{"assignees":[],"author":{"id":"MDQ6VXNlcjEyNzMzMjY=","is_bot":false,"login":"parkerzf","name":"zhao feng"},"body":"Hey I try to load the twitter graph in a AWS `p3.16xlarge` instance, which has 8 16GB memory GPUs, in total 128GB. However, it is OOM. Could you please take a look if I missed anything? Thanks so much!\r\n\r\n```python\r\nimport dask\r\nfrom dask_cuda import LocalCUDACluster\r\nfrom dask.distributed import Client\r\nimport dask_cudf\r\nimport cugraph\r\nimport cugraph.dask as dask_cugraph\r\nfrom cugraph.dask.common.mg_utils import get_visible_devices\r\nfrom cugraph.dask.comms import comms as Comms\r\nimport time\r\n\r\ncsv_file_name = \"twitter-2010.csv\"\r\n\r\nwith dask.config.set(jit_unspill=True):\r\n    with LocalCUDACluster(n_workers=8, device_memory_limit=\"16GB\") as cluster:\r\n        with Client(cluster) as client:\r\n            client.wait_for_workers(len(get_visible_devices()))\r\n            Comms.initialize(p2p=True)\r\n            chunksize = dask_cugraph.get_chunksize(csv_file_name)\r\n            ddf = dask_cudf.read_csv(csv_file_name, chunksize=chunksize, delimiter=' ', names=['src', 'dst'], dtype=['int32', 'int32'])\r\n            ddf.compute()\r\n            # G = cugraph.Graph(directed=True)\r\n            # G.from_dask_cudf_edgelist(ddf, source='src', destination='dst')\r\n```\r\n\r\nI can't find similar issues,  this [one](https://github.com/rapidsai/cudf/issues/6087) got similar errors but it is because LocalCUDACluster is not used.\r\n\r\nI used the docker approach to install the rapid frameworks:\r\n\r\n```cmd\r\ndocker pull rapidsai/rapidsai-dev:22.08-cuda11.5-devel-ubuntu20.04-py3.9\r\ndocker run --gpus all --rm -it \\\r\n    --shm-size=10g --ulimit memlock=-1 \\\r\n    -p 8888:8888 -p 8787:8787 -p 8786:8786 \\\r\n    rapidsai/rapidsai-dev:22.08-cuda11.5-devel-ubuntu20.04-py3.9\r\n```\r\n\r\nThe error log:\r\n\r\n```\r\n2022-09-27 13:03:21,520 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\r\n2022-09-27 13:03:21,520 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\r\n2022-09-27 13:03:21,524 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\r\n2022-09-27 13:03:21,524 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\r\n2022-09-27 13:03:21,544 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\r\n2022-09-27 13:03:21,544 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\r\n2022-09-27 13:03:21,554 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\r\n2022-09-27 13:03:21,554 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\r\n2022-09-27 13:03:21,555 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\r\n2022-09-27 13:03:21,555 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\r\n2022-09-27 13:03:21,556 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\r\n2022-09-27 13:03:21,556 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\r\n2022-09-27 13:03:21,597 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\r\n2022-09-27 13:03:21,597 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\r\n2022-09-27 13:03:21,673 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\r\n2022-09-27 13:03:21,673 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\r\n---------------------------------------------------------------------------\r\nMemoryError                               Traceback (most recent call last)\r\n/tmp/ipykernel_5947/1798640855.py in <module>\r\n      9             chunksize = dask_cugraph.get_chunksize(csv_file_name)\r\n     10             ddf = dask_cudf.read_csv(csv_file_name, chunksize=chunksize, delimiter=' ', names=['src', 'dst'], dtype=['int32', 'int32'])\r\n---> 11             ddf.compute()\r\n     12             # G = cugraph.Graph(directed=True)\r\n     13             # G.from_dask_cudf_edgelist(ddf, source='src', destination='dst')\r\n\r\n/opt/conda/envs/rapids/lib/python3.9/site-packages/dask/base.py in compute(self, **kwargs)\r\n    313         dask.base.compute\r\n    314         \"\"\"\r\n--> 315         (result,) = compute(self, traverse=False, **kwargs)\r\n    316         return result\r\n    317 \r\n\r\n/opt/conda/envs/rapids/lib/python3.9/site-packages/dask/base.py in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)\r\n    597 \r\n    598     results = schedule(dsk, keys, **kwargs)\r\n--> 599     return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\r\n    600 \r\n    601 \r\n\r\n/opt/conda/envs/rapids/lib/python3.9/site-packages/dask/base.py in <listcomp>(.0)\r\n    597 \r\n    598     results = schedule(dsk, keys, **kwargs)\r\n--> 599     return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\r\n    600 \r\n    601 \r\n\r\n/opt/conda/envs/rapids/lib/python3.9/site-packages/dask/dataframe/core.py in finalize(results)\r\n    136 \r\n    137 def finalize(results):\r\n--> 138     return _concat(results)\r\n    139 \r\n    140 \r\n\r\n/opt/conda/envs/rapids/lib/python3.9/site-packages/dask_cuda-22.8.0-py3.9.egg/dask_cuda/proxify_device_objects.py in wrapper(*args, **kwargs)\r\n    167     @functools.wraps(func)\r\n    168     def wrapper(*args, **kwargs):\r\n--> 169         ret = func(*args, **kwargs)\r\n    170         if dask.config.get(\"jit-unspill-compatibility-mode\", default=False):\r\n    171             ret = unproxify_device_objects(ret, skip_explicit_proxies=False)\r\n\r\n/opt/conda/envs/rapids/lib/python3.9/site-packages/dask/dataframe/core.py in _concat(args, ignore_index)\r\n    131         args[0]\r\n    132         if not args2\r\n--> 133         else methods.concat(args2, uniform=True, ignore_index=ignore_index)\r\n    134     )\r\n    135 \r\n\r\n/opt/conda/envs/rapids/lib/python3.9/site-packages/dask/dataframe/dispatch.py in concat(dfs, axis, join, uniform, filter_warning, ignore_index, **kwargs)\r\n     60     else:\r\n     61         func = concat_dispatch.dispatch(type(dfs[0]))\r\n---> 62         return func(\r\n     63             dfs,\r\n     64             axis=axis,\r\n\r\n/opt/conda/envs/rapids/lib/python3.9/site-packages/dask_cuda-22.8.0-py3.9.egg/dask_cuda/proxy_object.py in wrapper(*args, **kwargs)\r\n    900         args = [unproxy(d) for d in args]\r\n    901         kwargs = {k: unproxy(v) for k, v in kwargs.items()}\r\n--> 902         return func(*args, **kwargs)\r\n    903 \r\n    904     return wrapper\r\n\r\n/opt/conda/envs/rapids/lib/python3.9/site-packages/dask/dataframe/dispatch.py in concat(dfs, axis, join, uniform, filter_warning, ignore_index, **kwargs)\r\n     60     else:\r\n     61         func = concat_dispatch.dispatch(type(dfs[0]))\r\n---> 62         return func(\r\n     63             dfs,\r\n     64             axis=axis,\r\n\r\n/opt/conda/envs/rapids/lib/python3.9/contextlib.py in inner(*args, **kwds)\r\n     77         def inner(*args, **kwds):\r\n     78             with self._recreate_cm():\r\n---> 79                 return func(*args, **kwds)\r\n     80         return inner\r\n     81 \r\n\r\n/opt/conda/envs/rapids/lib/python3.9/site-packages/dask_cudf/backends.py in concat_cudf(dfs, axis, join, uniform, filter_warning, sort, ignore_index, **kwargs)\r\n    273         )\r\n    274 \r\n--> 275     return cudf.concat(dfs, axis=axis, ignore_index=ignore_index)\r\n    276 \r\n    277 \r\n\r\n/opt/conda/envs/rapids/lib/python3.9/site-packages/cudf/core/reshape.py in concat(objs, axis, join, ignore_index, sort)\r\n    397                 # don't filter out empty df's\r\n    398                 objs = old_objs\r\n--> 399             result = cudf.DataFrame._concat(\r\n    400                 objs,\r\n    401                 axis=axis,\r\n\r\n/opt/conda/envs/rapids/lib/python3.9/contextlib.py in inner(*args, **kwds)\r\n     77         def inner(*args, **kwds):\r\n     78             with self._recreate_cm():\r\n---> 79                 return func(*args, **kwds)\r\n     80         return inner\r\n     81 \r\n\r\n/opt/conda/envs/rapids/lib/python3.9/site-packages/cudf/core/dataframe.py in _concat(cls, objs, axis, join, ignore_index, sort)\r\n   1674         # Concatenate the Tables\r\n   1675         out = cls._from_data(\r\n-> 1676             *libcudf.concat.concat_tables(\r\n   1677                 tables, ignore_index=ignore_index or are_all_range_index\r\n   1678             )\r\n\r\nconcat.pyx in cudf._lib.concat.concat_tables()\r\n\r\nconcat.pyx in cudf._lib.concat.concat_tables()\r\n```\r\n","closed":false,"closedAt":null,"comments":[{"id":"IC_kwDOBWUGps5LGkv5","author":{"login":"beckernick"},"authorAssociation":"MEMBER","body":"When you call `compute`, you are bringing all of the data to a single GPU and a single Python process, which is likely not what you want to do. [These](https://coiled.io/blog/dask-compute/) two [blogs](https://coiled.io/blog/dask-persist-dataframe/) from Coiled can provide some context on how to handle distributed data in memory.","createdAt":"2022-09-27T20:31:37Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11796#issuecomment-1260014585","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5LITAT","author":{"login":"parkerzf"},"authorAssociation":"NONE","body":"Thanks @beckernick That makes total sense. I will read more about the blogs you shared. I have a follow up question, actually what I want is to load the data to `cugraph` and run the pagerank algorithm, not `compute`. However, it still shows the OOM error. You may find the detailed error message in this issue: https://github.com/rapidsai/cugraph/issues/2694. \r\n\r\n```python\r\nimport dask\r\nfrom dask_cuda import LocalCUDACluster\r\nfrom dask.distributed import Client\r\nimport dask_cudf\r\nimport cugraph\r\nimport cugraph.dask as dask_cugraph\r\nfrom cugraph.dask.common.mg_utils import get_visible_devices\r\nfrom cugraph.dask.comms import comms as Comms\r\nimport time\r\n\r\ncsv_file_name = \"twitter-2010.csv\"\r\n\r\nwith dask.config.set(jit_unspill=True):\r\n    with LocalCUDACluster(n_workers=8, device_memory_limit=\"16GB\") as cluster:\r\n        with Client(cluster) as client:\r\n            client.wait_for_workers(len(get_visible_devices()))\r\n            Comms.initialize(p2p=True)\r\n            chunksize = dask_cugraph.get_chunksize(csv_file_name)\r\n            ddf = dask_cudf.read_csv(csv_file_name, chunksize=chunksize, delimiter=' ', names=['src', 'dst'], dtype=['int32', 'int32'])\r\n            G = cugraph.Graph(directed=True)\r\n            G.from_dask_cudf_edgelist(ddf, source='src', destination='dst')\r\n```\r\n\r\nThis doesn't seem to be the same issue because I don't collect all the data to a single GPU. Do you maybe have a hint what could be the reason for that? Thanks!","createdAt":"2022-09-28T06:52:32Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11796#issuecomment-1260466195","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5LJZGB","author":{"login":"Ankitkurani1997"},"authorAssociation":"NONE","body":"@parkerzf , did you find any workaround for the above issue?","createdAt":"2022-09-28T11:14:27Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11796#issuecomment-1260753281","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5LNjRI","author":{"login":"parkerzf"},"authorAssociation":"NONE","body":"> \r\n\r\nNope, hope that someone else could share their experience to deal with large dataset. ","createdAt":"2022-09-29T06:50:53Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11796#issuecomment-1261843528","viewerDidAuthor":false}],"createdAt":"2022-09-27T20:05:54Z","id":"I_kwDOBWUGps5Sv8ZV","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NjQ=","name":"question","description":"Further information is requested","color":"D4C5F9"},{"id":"MDU6TGFiZWwxMDEzOTg3MzUy","name":"0 - Backlog","description":"In queue waiting for assignment","color":"d4c5f9"},{"id":"MDU6TGFiZWwxMTg1MjQwODk4","name":"dask","description":"Dask issue","color":"fcc25d"}],"milestone":{"number":20,"title":"Stabilizing large workflows (OOM, spilling, partitioning)","description":"","dueOn":null},"number":11796,"projectCards":[{"project":{"name":"Other Issues"},"column":{"name":"Needs prioritizing"}}],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[QST] OOM issue while loading the 26GB twitter dataset into 128GB GPU memory","updatedAt":"2022-10-21T19:31:39Z","url":"https://github.com/rapidsai/cudf/issues/11796"}
