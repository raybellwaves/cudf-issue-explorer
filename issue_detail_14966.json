{"assignees":[{"id":"MDQ6VXNlcjExNjY0MjU5","login":"galipremsagar","name":"GALI PREM SAGAR"}],"author":{"id":"MDQ6VXNlcjEyNzI1MTEx","is_bot":false,"login":"GregoryKimball","name":"Gregory Kimball"},"body":"**Is your feature request related to a problem? Please describe.**\r\nlibcudf provides a `chunked_parquet_reader` in its public API. This reader uses new reader options to process the data in a parquet file in sub-file units. The `chunk_read_limit` option limits the table size in bytes to be returned per read by only decoding a subset of pages per chunked read. The `pass_read_limit` option limits the memory used for reading and decompressing data by only decompressing a subset of pages per chunked read.\r\n\r\nThe chunked parquet reader allows cuDF-python to expose two types of useful functionality:\r\n1. an API that acts as an iterator to yield dataframe chunks. This is similar to the `iter_row_groups` behavior in [fastparquet](https://fastparquet.readthedocs.io/en/latest/api.html). This approach would let users work with parquet files that contain more rows than 2.1B rows (see #13159 for more information about the row limit in libcudf). \r\n2. a \"low_memory\" mode that reads the full file, but has a lower peak memory footprint thanks to the smaller sizes of intermediate allocations. This is similar to the the `low_memory` argument in [polars](https://docs.pola.rs/py-polars/html/reference/api/polars.read_parquet.html). This approach would make it easier to read large parquet datasets with limited GPU memory.\r\n\r\n**Describe the solution you'd like**\r\nWe should make chunked parquet reading available to cuDF-python users. Perhaps this functionality could be made available to `cudf.pandas` users as well. \r\n\r\n\r\n**Additional context**\r\nPandas does not seem to have a method for chunking parquet reads, and I'm not sure if pandas makes use of the `iter_row_groups` behavior in fastparquet as a pass-through parameter.\r\n\r\n\r\nAPI docs references:\r\n* pandas: [read_parquet](https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html)\r\n* pyarrow: [parquet.read_table](https://arrow.apache.org/docs/python/generated/pyarrow.parquet.read_table.html)\r\n* fastparquet: [read_parquet](https://fastparquet.readthedocs.io/en/latest/api.html)\r\n* polars: [read_parquet](https://docs.pola.rs/py-polars/html/reference/api/polars.read_parquet.html)\r\n* cudf: [read_parquet](https://docs.rapids.ai/api/cudf/nightly/user_guide/api_docs/api/cudf.read_parquet/)\r\n","closed":false,"closedAt":null,"comments":[],"createdAt":"2024-02-04T19:10:28Z","id":"I_kwDOBWUGps5-Mt5Y","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NjE=","name":"feature request","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwxMTM5NzQxMjEz","name":"cuDF (Python)","description":"Affects Python cuDF API.","color":"1d76db"}],"milestone":{"number":20,"title":"Stabilizing large workflows (OOM, spilling, partitioning)","description":"","dueOn":null},"number":14966,"projectCards":[],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[FEA] Incorporate chunked parquet reading into cuDF-python","updatedAt":"2024-05-10T22:32:47Z","url":"https://github.com/rapidsai/cudf/issues/14966"}
