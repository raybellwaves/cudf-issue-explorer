{"assignees":[],"author":{"id":"MDQ6VXNlcjE1MzgxNjU=","is_bot":false,"login":"vyasr","name":"Vyas Ramasubramani"},"body":"**Is your feature request related to a problem? Please describe.**\r\nThe use of type utilities in `cudf.api.types` is widespread throughout cudf. Some of this usage is problematic and should be removed -- in many places, we would be better off leveraging the columnar type hierarchy in cudf to implement additional functionality as methods of the appropriate subclass of `ColumnBase`. However, there are also many places where the usage of these utility functions is unavoidable, especially in functions like `astype` or `build_column` that need to dispatch based on an input dtype. The problem is that these methods are _slow_, especially when compared to the equivalent `isinstance` checks for primitive types. For instance, `pd.api.types.is_numeric_dtype` takes ~1.2 us as compared to ~50-80 ns for `isinstance(val, int)`. Although these absolute numbers are small, that factor of 10-20 is significant because many common cudf functions can call these functions dozens of times (consider e.g. merges or groupbys). We would benefit from reducing this overhead as much as possible.\r\n\r\n**Describe the solution you'd like**\r\nThese functions should all wrap their logic in a cache to bypass the calls on commonly reused types. Here is a simple example:\r\n```\r\nIn [48]: from functools import lru_cache\r\n\r\nIn [49]: import pandas as pd\r\n\r\nIn [50]: @lru_cache\r\n    ...: def is_numeric_type_cached(dtype):\r\n    ...:     return pd.api.types.is_numeric_dtype(dtype)\r\n    ...: \r\n\r\nIn [51]: def is_numeric_type(arr_or_dtype):\r\n    ...:     if dt := getattr(arr_or_dtype, \"dtype\", None):\r\n    ...:         arr_or_dtype = dt\r\n    ...:     return is_numeric_type_cached(arr_or_dtype)\r\n    ...: \r\n\r\nIn [52]: s = pd.Series([1])\r\n\r\nIn [53]: %timeit pd.api.types.is_numeric_dtype(s)\r\n1.88 µs ± 7.23 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\r\n\r\nIn [54]: %timeit pd.api.types.is_numeric_dtype(s.dtype)\r\n1.14 µs ± 7.26 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\r\n\r\nIn [55]: %timeit is_numeric_type(s.dtype)\r\n471 ns ± 1.2 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\r\n\r\nIn [56]: %timeit is_numeric_type(s)\r\n466 ns ± 1.98 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\r\n```\r\nNote the importance of the wrapper function to support calling these utilities with any object that exposes a `dtype` attribute such as a Series or a numpy/cupy array (we obviously don't want to cache those objects directly). We may need additional such logic in certain specific cases (perhaps categoricals or nested types). Nevertheless, as shown above using a cache could reduce the cost of the APIs of a factor of ~3 on average.\r\n\r\n**Describe alternatives you've considered**\r\nA more natural alternative that I considered was leveraging `functools.singledispatch`. Unfortunately, we are largely foiled in this attempt by the fact that the different possible \"dtype-like\" objects do not fit into simple class delinations, which is how dispatch is performed. In the worst case, all dtypes are specified with strings like \"int\" or \"float\", all of which end up dispatched to the same type.\r\n\r\n**Additional context**\r\nIn theory this could be abused to result in some ridiculous caching, e.g. if a user calls these methods with some arbitrary object that isn't a valid dtype-like object and ends up being stored in the cache. To prevent the worst excesses we should limit the cache size.","closed":false,"closedAt":null,"comments":[{"id":"IC_kwDOBWUGps5R6rsk","author":{"login":"shwina"},"authorAssociation":"CONTRIBUTOR","body":"I think there may be places that we call these utilities on things like Series objects. We could replace those instances with calling on the `.dtype` attribute instead.\r\n\r\nEdit: apologies, I jumped the gun on responding :) ","createdAt":"2023-01-07T01:36:13Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12494#issuecomment-1374337828","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5-PMOm","author":{"login":"vyasr"},"authorAssociation":"CONTRIBUTOR","body":"Related: #5695","createdAt":"2024-05-17T16:00:17Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12494#issuecomment-2117911462","viewerDidAuthor":false}],"createdAt":"2023-01-07T01:24:16Z","id":"I_kwDOBWUGps5azPme","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NjE=","name":"feature request","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwxMTM5NzQxMjEz","name":"cuDF (Python)","description":"Affects Python cuDF API.","color":"1d76db"},{"id":"MDU6TGFiZWwxMzIyMjUyNjE3","name":"Performance","description":"Performance related issue","color":"C2E0C6"}],"milestone":null,"number":12494,"projectCards":[],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[FEA] Cache the outputs of type utilities","updatedAt":"2024-05-17T16:00:17Z","url":"https://github.com/rapidsai/cudf/issues/12494"}
