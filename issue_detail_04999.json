{"assignees":[{"id":"MDQ6VXNlcjMxOTA0MDU=","login":"shwina","name":"Ashwin Srinath"}],"author":{"id":"MDQ6VXNlcjI2MTY5Nzcx","is_bot":false,"login":"miguelusque","name":"Miguel MartÃ­nez"},"body":"**Is your feature request related to a problem? Please describe.**\r\nHi,\r\n\r\nIt would be great if you could please evaluate the addition of the `low_memory` parameter in cuDF's `read_csv` method.\r\n\r\nThis parameter defaults to `True` in Pandas' `read_csv` method, and it indicates Pandas to read the CSV file in chunks, allowing to load a large CSV file on memory-constrained systems.\r\n\r\nCurrent cuDF's `read_csv` implementation first reads the whole file into GPU memory, and then creates the DataFrame. That has the benefit of a very fast DataFrame creation, but it also has the cons that it limits the size of the biggest dataset it can be created from a file.\r\n\r\nIn my tests, I was not able to create a DataFrame bigger than 7 GBs in a 16GBs v100 card.\r\n\r\nI think that, by reading the CSV file in chunks, the max size of the DataFrame that I could create from a CSV file would be much bigger, and I also think that the potential (if any) performance penalty might be well-worth.\r\n\r\n**Describe the solution you'd like**\r\nTo be able to load bigger datasets when reading CSV files with cuDF.\r\n\r\n**Describe alternatives you've considered**\r\nI have manually read the file in chunks, using the byte_range parameter, and then concatenating the different dataframes. I was able to load a DF up to 10GBs without a significate performance penalty. Also, my solution implies concat to dataframes, which would not be needed in a native libcudf implementation.\r\n\r\n**Additional context**\r\nDiscussed internally with @kkraus14 .","closed":false,"closedAt":null,"comments":[{"id":"MDEyOklzc3VlQ29tbWVudDYxODQ3MTI5Nw==","author":{"login":"kkraus14"},"authorAssociation":"COLLABORATOR","body":"cc @vuule @OlivierNV \r\n\r\nWe shouldn't consider this until we refactor the CSV reader based on #1229","createdAt":"2020-04-23T15:38:48Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/4999#issuecomment-618471297","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDYzMjYzOTE1Mg==","author":{"login":"miguelusque"},"authorAssociation":"MEMBER","body":"Hi all,\r\n\r\nI was wondering if there is any progress on this feature request.\r\n\r\nThanks!\r\n\r\nMiguel","createdAt":"2020-05-22T11:17:12Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/4999#issuecomment-632639152","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDYzMjk1NzEzNw==","author":{"login":"vuule"},"authorAssociation":"CONTRIBUTOR","body":"Hi Miguel, the team is still wrapping up the prioritized issues for the 0.14 release. ","createdAt":"2020-05-23T00:46:32Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/4999#issuecomment-632957137","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDcxNzc3ODYzNg==","author":{"login":"miguelusque"},"authorAssociation":"MEMBER","body":"Hi!\r\n\r\nI was wondering if you could please have a look at the feature request above.\r\n\r\nThanks!\r\n\r\nMiguel ","createdAt":"2020-10-28T08:28:18Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/4999#issuecomment-717778636","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDcxNzk4MTY5OQ==","author":{"login":"kkraus14"},"authorAssociation":"COLLABORATOR","body":"> Hi!\r\n> \r\n> I was wondering if you could please have a look at the feature request above.\r\n> \r\n> Thanks!\r\n> \r\n> Miguel\r\n\r\ncuIO is going through a large refactoring to make supporting requests like these easier moving forward. We'll re-evaluate this request once the refactoring is completed.","createdAt":"2020-10-28T14:45:04Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/4999#issuecomment-717981699","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDc5ODk2MjU4Mg==","author":{"login":"github-actions"},"authorAssociation":"NONE","body":"This issue has been labeled `inactive-90d` due to no recent activity in the past 90 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed.","createdAt":"2021-03-14T19:13:40Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/4999#issuecomment-798962582","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDc5OTYzNjU0OQ==","author":{"login":"miguelusque"},"authorAssociation":"MEMBER","body":"I think this feature request is still relevant.","createdAt":"2021-03-15T18:11:32Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/4999#issuecomment-799636549","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5F13_g","author":{"login":"GregoryKimball"},"authorAssociation":"CONTRIBUTOR","body":"Thank you @miguelusque for raising this idea. With byte-range support available for `read_csv`, we don't plan on implementing chunked CSV reading. ","createdAt":"2022-06-30T22:55:12Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/4999#issuecomment-1171750880","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5F17mz","author":{"login":"GregoryKimball"},"authorAssociation":"CONTRIBUTOR","body":"Re-opening this after a discussion with @miguelusque. We could implement CSV chunked reading at the cuDF-python layer or the libcudf layer to achieve the desired outcome. ","createdAt":"2022-06-30T23:21:38Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/4999#issuecomment-1171765683","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5Puhry","author":{"login":"shwina"},"authorAssociation":"CONTRIBUTOR","body":"@vuule, @GregoryKimball - it looks like chunking is implemented for `write_csv` at the `libcudf` level, but not `read_csv`. \r\n\r\n> We could implement CSV chunked reading at the cuDF-python layer\r\n\r\nWould this be accomplished via a combination of `skip_rows` and `nrows` to read the chunks, and `concat` to combine them? \r\n\r\nThat sounds doable, but I wanted to double check whether using `skip_rows` would actually result in lower (**peak**) memory usage than reading the entire `.csv`? ","createdAt":"2022-12-05T15:42:32Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/4999#issuecomment-1337596658","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5PvBrS","author":{"login":"shwina"},"authorAssociation":"CONTRIBUTOR","body":"> whether using skip_rows would actually result in lower (peak) memory usage than reading the entire .csv?\r\n\r\nEmperically, the answer is yes (using `byte_range` as suggested by @miguelusque as opposed to `skip_rows`):\r\n\r\n```python\r\nimport cudf\r\nfrom dask.utils import format_bytes, parse_bytes\r\nimport rmm\r\n\r\nfrom contextlib import contextmanager\r\n\r\n\r\ndef make_csv():\r\n    data = cudf.datasets.randomdata(10_000_000, dtypes={'a': str, 'b': str, 'c': str, 'd': str})\r\n    data.to_csv(\"test.csv\", index=False)\r\n\r\n@contextmanager\r\ndef peak_memory(*args, **kwargs):\r\n    old_mr = rmm.mr.get_current_device_resource()\r\n    mr = rmm.mr.StatisticsResourceAdaptor(old_mr)    \r\n    rmm.mr.set_current_device_resource(mr)\r\n    initial_peak_mem = mr.allocation_counts[\"peak_bytes\"]\r\n    try:\r\n        yield\r\n    finally:\r\n        final_peak_mem = mr.allocation_counts[\"peak_bytes\"]\r\n        print(format_bytes(final_peak_mem - initial_peak_mem))\r\n        rmm.mr.set_current_device_resource(old_mr)\r\n\r\ndef chunked_csv_reader(fname, chunksize, *args, **kwargs):\r\n    chunksize = parse_bytes(chunksize)\r\n    chunks = []\r\n    offset = 0\r\n    while True:\r\n        try:\r\n            chunk = cudf.read_csv(\"test.csv\", byte_range=[offset, chunksize], header=None, *args, **kwargs)\r\n        except RuntimeError as e:\r\n            if \"Offset is past end of file\" in str(e):\r\n                break\r\n        chunks.append(chunk) \r\n        offset += chunksize\r\n    return cudf.concat(chunks, ignore_index=True)\r\n\r\nif __name__ == \"__main__\":\r\n    make_csv()\r\n\r\n    with peak_memory():\r\n        cudf.read_csv(\"test.csv\", index_col=False)\r\n\r\n    with peak_memory():\r\n        chunked_csv_reader(\"test.csv\", \"100 MiB\", dtype=[str, str, str, str], index_col=False)\r\n```\r\n\r\nOutput:\r\n\r\n```\r\n1.35 GiB\r\n716.98 MiB\r\n```","createdAt":"2022-12-05T16:56:34Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"HEART","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/4999#issuecomment-1337727698","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5P9v6w","author":{"login":"vuule"},"authorAssociation":"CONTRIBUTOR","body":"Great demonstration of how `byte_range` helps here!\r\n`skip_rows` should also reduce the peak memory use, but it would be slower than `byte_range`. On the other hand, `byte_range` is not 100% reliable, see https://github.com/rapidsai/cudf/issues/7602.","createdAt":"2022-12-07T20:57:37Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/4999#issuecomment-1341587120","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5P9wPM","author":{"login":"vuule"},"authorAssociation":"CONTRIBUTOR","body":"related https://github.com/rapidsai/cudf/issues/10554","createdAt":"2022-12-07T20:58:53Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/4999#issuecomment-1341588428","viewerDidAuthor":false}],"createdAt":"2020-04-23T11:48:52Z","id":"MDU6SXNzdWU2MDU0NjgxOTU=","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NjE=","name":"feature request","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwxMTM5NzQwNjY2","name":"libcudf","description":"Affects libcudf (C++/CUDA) code.","color":"c5def5"},{"id":"MDU6TGFiZWwxMTM5NzQxMjEz","name":"cuDF (Python)","description":"Affects Python cuDF API.","color":"1d76db"},{"id":"MDU6TGFiZWwxMTg1MjQ0MTQy","name":"cuIO","description":"cuIO issue","color":"fef2c0"}],"milestone":{"number":12,"title":"CSV continuous improvement","description":"","dueOn":null},"number":4999,"projectCards":[{"project":{"name":"Feature Planning"},"column":{"name":"Needs prioritizing"}}],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[FEA] Add support to low_memory parameter in read_csv method ","updatedAt":"2023-04-06T05:29:42Z","url":"https://github.com/rapidsai/cudf/issues/4999"}
