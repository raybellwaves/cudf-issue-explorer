{"assignees":[],"author":{"id":"MDQ6VXNlcjQ4Mzc1NzE=","is_bot":false,"login":"VibhuJawa","name":"Vibhu Jawa"},"body":"**Is your feature request related to a problem? Please describe.**\r\nI think we should switch `chunksize` for `dask_cudf.read_csv` increase our `chunksize` from `256 MiB` to something bigger like `1 gb` to `2gb` . \r\n\r\nThe code change will go at line : \r\n\r\nhttps://github.com/rapidsai/cudf/blob/c382989330f97715e4aec3f2c677b0a6d2e99647/python/dask_cudf/dask_cudf/io/csv.py#L14\r\n\r\n## Why:\r\n\r\nFrom experience on `gpu-bdb`, workflows and other users,  it seems like current default of  `256 MiB`: \r\n\r\na. Is often too low to saturate GPU meaningfully especially with wider frames for both io and downstream operations. \r\n\r\nb.  Bigger chunk size often just sidesteps various communication and scheduler bottlenecks for shuffle operations like repartition/merge etc.\r\n\r\n\r\nCC: @beckernick , @ayushdg , @quasiben , @kkraus14 . ","closed":false,"closedAt":null,"comments":[{"id":"MDEyOklzc3VlQ29tbWVudDcxMjUyNTM1OQ==","author":{"login":"kkraus14"},"authorAssociation":"COLLABORATOR","body":"One note here is that per thread default stream / explicit CUDA streams could likely change this where we can run multiple threads on a GPU and saturate it more effectively via concurrent work instead of relying on only a single task to saturate it.\r\n\r\nThis is of course at the cost of a larger number of tasks and more work for the scheduler to handle the larger number of tasks.","createdAt":"2020-10-20T01:06:20Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/6560#issuecomment-712525359","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDcxMjUzMjQ0OQ==","author":{"login":"VibhuJawa"},"authorAssociation":"MEMBER","body":"> One note here is that per thread default stream / explicit CUDA streams could likely change this where we can run multiple threads on a GPU and saturate it more effectively via concurrent work instead of relying on only a single task to saturate it.\r\n> \r\n> This is of course at the cost of a larger number of tasks and more work for the scheduler to handle the larger number of tasks.\r\n\r\nAgreed that we will have to revisit this default when we have explicit CUDA streams but for now, I feel we should be safe to increase the default chunk size.\r\n\r\nIf we all end up agreeing on changing this default, happy to do a quick pr to fix this. ","createdAt":"2020-10-20T01:32:52Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/6560#issuecomment-712532449","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDcxMzEwMDkxOQ==","author":{"login":"kkraus14"},"authorAssociation":"COLLABORATOR","body":"The problem with straight up changing it to 2GB is if someone is using say a 4GB GPU they're going to run into problems. Can we somehow make it something like: `min(2GB, (1/8)*gpu_mem)`?","createdAt":"2020-10-20T19:52:46Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/6560#issuecomment-713100919","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDcxMzExNzU3OQ==","author":{"login":"VibhuJawa"},"authorAssociation":"MEMBER","body":"> The problem with straight up changing it to 2GB is if someone is using say a 4GB GPU they're going to run into problems. Can we somehow make it something like: `min(2GB, (1/8)*gpu_mem)`?\r\n\r\nAs this happens in the client process, I am unsure of a clean way to query `gpu_mem`,  as the worker processes might have different memory than the client.  Maybe @quasiben  will know. \r\n\r\nIf we can make the simplifying assumption here that `gpu_mem` is the memory of the client process this should be doable. ","createdAt":"2020-10-20T20:23:57Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/6560#issuecomment-713117579","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDcxMzE3NDQ5NQ==","author":{"login":"harrism"},"authorAssociation":"MEMBER","body":"Also, it's available / free GPU mem, not total memory, right? That also depends on what RMM memory resource you are using, and not all are queryable...","createdAt":"2020-10-20T22:23:18Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/6560#issuecomment-713174495","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDcxMzIyMjc2Nw==","author":{"login":"kkraus14"},"authorAssociation":"COLLABORATOR","body":"It would be based on available, not free. Dask manages spilling as needed so you don't want to change configuration based on the amount of GPU memory available generally.","createdAt":"2020-10-21T00:50:23Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/6560#issuecomment-713222767","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDcxMzIyMzc0Ng==","author":{"login":"harrism"},"authorAssociation":"MEMBER","body":"In my book, \"available\" and \"free\" memory are the same thing.","createdAt":"2020-10-21T00:54:05Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/6560#issuecomment-713223746","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDcxMzIyNDA0OA==","author":{"login":"kkraus14"},"authorAssociation":"COLLABORATOR","body":"> In my book, \"available\" and \"free\" memory are the same thing.\r\n\r\nSorry, by \"available\" I interpreted as the configured maximum pool size (if applicable) or the total amount of GPU memory.","createdAt":"2020-10-21T00:55:12Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/6560#issuecomment-713224048","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDcxMzY0MjI2Mw==","author":{"login":"rjzamora"},"authorAssociation":"MEMBER","body":"NVTabular uses pynvml to get the total memory, and uses that to set `chunksize`.  We used to use rmm, until it (understandably) dropped reliable support for this.","createdAt":"2020-10-21T15:02:42Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/6560#issuecomment-713642263","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDc4MDEyMzA5Ng==","author":{"login":"github-actions"},"authorAssociation":"NONE","body":"This issue has been marked rotten due to no recent activity in the past 90d. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed.","createdAt":"2021-02-16T21:16:59Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/6560#issuecomment-780123096","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps41CtZL","author":{"login":"beckernick"},"authorAssociation":"MEMBER","body":"@VibhuJawa is this still relevant?","createdAt":"2021-07-30T13:44:05Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/6560#issuecomment-889902667","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps41DXME","author":{"login":"VibhuJawa"},"authorAssociation":"MEMBER","body":"> @VibhuJawa is this still relevant?\r\n\r\nGiven the above problems i think changing default is not an option in the short term. \r\n\r\nThat said may be we need a best-practices page like dask has (see [doc]( https://docs.dask.org/en/latest/dataframe-best-practices.html )) to inform users about the best practices around chunk size .  ","createdAt":"2021-07-30T18:24:40Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/6560#issuecomment-890073860","viewerDidAuthor":false}],"createdAt":"2020-10-20T00:24:54Z","id":"MDU6SXNzdWU3MjUwNzg1MDQ=","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NjE=","name":"feature request","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwxMTM5NzQwNjY2","name":"libcudf","description":"Affects libcudf (C++/CUDA) code.","color":"c5def5"},{"id":"MDU6TGFiZWwxMTM5NzQxMjEz","name":"cuDF (Python)","description":"Affects Python cuDF API.","color":"1d76db"},{"id":"MDU6TGFiZWwxMTg1MjQwODk4","name":"dask","description":"Dask issue","color":"fcc25d"},{"id":"MDU6TGFiZWwxMTg1MjQ0MTQy","name":"cuIO","description":"cuIO issue","color":"fef2c0"}],"milestone":{"number":20,"title":"Stabilizing large workflows (OOM, spilling, partitioning)","description":"","dueOn":null},"number":6560,"projectCards":[{"project":{"name":"Feature Planning"},"column":{"name":"Needs prioritizing"}}],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[FEA] Switch default `chunksize` for `dask_cudf.read_csv` to `2gb` from `256 MiB`","updatedAt":"2023-04-06T05:27:52Z","url":"https://github.com/rapidsai/cudf/issues/6560"}
