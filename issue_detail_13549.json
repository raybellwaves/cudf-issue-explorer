{"assignees":[],"author":{"id":"MDQ6VXNlcjMxODI2NTg=","is_bot":false,"login":"gucasbrg","name":""},"body":"**Describe the bug**\r\n\r\n**Steps/Code to reproduce bug**\r\n```\r\nimport cudf\r\nfrom multiprocessing import get_context\r\nimport time\r\npdf = cudf.DataFrame({\r\n        'low':[i for i in range(1000)],\r\n        'close':[i for i in range(1000)],\r\n    })\r\n\r\ndef get_df(idx):\r\n    return idx.rolling(5).mean()\r\n\r\nif __name__ == \"__main__\":\r\n    ctx = get_context(\"spawn\")\r\n    start = time.time()\r\n    num =1\r\n    with ctx.Pool(num) as pool:\r\n        cudf.concat(pool.map(get_df, [pdf.a[i:] for i in range(100, 200)]))\r\n    print(time.time()-start)\r\n\r\n```\r\n\r\nnum =1 ,time 3.2659552097320557\r\nnum =2, time 7.382307291030884","closed":false,"closedAt":null,"comments":[{"id":"IC_kwDOBWUGps5e0qgq","author":{"login":"shwina"},"authorAssociation":"CONTRIBUTOR","body":"Hi @gucasbrg. Can you describe what exactly you'd like to do with `multiprocessing` + `cudf`? Are you looking to chunk your data up and hand off each chunk to a separate process? If so, perhaps you could use `dask`: https://medium.com/rapids-ai/easy-cpu-gpu-arrays-and-dataframes-run-your-dask-code-where-youd-like-e349d92351d?\r\n\r\n---\r\n\r\nI modified your script as follows:\r\n\r\n```python\r\nimport cudf\r\nfrom multiprocessing import get_context\r\nimport time\r\npdf = cudf.DataFrame({\r\n        'low':[i for i in range(1000)],\r\n        'close':[i for i in range(1000)],\r\n    })\r\n\r\ndef get_df(idx):\r\n    return idx.rolling(5).mean()\r\n\r\nif __name__ == \"__main__\":\r\n    ctx = get_context(\"spawn\")\r\n    for num in range(1, 10):\r\n        with ctx.Pool(num) as pool:\r\n            start = time.time()\r\n            cudf.concat(pool.map(get_df, [pdf.low[i:] for i in range(100, 200)]))\r\n            print(time.time()-start)\r\n```\r\n\r\nAnd the output is:\r\n\r\n```\r\n2.704458713531494\r\n2.7769949436187744\r\n2.8762805461883545\r\n2.846607208251953\r\n3.157398223876953\r\n3.077711582183838\r\n2.9359912872314453\r\n3.0212976932525635\r\n3.5092809200286865\r\n3.669832229614258\r\n```\r\n\r\nCould you please post your result for the same script? ","createdAt":"2023-06-14T09:47:55Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/13549#issuecomment-1590863914","viewerDidAuthor":false}],"createdAt":"2023-06-10T15:14:43Z","id":"I_kwDOBWUGps5oXj99","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NTk=","name":"bug","description":"Something isn't working","color":"d73a4a"},{"id":"MDU6TGFiZWwxMDEzOTg3Nzk5","name":"0 - Waiting on Author","description":"Waiting for author to respond to review","color":"ffb88c"}],"milestone":null,"number":13549,"projectCards":[],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"When the amount of data is not small,The parameter Num of the parallel process becomes larger, but the running time becomes longer","updatedAt":"2023-06-26T03:46:11Z","url":"https://github.com/rapidsai/cudf/issues/13549"}
