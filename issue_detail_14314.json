{"assignees":[],"author":{"id":"MDQ6VXNlcjE5MDEwNTk=","is_bot":false,"login":"abellina","name":"Alessandro Bellina"},"body":"We are investigating using pinned memory pool at the cuDF layer and replacing `cudaFreeHost` calls in `pinned_host_vector` due to traces we have seen that indicate synchronization or a \"lining up\" of kernels during parquet decode. Here's query88 from NDS at 3TB on our performance cluster running with an A100. In the nsys trace (pardon the amount of streams), we can see parquet nvcomp and decode kernels working on the first three quarters of the trace:\r\n\r\n![Screenshot from 2023-10-23 13-08-15](https://github.com/rapidsai/cudf/assets/1901059/b64915ab-c934-4d95-905e-851bcf689a66)\r\n\r\nThe bottom trace is cuDF without changes. The top trace is a modified cuDF where we replaced calls to [cudaMallocHost](https://github.com/rapidsai/cudf/blob/branch-23.12/cpp/include/cudf/detail/utilities/pinned_host_vector.hpp#L157) and [cudaFreeHost](https://github.com/rapidsai/cudf/blob/branch-23.12/cpp/include/cudf/detail/utilities/pinned_host_vector.hpp#L174) with `allocate` and `deallocate` against a modified RMM `pool_memory_resource` that isn't stream aware and  has a single free list.\r\n\r\nWhen we run with the modified cuDF, our NDS benchmark shows a 5% improvement at 3TB and a 6% improvement between old cuDF and new cuDF if we allow all 16 spark threads to submit work concurrently. In other words, we believe the `cudaFreeHost` calls specifically are preventing parquet heavy jobs from using more of the GPU due to synchronization.\r\n\r\nThe proposal here is to allow a pinned memory pool to be passed to parquet primarily, but there are probably other formats and areas in cuDF that might benefit from this. \r\n\r\nNote that another experiment we wanted to attempt was to remove pinned memory alltogether, which cuDF already has a flag for `LIBCUDF_IO_PREFER_PAGEABLE_TMP_MEMORY=1`, but we ran into issues for parquet only (https://github.com/rapidsai/cudf/issues/14311). Before I found this flag, I had tried replacing `cudaMallocHost` and `cudaFreeHost` with `malloc` and `free` and I ran into the same issue, so I think the parquet code is dependent on some sort of synchronization in the CUDA host pinned memory allocator.","closed":false,"closedAt":null,"comments":[{"id":"IC_kwDOBWUGps5qAPE6","author":{"login":"GregoryKimball"},"authorAssociation":"CONTRIBUTOR","body":"The new design for `cuda::mr::memory_resource`, which expands support in RMM to include pinned host memory pools, is tracked in this branch https://github.com/rapidsai/rmm/pull/1095.","createdAt":"2023-10-25T03:26:43Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/14314#issuecomment-1778446650","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5qAhx-","author":{"login":"harrism"},"authorAssociation":"MEMBER","body":"Note that cuda::mr::memory_resource doesn't directly expand support to include pinned host memory pools, it just enables us to more easily reuse the implementation of stream-ordered device memory pools for other kinds of device-accessible memory.","createdAt":"2023-10-25T05:09:44Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/14314#issuecomment-1778523262","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5qAh-t","author":{"login":"harrism"},"authorAssociation":"MEMBER","body":"@abellina It's really hard to tell in the image how much time is saved by this change. Can you provide comparable benchmark results?","createdAt":"2023-10-25T05:10:39Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/14314#issuecomment-1778524077","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5qGwkg","author":{"login":"abellina"},"authorAssociation":"CONTRIBUTOR","body":"Sure, this is for NDS @ 3TB in our performance cluster, the benchmark was executed 3 times for baseline vs test. We see around 6% improvement. I need to look at query95 more because it found a regression there, but overall this was a win:\r\n\r\n```\r\n--------------------------------------------------------------------\r\nName = query2\r\nMeans = 3088.3333333333335, 2494.6666666666665\r\nTime diff = 593.666666666667\r\nSpeedup = 1.2379743452699092\r\nT-Test (test statistic, p value, df) = 7.8686774190832445, 0.0014098464938042464, 4.0\r\nT-Test Confidence Interval = 384.1927204113131, 803.1406129220209\r\nALERT: significant change has been detected (p-value < 0.05)\r\nALERT: improvement in performance has been observed\r\n--------------------------------------------------------------------\r\nName = query9\r\nMeans = 11011.666666666666, 8408.0\r\nTime diff = 2603.666666666666\r\nSpeedup = 1.3096653980336188\r\nT-Test (test statistic, p value, df) = 8.329919755519573, 0.0011349387959612939, 4.0\r\nT-Test Confidence Interval = 1735.8386701912746, 3471.4946631420576\r\nALERT: significant change has been detected (p-value < 0.05)\r\nALERT: improvement in performance has been observed\r\n--------------------------------------------------------------------\r\nName = query23_part2\r\nMeans = 23309.0, 22541.666666666668\r\nTime diff = 767.3333333333321\r\nSpeedup = 1.0340406654343808\r\nT-Test (test statistic, p value, df) = 3.5857045809141983, 0.023049979254582895, 4.0\r\nT-Test Confidence Interval = 173.17984709011603, 1361.4868195765482\r\nALERT: significant change has been detected (p-value < 0.05)\r\nALERT: improvement in performance has been observed\r\n--------------------------------------------------------------------\r\nName = query28\r\nMeans = 8749.666666666666, 7877.0\r\nTime diff = 872.6666666666661\r\nSpeedup = 1.1107866785155092\r\nT-Test (test statistic, p value, df) = 3.675737490963056, 0.021283509364083526, 4.0\r\nT-Test Confidence Interval = 213.50341001605773, 1531.8299233172743\r\nALERT: significant change has been detected (p-value < 0.05)\r\nALERT: improvement in performance has been observed\r\n--------------------------------------------------------------------\r\nName = query31\r\nMeans = 3846.3333333333335, 3217.6666666666665\r\nTime diff = 628.666666666667\r\nSpeedup = 1.1953796747125247\r\nT-Test (test statistic, p value, df) = 3.476522797723856, 0.02543224663133342, 4.0\r\nT-Test Confidence Interval = 126.59646864855881, 1130.7368646847751\r\nALERT: significant change has been detected (p-value < 0.05)\r\nALERT: improvement in performance has been observed\r\n--------------------------------------------------------------------\r\nName = query74\r\nMeans = 5813.333333333333, 5295.0\r\nTime diff = 518.333333333333\r\nSpeedup = 1.0978910922253697\r\nT-Test (test statistic, p value, df) = 4.145944413928419, 0.014307288000811205, 4.0\r\nT-Test Confidence Interval = 171.21723564533386, 865.4494310213322\r\nALERT: significant change has been detected (p-value < 0.05)\r\nALERT: improvement in performance has been observed\r\n--------------------------------------------------------------------\r\nName = query75\r\nMeans = 8002.0, 7139.0\r\nTime diff = 863.0\r\nSpeedup = 1.120885278050147\r\nT-Test (test statistic, p value, df) = 6.9462130566740035, 0.0022563913623439248, 4.0\r\nT-Test Confidence Interval = 518.0534649259677, 1207.9465350740325\r\nALERT: significant change has been detected (p-value < 0.05)\r\nALERT: improvement in performance has been observed\r\n--------------------------------------------------------------------\r\nName = query83\r\nMeans = 11889.0, 10728.333333333334\r\nTime diff = 1160.666666666666\r\nSpeedup = 1.1081870436538759\r\nT-Test (test statistic, p value, df) = 7.983215413800722, 0.0013345137828189723, 4.0\r\nT-Test Confidence Interval = 757.0038418026304, 1564.3294915307017\r\nALERT: significant change has been detected (p-value < 0.05)\r\nALERT: improvement in performance has been observed\r\n--------------------------------------------------------------------\r\nName = query88\r\nMeans = 8288.333333333334, 6815.666666666667\r\nTime diff = 1472.666666666667\r\nSpeedup = 1.2160708172348023\r\nT-Test (test statistic, p value, df) = 14.785515131088996, 0.00012180808800452909, 4.0\r\nT-Test Confidence Interval = 1196.1272209995159, 1749.206112333818\r\nALERT: significant change has been detected (p-value < 0.05)\r\nALERT: improvement in performance has been observed\r\n--------------------------------------------------------------------\r\nName = query95\r\nMeans = 8023.0, 9527.333333333334\r\nTime diff = -1504.333333333334\r\nSpeedup = 0.8421034217339584\r\nT-Test (test statistic, p value, df) = -5.098045536832821, 0.006992118575497059, 4.0\r\nT-Test Confidence Interval = -2323.6078748695036, -685.0587917971645\r\nALERT: significant change has been detected (p-value < 0.05)\r\nALERT: regression in performance has been observed\r\n--------------------------------------------------------------------\r\nName = benchmark\r\nMeans = 418333.3333333333, 394333.3333333333\r\nTime diff = 24000.0\r\nSpeedup = 1.0608622147083686\r\nT-Test (test statistic, p value, df) = 5.622255427989818, 0.004920931820960854, 4.0\r\nT-Test Confidence Interval = 12148.051368670785, 35851.948631329215\r\nALERT: significant change has been detected (p-value < 0.05)\r\nALERT: improvement in performance has been observed\r\n```","createdAt":"2023-10-25T22:42:12Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/14314#issuecomment-1780156704","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5qHHf6","author":{"login":"harrism"},"authorAssociation":"MEMBER","body":"So what are the other ones with speedups of 21%, 19%, etc.?","createdAt":"2023-10-26T00:50:41Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/14314#issuecomment-1780250618","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5qKRj8","author":{"login":"abellina"},"authorAssociation":"CONTRIBUTOR","body":"> So what are the other ones with speedups of 21%, 19%, etc.?\r\n\r\nSorry I should describe the benchmark better. This is NDS https://github.com/NVIDIA/spark-rapids-benchmarks/tree/dev/nds where we are running it in \"power run\" mode. In this case we are running queries in series one after the other in a cluster running spark rapids and 8xA100 GPUs. The results above show when one of the queries has significant regressions or speedups, and it also has a \"benchmark\" section at the end for the overall (sum of all query times compared between baseline and test). I ran these sets of tests three times, so the comparison tool is looking at the means and the variance and figuring out what is noise and what isn't.\r\n\r\nFrom the queries that have significant speedup, query9 and query88 are two queries we know are parquet/scan bound. Looking at traces for these, you'll mostly fine unsnap and parquet decode kernels. We see a lot of benefit here.\r\n\r\nIs this helping with your question @harrism, or am I missing it?","createdAt":"2023-10-26T13:01:14Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/14314#issuecomment-1781078268","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5qM_hE","author":{"login":"harrism"},"authorAssociation":"MEMBER","body":"Yes. So the 6% is overall average benefit. Good to know.","createdAt":"2023-10-26T19:41:52Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/14314#issuecomment-1781790788","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5xzNI4","author":{"login":"harrism"},"authorAssociation":"MEMBER","body":"As of 24.02 you can create a `pool_memory_resource<pinned_host_memory_resource>`. I suggest that libcudf / cuIO adds a `cudf::get/set_current_pinned_host_memory_resource` to use for this.  This could also be added to RMM, but \r\n\r\na) The static resource RMM currently stores for `get/set_current_device_memory_resource` is one of the reasons RMM is not Windows compatible, so I'm averse to adding a second instance of that. Whereas libcudf has a binary library component so it can maintain that state without affecting platform compatibility.\r\nb) I think it's good to try out this feature in libcudf and then move it lower in the stack later if it would be useful to other clients.\r\n\r\nThere is some design work around how and where to put this, and how to wire up configuration knobs for the initial / maximum pool size. Also how to expose that to Python and Spark.\r\n\r\nThere are other places where this pool could be useful but let's start here.","createdAt":"2024-01-25T02:37:49Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/14314#issuecomment-1909248568","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5yV7Ly","author":{"login":"GregoryKimball"},"authorAssociation":"CONTRIBUTOR","body":"Thank you @harrism for kicking off the design discussion. \r\na) would you please clarify the Windows compatibility issue? Is it the presence of a static object with certain properties in a header, or some other issue?\r\nb) If libcudf added `cudf::get/set_current_pinned_host_memory_resource`, would you please share a bit more about the components in RMM this would be modeled after? I'm not familiar enough with RMM to understand the scope of this suggestion.\r\n\r\nThank you for your help","createdAt":"2024-01-31T04:15:27Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/14314#issuecomment-1918350066","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5yW5aL","author":{"login":"harrism"},"authorAssociation":"MEMBER","body":"This issue describes a): https://github.com/rapidsai/rmm/issues/826\n\nBasically the function local static works on Linux but not on windows because each binary gets a it's own instance and so they won't be the same across DLLs.\n\nAlthough I suppose adding a duplicate of the incompatibility reason doesn't make it more incompatible, it's just more of the same.\n\nI still think it should be tested in libcudf first.\n\nFor b) the answer is to model it after get/set_current_device_resource(), which is the function with the function local static.\n\nBut mostly I was suggesting that libcudf should implement it the way libcudf wants it to be for libcudf.\n\nThank you too!","createdAt":"2024-01-31T08:19:42Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/14314#issuecomment-1918604939","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5yc-EE","author":{"login":"bdice"},"authorAssociation":"CONTRIBUTOR","body":"I just discussed this idea a bit more with @abellina, @mattahrens, @GregoryKimball, and @vuule.\r\n\r\nWe're leaning towards adding a parameter for `host_mr` to the Parquet reading APIs. We're leaning away from using a global/static host allocator because we want this behavior to be friendly/simple if dealing with multiple threads like Spark does. Also callers may want to use different host allocators.\r\n\r\nThe `host_mr` would be optional, and would use a non-pooled allocator by default. This avoids some of the concerns about determining a default host pool size, and how that would be handled by Python cudf and other consumers of the library. The `host_mr` should allow any of: a pinned host allocator (which is effectively the current behavior), a pooling pinned host allocator, or possibly even an adapter that implements an \"opportunistically pinned pool\" (Spark's desired use case; this would attempt allocating pinned memory, and fall back to allocating paged memory rather than failing to allocate).\r\n\r\n@abellina is going to investigate a bit further and pursue an implementation for review.","createdAt":"2024-01-31T23:52:47Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/14314#issuecomment-1920196868","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5ydQmD","author":{"login":"harrism"},"authorAssociation":"MEMBER","body":"Sounds good. Because `rmm::mr::pinned_host_memory_resource` is not a `device_memory_resource`, you will need to use the new `rmm::device_async_resource_ref` for the `host_mr` parameter in the Parquet reading APIs. We will be transitioning to that in all of RAPIDS, and in the interim, this will make the parameter compatible with both legacy `device_memory_resource`/`host_memory_resource` MRs AND the newer MRs like `pinned_host_memory_resource` that just implement the `cuda::async_memory_resource` policy.\r\n\r\nFor an example, see #14873 ","createdAt":"2024-02-01T00:54:26Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/14314#issuecomment-1920272771","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5yd8Mk","author":{"login":"harrism"},"authorAssociation":"MEMBER","body":"One thing to be aware of for deciding your initial pool size is that if you make it too small, you will get unnecessary fragmentation. The reason is that the RMM pool MR grows by just allocating a new chunk. This isn't TOO bad because it uses a geometric growth strategy, but allocations that don't fit will cause new allocations that can't be merged with the previous pool chunks, hence fragmentation.\r\n\r\nSee https://github.com/harrism/rmm/blob/6bb0ef2973821c2a1b8f952298046e7b406dc9d2/include/rmm/mr/device/pool_memory_resource.hpp#L352","createdAt":"2024-02-01T03:46:52Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/14314#issuecomment-1920451364","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5yeRpa","author":{"login":"GregoryKimball"},"authorAssociation":"CONTRIBUTOR","body":"Thank you for the meeting today and thank you @bdice for sharing this summary. \r\n\r\n* Adding a `host_mr` parameter to the Parquet reading APIs would be consistent with the way we use the `mr` parameter today. For this approach I would love to sketch out the design changes to `hostdevice_vector` and `pinned_host_vector` that would be required to make use of an input `host_mr`. \r\n* Using a static memory resource in `pinned_host_vector` as in the [prototype](https://github.com/abellina/cudf/commit/a73fbd8a56901f7bb5d6f75ffdcb37f2fd37bdec) by @abellina seems to have the advantage of not adding additional plumbing to the IO modules that use the `hostdevice_vector` utility. \r\n\r\nI'm curious also to hear from @vuule. Plus @nvdbaranec and @etseidl if you would like to weigh in.","createdAt":"2024-02-01T05:24:30Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/14314#issuecomment-1920539226","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5yjoie","author":{"login":"vuule"},"authorAssociation":"CONTRIBUTOR","body":"The introduction of the `host_mr` option would require substantial code changes to pass it to all places where the `hostdevice_vector`s are created. And this would still be limited the PQ reader, until we do the same for other components. \r\nI don't think we know at this point if this would bring additional benefit compared to a global API to set the resource. My proposal is to start with a global resource (that keeps the default behavior) and consider modifying the APIs once we see the effect of the global resource in production. Development would be pretty similar to the stream support (AFAICT), with a global default, and recently included stream parameters in most APIs.\r\n\r\nI have a separate concern about having a `host_mr` parameter that is not an equivalent to the existing mr param. The pinned mem allocator does not change the output for the user, it's purely an internal optimization. But this is maybe just a naming issue, very secondary at this point.","createdAt":"2024-02-01T18:19:36Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/14314#issuecomment-1921943710","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5ykC1K","author":{"login":"nvdbaranec"},"authorAssociation":"CONTRIBUTOR","body":"I am more of a fan of having a global function that sets an rmm allocator to be used for pinned allocations.  For now we could limit it to cuIO only (which would really mean just `hostdevice_vector`).  Something similar to the way we initialize rmm (`rmm::mr::set_current_device_resource(resource.get());`)\r\n\r\nMaybe `cudf::io::set_current_pinned_memory_resource`","createdAt":"2024-02-01T19:17:06Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/14314#issuecomment-1922051402","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5yl9st","author":{"login":"bdice"},"authorAssociation":"CONTRIBUTOR","body":"> The pinned mem allocator does not change the output for the user, it's purely an internal optimization\r\n\r\nThis is a good observation. My primary concern was about ensuring thread safety for the global/static approach (what happens if that allocator is changed while in use / before its previous allocations are freed?). If that's not a problem, then I'm okay with using a global/static pinned (pool) host memory resource. I thought the `host_mr` argument might give us more flexibility for our potential long term needs, but I don't want to overengineer for that case given how little we rely on pinned memory today.","createdAt":"2024-02-02T00:23:07Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"HEART","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/14314#issuecomment-1922554669","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5ymoxf","author":{"login":"vuule"},"authorAssociation":"CONTRIBUTOR","body":"Just measured the pinned memory peak use in the Parquet reader. In the benchmarks, which create tables with 512MB of data, the largest peak pinned memory use I saw was about 3.8MB, with integer columns. Many cases use around 1MB. So we should be able to make great use of a <1GB pool even with many threads.","createdAt":"2024-02-02T03:13:52Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/14314#issuecomment-1922731103","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5yo2X1","author":{"login":"harrism"},"authorAssociation":"MEMBER","body":"But isn't 512MB much smaller than real world use cases?","createdAt":"2024-02-02T08:26:13Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/14314#issuecomment-1923311093","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5yo3iA","author":{"login":"harrism"},"authorAssociation":"MEMBER","body":"BTW after @vuule and @nvdbaranec pointed it out I also think starting with a global config for an allocator that should be used internally in cuIO is a much better place to start than plumbing it in everywhere for flexibility. ","createdAt":"2024-02-02T08:28:32Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/14314#issuecomment-1923315840","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5ysDfA","author":{"login":"abellina"},"authorAssociation":"CONTRIBUTOR","body":"> The introduction of the `host_mr` option would require substantial code changes to pass it to all places where the `hostdevice_vector`s are created. And this would still be limited the PQ reader, until we do the same for other components. I don't think we know at this point if this would bring additional benefit compared to a global API to set the resource. My proposal is to start with a global resource (that keeps the default behavior) and consider modifying the APIs once we see the effect of the global resource in production. Development would be pretty similar to the stream support (AFAICT), with a global default, and recently included stream parameters in most APIs.\r\n> \r\n> I have a separate concern about having a `host_mr` parameter that is not an equivalent to the existing mr param. The pinned mem allocator does not change the output for the user, it's purely an internal optimization. But this is maybe just a naming issue, very secondary at this point.\r\n\r\n@vuule could spark-rapids still provide its own memory pool for this? We wouldn't want a default pinned allocator to initialize, then uninitialized because it is going to be replaced by our own allocator.","createdAt":"2024-02-02T15:52:43Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/14314#issuecomment-1924151232","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5ytR4k","author":{"login":"vuule"},"authorAssociation":"CONTRIBUTOR","body":"> But isn't 512MB much smaller than real world use cases?\r\n\r\nSure, but the measurements show that pinned memory requirement are less than 1% of the device memory required to read a PQ file. Even when fully using a 50GB GPU we won't fill a 1GB pinned pool. ","createdAt":"2024-02-02T18:40:54Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/14314#issuecomment-1924472356","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5y9Ump","author":{"login":"harrism"},"authorAssociation":"MEMBER","body":"Oh, I guess I don't know how parquet reading works. :)","createdAt":"2024-02-06T02:40:20Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/14314#issuecomment-1928677801","viewerDidAuthor":false}],"createdAt":"2023-10-23T18:16:42Z","id":"I_kwDOBWUGps50sNgB","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NjE=","name":"feature request","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwxMTM5NzQwNjY2","name":"libcudf","description":"Affects libcudf (C++/CUDA) code.","color":"c5def5"},{"id":"MDU6TGFiZWwxMTY1NzE0MDY3","name":"0 - Blocked","description":"Cannot progress due to external reasons","color":"e07d6b"},{"id":"MDU6TGFiZWwxMzIyMjUyNjE3","name":"Performance","description":"Performance related issue","color":"C2E0C6"},{"id":"MDU6TGFiZWwxNDA1MTQ2OTc1","name":"Spark","description":"Functionality that helps Spark RAPIDS","color":"7400ff"}],"milestone":{"number":22,"title":"Parquet continuous improvement","description":"","dueOn":null},"number":14314,"projectCards":[],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[FEA] Pinned memory pools for parquet decode","updatedAt":"2024-02-16T23:58:04Z","url":"https://github.com/rapidsai/cudf/issues/14314"}
