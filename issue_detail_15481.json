{"assignees":[],"author":{"id":"MDQ6VXNlcjEzMjk3NTAz","is_bot":false,"login":"pmixer","name":"黄(Huáng)瓒(Zàn)"},"body":"**Describe the bug**\r\nPerformance improvement proposal for cudf parquet file reading efficiency.\r\n\r\n**Steps/Code to reproduce bug**\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({'jnac': [None] * 1000})\r\ndf.to_parquet('/dev/shm/jnac.parquet', compression='ZSTD')\r\n\r\n# cd to /dev/shm now\r\n\r\nimport cudf\r\nimport pandas\r\nimport pyarrow.parquet\r\n\r\nimport time\r\n\r\n# not accurate timing, while the diff is so obvious which do not require more accurate timing temporrally\r\n\r\nts = time.time(); tb = cudf.read_parquet('/dev/shm/jnac.parquet'); te = time.time()\r\ntime.sleep(1)\r\nts = time.time(); tb = cudf.read_parquet('/dev/shm/jnac.parquet'); te = time.time()\r\nprint(te - ts)\r\n\r\nts = time.time(); tb = pandas.read_parquet('/dev/shm/jnac.parquet'); te = time.time()\r\ntime.sleep(1)\r\nts = time.time(); tb = pandas.read_parquet('/dev/shm/jnac.parquet'); te = time.time()\r\nprint(te - ts)\r\n\r\nts = time.time(); tb = pyarrow.parquet.read_table('/dev/shm/jnac.parquet'); te = time.time()\r\ntime.sleep(1)\r\nts = time.time(); tb = pyarrow.parquet.read_table('/dev/shm/jnac.parquet'); te = time.time()\r\nprint(te - ts)\r\n```\r\n\r\n**Expected behavior**\r\n\r\n```python\r\n>>> ts = time.time(); tb = cudf.read_parquet('jnac.parquet'); te = time.time()\r\n>>> print(te - ts)\r\n0.006829023361206055\r\n>>>\r\n>>> ts = time.time(); tb = pandas.read_parquet('jnac.parquet'); te = time.time()\r\n>>> time.sleep(1)\r\n\r\n>>> ts = time.time(); tb = pandas.read_parquet('jnac.parquet'); te = time.time()\r\n>>> print(te - ts)\r\n0.003950357437133789\r\n>>>\r\n>>> ts = time.time(); tb = pyarrow.parquet.read_table('jnac.parquet'); te = time.time()\r\n>>> time.sleep(1)\r\n>>> ts = time.time(); tb = pyarrow.parquet.read_table('jnac.parquet'); te = time.time()\r\n>>> print(te - ts)\r\n0.0013420581817626953\r\n>>>\r\n\r\n```\r\n\r\n**Environment overview (please complete the following information)**\r\ninternal T4 node, py3.9, cudf 24.02.02\r\n\r\n\r\n**Additional context**\r\n\r\nIt just takes too much time to process <NA> entries, especially for cudf when num rows is just 1K(similar latency cost for 10M rows NA though).\r\n","closed":false,"closedAt":null,"comments":[{"id":"IC_kwDOBWUGps55vNus","author":{"login":"pmixer"},"authorAssociation":"NONE","body":"![cudf-na-issue](https://github.com/rapidsai/cudf/assets/13297503/fb225630-4718-4d57-9925-8514fb7ca3e7)\r\n[read_jnac_parquet_and_nsys_file.zip](https://github.com/rapidsai/cudf/files/14904390/read_jnac_parquet_and_nsys_file.zip)\r\nget to know what's happening under the hood, gpu kernel trace very sparse, but CUDA API calls takes so long...","createdAt":"2024-04-08T10:41:16Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/15481#issuecomment-2042420140","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps551g_F","author":{"login":"pmixer"},"authorAssociation":"NONE","body":"There might be some tricks to avoid the long time 1st round run cudaHostAlloc, which I haven't figured out yet, code as below may only handle gpu side mem pre-alloc.\r\n\r\n```python\r\nimport rmm\r\n\r\n# rmm.reinitialize(pool_allocator=True, initial_pool_size= 4 * 10 ** 9)\r\n```","createdAt":"2024-04-09T03:05:02Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/15481#issuecomment-2044071877","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps551h-a","author":{"login":"pmixer"},"authorAssociation":"NONE","body":"I also tried 1M rows all (same) integers and 1M rows all (same) string column, cudf.read_parquet still suffering the perf issue, very likely due to the long cudaMallocHost call.\r\n\r\n```python\r\ndf = pandas.DataFrame({'j2333c': [2333] * 1000000})\r\ndf.to_parquet('/dev/shm/j2333c.parquet', compression='ZSTD')\r\n\r\n>>> import cudf\r\n>>> import pandas\r\n>>> import pyarrow.parquet\r\n>>> \r\n>>> import time\r\n>>> \r\n>>> # not accurate timing, while the diff is so obvious which do not require more accurate timing temporrally\r\n>>> \r\n>>> ts = time.time(); tb = cudf.read_parquet('j2333c.parquet'); te = time.time()\r\n>>> time.sleep(1)\r\n>>> ts = time.time(); tb = cudf.read_parquet('j2333c.parquet'); te = time.time()\r\n>>> print(te - ts)\r\n0.08919477462768555\r\n>>> \r\n>>> ts = time.time(); tb = pandas.read_parquet('j2333c.parquet'); te = time.time()\r\n>>> time.sleep(1)\r\n>>> ts = time.time(); tb = pandas.read_parquet('j2333c.parquet'); te = time.time()\r\n>>> print(te - ts)\r\n0.026215314865112305\r\n>>> \r\n>>> ts = time.time(); tb = pyarrow.parquet.read_table('j2333c.parquet'); te = time.time()\r\n>>> time.sleep(1)\r\n>>> ts = time.time(); tb = pyarrow.parquet.read_table('j2333c.parquet'); te = time.time()\r\n>>> print(te - ts)\r\n0.014030933380126953\r\n\r\n\r\n>>> import cudf\r\n>>> import pandas\r\n>>> import pyarrow.parquet\r\n>>> \r\n>>> import time\r\n>>> \r\n>>> import rmm\r\n>>> \r\n>>> rmm.reinitialize(pool_allocator=True, initial_pool_size= 4 * 10 ** 9)\r\n>>> \r\n>>> # not accurate timing, while the diff is so obvious which do not require more accurate timing temporrally\r\n>>> \r\n>>> ts = time.time(); tb = cudf.read_parquet('j2333c.parquet'); te = time.time()\r\n>>> time.sleep(1)\r\n>>> ts = time.time(); tb = cudf.read_parquet('j2333c.parquet'); te = time.time()\r\n\r\n>>> print(te - ts)\r\n0.08475613594055176\r\n>>> \r\n>>> ts = time.time(); tb = pandas.read_parquet('j2333c.parquet'); te = time.time()\r\n>>> time.sleep(1)\r\n>>> ts = time.time(); tb = pandas.read_parquet('j2333c.parquet'); te = time.time()\r\n>>> print(te - ts)\r\n0.025774002075195312\r\n>>> \r\n>>> ts = time.time(); tb = pyarrow.parquet.read_table('j2333c.parquet'); te = time.time()\r\n>>> time.sleep(1)\r\n>>> ts = time.time(); tb = pyarrow.parquet.read_table('j2333c.parquet'); te = time.time()\r\n>>> print(te - ts)\r\n0.011544227600097656\r\n\r\ndf = pandas.DataFrame({'jstrc', ['2333'] * 1000000})\r\ndf.to_parquet('/dev/shm/jstrc.parquet', compression='ZSTD')\r\n\r\n>>> import cudf\r\n>>> import pandas\r\n>>> import pyarrow.parquet\r\n>>> \r\n>>> import time\r\n>>> \r\n>>> import rmm\r\n>>> \r\n>>> # rmm.reinitialize(pool_allocator=True, initial_pool_size= 4 * 10 ** 9)\r\n>>> \r\n>>> # not accurate timing, while the diff is so obvious which do not require more accurate timing temporrally\r\n>>> \r\n>>> ts = time.time(); tb = cudf.read_parquet('/dev/shm/jstrc.parquet'); te = time.time()\r\n>>> time.sleep(1)\r\n\r\n>>> ts = time.time(); tb = cudf.read_parquet('/dev/shm/jstrc.parquet'); te = time.time()\r\n>>> print(te - ts)\r\n0.08581995964050293\r\n>>> \r\n>>> ts = time.time(); tb = pandas.read_parquet('/dev/shm/jstrc.parquet'); te = time.time()\r\n>>> time.sleep(1)\r\n>>> ts = time.time(); tb = pandas.read_parquet('/dev/shm/jstrc.parquet'); te = time.time()\r\n>>> print(te - ts)\r\n0.057205915451049805\r\n>>> \r\n>>> ts = time.time(); tb = pyarrow.parquet.read_table('/dev/shm/jstrc.parquet'); te = time.time()\r\n>>> time.sleep(1)\r\n>>> ts = time.time(); tb = pyarrow.parquet.read_table('/dev/shm/jstrc.parquet'); te = time.time()\r\n>>> print(te - ts)\r\n0.022694826126098633\r\n```\r\n","createdAt":"2024-04-09T03:10:39Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/15481#issuecomment-2044075930","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps551kCm","author":{"login":"pmixer"},"authorAssociation":"NONE","body":"Well, as GPUs are throughput machine, if increasing rows num from millions to billions, the advantages got well shown:\r\n\r\n```python\r\n\r\n>>> import cudf\r\n>>> import pandas as pd\r\n>>> \r\n>>> df = pd.DataFrame({'jnac': [None] * 1000000000})\r\n>>> df.to_parquet('/dev/shm/jnac.parquet', compression='ZSTD')\r\n>>> import cudf\r\n>>> import pandas\r\n>>> import pyarrow.parquet\r\n>>> \r\n>>> import time\r\n>>> \r\n>>> # not accurate timing, while the diff is so obvious which do not require more accurate timing temporrally\r\n>>> \r\n>>> ts = time.time(); tb = cudf.read_parquet('/dev/shm/jnac.parquet'); te = time.time()\r\n>>> time.sleep(1)\r\n>>> ts = time.time(); tb = cudf.read_parquet('/dev/shm/jnac.parquet'); te = time.time()\r\n>>> print(te - ts)\r\n0.15029525756835938\r\n>>> \r\n>>> ts = time.time(); tb = pandas.read_parquet('/dev/shm/jnac.parquet'); te = time.time()\r\n>>> time.sleep(1)\r\n>>> ts = time.time(); tb = pandas.read_parquet('/dev/shm/jnac.parquet'); te = time.time()\r\n>>> print(te - ts)\r\n7.30379843711853\r\n>>> \r\n>>> ts = time.time(); tb = pyarrow.parquet.read_table('/dev/shm/jnac.parquet'); te = time.time()\r\n>>> time.sleep(1)\r\n>>> ts = time.time(); tb = pyarrow.parquet.read_table('/dev/shm/jnac.parquet'); te = time.time()\r\n>>> print(te - ts)\r\n1.51247239112854\r\n>>> \r\n\r\n```\r\n\r\nso, now the major problem is how to resolve the issue for millions scale row num chunked tables.","createdAt":"2024-04-09T03:23:29Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/15481#issuecomment-2044084390","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps56xLIY","author":{"login":"etseidl"},"authorAssociation":"CONTRIBUTOR","body":"TL;DR There is a fixed overhead to using CUDA and cuDF, so very small files are not going to show any improvement, but you can cut down on the startup overhead some, and there are other ways to improve efficiency some.\r\n\r\nI'll show some example traces on my RTX A6000 using the 1M integer example from above. First the output:\r\n```\r\n% python3 jnac.py \r\n0.00894021987915039\r\n0.005298137664794922\r\n0.0025205612182617188\r\n```\r\nand the associated trace\r\n![Screenshot from 2024-04-16 10-22-41](https://github.com/rapidsai/cudf/assets/25541553/8a1d0fc8-08f1-4a46-b9b8-294d58c5a92c)\r\nThe area highlighted in green is the actual decode time and is around 21ms. You can see the time is dominated by CUDA initialization (54ms) and setup involved in using kvikio/cuFile (187ms).\r\n\r\nThe second `read_parquet` call is then much faster (9ms), and is dominated by the decode kernel (6.5ms).\r\n![Screenshot from 2024-04-16 10-25-18](https://github.com/rapidsai/cudf/assets/25541553/b10101ae-50ee-4236-8c72-96d48fb2ed50)\r\n\r\nSince the file is so small, we can skip using cuFile by setting the `LIBCUDF_CUFILE_POLICY` envvar to `OFF`. This has no impact on the measured read time, but greatly reduces the setup time.\r\n```\r\n% env LIBCUDF_CUFILE_POLICY=OFF python3 jnac.py \r\n0.009036540985107422\r\n0.007204532623291016\r\n0.0022492408752441406\r\n```\r\n\r\n![Screenshot from 2024-04-16 10-29-08](https://github.com/rapidsai/cudf/assets/25541553/84fa9690-ed2a-4070-8527-1265a17b9492)\r\n\r\n\r\nAnother thing to notice from the above is that pandas is writing all 1M rows into a single page. But libcudf parallelizes parquet reads at the page level, so to see any improvement you'll want more pages. Parquet-mr and libcudf default to 20000 rows max per page, so using cudf to write the initial file has a measurable impact for the 1M row case.\r\n```py\r\ndf = pd.DataFrame({'jnac': [2333] * 1000000})\r\n#df.to_parquet(fname, compression='ZSTD')\r\ncdf = cudf.from_pandas(df)\r\ncdf.to_parquet(fname, compression='ZSTD')\r\n```\r\n```\r\n% env LIBCUDF_CUFILE_POLICY=OFF python3 jnac.py\r\n0.002679586410522461\r\n0.005636692047119141\r\n0.0023217201232910156\r\n```\r\n\r\n![Screenshot from 2024-04-16 10-31-34](https://github.com/rapidsai/cudf/assets/25541553/f680afce-be4f-4756-9d96-d21ba60c039d)\r\n\r\nNow the `to_parquet()` call is bearing the price of cuda initialization. The decode time (again in highlighted in green) has gone from 20ms to around 11, and the decode kernel has gone from a single threadblock and 6.5ms to 50 threadblocks and 140us. And the second `read_parquet` is now down to around 2.6ms.\r\n\r\nIn summary, we're seeing a pretty fixed cost of 50ms for CUDA setup, 190ms for cuFile setup, and around 10ms for cuDF setup (there are some buffer initializations and a stream pool to set up). Python adds its own latency too, and there's the actual file I/O to take into account. So the minimum time you'll see for a single file read is going to be something over 60ms.  If you can read files in batches and amortize the startup penalty, you'll still only see performance on par with arrow for such small files.  As you've discovered already, to really see the benefit you need files that are large enough to move the bottleneck from setup/IO to the actual compute kernels that will show the parallelization benefit. ","createdAt":"2024-04-16T18:36:38Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}},{"content":"HEART","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/15481#issuecomment-2059711000","viewerDidAuthor":false}],"createdAt":"2024-04-08T10:05:14Z","id":"I_kwDOBWUGps6E9xl6","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NTk=","name":"bug","description":"Something isn't working","color":"d73a4a"}],"milestone":null,"number":15481,"projectCards":[],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[BUG] cudf.read_parquet takes too much time(due to cudaMallocHost overhead etc.) to load the zstd compressed parquet files with few thousands to millions of rows","updatedAt":"2024-04-16T18:36:39Z","url":"https://github.com/rapidsai/cudf/issues/15481"}
