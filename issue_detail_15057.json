{"assignees":[],"author":{"id":"MDQ6VXNlcjEyNzI1MTEx","is_bot":false,"login":"GregoryKimball","name":"Gregory Kimball"},"body":"**Is your feature request related to a problem? Please describe.**\r\nThe `BM_parquet_read_chunks` benchmark in `benchmarks/io/parquet/parquet_reader_input.cpp` includes a `byte_limit` nvbench axis. This axis controls the `chunk_read_limit`. With the new features added in #14360, there is a new `chunked_parquet_reader` API that exposes both `chunk_read_limit` and `pass_read_limit` parameters to control reader behavior. We currently do not have a method for benchmarking `pass_read_limit` values.\r\n\r\n**Describe the solution you'd like**\r\n- [ ] Add a new benchmark, such as `BM_parquet_read_subrowgroup_chunks`, that provides nvbench axes for both `chunk_read_limit` and `pass_read_limit`\r\n- [ ] Rename `byte_limit` to `chunk_read_limit` in `BM_parquet_read_chunks` for clarity, now that we have both input and output byte limits in chunked parquet reading.\r\n- [ ] Also, please consider adding an nvbench axis for `data_size` for at least the chunked parquet reader benchmarks. It would be useful to allow the benchmarks to operate on tables larger than 536 MB.\r\n\r\n**Describe alternatives you've considered**\r\nn/a","closed":false,"closedAt":null,"comments":[],"createdAt":"2024-02-14T21:27:40Z","id":"I_kwDOBWUGps5_RRGF","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NjE=","name":"feature request","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwxMDEzOTg3MzUy","name":"0 - Backlog","description":"In queue waiting for assignment","color":"d4c5f9"},{"id":"MDU6TGFiZWwxMTM5NzQwNjY2","name":"libcudf","description":"Affects libcudf (C++/CUDA) code.","color":"c5def5"},{"id":"MDU6TGFiZWwxMTg1MjQ0MTQy","name":"cuIO","description":"cuIO issue","color":"fef2c0"},{"id":"MDU6TGFiZWwxNDA1MTQ2OTc1","name":"Spark","description":"Functionality that helps Spark RAPIDS","color":"7400ff"}],"milestone":{"number":22,"title":"Parquet continuous improvement","description":"","dueOn":null},"number":15057,"projectCards":[],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[FEA] Update chunked parquet reader benchmarks to include `pass_read_limit`","updatedAt":"2024-02-16T23:45:28Z","url":"https://github.com/rapidsai/cudf/issues/15057"}
