{"assignees":[],"author":{"id":"MDQ6VXNlcjE3ODE3NzY4","is_bot":false,"login":"MikeChenfu","name":"Cg Lai"},"body":"**Describe the bug**\r\nHi Guys,  I got an error about `Total number of concatenated rows exceeds size_type range` after  doing an`inner join` on two `dask_cudf` dfs. It seems that some partitions contains a large number of rows. However, the code works good when I `concat` these two dfs.\r\n\r\n**Steps/Code to reproduce bug**\r\nIn my understanding, the rows in q2 should be larger than q1 .\r\n```\r\n# setup\r\nc = LocalCUDACluster( device_memory_limit=0.8, rmm_managed_memory=True, jit_unspill=True)\r\nc = Client(c)\r\n\r\n# 40G data\r\na = dask_cudf.read_orc('a/*.orc')\r\n# 4G data\r\nb = dask_cudf.read_orc('b/*.orc')\r\n\r\n# Works good\r\nq2 = dask_cudf.concat([a,b])\r\nq2.map_partitions(len).compute()\r\n\r\n0        307200\r\n1        291840\r\n2        337920\r\n3        261120\r\n4        256000\r\n          ...  \r\n21529    100525\r\n21530     94142\r\n21531     86762\r\n21532     94782\r\n21533     12502\r\nLength: 21534, dtype: int64\r\n\r\n# Errors with concatenated rows exceeds size_type range\r\nq1 = a.merge(b, on=['a'],how='inner' )\r\nlength_partition = q1.map_partitions(len)\r\nlength_partition.compute()\r\n\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-8-efedf1b4ef97> in <module>\r\n      1 length_partition = q1.map_partitions(len)\r\n----> 2 length_partition.compute()\r\n\r\n/conda/envs/rapids/lib/python3.7/site-packages/dask/base.py in compute(self, **kwargs)\r\n    282         dask.base.compute\r\n    283         \"\"\"\r\n--> 284         (result,) = compute(self, traverse=False, **kwargs)\r\n    285         return result\r\n    286 \r\n\r\n/conda/envs/rapids/lib/python3.7/site-packages/dask/base.py in compute(*args, **kwargs)\r\n    564         postcomputes.append(x.__dask_postcompute__())\r\n    565 \r\n--> 566     results = schedule(dsk, keys, **kwargs)\r\n    567     return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\r\n    568 \r\n\r\n/conda/envs/rapids/lib/python3.7/site-packages/distributed/client.py in get(self, dsk, keys, workers, allow_other_workers, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)\r\n   2664                     should_rejoin = False\r\n   2665             try:\r\n-> 2666                 results = self.gather(packed, asynchronous=asynchronous, direct=direct)\r\n   2667             finally:\r\n   2668                 for f in futures.values():\r\n\r\n/conda/envs/rapids/lib/python3.7/site-packages/distributed/client.py in gather(self, futures, errors, direct, asynchronous)\r\n   1979                 direct=direct,\r\n   1980                 local_worker=local_worker,\r\n-> 1981                 asynchronous=asynchronous,\r\n   1982             )\r\n   1983 \r\n\r\n/conda/envs/rapids/lib/python3.7/site-packages/distributed/client.py in sync(self, func, asynchronous, callback_timeout, *args, **kwargs)\r\n    842         else:\r\n    843             return sync(\r\n--> 844                 self.loop, func, *args, callback_timeout=callback_timeout, **kwargs\r\n    845             )\r\n    846 \r\n\r\n/conda/envs/rapids/lib/python3.7/site-packages/distributed/utils.py in sync(loop, func, callback_timeout, *args, **kwargs)\r\n    351     if error[0]:\r\n    352         typ, exc, tb = error[0]\r\n--> 353         raise exc.with_traceback(tb)\r\n    354     else:\r\n    355         return result[0]\r\n\r\n/conda/envs/rapids/lib/python3.7/site-packages/distributed/utils.py in f()\r\n    334             if callback_timeout is not None:\r\n    335                 future = asyncio.wait_for(future, callback_timeout)\r\n--> 336             result[0] = yield future\r\n    337         except Exception as exc:\r\n    338             error[0] = sys.exc_info()\r\n\r\n/conda/envs/rapids/lib/python3.7/site-packages/tornado/gen.py in run(self)\r\n    760 \r\n    761                     try:\r\n--> 762                         value = future.result()\r\n    763                     except Exception:\r\n    764                         exc_info = sys.exc_info()\r\n\r\n/conda/envs/rapids/lib/python3.7/site-packages/distributed/client.py in _gather(self, futures, errors, direct, local_worker)\r\n   1838                             exc = CancelledError(key)\r\n   1839                         else:\r\n-> 1840                             raise exception.with_traceback(traceback)\r\n   1841                         raise exc\r\n   1842                     if errors == \"skip\":\r\n\r\n/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/core.py in _concat()\r\n    101         args[0]\r\n    102         if not args2\r\n--> 103         else methods.concat(args2, uniform=True, ignore_index=ignore_index)\r\n    104     )\r\n    105 \r\n\r\n/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/methods.py in concat()\r\n    434             filter_warning=filter_warning,\r\n    435             ignore_index=ignore_index,\r\n--> 436             **kwargs\r\n    437         )\r\n    438 \r\n\r\n/conda/envs/rapids/lib/python3.7/site-packages/dask_cuda/proxy_object.py in wrapper()\r\n    708         args = [unproxy(d) for d in args]\r\n    709         kwargs = {k: unproxy(v) for k, v in kwargs.items()}\r\n--> 710         return func(*args, **kwargs)\r\n    711 \r\n    712     return wrapper\r\n\r\n/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/methods.py in concat()\r\n    434             filter_warning=filter_warning,\r\n    435             ignore_index=ignore_index,\r\n--> 436             **kwargs\r\n    437         )\r\n    438 \r\n\r\n/conda/envs/rapids/lib/python3.7/site-packages/dask_cudf/backends.py in concat_cudf()\r\n    223         )\r\n    224 \r\n--> 225     return cudf.concat(dfs, axis=axis, ignore_index=ignore_index)\r\n    226 \r\n    227 \r\n\r\n/conda/envs/rapids/lib/python3.7/site-packages/cudf/core/reshape.py in concat()\r\n    370                 ignore_index=ignore_index,\r\n    371                 # Explicitly cast rather than relying on None being falsy.\r\n--> 372                 sort=bool(sort),\r\n    373             )\r\n    374         return result\r\n\r\n/conda/envs/rapids/lib/python3.7/contextlib.py in inner()\r\n     72         def inner(*args, **kwds):\r\n     73             with self._recreate_cm():\r\n---> 74                 return func(*args, **kwds)\r\n     75         return inner\r\n     76 \r\n\r\n/conda/envs/rapids/lib/python3.7/site-packages/cudf/core/frame.py in _concat()\r\n    454         # Concatenate the Tables\r\n    455         out = cls._from_table(\r\n--> 456             libcudf.concat.concat_tables(tables, ignore_index=ignore_index)\r\n    457         )\r\n    458 \r\n\r\ncudf/_lib/concat.pyx in cudf._lib.concat.concat_tables()\r\n\r\ncudf/_lib/concat.pyx in cudf._lib.concat.concat_tables()\r\n\r\nRuntimeError: cuDF failure at: ../src/copying/concatenate.cu:364: Total number of concatenated rows exceeds size_type range\r\n\r\n\r\n\r\n```\r\n\r\n\r\n**Environment overview (please complete the following information)**\r\n - Environment location: Docker\r\n - Method of cuDF install: conda\r\n","closed":false,"closedAt":null,"comments":[{"id":"MDEyOklzc3VlQ29tbWVudDgzNDgzNDkxMA==","author":{"login":"VibhuJawa"},"authorAssociation":"MEMBER","body":"Hey  Lai, \r\n\r\nSeems like you are running into this issue because the output merge of small chunks may be too big (> 2,147,483,648) .  \r\n\r\nThis can happen if you have a lot of repeated keys as we repeat each row in the right table that matches the left table. See the below example for an explanation. \r\n\r\n```python\r\ndf_1 = cudf.DataFrame({'id_a':[1,2,3,4,5,6],'key':[0,0,0,0,0,0]})\r\ndf_2 = cudf.DataFrame({'id_b':[10,20],'key':[0,0]})\r\ndf_1.merge(df_2)\r\n```\r\n\r\n```\r\n\tid_a\tkey\tid_b\r\n0\t1\t0\t10\r\n1\t2\t0\t10\r\n2\t3\t0\t10\r\n3\t4\t0\t10\r\n4\t5\t0\t10\r\n5\t6\t0\t10\r\n6\t1\t0\t20\r\n7\t2\t0\t20\r\n8\t3\t0\t20\r\n9\t4\t0\t20\r\n10\t5\t0\t20\r\n11\t6\t0\t20\r\n````\r\n\r\nNow mitigating this can be done in multiple ways : \r\n\r\n### 1.  If one of the frames is a single partitions (For broadcast joints) :\r\n\r\nRepartition the multi partitioned table upfront into  more partitions:\r\n\r\n```python\r\n## the right frame is a single partition\r\nassert ddf_2.npartitions == 1 \r\n\r\nmerged_df = ddf_1 = ddf_1.repartition(npartitions=ddf_1.npartitions*2) \r\nddf_1.merge(ddf_2,how=inner)\r\n\r\n```\r\n\r\n### 2.  If both of them have multiple partitions (Hash-Join)\r\n\r\nUsing `npartitions`: This argument controls the number of output partitions. \r\n\r\no_partitions = max(ddf_1.npartitions, ddf_2.npartitions )*2 \r\n\r\n```python\r\nmerged_df = ddf_1.merge(ddf_2,npartitions= o_partitions,  how=inner)\r\n``` \r\n\r\n\r\n\r\nIt's still tough to know how to mitigate this error without more information like the `number of left `and `right` partitions but I hope that this still helps. \r\n","createdAt":"2021-05-07T22:47:58Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/8182#issuecomment-834834910","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDgzNDkwNDcyOQ==","author":{"login":"kkraus14"},"authorAssociation":"COLLABORATOR","body":"@rjzamora @shwina this is something that we have primitives to potential handle within dask-cudf. We have APIs that can now return a pair of device vectors to use for gather maps to materialize the join output. The device vectors don't have this int32 size limitation, so we can logically split them into multiple gather calls and output multiple partitions instead of a single partition.","createdAt":"2021-05-08T00:37:44Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/8182#issuecomment-834904729","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDgzNTI2MDQyMQ==","author":{"login":"shwina"},"authorAssociation":"CONTRIBUTOR","body":"Ideally I'd like for dask-cudf to do this in a way that doesn't interact with cudf internals. To that end, I think it makes sense to expose a join API that returns either (1) chunked results or (2) the gathermaps as CuPy arrays.","createdAt":"2021-05-08T10:01:20Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/8182#issuecomment-835260421","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps45xqMR","author":{"login":"github-actions"},"authorAssociation":"NONE","body":"This issue has been labeled `inactive-90d` due to no recent activity in the past 90 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed.","createdAt":"2021-11-15T21:04:19Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/8182#issuecomment-969319185","viewerDidAuthor":false}],"createdAt":"2021-05-07T15:42:19Z","id":"MDU6SXNzdWU4NzkyMzgyODI=","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NTk=","name":"bug","description":"Something isn't working","color":"d73a4a"},{"id":"MDU6TGFiZWwxMTM5NzQxMjEz","name":"cuDF (Python)","description":"Affects Python cuDF API.","color":"1d76db"},{"id":"MDU6TGFiZWwxMTg1MjQwODk4","name":"dask","description":"Dask issue","color":"fcc25d"}],"milestone":null,"number":8182,"projectCards":[{"project":{"name":"Bug Squashing"},"column":{"name":"Needs prioritizing"}}],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"Concatenated rows exceeds size_type range after merge operation[BUG]","updatedAt":"2023-09-09T03:56:30Z","url":"https://github.com/rapidsai/cudf/issues/8182"}
