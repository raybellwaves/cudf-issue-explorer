,title,body,createdAt,n_body_reactions_thumbs_up,n_body_reactions_thumbs_down,author.name,company,is_nvidia_employee
0,Support for windows? ,"I use Linux at work but at home I have windows and would like to be able to run it on my main machine with conda. Currently when I run:

`conda env create --name pygdf_dev --file conda_environments/testing_py35.yml`

I am seeing this error:

```
NoPackagesFoundError: Package missing in current win-64 channels:
  - libgdf_cffi >=0.1.0a1.dev
```

I am hoping that this could be easily added to the win-64 channels?",2017-07-01T05:54:03Z,43,0,stephenmm,,False
1,Compile times are growing significantly,"<!--

Thanks for opening an issue! To help the libGDF team handle your information
efficiently, please first ensure that there is no other issue present that
already describes the issue you have
(search at https://github.com/gpuopenanalytics/libgdf/issues?&q=is%3Aissue).

If there is no issue present please jump to a section below and delete the
irrelevant one depending on whether you are:

 * Making a feature request.
 * Reporting a bug.

For more general ""how do I do X?"" type questions, please speak visit
http://gpuopenanalytics.com/#/COMMUNITY for links to Slack and Google
Groups.

-->

## Feature request

<!--

Please include details of the feature you would like to see, why you would
like to see it/the use case

-->


As anyone who has built libgdf recently has surely noticed, the time to compile the library from scratch has grown significantly in the last few months. For example, compiling on all 10 cores of my i9-7900X  @ 3.30GHz takes 11 minutes as reported by `time make -j`.

Compiling with `-ftime-report` may be a good place to start to see where all the compilation time is being spent.

This is likely due to the large amount of template instantiation that is required for instantiating functions for all possible types. We should make sure that best practices are being followed in template instantiation such that a template for a given type is only having to be instantiated once via explicit instantiation.

Much of our code is implemented in headers, which causes it to be recompiled everywhere that header is included. Using pre-compiled headers may help: 
https://gcc.gnu.org/onlinedocs/gcc/Precompiled-Headers.html
http://itscompiling.eu/2017/01/12/precompiled-headers-cpp-compilation/

Furthermore, code should be scrubbed from excessive and unnecessary `#include`s. Compiling with `-MD` will show what files are being included 

Here's a Clang tool that ensures you only include the necessary headers: https://github.com/include-what-you-use/include-what-you-use

Here's a Clang tool to profile time spent in template instantiation: https://github.com/mikael-s-persson/templight





",2018-09-09T17:57:58Z,0,0,Jake Hemstad,@NVIDIA,True
2,Add clang-tidy for automatic linting,"<!--

Thanks for opening an issue! To help the libGDF team handle your information
efficiently, please first ensure that there is no other issue present that
already describes the issue you have
(search at https://github.com/gpuopenanalytics/libgdf/issues?&q=is%3Aissue).

If there is no issue present please jump to a section below and delete the
irrelevant one depending on whether you are:

 * Making a feature request.
 * Reporting a bug.

For more general ""how do I do X?"" type questions, please speak visit
http://gpuopenanalytics.com/#/COMMUNITY for links to Slack and Google
Groups.

-->

## Feature request

In similar spirit of #117, would be good to start using [`clang-tidy`](https://clang.llvm.org/extra/clang-tidy/) as a linter on the codebase to catch some errors, style violations, etc.

Arrow's setup would be a good place to start, which includes a [CMake target](https://github.com/apache/arrow/blob/44c2fa7d7d4e7a1c1645c85f87d557c4fc510c33/cpp/CMakeLists.txt#L552-L560) to run the tool and a [custom version of `run-clang-tidy`](https://github.com/apache/arrow/blob/master/cpp/build-support/run-clang-tidy.sh) which supports ignoring some files.

<!--

Please include details of the feature you would like to see, why you would
like to see it/the use case

-->",2018-08-29T18:00:18Z,0,0,Andrew Seidl,,False
3,[BUG] Long import times,"**Describe the bug**

It takes a few seconds to import cudf today

**Steps/Code to reproduce bug**

```
In [1]: import numpy, pandas

In [2]: %time import cudf
CPU times: user 432 ms, sys: 132 ms, total: 564 ms
Wall time: 2.44 s
```

<details>

```
**git***
commit ab3f45857f641548f6d64d977908075d63c193bf (HEAD -> repr-html, mrocklin/repr-html)
Author: Matthew Rocklin <mrocklin@gmail.com>
Date:   Thu Jan 3 17:35:10 2019 -0800

    Test repr for both large and small dataframes

***OS Information***
DGX_NAME=""DGX Server""
DGX_PRETTY_NAME=""NVIDIA DGX Server""
DGX_SWBUILD_DATE=""2018-03-20""
DGX_SWBUILD_VERSION=""3.1.6""
DGX_COMMIT_ID=""1b0f58ecbf989820ce745a9e4836e1de5eea6cfd""
DGX_SERIAL_NUMBER=QTFCOU8310024

DGX_OTA_VERSION=""3.1.7""
DGX_OTA_DATE=""Thu Sep 27 20:07:53 PDT 2018""
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=16.04
DISTRIB_CODENAME=xenial
DISTRIB_DESCRIPTION=""Ubuntu 16.04.5 LTS""
NAME=""Ubuntu""
VERSION=""16.04.5 LTS (Xenial Xerus)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 16.04.5 LTS""
VERSION_ID=""16.04""
HOME_URL=""http://www.ubuntu.com/""
SUPPORT_URL=""http://help.ubuntu.com/""
BUG_REPORT_URL=""http://bugs.launchpad.net/ubuntu/""
VERSION_CODENAME=xenial
UBUNTU_CODENAME=xenial
Linux dgx16 4.4.0-135-generic #161-Ubuntu SMP Mon Aug 27 10:45:01 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

***GPU Information***
Thu Jan  3 18:42:10 2019
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 396.44                 Driver Version: 396.44                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |
| N/A   42C    P0    59W / 300W |   1085MiB / 32510MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |
| N/A   43C    P0    46W / 300W |     11MiB / 32510MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |
| N/A   42C    P0    46W / 300W |     11MiB / 32510MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |
| N/A   41C    P0    47W / 300W |     11MiB / 32510MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |
| N/A   42C    P0    46W / 300W |     11MiB / 32510MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |
| N/A   42C    P0    44W / 300W |     11MiB / 32510MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |
| N/A   44C    P0    45W / 300W |     11MiB / 32510MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |
| N/A   41C    P0    44W / 300W |     11MiB / 32510MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     64394      C   ...klin/miniconda/envs/cudf_dev/bin/python   644MiB |
|    0     65545      C   ...klin/miniconda/envs/cudf_dev/bin/python   430MiB |
+-----------------------------------------------------------------------------+

***CPU***
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                80
On-line CPU(s) list:   0-79
Thread(s) per core:    2
Core(s) per socket:    20
Socket(s):             2
NUMA node(s):          2
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 79
Model name:            Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz
Stepping:              1
CPU MHz:               2030.789
CPU max MHz:           3600.0000
CPU min MHz:           1200.0000
BogoMIPS:              4391.76
Virtualization:        VT-x
L1d cache:             32K
L1i cache:             32K
L2 cache:              256K
L3 cache:              51200K
NUMA node0 CPU(s):     0-19,40-59
NUMA node1 CPU(s):     20-39,60-79
Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb invpcid_single intel_pt ssbd ibrs ibpb stibp kaiser tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts flush_l1d

***CMake***
/home/nfs/mrocklin/miniconda/envs/cudf_dev/bin/cmake
cmake version 3.13.2

CMake suite maintained and supported by Kitware (kitware.com/cmake).

***g++***
/usr/bin/g++
g++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


***nvcc***

***Python***
/home/nfs/mrocklin/miniconda/envs/cudf_dev/bin/python
Python 3.5.5

***Environment Variables***
PATH                            : /home/nfs/mrocklin/miniconda/envs/cudf_dev/bin:/home/nfs/mrocklin/miniconda/bin:/home/nfs/mrocklin/miniconda/bin:/home/nfs/mrocklin/miniconda/bin:/home/nfs/mrocklin/bin:/home/nfs/mrocklin/.local/bin:/home/nfs/mrocklin/miniconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games
LD_LIBRARY_PATH                 :
NUMBAPRO_NVVM                   :
NUMBAPRO_LIBDEVICE              :
CONDA_PREFIX                    : /home/nfs/mrocklin/miniconda/envs/cudf_dev
PYTHON_PATH                     :

***conda packages***
/home/nfs/mrocklin/miniconda/bin/conda
# packages in environment at /home/nfs/mrocklin/miniconda/envs/cudf_dev:
#
# Name                    Version                   Build  Channel
alabaster                 0.7.12                     py_0    conda-forge
arrow-cpp                 0.10.0           py35h70250a7_0    conda-forge
asn1crypto                0.24.0                   py35_3    conda-forge
atomicwrites              1.2.1                      py_0    conda-forge
attrs                     18.2.0                     py_0    conda-forge
babel                     2.6.0                      py_1    conda-forge
backcall                  0.1.0                      py_0    conda-forge
blas                      1.0                         mkl
bleach                    3.0.2                      py_1    conda-forge
bokeh                     0.13.0                   py35_0
boost                     1.67.0           py35h3e44d54_0    conda-forge
boost-cpp                 1.67.0               h3a22d5f_0    conda-forge
bzip2                     1.0.6                h470a237_2    conda-forge
ca-certificates           2018.03.07                    0
certifi                   2018.8.24                py35_1
cffi                      1.11.5           py35h5e8e0c9_1    conda-forge
chardet                   3.0.4                    py35_3    conda-forge
click                     7.0                        py_0    conda-forge
cloudpickle               0.6.1                      py_0    conda-forge
cmake                     3.13.2               h011004d_0    conda-forge
commonmark                0.5.4                      py_2    conda-forge
cryptography              2.3.1            py35hdffb7b8_0    conda-forge
cryptography-vectors      2.3.1                    py35_0    conda-forge
cudatoolkit               9.2                           0
cudf                      0.4.0+385.g81a187d           <pip>
curl                      7.63.0               h74213dd_0    conda-forge
cython                    0.28.5           py35hfc679d8_0    conda-forge
cytoolz                   0.9.0.1          py35h470a237_0    conda-forge
dask                      1.0.0+51.g2ca205b           <pip>
dask-core                 1.0.0                      py_0    conda-forge
dask-cuda                 0.0.0                     <pip>
dask-cudf                 0.0.1+222.g10a9f90.dirty           <pip>
decorator                 4.3.0                      py_0    conda-forge
distributed               1.23.2                   py35_1    conda-forge
distributed               1.25.1+10.ga0d0ed2           <pip>
docutils                  0.14                     py35_1    conda-forge
entrypoints               0.2.3                    py35_2    conda-forge
expat                     2.2.5                hfc679d8_2    conda-forge
future                    0.16.0                   py35_2    conda-forge
gmp                       6.1.2                hfc679d8_0    conda-forge
heapdict                  1.0.0                 py35_1000    conda-forge
icu                       58.2                 hfc679d8_0    conda-forge
idna                      2.7                      py35_2    conda-forge
imagesize                 1.1.0                      py_0    conda-forge
intel-openmp              2019.1                      144
ipykernel                 5.1.0              pyh24bf2e0_0    conda-forge
ipython                   7.0.1            py35h24bf2e0_0    conda-forge
ipython_genutils          0.2.0                      py_1    conda-forge
jedi                      0.12.1                   py35_0    conda-forge
jinja2                    2.10                       py_1    conda-forge
jsonschema                2.6.0                    py35_2    conda-forge
jupyter_client            5.2.4                      py_0    conda-forge
jupyter_core              4.4.0                      py_0    conda-forge
krb5                      1.16.2               hbb41f41_0    conda-forge
libcurl                   7.63.0               hbdb9355_0    conda-forge
libedit                   3.1.20170329         haf1bffa_1    conda-forge
libffi                    3.2.1                hfc679d8_5    conda-forge
libgcc-ng                 8.2.0                hdf63c60_1
libgdf-cffi               0.4.0                     <pip>
libgfortran-ng            7.2.0                hdf63c60_3    conda-forge
librmm-cffi               0.4.0                     <pip>
libsodium                 1.0.16               h470a237_1    conda-forge
libssh2                   1.8.0                h5b517e9_3    conda-forge
libstdcxx-ng              8.2.0                hdf63c60_1
libuv                     1.24.1               h470a237_0    conda-forge
llvmlite                  0.27.0           py35hf484d3e_0    numba
Markdown                  2.6.11                    <pip>
markupsafe                1.0              py35h470a237_1    conda-forge
mistune                   0.8.3            py35h470a237_2    conda-forge
mkl                       2018.0.3                      1
mkl_fft                   1.0.9                    py35_0    conda-forge
mkl_random                1.0.1                    py35_0    conda-forge
more-itertools            4.3.0                    py35_0    conda-forge
msgpack-python            0.5.6            py35h2d50403_3    conda-forge
nbconvert                 5.3.1                      py_1    conda-forge
nbformat                  4.4.0                      py_1    conda-forge
ncurses                   6.1                  hfc679d8_2    conda-forge
notebook                  5.7.0                    py35_0    conda-forge
numba                     0.42.0          np115py35hf484d3e_0    numba
numpy                     1.15.2           py35h1d66e8a_0
numpy-base                1.15.2           py35h81de0dd_0
numpydoc                  0.8.0                      py_1    conda-forge
nvstrings                 0.1.0            cuda9.2_py35_0    nvidia
openssl                   1.0.2p               h14c3975_0
packaging                 18.0                       py_0    conda-forge
pandas                    0.20.3                   py35_1    conda-forge
pandoc                    2.5                           0    conda-forge
pandocfilters             1.4.2                      py_1    conda-forge
parquet-cpp               1.5.0.pre            h83d4a3d_0    conda-forge
parso                     0.3.1                      py_0    conda-forge
pathlib2                  2.3.2                    py35_0    conda-forge
pexpect                   4.6.0                    py35_0    conda-forge
pickleshare               0.7.5                    py35_0    conda-forge
pip                       18.0                  py35_1001    conda-forge
pluggy                    0.8.0                      py_0    conda-forge
prometheus_client         0.5.0                      py_0    conda-forge
prompt_toolkit            2.0.7                      py_0    conda-forge
psutil                    5.4.7            py35h470a237_1    conda-forge
ptyprocess                0.6.0                 py35_1000    conda-forge
py                        1.7.0                      py_0    conda-forge
pyarrow                   0.10.0           py35hfc679d8_0    conda-forge
pycparser                 2.19                       py_0    conda-forge
pygments                  2.3.1                      py_0    conda-forge
pyopenssl                 18.0.0                   py35_0    conda-forge
pyparsing                 2.3.0                      py_0    conda-forge
pysocks                   1.6.8                    py35_2    conda-forge
pytest                    4.0.2                     <pip>
pytest                    3.8.1                    py35_0
python                    3.5.5                h5001a0f_2    conda-forge
python-dateutil           2.7.5                      py_0    conda-forge
pytz                      2018.7                     py_0    conda-forge
pyyaml                    3.13             py35h470a237_1    conda-forge
pyzmq                     17.1.2           py35hae99301_0    conda-forge
readline                  7.0                  haf1bffa_1    conda-forge
recommonmark              0.4.0                      py_2    conda-forge
requests                  2.19.1                   py35_1    conda-forge
rhash                     1.3.6                h470a237_1    conda-forge
send2trash                1.5.0                      py_0    conda-forge
setuptools                40.4.3                   py35_0    conda-forge
simplegeneric             0.8.1                      py_1    conda-forge
six                       1.11.0                   py35_1    conda-forge
snowballstemmer           1.2.1                      py_1    conda-forge
sortedcontainers          2.1.0                      py_0    conda-forge
sphinx                    1.8.1                    py35_0    conda-forge
sphinx-markdown-tables    0.0.9                     <pip>
sphinx_rtd_theme          0.4.2                      py_0    conda-forge
sphinxcontrib-websupport  1.1.0                      py_1    conda-forge
sqlite                    3.26.0               hb1c47c0_0    conda-forge
tblib                     1.3.2                      py_1    conda-forge
terminado                 0.8.1                    py35_1    conda-forge
testpath                  0.3.1                    py35_1    conda-forge
tk                        8.6.9                ha92aebf_0    conda-forge
toolz                     0.9.0                      py_1    conda-forge
tornado                   5.1.1            py35h470a237_0    conda-forge
traitlets                 4.3.2                    py35_0    conda-forge
urllib3                   1.23                     py35_1    conda-forge
wcwidth                   0.1.7                      py_1    conda-forge
webencodings              0.5.1                      py_1    conda-forge
wheel                     0.32.0                py35_1000    conda-forge
xz                        5.2.4                h470a237_1    conda-forge
yaml                      0.1.7                h470a237_1    conda-forge
zeromq                    4.2.5                hfc679d8_6    conda-forge
zict                      0.1.3                      py_0    conda-forge
zlib                      1.2.11               h470a237_3    conda-forge
```

</details>",2019-01-04T02:46:53Z,0,0,Matthew Rocklin,@coiled ,False
4,[FEA] Support escapechar in read_csv,"This corresponds to 'escapechar' parameter in pandas.  One-character string used to escape delimiter when quoting is QUOTE_NONE.
",2019-01-16T23:00:29Z,0,0,Thomas Meier,NVIDIA,True
5,[FEA] need official EWM function,"**Is your feature request related to a problem? Please describe.**
EWM is a very popular method used in time series analysis, especially in the domain of FSI. cuIndicator is using EWM a lot to compute the technical indicators. It is good to have official support in the cuDF. 

**Describe the solution you'd like**
DataFrame.ewm(com=None, span=None, halflife=None, alpha=None, min_periods=0, adjust=True, ignore_na=False, axis=0). The same interface as the Pandas [EWM function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ewm.html)

**Describe alternatives you've considered**
cuIndicator has the implementation that is based on rolling window methods. [cuIndicator EWM](https://yagr.nvidia.com/saefsi/cuindicator/blob/master/cuindicator/ewm.py). 

**Additional context**
EWM can be implemented by prefix-sum method if we weight the past carefully. I have the example implementation for it.


- [ ] mean
- [ ] standard deviation
- [ ] variance
- [ ] covariance
- [ ] correlation",2019-03-22T19:05:54Z,1,0,Yi Dong,Nvidia,True
6,[FEA] libcudf Series correlation (Pearson),"**Is your feature request related to a problem? Please describe.**
As a cuDF user, I want to calculate the correlation of two series. Pearson correlation is likely the most commonly used as it is the default in Pandas ([API docs](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.corr.html#pandas.Series.corr)).

**Describe the solution you'd like**
I'd like to be able to do this with `series1.corr(series2)` and also on DataFrame and Groupby objects.

**Describe alternatives you've considered**
The alternative is to actually calculate the correlation manually, which is cumbersome.

",2019-03-22T22:13:19Z,0,0,Nick Becker,@NVIDIA,True
7,[FEA] Explore performance impact of enabling relocatable device code in libcudf,"**Is your feature request related to a problem? Please describe.**

It would be nice to quantify the impact is of enabling [reloctable device code](https://devblogs.nvidia.com/separate-compilation-linking-cuda-device-code/) in libcudf.

**Describe the solution you'd like**

A thorough study of micro-benchmark performance should be conducted comparing performance of key libcudf/cuDF operations with and without relocatable device code enabled. 

The obvious candidate would be to start with the Air Speed Velocity benchmarks that have been created (but not yet released). 
",2019-04-16T19:26:47Z,0,0,Jake Hemstad,@NVIDIA,True
8,[FEA] Implement find_valid_index and its permutations (first_ and last_),"**Is your feature request related to a problem? Please describe.**
As a user, I'd like to be able to call `last_valid_index` or `first_valid_index` to get the index of the first/last non-null value in a Series.

**Describe the solution you'd like**
I'd like to be able to do the following with cudf:
```
import pandas as pd
import numpy as np

ser1 = [1,2,3,4]
ser2 = [1,2,3,None]
pdf = pd.DataFrame({'a':ser1, 'b':ser2})
print(pdf.b.last_valid_index())
2
```

**Additional context**
The pandas implementation is [here](https://github.com/pandas-dev/pandas/blob/v0.24.2/pandas/core/generic.py#L10195). This will help with API compatibility.
",2019-04-22T18:36:54Z,0,0,Nick Becker,@NVIDIA,True
9,[FEA] DatetimeIndex Frequency/TZ,"In Pandas, DatetimeIndexes can report back the frequency and timezone.  I think this would be useful/nice for cuDF but it's not a high priority at all.  

```
In [70]: pd.date_range(end='1/1/2018', periods=8).freq
Out[70]: <Day>
```

For more info on these extended dtypes: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#period-dtypes",2019-06-06T16:09:44Z,0,0,Benjamin Zaitlen,,False
10,[FEA] Add more functionality to rolling objects,"For our use cases, it would be helpful if rolling window objects have more functionality.

Basically, this would mean adding support for median, variance, standard deviation, quantile, and aggregate functions for rolling objects.

An example of support for aggregate() is below:
```
import cudf
df = cudf.DataFrame({'x': np.arange(10)})
print(df.x.rolling(2).aggregate({'x':'max'}))
```

**EDIT**
As of 7/11/2022, the following methods exist: min, mean, max, std, var, sum, count, apply. What remains is to implement:
- [ ] median (see also #6276)
- [ ] quantile
- [ ] aggregate",2019-06-27T17:30:18Z,0,0,Chinmay Chandak,NVIDIA,True
11,[FEA]Add merge_asof to cudf,Please add merge_asof to cudf to match pandas merge_asof capabilities. Thanks!,2019-07-12T17:04:59Z,0,0,,,False
12,[BUG] read_csv fails to correctly handle misplaced quotes ,"**Describe the bug**
Often csv files have misplaced quotes and sometime there is a quotation mark as a part of one of the string fields. This should not be interpreted as a quotation mark indicating that a field has delimiters in it and therefore uses `""`. 
**Steps/Code to reproduce bug**
```
import cudf
df = cudf.read_csv('quoting.csv')
df.to_pandas()
```
```
	a	b	c	d	e
0	1	192.16.1.2	/abc/def/ghi	200	1
1	2	nvidia.com	/abc/def/ghi.html	200	1
2	3	0.0.0.0	/images"",500,1\n4,0.0.0.0,/abc/def,200,2\n5,ra...	-1	-1
```
**Expected behavior**
```
import pandas as pd
df = pd..read_csv('quoting.csv')
df
```
```
	a	b	c	d	e
0	1	192.16.1.2	/abc/def/ghi	200	1
1	2	nvidia.com	/abc/def/ghi.html	200	1
2	3	0.0.0.0	/images""	500	1
3	4	0.0.0.0	/abc/def	200	2
4	5	rapids.ai	/images	200	2
```

Workaround: Use cudf.read_csv with `quoting=3`. Pandas gives correct output for all quotation modes. 

**Environment overview (please complete the following information)**
 - Environment location: Docker nightly
 - Method of cuDF install: Docker
   - `docker pull rapidsai/rapidsai-nightly:0.9-cuda10.0-runtime-ubuntu16.04-gcc5-py3.7`

**Additional context**
I might be wrong but maybe checking if opening quote exists just after delimiter and the second one before another delimiter might be the way? (Just a guess)

[quoting.csv.zip](https://github.com/rapidsai/cudf/files/3433203/quoting.csv.zip)

",2019-07-25T20:42:28Z,0,0,Ayush Dattagupta,Nvidia,True
13,[FEA] Throw exception on non utf-8 charsets during io operations,"**What is your question?**
Often when reading from files, the data is not utf-8 encoded. In such situations pandas throws a clear `utf-8 decoding error` unless the encoding is specified.

Cudf read_csv on the other hand reads the data without throwing any errors. I can run operations like groupby, etc on my Dataframe. But trying to print or convert to pandas throws the utf-8 errors. 

Would it make sense to throw an error if we encounter non utf-8 charsets (like iso-8859-1 encoding)?
Since cudf does not allow specifying encoding. Would it make more sense to let the behavior stay and allow users to do operations with df's and throw errors while print, converting to string, pandas etc.
Or possibly another option could raise some sort of warning mentioning that the data contains non-utf8 chars (if that's even possible) and let everything run as usual.",2019-07-26T19:49:11Z,0,0,Ayush Dattagupta,Nvidia,True
14,[FEA] Timezone conversions for timestamps,"**Is your feature request related to a problem? Please describe.**
Apache Spark internally tracks timestamps relative to the epoch in the JVM's default timezone.  cudf uses timestamps relative to the epoch in the UTC timezone (at least in the ORC loader).  When Spark tries to interpret timestamp data created/loaded by cudf the values are incorrect (from Spark's viewpoint) unless the JVM's default timezone is UTC.

**Describe the solution you'd like**
A timezone conversion utility function for timestamps in libcudf would help, as the timestamp column could be converted from UTC to the JVM's timezone.  The utility function would take an input timestamp column, a source timezone and a destination timezone, and would return a new timestamp column with the timestamp values adjusted by the time delta difference between the two timezones at the source timestamp's time.  Note that due to effects like daylight savings, the delta between timezones is not necessarily constant and therefore this utility method can't be obviated by a simple scalar addition on the timestamp column data.

**Describe alternatives you've considered**
Adding an optional, target timezone parameter to the various cudf data loaders (like the ORC reader) could work as well and potentially be more efficient if it avoids the separate CUDA kernel to do the conversion from UTC.  However the utility method approach seems like a more flexible approach to cover cases where the timestamp data source didn't originate from a cudf data loader.",2019-08-06T20:39:48Z,0,0,Jason Lowe,NVIDIA,True
15,Support callables in DataFrame.assign,"Value I am trying to compute is a range between two measure variables `v1, v2` within groups defined by `id2, id4` categories.
The following pandas/dask syntax could work
```py
ans = x.groupby(['id2','id4']).agg({'v1': 'max', 'v2': 'min'}).assign(range_v1_v2=lambda x: x['v1'] - x['v2'])[['range_v1_v2']]
#  File ""pyarrow/array.pxi"", line 536, in pyarrow.lib.Array.from_pandas
#  File ""pyarrow/array.pxi"", line 176, in pyarrow.lib.array
#  File ""pyarrow/array.pxi"", line 85, in pyarrow.lib._ndarray_to_array
#  File ""pyarrow/error.pxi"", line 81, in pyarrow.lib.check_status
#pyarrow.lib.ArrowInvalid: Only 1D arrays accepted
```
reproducible example
```py
import os
import gc
import cudf as cu
ver = cu.__version__
print(ver)
#0.8.0+0.g8fa7bd3.dirty
src_grp = ""G1_1e7_1e2_0_0.csv""
x = cu.read_csv(src_grp, skiprows=1,
                names=['id1','id2','id3','id4','id5','id6','v1','v2','v3'],
                dtype=['str','str','str','int','int','int','int','int','float'])
ans = x.groupby(['id2','id4']).agg({'v1': 'max', 'v2': 'min'}).assign(range_v1_v2=lambda x: x['v1'] - x['v2'])[['range_v1_v2']]
```
generate data according to https://github.com/rapidsai/cudf/issues/2494",2019-08-15T09:19:19Z,0,0,Jan Gorecki,,False
16,[FEA] Performance improvements for csv-writer,"I've been working on improvements to the csv-writer. The changes may require multiple PRs and are as follows:

1. The current implementation formats the CSV (in row chunks) into CPU memory before writing the file. Profiling should transposing the columns from device memory to host memory was taking more than half the total time to generate the file. Modifying the logic to create the format in device memory first and then copying to host before writing the file improved performance by 20-30%.
This item requires no change to the API.
2. When chunking the rows, writing the chunks to individual files did not provide performance improvement but generating multiple files may improve read speed. If this becomes an option, the code can launch separate CPU threads when writing each chunk from host memory. This provided a 2-3x speedup over creating a single output file.
This item would require a new parameter to tell the csv-writer to create individual files for each chunk.
3. After the first 2 are implemented, it would be possible to support gzip compression of the file chunks without too significant of a performance penalty. Adding gzip without these measures increased the write time 3-4x.
This item would also require a new parameter indicating that compression is desired.

Recommend adding these improvements in order since each subsequent item gets its advantage from the previous.
The first item also makes GDS an option for speeding up the actual file write since copying the data to the host would not be required.


",2019-08-23T21:08:09Z,0,0,David Wendt,NVIDIA,True
17,[FEA] Backstick support in DataFrame.query() method,"**Is your feature request related to a problem? Please describe.**
Hi,

While trying to use the `DataFrame.query()` with columns whose name is a string with blank spaces, I have noticed that I didn't know how to do that with RAPIDS nor Pandas.

Example below:
```
df = pd.DataFrame({'A': range(1, 6), 'B': range(10, 0, -2), 'C C': range(10, 5, -1)})
df.query('B == `C C`')
```

I have gone to Pandas doc, and I have found they support the use of backstick from version 0.25.1. Backstick has been deprecated in Python 3, but it has been introduced in Pandas to accomplish the scenario above.

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html

**Describe the solution you'd like**
I'd like to have 'backstick' support in .query method in cudf.

**Describe alternatives you've considered**
As a workaround:
`df[df.B == df['C C']]`


**Additional context**
Not a stopper at all. I have just thought it might be handy to have backstick support in RAPIDS cuDF.",2019-10-08T07:38:16Z,0,0,Miguel Martínez,NVIDIA,True
18,[FEA] CUDF_VERSION in cudf.h ?,"**Is your feature request related to a problem? Please describe.**
Changes to cudf functionality may break existing code that relies on cudf. The lack of versioning makes it difficult to have a code base that could support multiple cudf versions. 

**Describe the solution you'd like**
Seems like a CUDF_VERSION, perhaps along with CUDF_VERSION_{MAJOR|MINOR} definitions would be useful. Would also be useful for cudf-io writers for putting cudf version in parquet/orc files.

",2019-10-18T19:32:51Z,1,0,,NVIDIA,True
19,[FEA] String timestamp parsing to match Spark casting string to timestamp,"Unlike timestamp2long, there is no timestamp format string used when casting string to timestamp in spark. Instead, the cast uses an incremental parsing approach that allows for a large set of parsable/accepted formats including single digit month, day, hour, minute, and second values.

Here is the full list of supported timestamp formats https://github.com/apache/spark/blob/a4382f7fe1c36a51c64f460c6cb91e93470e0825/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala#L192
```
   * `yyyy`
   * `yyyy-[m]m`
   * `yyyy-[m]m-[d]d`
   * `yyyy-[m]m-[d]d `
   * `yyyy-[m]m-[d]d [h]h:[m]m:[s]s.[ms][ms][ms][us][us][us]`
   * `yyyy-[m]m-[d]d [h]h:[m]m:[s]s.[ms][ms][ms][us][us][us]Z`
   * `yyyy-[m]m-[d]d [h]h:[m]m:[s]s.[ms][ms][ms][us][us][us]-[h]h:[m]m`
   * `yyyy-[m]m-[d]d [h]h:[m]m:[s]s.[ms][ms][ms][us][us][us]+[h]h:[m]m`
   * `yyyy-[m]m-[d]dT[h]h:[m]m:[s]s.[ms][ms][ms][us][us][us]`
   * `yyyy-[m]m-[d]dT[h]h:[m]m:[s]s.[ms][ms][ms][us][us][us]Z`
   * `yyyy-[m]m-[d]dT[h]h:[m]m:[s]s.[ms][ms][ms][us][us][us]-[h]h:[m]m`
   * `yyyy-[m]m-[d]dT[h]h:[m]m:[s]s.[ms][ms][ms][us][us][us]+[h]h:[m]m`
   * `[h]h:[m]m:[s]s.[ms][ms][ms][us][us][us]`
   * `[h]h:[m]m:[s]s.[ms][ms][ms][us][us][us]Z`
   * `[h]h:[m]m:[s]s.[ms][ms][ms][us][us][us]-[h]h:[m]m`
   * `[h]h:[m]m:[s]s.[ms][ms][ms][us][us][us]+[h]h:[m]m`
   * `T[h]h:[m]m:[s]s.[ms][ms][ms][us][us][us]`
   * `T[h]h:[m]m:[s]s.[ms][ms][ms][us][us][us]Z`
   * `T[h]h:[m]m:[s]s.[ms][ms][ms][us][us][us]-[h]h:[m]m`
   * `T[h]h:[m]m:[s]s.[ms][ms][ms][us][us][us]+[h]h:[m]m`
```

A custring function that supports this conversion and returns `null` when a given string is not parsable. We've tried using the timestamp2long function, but the lack of generic format support makes it impossible to match the spark cast functionality in columns with heterogeneous formats.",2019-11-07T19:30:21Z,0,0,Ryan Lee,NVIDIA,True
20,[FEA] Series and DataFrame mean absolute deviation,"For API compatibility and supporting exploratory analysis, we should support Series and DataFrame mean absolute deviation. See the pandas [mean absolute deviation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.mad.html) for more information.

This can be implemented for Python Series and DataFrame as a stopgap as `Series.mad` and then leverage `_apply_support_method` for `DataFrame.mad`. It's not 10x faster, but it gets the job done well.

```python
import cudf
import numpy as np
​
def mad(self):
    # mad formula
    n = len(self)
    m = self.mean()
    mad = ((self - m).abs() / n).sum()
    return mad
​
​# 1 million rows
s = cudf.Series(np.random.normal(10,5,1_000_000))
ps = s.to_pandas()
​
%time mp = ps.mad()
%time mg = mad(s)
print(mp)
print(mg)
CPU times: user 31.9 ms, sys: 0 ns, total: 31.9 ms
Wall time: 32 ms
CPU times: user 0 ns, sys: 7.24 ms, total: 7.24 ms
Wall time: 23.2 ms
3.990811998439671
3.990811998439673
```",2019-12-30T15:20:14Z,0,0,Nick Becker,@NVIDIA,True
21,[BUG]Cannot query dataframes with categorical columns ,"**Describe the bug**
if i do a simple query on a categorical column, i get an error stating that  `This error is usually caused by passing an argument of a type that is unsupported by the named function.`

**Steps/Code to reproduce bug**
```
import cudf
import pandas as pd

fn = 'test.csv'
lines = """"""id1,id2
1,45
2,3
3, 7
1, 25
""""""
with open(fn, 'w') as fp:
    fp.write(lines)
pdf = pd.read_csv(fn, header=0, dtype={""id1"":""category"", ""id2"":""int32""})
cdf = cudf.read_csv(fn, header=0, dtype={""id1"":""int32"", ""id2"":""int32""}) #see #3960 for why i have to do this
cdf['id1'] = cdf['id1'].astype(""category"")
pdf.query(""id1 == ['1'] and id2 == 45"")
cdf.query(""id1 == ['1'] and id2 == 45"")
```
The cdf query outputs a rather large error
```
---------------------------------------------------------------------------
TypingError                               Traceback (most recent call last)
<ipython-input-27-28a794912e6e> in <module>
----> 1 cdf2.query(""id1 == ['1'] and id2 == 45"")

/opt/conda/envs/rapids/lib/python3.7/site-packages/cudf/core/dataframe.py in query(self, expr, local_dict)
   2893         }
   2894         # Run query
-> 2895         boolmask = queryutils.query_execute(self, expr, callenv)
   2896 
   2897         selected = Series(boolmask)

/opt/conda/envs/rapids/lib/python3.7/site-packages/cudf/utils/queryutils.py in query_execute(df, expr, callenv)
    223     # run kernel
    224     args = [out] + colarrays + envargs
--> 225     kernel.forall(nrows)(*args)
    226     out_mask = applyutils.make_aggregate_nullmask(df, columns=columns)
    227     if out_mask is not None:

/opt/conda/envs/rapids/lib/python3.7/site-packages/numba/cuda/compiler.py in __call__(self, *args)
    264     def __call__(self, *args):
    265         if isinstance(self.kernel, AutoJitCUDAKernel):
--> 266             kernel = self.kernel.specialize(*args)
    267         else:
    268             kernel = self.kernel

/opt/conda/envs/rapids/lib/python3.7/site-packages/numba/cuda/compiler.py in specialize(self, *args)
    808         argtypes = tuple(
    809             [self.typingctx.resolve_argument_type(a) for a in args])
--> 810         kernel = self.compile(argtypes)
    811         return kernel
    812 

/opt/conda/envs/rapids/lib/python3.7/site-packages/numba/cuda/compiler.py in compile(self, sig)
    824                 self.targetoptions['link'] = ()
    825             kernel = compile_kernel(self.py_func, argtypes,
--> 826                                     **self.targetoptions)
    827             self.definitions[(cc, argtypes)] = kernel
    828             if self.bind:

/opt/conda/envs/rapids/lib/python3.7/site-packages/numba/compiler_lock.py in _acquire_compile_lock(*args, **kwargs)
     30         def _acquire_compile_lock(*args, **kwargs):
     31             with self:
---> 32                 return func(*args, **kwargs)
     33         return _acquire_compile_lock
     34 

/opt/conda/envs/rapids/lib/python3.7/site-packages/numba/cuda/compiler.py in compile_kernel(pyfunc, args, link, debug, inline, fastmath, extensions, max_registers)
     60 def compile_kernel(pyfunc, args, link, debug=False, inline=False,
     61                    fastmath=False, extensions=[], max_registers=None):
---> 62     cres = compile_cuda(pyfunc, types.void, args, debug=debug, inline=inline)
     63     fname = cres.fndesc.llvm_func_name
     64     lib, kernel = cres.target_context.prepare_cuda_kernel(cres.library, fname,

/opt/conda/envs/rapids/lib/python3.7/site-packages/numba/compiler_lock.py in _acquire_compile_lock(*args, **kwargs)
     30         def _acquire_compile_lock(*args, **kwargs):
     31             with self:
---> 32                 return func(*args, **kwargs)
     33         return _acquire_compile_lock
     34 

/opt/conda/envs/rapids/lib/python3.7/site-packages/numba/cuda/compiler.py in compile_cuda(pyfunc, return_type, args, debug, inline)
     49                                   return_type=return_type,
     50                                   flags=flags,
---> 51                                   locals={})
     52 
     53     library = cres.library

/opt/conda/envs/rapids/lib/python3.7/site-packages/numba/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class)
    526     pipeline = pipeline_class(typingctx, targetctx, library,
    527                               args, return_type, flags, locals)
--> 528     return pipeline.compile_extra(func)
    529 
    530 

/opt/conda/envs/rapids/lib/python3.7/site-packages/numba/compiler.py in compile_extra(self, func)
    324         self.state.lifted = ()
    325         self.state.lifted_from = None
--> 326         return self._compile_bytecode()
    327 
    328     def compile_ir(self, func_ir, lifted=(), lifted_from=None):

/opt/conda/envs/rapids/lib/python3.7/site-packages/numba/compiler.py in _compile_bytecode(self)
    383         """"""
    384         assert self.state.func_ir is None
--> 385         return self._compile_core()
    386 
    387     def _compile_ir(self):

/opt/conda/envs/rapids/lib/python3.7/site-packages/numba/compiler.py in _compile_core(self)
    363                 self.state.status.fail_reason = e
    364                 if is_final_pipeline:
--> 365                     raise e
    366         else:
    367             raise CompilerError(""All available pipelines exhausted"")

/opt/conda/envs/rapids/lib/python3.7/site-packages/numba/compiler.py in _compile_core(self)
    354             res = None
    355             try:
--> 356                 pm.run(self.state)
    357                 if self.state.cr is not None:
    358                     break

/opt/conda/envs/rapids/lib/python3.7/site-packages/numba/compiler_machinery.py in run(self, state)
    326                     (self.pipeline_name, pass_desc)
    327                 patched_exception = self._patch_error(msg, e)
--> 328                 raise patched_exception
    329 
    330     def dependency_analysis(self):

/opt/conda/envs/rapids/lib/python3.7/site-packages/numba/compiler_machinery.py in run(self, state)
    317                 pass_inst = _pass_registry.get(pss).pass_inst
    318                 if isinstance(pass_inst, CompilerPass):
--> 319                     self._runPass(idx, pass_inst, state)
    320                 else:
    321                     raise BaseException(""Legacy pass in use"")

/opt/conda/envs/rapids/lib/python3.7/site-packages/numba/compiler_lock.py in _acquire_compile_lock(*args, **kwargs)
     30         def _acquire_compile_lock(*args, **kwargs):
     31             with self:
---> 32                 return func(*args, **kwargs)
     33         return _acquire_compile_lock
     34 

/opt/conda/envs/rapids/lib/python3.7/site-packages/numba/compiler_machinery.py in _runPass(self, index, pss, internal_state)
    279             mutated |= check(pss.run_initialization, internal_state)
    280         with SimpleTimer() as pass_time:
--> 281             mutated |= check(pss.run_pass, internal_state)
    282         with SimpleTimer() as finalize_time:
    283             mutated |= check(pss.run_finalizer, internal_state)

/opt/conda/envs/rapids/lib/python3.7/site-packages/numba/compiler_machinery.py in check(func, compiler_state)
    266 
    267         def check(func, compiler_state):
--> 268             mangled = func(compiler_state)
    269             if mangled not in (True, False):
    270                 msg = (""CompilerPass implementations should return True/False. ""

/opt/conda/envs/rapids/lib/python3.7/site-packages/numba/typed_passes.py in run_pass(self, state)
     92                 state.args,
     93                 state.return_type,
---> 94                 state.locals)
     95             state.typemap = typemap
     96             state.return_type = return_type

/opt/conda/envs/rapids/lib/python3.7/site-packages/numba/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals)
     64 
     65         infer.build_constraint()
---> 66         infer.propagate()
     67         typemap, restype, calltypes = infer.unify()
     68 

/opt/conda/envs/rapids/lib/python3.7/site-packages/numba/typeinfer.py in propagate(self, raise_errors)
    949                                   if isinstance(e, ForceLiteralArg)]
    950                 if not force_lit_args:
--> 951                     raise errors[0]
    952                 else:
    953                     raise reduce(operator.or_, force_lit_args)

TypingError: Failed in nopython mode pipeline (step: nopython frontend)
Invalid use of Function(<numba.cuda.compiler.DeviceFunctionTemplate object at 0x7f914147a320>) with argument(s) of type(s): (int32, int32)
 * parameterized
In definition 0:
    TypingError: Failed in nopython mode pipeline (step: nopython frontend)
Invalid use of Function(<built-in function eq>) with argument(s) of type(s): (int32, list(unicode_type))
Known signatures:
 * (bool, bool) -> bool
 * (int8, int8) -> bool
 * (int16, int16) -> bool
 * (int32, int32) -> bool
 * (int64, int64) -> bool
 * (uint8, uint8) -> bool
 * (uint16, uint16) -> bool
 * (uint32, uint32) -> bool
 * (uint64, uint64) -> bool
 * (float32, float32) -> bool
 * (float64, float64) -> bool
 * (complex64, complex64) -> bool
 * (complex128, complex128) -> bool
 * parameterized
In definition 0:
    All templates rejected with literals.
In definition 1:
    All templates rejected without literals.
In definition 2:
    All templates rejected with literals.
In definition 3:
    All templates rejected without literals.
In definition 4:
    All templates rejected with literals.
In definition 5:
    All templates rejected without literals.
In definition 6:
    All templates rejected with literals.
In definition 7:
    All templates rejected without literals.
In definition 8:
    All templates rejected with literals.
In definition 9:
    All templates rejected without literals.
In definition 10:
    All templates rejected with literals.
In definition 11:
    All templates rejected without literals.
In definition 12:
    All templates rejected with literals.
In definition 13:
    All templates rejected without literals.
In definition 14:
    All templates rejected with literals.
In definition 15:
    All templates rejected without literals.
In definition 16:
    All templates rejected with literals.
In definition 17:
    All templates rejected without literals.
In definition 18:
    All templates rejected with literals.
In definition 19:
    All templates rejected without literals.
This error is usually caused by passing an argument of a type that is unsupported by the named function.
[1] During: typing of intrinsic-call at <string> (2)

File ""<string>"", line 2:
<source missing, REPL/exec in use?>

    raised from /opt/conda/envs/rapids/lib/python3.7/site-packages/numba/typeinfer.py:951
In definition 1:
    TypingError: Failed in nopython mode pipeline (step: nopython frontend)
Invalid use of Function(<built-in function eq>) with argument(s) of type(s): (int32, list(unicode_type))
Known signatures:
 * (bool, bool) -> bool
 * (int8, int8) -> bool
 * (int16, int16) -> bool
 * (int32, int32) -> bool
 * (int64, int64) -> bool
 * (uint8, uint8) -> bool
 * (uint16, uint16) -> bool
 * (uint32, uint32) -> bool
 * (uint64, uint64) -> bool
 * (float32, float32) -> bool
 * (float64, float64) -> bool
 * (complex64, complex64) -> bool
 * (complex128, complex128) -> bool
 * parameterized
In definition 0:
    All templates rejected with literals.
In definition 1:
    All templates rejected without literals.
In definition 2:
    All templates rejected with literals.
In definition 3:
    All templates rejected without literals.
In definition 4:
    All templates rejected with literals.
In definition 5:
    All templates rejected without literals.
In definition 6:
    All templates rejected with literals.
In definition 7:
    All templates rejected without literals.
In definition 8:
    All templates rejected with literals.
In definition 9:
    All templates rejected without literals.
In definition 10:
    All templates rejected with literals.
In definition 11:
    All templates rejected without literals.
In definition 12:
    All templates rejected with literals.
In definition 13:
    All templates rejected without literals.
In definition 14:
    All templates rejected with literals.
In definition 15:
    All templates rejected without literals.
In definition 16:
    All templates rejected with literals.
In definition 17:
    All templates rejected without literals.
In definition 18:
    All templates rejected with literals.
In definition 19:
    All templates rejected without literals.
This error is usually caused by passing an argument of a type that is unsupported by the named function.
[1] During: typing of intrinsic-call at <string> (2)

File ""<string>"", line 2:
<source missing, REPL/exec in use?>

    raised from /opt/conda/envs/rapids/lib/python3.7/site-packages/numba/typeinfer.py:951
This error is usually caused by passing an argument of a type that is unsupported by the named function.
[1] During: resolving callee type: Function(<numba.cuda.compiler.DeviceFunctionTemplate object at 0x7f914147a320>)
[2] During: typing of call at <string> (6)


File ""<string>"", line 6:
<source missing, REPL/exec in use?>
```
**Expected behavior**
I expect it to output similar to the `pdf.query`, `pdf.query(""id1 == ['1'] and id2 == 45"")`

id1 | id2
-- | --
1 | 45

**Environment overview (please complete the following information)**
 - Environment location: [Docker]
 - Method of cuDF install: [Docker]

**Additional context**
Converting from cudf to pandas to do the query also inexplicitly fails
```
tdf = cdf.to_pandas()
tdf['id1']
```
will output correctly with 
```
0    1
1    2
2    3
3    1
Name: id1, dtype: category
Categories (3, int64): [1, 2, 3]
```
but when you run the query...
```
tdf.query(""id1 == ['1'] and id2 == 45"")
```
Outputs an empty table 

  | id1 | id2
-- | -- | --






",2020-01-28T11:55:24Z,0,0,Taurean Dyer,,False
22,[FEA] Support nanValue Spark CSV parse option in cudf CSV reader,"_Description of the request:_ 
Apache Spark CSV reader options include specifying values that should be interpreted as NaNs via `nanValue`.
Refer to  [https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/DataFrameReader.html](url) 
cuDF CSV reader seems to lack an equivalent

_Description of a possible solution_:
It would helpful to have a translation/support for that option in the cudf CSV reader options. (analogous to its na_values)",2020-01-29T22:12:37Z,0,0,Kuhu Shukla,,False
23,[BUG] expand_bits_to_bytes could be improved by returning ndarray of np.bool,"**Describe the bug**
`expand_bits_to_bytes`, defined below, is a utility function for unit tests which unpacks a bitmask into a list of valids per element. As currently defined, this returns a `list` of `int` where 1 is valid and 0 is invalid. However, in every case where this is used, the caller converts the result to `ndarray` of `bool`. The input to the function is expected to be an `ndarray` anyway, so there could be some numpy transform which gives us the result as `ndarray` directly.
https://github.com/rapidsai/cudf/blob/a831f11bdeb2cb8dfcc2f47ef69b596a2334c3d2/python/cudf/cudf/tests/utils.py#L36-L43

**Expected behavior**
Refactor `expand_bits_to_bytes` to return `ndarray` of `bool`. [unpackbits](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.unpackbits.html) looks like it would be useful here.",2020-02-20T03:05:49Z,0,0,Trevor Smith,,False
24,[FEA] Make `column_view` _actually_ be a view,"**Is your feature request related to a problem? Please describe.**

`column_view` is so named because it is supposed to be a non-owning, ""view"" type. However, this is currently a half-truth because `column_view` _is_ an owning class! 

`column_view` stores a `vector<column_view>` internally for it's children. Since `vector` is an owning type, that makes `column_view` an owning type. 

This has several ramifications. 

First of all `column_view` is not trivially copyable (which view types should be!).

The larger problem is more insidious. The fact that `column_view` contains a `vector` object is the ultimate cause of the many compiler errors that libcudf developers have seen about calling a `__host__` function from a `__host__ __device__` context. The full causal chain is complicated. 

As @harrism described in https://github.com/rapidsai/rmm/pull/312:

>Unfortunately, NVCC implicitly adds __host__ __device__ specifiers to explicitly defaulted functions that are called from both __host__ and __device__ (or __host__ __device__) functions. In libcudf, the type_dispatcher uses a __host__ __device__ function, so the above changed resulted in compiler errors since the default compiler-generated constructor necessarily invokes a host-only function. 

In short, `column_view` has defaulted ctors. Since these ctors can be used within the context of the `type_dispatcher` (which is `__host__ __device__`), nvcc will implicitly add `__host__ __device__` to these ctors. These defaulted `column_view` ctors will invoke the member `vector` ctors (which are intrinsically `__host__` only). Thus, we end up with trying to call a `__host__` function (`vector` ctor/dtor) inside a `__host__ __device__` function (`column_view` ctor/dtor).

**Describe the solution you'd like**

Remove the `vector<column_view>` from `column_view` and instead replace it with `column_view* children` and `size_type num_children`:

```c++
class column_view{

   column_view(...., column_view * children, size_type num_children);

private:
   column_view * children; // pointer to array of child `column_view` objects
   size_type num_children;
};
```

This will make `column_view` a true ""view"" type by being trivially copyable and eliminate any possibility for the host/device errors we've encountered numerous times in the past. 

But what about the children!? 

The catch is, someone has to own the children `column_view` objects, i.e., the `children` pointer in `column_view` has to point to `column_view` objects that are constructed/owned by someone else. There's two scenarios we have to consider:

1. `column_view`s that view `cudf::column` objects
2. `column_view`s that _do not_ view ` cudf::column` object (e.g., viewing a Python owned column)

In 1. the fix is pretty easy. We can just add a `vector<column_view>` to `cudf::column`:

```c++
class column{

private:
   vector<unique_ptr<column>> children; // here's the owning child objects
   vector<column_view> child_views; // non-owning views to the same children

public:
   operator column_view(){
       // pass pointer to the `child_view`s data when converting to a `column_view`
       return column_view{..., child_views.data(), child_views.size()};
   }
};
```

Situation 2. shouldn't be too onerous either. The caller was already required to construct a `std::vector<column_view>` to pass the children to the `column_view` ctor. However, instead of copying that vector, we just take a pointer to it's contents. This does add the added responsibility of the user to ensure that the `column_view` does not outlive the `vector<column_view>` for the children. @shwina correct me if I'm wrong, but I don't anticipate this should be a problem from the Python/Cython side nor require significant changes? 

",2020-02-27T18:10:01Z,0,0,Jake Hemstad,@NVIDIA,True
25,[BUG] dask_cudf.DataFrame.values shows chunktype of numpy ndarray,"**Describe the bug**
`dask_cudf.DataFrame.values` shows `chunktype` of `numpy.ndarray` but it is actually `cupy.ndarray`.

**Steps/Code to reproduce bug**
```
import cudf
import dask_cudf
df = cudf.DataFrame({'a': list(range(20)),
'b': list(reversed(range(20))),
'c': list(range(20))})
ddf = dask_cudf.from_cudf(df, npartitions=2)
print(ddf.values)

dask.array<values, shape=(nan, 3), dtype=int64, chunksize=(nan, 3), chunktype=numpy.ndarray>
```
**Expected behavior**
The chunktype should be `cupy.ndarray`

**Environment overview (please complete the following information)**
 - Environment location: Bare-metal
 - Method of cuDF install: conda

**Environment details**
cudf `0.12.0`. cuda 10.0. python 3.7

",2020-03-03T00:25:58Z,0,0,Jiwei Liu,Nvidia,True
26,[FEA] Support additional pandas features in strings wrap implementation,"**Describe the bug**
The current libcudf implementation & old nvstrings implementation of wrap are not in-line with that of pandas `.str.wrap`: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.wrap.html#pandas.Series.str.wrap
Our implementation seems to close to R's stringr library str_wrap function and to achieve that in Pandas we need to fix to a specific parameter setting like as follows:
```
expand_tabs = False

replace_whitespace = True

drop_whitespace = True

break_long_words = False

break_on_hyphens = False
```

There is inconsistency even if we compare with above setting of pandas to our present implementation, we seem to be maintaining tab spaces. Simple code example below.

**Steps/Code to reproduce bug**
```python
data = [' ', '\t\r\n ', ''], width = 100

    def test_string_wrap(data, width):
        gs = cudf.Series(data)
        ps = pd.Series(data)
    
>       assert_eq(gs.str.wrap(width=width), ps.str.wrap(width=width, break_long_words=False, expand_tabs=False, replace_whitespace=True, drop_whitespace=True, break_on_hyphens=False))

python/cudf/cudf/tests/test_string.py:1145: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
pandas/_libs/testing.pyx:65: in pandas._libs.testing.assert_almost_equal
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   AssertionError: Series are different
E   
E   Series values are different (66.66667 %)
E   [left]:  [ ,     , ]
E   [right]: [, , ]

```

This behavior seems to be for any value of `width`, not just 100.


If we ignore the specific parameter setting mentioned above, out of the box this is the result inconsistency between cudf and pandas implementation:
```python


data = ['line to be wrapped', 'another line to be wrapped'], width = 1

    def test_string_wrap(data, width):
        gs = cudf.Series(data)
        ps = pd.Series(data)
    
>     assert_eq(gs.str.wrap(width=width),ps.str.wrap(width=width))

python/cudf/cudf/tests/test_string.py:1148: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
pandas/_libs/testing.pyx:65: in pandas._libs.testing.assert_almost_equal
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   AssertionError: Series are different
E   
E   Series values are different (100.0 %)
E   [left]:  [line
E   to
E   be
E   wrapped, another
E   line
E   to
E   be
E   wrapped]
E   [right]: [l
E   i
E   n
E   e
E   t
E   o
E   b
E   e
E   w
E   r
E   a
E   p
E   p
E   e
E   d, a
E   n
E   o
E   t
E   h
E   e
E   r
E   l
E   i
E   n
E   e
E   t
E   o
E   b
E   e
E   w
E   r
E   a
E   p
E   p
E   e
E   d]

pandas/_libs/testing.pyx:178: AssertionError
```


**Expected behavior**
1. The values returned should be similar to that of the above.
2. I'd also like to make this as a feature request to support the currently un-supported parameters:
```
expand_tabs
replace_whitespace
drop_whitespace
break_long_words
break_on_hyphens
```

**Environment overview (please complete the following information)**
 - Environment location: Docker
 - Method of cuDF install: from source


**Environment details**
Output of the `cudf/print_env.sh` script here:
[env.txt](https://github.com/rapidsai/cudf/files/4295897/env.txt)

**Additional context**
Surfaced when testing this PR: #4339 
",2020-03-06T00:56:16Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
27,[BUG] `get_scalar_device_view` doesn't support `const scalar` ,"**Describe the bug**

In order to use a scalar in device code, you have to use [`get_scalar_device_view`](https://github.com/rapidsai/cudf/blob/1ead2d55562b9cd1b80568b86b8b51ab616f3cb8/cpp/include/cudf/scalar/scalar_device_view.cuh#L194). The problem is, `get_scalar_device_view` takes a `scalar` by non-const ref, which means if your scalar is a `const`, then you have to do a `const_cast` to cast away the constness, which invokes undefined behavior. This is bad. 

**Expected behavior**

I shouldn't have to `const_cast` to use `get_scalar_device_view` with a const scalar. 


",2020-03-06T20:21:37Z,0,0,Jake Hemstad,@NVIDIA,True
28,[FEA] predicate push down such as bloom filters added to read_orc,"having predicate pushdowns such as bloom filters work as in hive to skip over sections of data on disk during the read would enable better overall performance as the data would never have to be loaded to memory and then discarded if the predicate didn't match in the row group.

support predicate pushdown similar to what is done with hive in read_orc, primary support for bloom filter but also for sorted data as well to enable only loading relevant data to query.


",2020-03-10T22:22:06Z,0,0,,Walmart,False
29,"[FEA] Async, especially for heavier ops","**Is your feature request related to a problem? Please describe.**

We are currently putting manual `await sleep(0)` points into our cudf code to enable use of cudf alongside Python async code in web server scenarios. Otherwise, cudf hangs our web server when it is used for handling requests. This gets worse as tasks get bigger, complicates having mixed CPU/GPU tasks, and unnecessarily forces architectural decisions like carefully separating processes and 2-level scheduling. 

**Describe the solution you'd like**

Support for Python3's async/await constructor. Part of the Python 2 -> 3 shift is native support for `async`, especially for IO (e.g., when handling web requests) and compute (e.g., compression tasks).  

Ex:

```
@route(/cluster/big/dataset/<datasetid>, method=POST)
async def cluster_big_dataset(dataset_id):
    
    df = await cudf.read_parquet(f'/files/{dataset_id}.parquet')
    ...
```

This would be great universally, but there's probably a ~top 10 list for most slow in practice: to/from I/O, groupby & merge, ... .

**Describe alternatives you've considered**

* Going via Dask also supports this, but with way more overhead and complexity. 

* We currently use multiple Python processes to help with SLAs, but it's clearly avoidable.",2020-03-16T21:44:13Z,0,0,,Graphistry,False
30,[BUG] split/slice APIs do not align with partitioning APIs,"**Describe the bug**

Partitioning APIs that partition a table into `n` partitions, like [`hash_partition`](https://github.com/rapidsai/cudf/blob/branch-0.14/cpp/include/cudf/partitioning.hpp#L43) or [`round_robin_partition`](https://github.com/rapidsai/cudf/blob/branch-0.14/cpp/include/cudf/partitioning.hpp#L185), return a single table and a vector of `n+1` offsets that points to the beginning of each partition and where the size of any partition `i` can be determined by `offsets[i+1] - offsets[i]`.

For example:
```
partitioned_table = {7}, {}, {3, 8, 9}, {42};
offsets = [0, 1, 1, 4, 5]
```


I would expect to be able to trivially pass the output of a partitioning API into an API like [`split`](https://github.com/rapidsai/cudf/blob/branch-0.13/cpp/include/cudf/copying.hpp#L389) or [`slice`](https://github.com/rapidsai/cudf/blob/branch-0.13/cpp/include/cudf/copying.hpp#L358) in order to get a vector of zero-copy `table_view`s for each partition. 

However, this is not possible because the expected inputs for `split` or `slice` are incompatible with the `offsets` vector returned from a partitioning API.

`slice` expects a vector of index pairs:
```
 input:   [{10, 12, 14, 16, 18, 20, 22, 24, 26, 28},
           {50, 52, 54, 56, 58, 60, 62, 64, 66, 68}]
 indices: {1, 3, 5, 9, 2, 4, 8, 8}
 output:  [{{12, 14}, {20, 22, 24, 26}, {14, 16}, {}},
           {{52, 54}, {60, 22, 24, 26}, {14, 16}, {}}]
```

`split` expects a vector of the split points:
```
 input:   {10, 12, 14, 16, 18, 20, 22, 24, 26, 28}
 splits:  {2, 5, 9}
 output:  {{10, 12}, {14, 16, 18}, {20, 22, 24, 26}, {28}}
```

Neither of these are trivially compatible with the output of a partitioning API.

`split` is the closest. You can obtain the `splits` vector from the `offsets` vector by dropping the first and last element from `offsets`. However, that is inconvenient. 


**Expected behavior**

There should be an API that allows naively passing in the vector of offsets from a partitioning API and it returns a vector of zero-copy views for each partition. 


",2020-03-19T17:40:23Z,0,0,Jake Hemstad,@NVIDIA,True
31,[FEA] dask-cudf groupby with quantile and median methods,"**Is your feature request related to a problem? Please describe.**
I'd like to calculate median and/or quantile on a column after groupbying a dask-cudf data frame. 

**EDIT** 5/10/2024: median is now implemented

**Describe the solution you'd like**
I want the following code to work and generate correct results:

```
cdf =cudf.DataFrame({'id4': 4*list(range(6)), 'id5': 4*list(reversed(range(6))), 'v3': 6*list(range(4))})
ddf = dcu.from_cudf(cdf, npartitions= 1)

ddf.dtypes
id4    int64
id5    int64
v3     int64
dtype: object

ddf.head()

        id4   id5     v3

0	0	5	0
1	1	4	1
2	2	3	2
3	3	2	3
4	4	1	0

#these groupby operations do not work
ans = ddf.groupby(['id4', 'id5'])[['v3']].median().compute()

OR 

ans = ddf.groupby(['id4', 'id5'])[['v3']].quantile(q=0.5).compute()
```

**Additional context**
I am using Rapids 0.13 nightly release in conda env, with dask 2.12.0 version.
",2020-03-26T16:49:09Z,0,0,,,False
32,[FEA] Groupby MIN/MAX with NaN values does not match what Spark expects,"**Describe the bug**
Running min aggregate on a table returns the NaN value as its long value instead of the literal ""nan"" as it does for the other aggregates. I haven't gotten around to writing a unit test for this but can do if so required 

**Steps/Code to reproduce bug**
Create the following table 
```
scala> spark.sql(""""select * from floatsAndDoubles"""").show
+-----+------+
|float|double|
+-----+------+
|  NaN|   NaN|
| 1.02|   NaN|
|  NaN|   4.5|
+-----+------+
```

running an aggregate(min) op on the double column will result in the following table 

```
+----------+-----------+
| float    |min(double)|
+----------+-----------+
| 1.020000 | 179769313486231570000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000.000000 |
| NaN      | 4.500000  |
+----------+-----------+
```

**Expected behavior**
It should output this 

```
scala> spark.sql(""""select float, min(double) from floatsAndDoubles group by float"""").show
+-----+-----------+
|float|min(double)|
+-----+-----------+
| 1.02|        NaN|
|  NaN|        4.5|
+-----+-----------+
```

**Additional context**
For context here is what aggregate(sum) does in cudf
```
+------+-----+
|float | sum |
+------+-----+
| 1.02 | NaN |
| NaN  | NaN |
+------+-----+
```
",2020-03-31T05:12:45Z,0,0,Raza Jafri,NVIDIA,True
33,[FEA] Add support to low_memory parameter in read_csv method ,"**Is your feature request related to a problem? Please describe.**
Hi,

It would be great if you could please evaluate the addition of the `low_memory` parameter in cuDF's `read_csv` method.

This parameter defaults to `True` in Pandas' `read_csv` method, and it indicates Pandas to read the CSV file in chunks, allowing to load a large CSV file on memory-constrained systems.

Current cuDF's `read_csv` implementation first reads the whole file into GPU memory, and then creates the DataFrame. That has the benefit of a very fast DataFrame creation, but it also has the cons that it limits the size of the biggest dataset it can be created from a file.

In my tests, I was not able to create a DataFrame bigger than 7 GBs in a 16GBs v100 card.

I think that, by reading the CSV file in chunks, the max size of the DataFrame that I could create from a CSV file would be much bigger, and I also think that the potential (if any) performance penalty might be well-worth.

**Describe the solution you'd like**
To be able to load bigger datasets when reading CSV files with cuDF.

**Describe alternatives you've considered**
I have manually read the file in chunks, using the byte_range parameter, and then concatenating the different dataframes. I was able to load a DF up to 10GBs without a significate performance penalty. Also, my solution implies concat to dataframes, which would not be needed in a native libcudf implementation.

**Additional context**
Discussed internally with @kkraus14 .",2020-04-23T11:48:52Z,0,0,Miguel Martínez,NVIDIA,True
34,[FEA] read_csv -> skiprows parameter: support list-like and callable arguments,"**Is your feature request related to a problem? Please describe.**
I am porting some existing code from Pandas to cuDF, when I have noticed that `skiprows` parameter in `read_csv` method does not suport neither `list-like` nor `callable` arguments.

**Describe the solution you'd like**
Support the same argument types as Pandas: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html

It is very useful to load subsets of huge files:
df = pd.read_csv(""dataset.csv"", skiprows = lambda x: x>0 and np.random.rand() > 0.01)

**Describe alternatives you've considered**
None

**Additional context**
Tested with RAPIDS 0.13",2020-04-24T22:59:12Z,0,0,Miguel Martínez,NVIDIA,True
35,[PERF] CSV reader data type detection is slow,"**Describe the bug**
data type detection represents ~25% of the CSV reader total time

**Steps/Code to reproduce bug**
nvprof read_csv(<large csv file>)

**Expected behavior**
data type detection time should be negligible
",2020-05-02T01:22:48Z,0,0,,NVIDIA,True
36,[FEA] Snappy Compressed CSV Not Implemented,read_csv doesn't support snappy compression. As a workaround we use snappy-python to uncompress csv files and then load using cudf.,2020-05-08T14:57:47Z,0,0,Lahir Marni,,False
37,[FEA] Implement pandas.Categorical,"**Is your feature request related to a problem? Please describe.**
While porting some existing code to cuDF, I have notices `pandas.Categorical` method is not implemented yet.

**Describe the solution you'd like**
To have it implemented.

**Describe alternatives you've considered**
I have used the following workaround it as follows (column 'D'):

```
df = cudf.DataFrame({'A': 1.,
                     'B': 2,
                     'C': np.array([3] * 4, dtype='int32'),
                     'D': cudf.Series([""cat"", ""dog"", ""lion"", ""fish""], dtype='category'),
                     'E': 'foo'})

```

**Additional context**
Using RAPIDS 0.13",2020-05-25T17:29:02Z,0,0,Miguel Martínez,NVIDIA,True
38,"[FEA] Remove from_xxx methods in DataFrame interface, or at least add other datatype sources to DataFrame object","**Is your feature request related to a problem? Please describe.**
Hi,

I was wondering if, instead of providing the following methods, it would be better to accept different datatypes when creating a DataFrame:
 
  - DataFrame.from_pandas
  - DataFrame.from_records
  - DataFrame.from_gpu_matrix
  - DataFrame.from_arrow

I think it would make easier to create DataFrames by allowing these datatypes as input when creating a DataFrame, instead of having specific methods. 

In the future, if other datatypes are supported, the code would remain valid without any change.

I am looking forward for your comments on this.

Regards,
Miguel",2020-05-25T17:37:49Z,0,0,Miguel Martínez,NVIDIA,True
39,[FEA] cuDF doesn’t support custom class objects as columns,"
**Is your feature request related to a problem? Please describe.**
cuDF doesn’t support custom class objects as columns

**Describe the solution you'd like**
Add support for custom class objects to be used as columns

**Describe alternatives you've considered**
Use strings instead

**Additional context**

Pandas sample
```python

import pandas as pd

class Header:
    def __init__(self, name):
        self.name = name

pd.DataFrame({Header(""name""): [1, 2, 3]})
```

cuDF sample
```python
import cudf as pd

pd.DataFrame({Header(""name""): [1, 2, 3]})
# TypeError: __setitem__ on type <class '__main__.Header'> is not supported

```

Traceback:
```python
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)

~/miniconda3/envs/rapids14/lib/python3.7/site-packages/cudf/core/dataframe.py in __setitem__(self, arg, value)
    616         else:
    617             msg = ""__setitem__ on type {!r} is not supported""
--> 618             raise TypeError(msg.format(type(arg)))
    619 
    620     def __delitem__(self, name):

TypeError: __setitem__ on type <class '__main__.Header'> is not supported
       
```
",2020-06-23T07:19:58Z,0,0,,,False
40,[FEA] cuDF doesn't work out of the box with `NamedTuple` when constructing a `DataFrame`,"
**Is your feature request related to a problem? Please describe.**

cuDF doesn't work out of the box with `NamedTuple` when constructing a `DataFrame`

**Describe the solution you'd like**

Add support for `NamedTuple`

**Describe alternatives you've considered**

First, create a `NamedTuple` object:

```python
from typing import NamedTuple, Optional, List
class ModelPrediction(NamedTuple):
    suspicious: Optional[bool]
    confidence: Optional[float]
    prediction: Optional[List[float]]

outputs = [ModelPrediction(suspicious=True, confidence=0.1, prediction=60), ModelPrediction(suspicious=False, confidence=0.6, prediction=40), ModelPrediction(suspicious=True, confidence=0.8, prediction=30)]
```

cuDF workaround:
```python
import cudf as pd
pd.DataFrame(list(iter(outputs)), columns=ModelPrediction.__annotations__.keys())
```

**Additional context**

Pandas works out of the box:
```python
import pandas as pd
pd.DataFrame(iter(outputs))
```

while cuDF produces a DataFrame with column names missing:
```python
import cudf as pd
pd.DataFrame(list(iter(outputs))) # column names missing

```

Output:
```
suspicious	confidence	prediction
0	True	0.1	60
1	False	0.6	40
2	True	0.8	30
```",2020-06-23T07:22:34Z,0,0,,,False
41,[FEA] `fillna()` doesn't accept axis keyword,"
**Is your feature request related to a problem? Please describe.**

`fillna()` doesn't accept axis keyword

**Describe the solution you'd like**

Add support for the `axis` keyword

**Describe alternatives you've considered**

In our use case `axis` keyword was redundant, it worked without it

**Additional context**

Pandas showcase:
```python

import pandas as pd

data = pd.DataFrame(
   data=([[None, None, 100, 1000, 10000], [2, 20, 200, 2000, 20000]]), columns=['a', 'b', 'c', 'd', 'e']
)

data1 = pd.DataFrame(
   data=([[3, 33], [2, 22]]), columns=['a', 'b']
)
data.fillna(data1, axis=1)
```

cuDF showcase:
```python
import cudf as pd

data = pd.DataFrame(
   data=([[None, None, 100, 1000, 10000], [2, 20, 200, 2000, 20000]]), columns=['a', 'b', 'c', 'd', 'e']
)

data1 = pd.DataFrame(
   data=([[3, 33], [2, 22]]), columns=['a', 'b']
)
data.fillna(data1, axis=1) # the axis keyword is not supported

```

Traceback:
```python
---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
<ipython-input-9-3b5ff28bd8cb> in <module>
      9    data=([[3, 33], [2, 22]]), columns=['a', 'b']
     10 )
---> 11 data.fillna(data1, axis=1) # the axis keyword is not supported

~/miniconda3/envs/rapids14/lib/python3.7/site-packages/cudf/core/dataframe.py in fillna(self, value, method, axis, inplace, limit)
   3815                 axis=axis,
   3816                 inplace=inplace,
-> 3817                 limit=limit,
   3818             )
   3819 

~/miniconda3/envs/rapids14/lib/python3.7/site-packages/cudf/core/series.py in fillna(self, value, method, axis, inplace, limit)
   1605             raise NotImplementedError(""The limit keyword is not supported"")
   1606         if axis:
-> 1607             raise NotImplementedError(""The axis keyword is not supported"")
   1608 
   1609         data = self._column.fillna(value)

NotImplementedError: The axis keyword is not supported
    ```
",2020-06-23T07:26:33Z,0,0,,,False
42,[FEA] Google mock for KafkaConsumer,"Testing around Kafka is currently limited. A Google mock class ""MockKafkaConsumer"" should be introduced so that tests against reading messages from Kafka can be performed.",2020-06-24T19:42:00Z,0,0,Jeremy Dyer,Nvidia,True
43,[BUG] Rolling count aggregations produce different results than Pandas 1.0+,"**Describe the bug**
Count aggregations on rolling windows do not match Pandas behavior when using pandas 1.0+. This is due to changes to handling of NaNs and the conditions under which a non-NaN value is allowed to be produced for a particular data window. More discussion in https://github.com/rapidsai/cudf/pull/4546


**Steps/Code to reproduce bug**
```
>>> cudf.Series.from_pandas(pd.Series([1,1,1,None])).rolling(2, min_periods=2, center=True).count()
0    null
1       2
2       2
3    null
dtype: int32

>>> pd.__version__
'1.0.3'
>>> pd.Series([1,1,1,None]).rolling(2, min_periods=2, center=True).count()
0    NaN
1    2.0
2    2.0
3    1.0
dtype: float64
```

**Expected behavior**
Either matching behavior or an understanding of why we differ. 


**Environment overview (please complete the following information)**
 - Environment location: Bare-metal
 - Method of cuDF install: Source
   - If method of install is [Docker], provide `docker pull` & `docker run` commands used

**Environment details**
cuDF 0.15

**Additional context**
https://github.com/pandas-dev/pandas/pull/30923
https://github.com/pandas-dev/pandas/issues/34466

",2020-06-25T20:40:34Z,0,0,,NVIDIA,True
44,[FEA] Hash values in nested columns,"Hashing categorical features to a fixed number of bins is a common preprocessing operation, particularly for tabular deep learning models where memory requirements scale with the number of bins. For extension of CuDF to nested columns, it would be helpful if calls to `Series.hash_values` hashed the values *in* each element's list and not the list itself. This would allow categorical hashing to extend to multi-hot categorical features.

As an example:
```
df = cudf.DataFrame({'a': [[0, 1, 2], [3, 4], [], [5], [6, 7, 8, 9]]})
df['a'].hash_values()

# not sure what this representation will
# print like but some like this
[[ 29140149,  -247539971,  1683430573],  [1098043756,  1851360991], [],
        [100260016],   [154726282, -1778135556, -1793932552,   246633392]]

df['a'].hash_values() % 4
[[1, 1, 1], [0, 3], [], [0], [2, 0, 0, 0]]
```

**Additional context**
Necessary for extension of [NVTabular HashBucket op](https://github.com/NVIDIA/NVTabular/blob/9f8216a89d565e00d8356ffef62f4437f3e2dee3/nvtabular/ops.py#L498) to multi-hot categorical data
",2020-07-06T13:43:29Z,0,0,Alec Gunny,MIT,False
45,[ENH] Identify opportunities for making properties cached in cuDF,"Wherever possible, cache properties using [`cached_property`](https://github.com/rapidsai/cudf/blob/branch-0.15/python/cudf/cudf/utils/utils.py#L310) to avoid computing them every time they are needed.",2020-07-15T12:27:36Z,0,0,Ashwin Srinath,Voltron Data,False
46,[FEA] Remove `object` dtype,"**Is your feature request related to a problem? Please describe.**
The `np.object` data type is used within cuDF python to represent string data, but this doesn't really mirror the purpose or function of `object` datatypes in CPU based python libraries, particularly numpy and pandas which can work with truly general python objects. This results in a lot of special handling in certain places in the python code as well as giving the wrong impression to users about our handling of python objects. We should replace it with something better, possibly an extension type similar to list/categorical in the short term. 

**Describe the solution you'd like**
Replace `np.object` in the cuDF codebase with a `cudf.StringDtype` object.

**Describe alternatives you've considered**
We could also try and shoehorn `pd.StringDtype()` in instead or wait and handle this as part of a wider `cud.Dtype` effort.

",2020-07-17T14:26:08Z,0,0,,NVIDIA,True
47,[FEA] Rolling slope calculation with groupby,"Hello.

I would like to calculate the rolling slope of y_value over x_value using cuML LinearRegression.

Sample data (cuDF dataframe):
```
| date       | x_value | y_value |
| ------     | ------  |  ----   |
| 2020-01-01 | 900     | 10      |
| 2020-01-01 | 905     | 15      |
| 2020-01-01 | 910     | 15      |
| 2020-01-01 | 915     | 15      |
| 2020-01-02 | 900     | 30      |
| 2020-01-02 | 905     | 40      |
| 2020-01-02 | 910     | 50      |
| ------     | ------  | ------  |
```
A simple function to use LinearRegression:
```
def RollingOLS(x, y):
    lr = LinearRegression(fit_intercept = True, normalize = False, algorithm = 'svd')
    reg = lr.fit(x, y)
    
    return reg.coef_
```

What I would like to do:
```
data.groupby('date').rolling(2).apply(RollingOLS, x=x_value, y=y_value)
```

However, I am getting an error: ```NotImplementedError: Handling UDF with null values is not yet supported```. Is there any way to overcome this error? Thank you.",2020-08-02T16:43:06Z,0,0,,,False
48,[FEA] Add support for cudf.Timedelta scalar support,"**Is your feature request related to a problem? Please describe.**
This is a Feature request for `cudf.Timedelta` similar to `pd.Timedelta` scalar duration type.

**Describe the solution you'd like**
`cudf.Timedelta` can be a wrapper on `Scalar(dtype='np.timedelta64')` object.


**Additional context**
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timedelta.html
https://github.com/rapidsai/cudf/pull/5781

",2020-08-07T15:47:03Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
49,[FEA] Make java test utlities for Lists and nested types user-friendly and less verbose,"**Is your feature request related to a problem? Please describe.**
Add test utilities to make complex type testing simpler and readable.


**Describe the solution you'd like**
We need some modifications to fromLists and getList methods in cudf java to auto detect number of elements, assert when numRows and data size may not line up and make `DataType` more palatable in terms of verbosity.

",2020-08-07T21:20:07Z,0,0,Kuhu Shukla,,False
50,[FEA] Pass `cudaStreamPerThread` to numba/CuPy kernels,"Memory allocations should already use PTDS since both numba and CuPy allocate memory using RMM. Kernels on the other hand, may explicitly need to be passed the `cudaStreamPerThread` stream handle.

cc: @jakirkham @kkraus14",2020-08-11T13:45:41Z,0,0,Ashwin Srinath,Voltron Data,False
51,[FEA] Dask_cudf support to read partitioned orc datasets,"**Is your feature request related to a problem? Please describe.**
Often, datasets stored in the orc format are partitioned (based on standard hive partitioning format) similar to parquet. Dask_cudf  (and dask_dataframe) currently supports reading partitioned parquet datasets, but does not support reading partitioned orc datasets.

**Describe the solution you'd like**
`dask_cudf.read_orc` works when provided a path to a partitioned orc dataset, without errors (similar to how this works now for `read_parquet`).
If the solution is general, this could be upstreamed to dask dataframe as well.

**Describe alternatives you've considered**
Current alternatives would involve walking through subfolders and reading the orc files separately, while using some custom logic (like looking at folder names) to determine the values for the partitioned columns.

**Additional context**
Here is an example of a partitioned orc dataset.
[test_orc.zip](https://github.com/rapidsai/cudf/files/5070526/test_orc.zip)

This is the existing output when trying to read this dataset with dask_cudf

```python
In [1]: import dask_cudf                                                                                                                                               
In [2]: dask_cudf.read_orc(""test_orc"")                                                                                                                                 
---------------------------------------------------------------------------
IsADirectoryError                         Traceback (most recent call last)
<ipython-input-2-711da5386bc8> in <module>
----> 1 dask_cudf.read_orc(""test_orc"")

/opt/conda/envs/rapids/lib/python3.7/site-packages/dask_cudf/io/orc.py in read_orc(path, columns, storage_options, **kwargs)
     47     nstripes_per_file = []
     48     for path in paths:
---> 49         with fs.open(path, ""rb"") as f:
     50             o = orc.ORCFile(f)
     51             if schema is None:

/opt/conda/envs/rapids/lib/python3.7/site-packages/fsspec/spec.py in open(self, path, mode, block_size, cache_options, **kwargs)
    842                 autocommit=ac,
    843                 cache_options=cache_options,
--> 844                 **kwargs
    845             )
    846             if not ac:

/opt/conda/envs/rapids/lib/python3.7/site-packages/fsspec/implementations/local.py in _open(self, path, mode, block_size, **kwargs)
    113         if self.auto_mkdir and ""w"" in mode:
    114             self.makedirs(self._parent(path), exist_ok=True)
--> 115         return LocalFileOpener(path, mode, fs=self, **kwargs)
    116 
    117     def touch(self, path, **kwargs):

/opt/conda/envs/rapids/lib/python3.7/site-packages/fsspec/implementations/local.py in __init__(self, path, mode, autocommit, fs, **kwargs)
    195         self.autocommit = autocommit
    196         self.blocksize = io.DEFAULT_BUFFER_SIZE
--> 197         self._open()
    198 
    199     def _open(self):

/opt/conda/envs/rapids/lib/python3.7/site-packages/fsspec/implementations/local.py in _open(self)
    200         if self.f is None or self.f.closed:
    201             if self.autocommit or ""w"" not in self.mode:
--> 202                 self.f = open(self.path, mode=self.mode)
    203             else:
    204                 # TODO: check if path is writable?

IsADirectoryError: [Errno 21] Is a directory: '/workdir/data/test_orc'
```

",2020-08-13T17:38:35Z,1,0,Ayush Dattagupta,Nvidia,True
52,[FEA] Profiling duplicate reading of metadata,"The row-group-level filtered reading for Parquet that is introduced by #5843 creates an issue of duplicate metadata (metadata is stored in the footers of Parquet files) reading in the case when filters are specified. Arrow is used to read metadata and select a subset of data to read given user-provided filters [4] . Information about this subset is then passed to libcudf which reads in the subset [5]. The issue is that metadata gets read twice - first when Arrow reads metadata to do filtering and second when libcudf reads data.

This issue was initially raised here [1].

# What to profile

- [ ] Perf penalty of reading metadata using Arrow for filtering in the same vein as [2] but with datasets of varying # of files
- [ ] Perf penalty of parsing metadata buffer [3] as fraction of total time Arrow spends reading metadata

# What to determine

- [ ] Determine whether or not perf penalty of the additional reading of metadata using Arrow is significant
- [ ] Determine whether the duplicate reading should be resolved by passing metadata struct (steps to implement [6]) or metadata buffer (which is then parsed into metadata struct in libcudf) (steps to implement [7]) from Arrow `Dataset` to libcudf reader functions

# Relevant discussion/code

[1] https://github.com/rapidsai/cudf/pull/5843#discussion_r467191621
[2] https://github.com/rapidsai/cudf/pull/5843#issuecomment-673566456
[3] https://github.com/apache/arrow/blob/2e6009621011d7df43882aa883905b84d1647018/cpp/src/parquet/file_reader.cc#L532
[4] https://github.com/rapidsai/cudf/pull/5843/files#diff-deac873508aaa12ca2e7c0a2c9035230R316
[5] https://github.com/rapidsai/cudf/pull/5843/files#diff-deac873508aaa12ca2e7c0a2c9035230R359-R375
[6] https://github.com/rapidsai/cudf/pull/5843#issuecomment-674437264
[7] https://github.com/rapidsai/cudf/pull/5843#issuecomment-674437709",2020-08-17T19:09:26Z,0,0,Caleb Winston,Stanford,False
53,"[BUG] Series.equals() raises error with ListColumns, while DataFrame.equals() works","**Describe the bug**
When trying to compare two Series with ListColumns, the following error is displayed:

```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-11-6bd0d51bd16c> in <module>
      4 series2 = cudf.Series({1: [10, 20, 30]})
      5 
----> 6 series1.equals(series2)

/opt/conda/envs/rapids/lib/python3.7/site-packages/cudf/core/series.py in equals(self, other)
   1499         if other is None or len(self) != len(other):
   1500             return False
-> 1501         return self._binaryop(other, ""eq"").min()
   1502 
   1503     def ne(self, other, fill_value=None, axis=0):

/opt/conda/envs/rapids/lib/python3.7/contextlib.py in inner(*args, **kwds)
     72         def inner(*args, **kwds):
     73             with self._recreate_cm():
---> 74                 return func(*args, **kwds)
     75         return inner
     76 

/opt/conda/envs/rapids/lib/python3.7/site-packages/cudf/core/series.py in _binaryop(self, other, fn, fill_value, reflect)
   1081                     rhs = rhs.fillna(fill_value)
   1082 
-> 1083         outcol = lhs._column.binary_operator(fn, rhs, reflect=reflect)
   1084         result = lhs._copy_construct(data=outcol, name=result_name)
   1085         return result

AttributeError: 'ListColumn' object has no attribute 'binary_operator'
```

**Steps/Code to reproduce bug**
```
import cudf

series1 = cudf.Series({1: [10, 20, 30]})
series2 = cudf.Series({1: [10, 20, 30]})

series1.equals(series2)
```

**Expected behaviour**
No error message. It works with DataFrames instead:

```
import cudf

df1 = cudf.DataFrame({1: [10, 20, 30], 2: [20, 30, 40]})
df2 = cudf.DataFrame({1: [10, 20, 30], 2: [20, 30, 40]})

df1.equals(df2)
```

**Environment overview (please complete the following information)**
DGX1

**Environment details**
cuDF version:  0.15.0a+4666.g1778921b0
",2020-08-19T13:58:55Z,0,0,Miguel Martínez,NVIDIA,True
54,[FEA] Support packing to a max input sequence length with cudf-subword tokenizer,"**Is your feature request related to a problem? Please describe.**

Currently, the tokenized string is shorter than max_length, output is be padded with 0s.  So if `max( tokenized string lengths)` < `max_length`, it  leads to performance penalties as the compute time for `Transformer` models is often proportional to the sequence length of the input . 

HuggingFace's tokenizer defaults to padding to max input sequence length if `max_length` and `pad_to_max_length` are not provided . We should try to follow that, this is especially beneficial for streaming cases that feature https://github.com/rapidsai/cudf/issues/5868 will help. 


##### See below example: 

#### Padding to max sequence length.(Proposed Default Behaviour)  
```python
from transformers import BertTokenizerFast


tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', do_lower_case=True)
data = ['a', 'a b', 'a b c d']
output = tokenizer.batch_encode_plus(data,padding=True,add_special_tokens=False, return_tensors = 'pt')
output['input_ids']

tensor([[1037,    0,    0,    0],
        [1037, 1038,    0,    0],
        [1037, 1038, 1039, 1040]])
```

####  Padding to max_length (Current Default Behavior) 
```python
tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', do_lower_case=True)
output = tokenizer.batch_encode_plus(
        data, truncation=True, max_length=64, pad_to_max_length=True,
        add_special_tokens=False, return_tensors = 'pt'
    )
output['input_ids']
```
```
tensor([[1037,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0],
        [1037, 1038,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0],
        [1037, 1038, 1039, 1040,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0]])
```



**Related Implications:**

a. We might have to switch from returning one-dimensional cupy arrays to 2-dimensional arrays for token-ids and attention masks which we allready do for most workflow cases so should not have performance penalties.   


**Describe alternatives you've considered**

Currently, a user can do the tokenization twice. 

1. First time to get maximum sequence length, do this without a `to_dlpack` call. 
2. Inputting that sequence length to tokenizer again and then convert to tensors using `dlpack` 

I do above for gpu-bdb q27 HF. 
), As most of the time is spent doing `to_dlpack` so this workaround should not have big performance implications. 



CC: @raykallen , @randerzander , @davidwendt 
",2020-08-25T17:48:07Z,0,0,Vibhu Jawa,Nvidia,True
55,[FEA] Filtering/reading statistics of ORC data with legacy TimestampStatistics,"The ORC statistics reading introduced in #6142 and stats-based ORC filtering in #6116 do not support the legacy version of `TimestampStatistics` in the ORC format. This legacy version uses time that is adjusted to the local timezone that the ORC data was written in. Fortunately, the local timezone is contained in ORC metadata so it is possible for us to support this version.

I'm creating this issue mainly to see if there is a need for this (user/workflow). If there is, the necessary changes would involve an interface between cuDF and libcudf for getting `writerTimezone` from each `StripeFooter`. This `writerTimezone` would be used to decode `minimum` and `maximum` into Python datetimes.",2020-09-04T19:15:28Z,0,0,Caleb Winston,Stanford,False
56,[FEA] Nth element support in dask cudf,"Requesting `nth` support in dask cudf after groupby.
ex. 
```
from cudf import DataFrame
import dask_cudf
df = DataFrame()
df['key'] = [1, 1, 1, 1, 2, 2, 2]
df['val_0']= [13, 15, 20, 27, 60, 17, 90]
df['val_1'] = [5, 1, 4, 9, 2, 7, 8]
meta_format = DataFrame()
ddf = dask_cudf.from_cudf(df, npartitions=1)
groups = ddf.groupby(['key']).nth(0)
```
Currently it fails with :
```
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
~/miniconda3/envs/branch15/lib/python3.8/site-packages/dask/dataframe/groupby.py in __getattr__(self, key)
   1749         try:
-> 1750             return self[key]
   1751         except KeyError as e:

~/miniconda3/envs/branch15/lib/python3.8/site-packages/dask/dataframe/groupby.py in __getitem__(self, key)
   1735         # error is raised from pandas
-> 1736         g._meta = g._meta[key]
   1737         return g

~/miniconda3/envs/branch15/lib/python3.8/site-packages/cudf/core/groupby/groupby.py in __getitem__(self, key)
    623     def __getitem__(self, key):
--> 624         return self.obj[key].groupby(self.grouping, dropna=self._dropna)
    625

~/miniconda3/envs/branch15/lib/python3.8/contextlib.py in inner(*args, **kwds)
     74             with self._recreate_cm():
---> 75                 return func(*args, **kwds)
     76         return inner

~/miniconda3/envs/branch15/lib/python3.8/site-packages/cudf/core/dataframe.py in __getitem__(self, arg)
    640         if is_scalar(arg) or isinstance(arg, tuple):
--> 641             return self._get_columns_by_label(arg, downcast=True)
    642

~/miniconda3/envs/branch15/lib/python3.8/site-packages/cudf/core/frame.py in _get_columns_by_label(self, labels, downcast)
    466         """"""
--> 467         new_data = self._data.select_by_label(labels)
    468         if downcast:

~/miniconda3/envs/branch15/lib/python3.8/site-packages/cudf/core/column_accessor.py in select_by_label(self, key)
    216                     return self._select_by_label_with_wildcard(key)
--> 217             return self._select_by_label_grouped(key)
    218

~/miniconda3/envs/branch15/lib/python3.8/site-packages/cudf/core/column_accessor.py in _select_by_label_grouped(self, key)
    264     def _select_by_label_grouped(self, key):
--> 265         result = self._grouped_data[key]
    266         if isinstance(result, cudf.core.column.ColumnBase):

KeyError: 'nth'

The above exception was the direct cause of the following exception:

AttributeError                            Traceback (most recent call last)
<ipython-input-2-9e488bd207ec> in <module>
      7 meta_format = DataFrame()
      8 ddf = dask_cudf.from_cudf(df, npartitions=1)
----> 9 groups = ddf.groupby(['key']).nth(1)

~/miniconda3/envs/branch15/lib/python3.8/site-packages/dask/dataframe/groupby.py in __getattr__(self, key)
   1750             return self[key]
   1751         except KeyError as e:
-> 1752             raise AttributeError(e) from e
   1753
   1754     @derived_from(pd.core.groupby.DataFrameGroupBy)

AttributeError: 'nth'
```
Once this functionality is implemented it should return a dask cudf dataframe that would contain the first row of each groupby

```
     val_0  val_1
key
1       13      5
2       60      2

```",2020-09-08T18:19:17Z,0,0,,,False
57,[BUG] `cudf.read_parquet` errors when non-cudf engine is used,"**Describe the bug**
When we save a dataframe containing a series with duration type using `fastparquet` engine and trying to load the same parquet file works in pandas but not in cudf.

**Steps/Code to reproduce bug**
```python
In[197]: df = pd.DataFrame({""a"":pd.Series([234334353455, 4354353455445, 54546344356], dtype='timedelta64[ns]')})
In[198]: df
Out[198]: 
                          a
0 0 days 00:03:54.334353455
1 0 days 01:12:34.353455445
2 0 days 00:00:54.546344356
In[199]: df.to_parquet(""temp_file"", engine='fastparquet', compression='GZIP')
In[200]: pd.read_parquet(""temp_file"", engine='fastparquet')
Out[200]: 
                       a
0 0 days 00:03:54.334353
1 0 days 01:12:34.353455
2 0 days 00:00:54.546344
In[201]: cudf.read_parquet(""temp_file"", engine='fastparquet')
/home/pgali/anaconda3/envs/cudf_dev11/lib/python3.7/site-packages/cudf/io/parquet.py:261: UserWarning: Using CPU via PyArrow to read Parquet dataset.
  warnings.warn(""Using CPU via PyArrow to read Parquet dataset."")
Traceback (most recent call last):
  File ""/home/pgali/anaconda3/envs/cudf_dev11/lib/python3.7/site-packages/IPython/core/interactiveshell.py"", line 3417, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-201-cc14a22603f4>"", line 1, in <module>
    cudf.read_parquet(""temp_file"", engine='fastparquet')
  File ""/home/pgali/anaconda3/envs/cudf_dev11/lib/python3.7/site-packages/cudf/io/parquet.py"", line 264, in read_parquet
    columns=columns, *args, **kwargs
  File ""/home/pgali/anaconda3/envs/cudf_dev11/lib/python3.7/site-packages/cudf/core/dataframe.py"", line 4809, in from_arrow
    out = super().from_arrow(table)
  File ""/home/pgali/anaconda3/envs/cudf_dev11/lib/python3.7/contextlib.py"", line 74, in inner
    return func(*args, **kwds)
  File ""/home/pgali/anaconda3/envs/cudf_dev11/lib/python3.7/site-packages/cudf/core/frame.py"", line 1935, in from_arrow
    else libcudf.interop.from_arrow(data, data.column_names)
  File ""cudf/_lib/interop.pyx"", line 154, in cudf._lib.interop.from_arrow
RuntimeError: cuDF failure at: /home/pgali/Desktop/cudf/cpp/src/interop/from_arrow.cu:76: Unsupported type_id conversion to cudf
```
**Expected behavior**
To not error and load the dataframe, like in `pd.read_parquet`

**Environment overview (please complete the following information)**
 - Environment location: Bare-metal
 - Method of cuDF install: from source

**Environment details**
Please run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details

<details><summary>Click here to see environment details</summary><pre>
     
     **git***
     commit 8e0d85cfd4512bb3394104487c908e07d875f667 (HEAD -> 6001, origin/6001)
     Merge: 073534b263 19237a09fe
     Author: galipremsagar <sagarprem75@gmail.com>
     Date:   Tue Sep 8 12:24:30 2020 -0500
     
     Merge remote-tracking branch 'upstream/branch-0.16' into 6001
     **git submodules***
     
     ***OS Information***
     DISTRIB_ID=Ubuntu
     DISTRIB_RELEASE=20.04
     DISTRIB_CODENAME=focal
     DISTRIB_DESCRIPTION=""Ubuntu 20.04.1 LTS""
     NAME=""Ubuntu""
     VERSION=""20.04.1 LTS (Focal Fossa)""
     ID=ubuntu
     ID_LIKE=debian
     PRETTY_NAME=""Ubuntu 20.04.1 LTS""
     VERSION_ID=""20.04""
     HOME_URL=""https://www.ubuntu.com/""
     SUPPORT_URL=""https://help.ubuntu.com/""
     BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
     PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
     VERSION_CODENAME=focal
     UBUNTU_CODENAME=focal
     Linux pgali-HP-Z8-G4-Workstation 5.4.0-47-generic #51-Ubuntu SMP Fri Sep 4 19:50:52 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
     
     ***GPU Information***
     Tue Sep  8 17:14:57 2020
     +-----------------------------------------------------------------------------+
     | NVIDIA-SMI 450.66       Driver Version: 450.66       CUDA Version: 11.0     |
     |-------------------------------+----------------------+----------------------+
     | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
     | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
     |                               |                      |               MIG M. |
     |===============================+======================+======================|
     |   0  Quadro RTX 8000     Off  | 00000000:22:00.0 Off |                  Off |
     | 34%   33C    P8     5W / 260W |    663MiB / 48601MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   1  Quadro RTX 8000     Off  | 00000000:2D:00.0  On |                  Off |
     | 33%   37C    P8    22W / 260W |    353MiB / 48592MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     
     +-----------------------------------------------------------------------------+
     | Processes:                                                                  |
     |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
     |        ID   ID                                                   Usage      |
     |=============================================================================|
     |    0   N/A  N/A       955      G   /usr/lib/xorg/Xorg                  4MiB |
     |    0   N/A  N/A      1848      G   /usr/lib/xorg/Xorg                  4MiB |
     |    0   N/A  N/A      5155      C   ...nvs/cudf_dev11/bin/python      649MiB |
     |    1   N/A  N/A       955      G   /usr/lib/xorg/Xorg                 39MiB |
     |    1   N/A  N/A      1848      G   /usr/lib/xorg/Xorg                128MiB |
     |    1   N/A  N/A      2050      G   /usr/bin/gnome-shell              158MiB |
     |    1   N/A  N/A      4900      G   /usr/lib/firefox/firefox            3MiB |
     |    1   N/A  N/A      5063      G   /usr/lib/firefox/firefox            3MiB |
     |    1   N/A  N/A      5372      G   /usr/lib/firefox/firefox            3MiB |
     |    1   N/A  N/A      7190      G   /usr/lib/firefox/firefox            3MiB |
     +-----------------------------------------------------------------------------+
     
     ***CPU***
     Architecture:                    x86_64
     CPU op-mode(s):                  32-bit, 64-bit
     Byte Order:                      Little Endian
     Address sizes:                   46 bits physical, 48 bits virtual
     CPU(s):                          12
     On-line CPU(s) list:             0-11
     Thread(s) per core:              2
     Core(s) per socket:              6
     Socket(s):                       1
     NUMA node(s):                    1
     Vendor ID:                       GenuineIntel
     CPU family:                      6
     Model:                           85
     Model name:                      Intel(R) Xeon(R) Gold 6128 CPU @ 3.40GHz
     Stepping:                        4
     CPU MHz:                         1200.111
     CPU max MHz:                     3700.0000
     CPU min MHz:                     1200.0000
     BogoMIPS:                        6800.00
     Virtualization:                  VT-x
     L1d cache:                       192 KiB
     L1i cache:                       192 KiB
     L2 cache:                        6 MiB
     L3 cache:                        19.3 MiB
     NUMA node0 CPU(s):               0-11
     Vulnerability Itlb multihit:     KVM: Vulnerable
     Vulnerability L1tf:              Mitigation; PTE Inversion
     Vulnerability Mds:               Mitigation; Clear CPU buffers; SMT vulnerable
     Vulnerability Meltdown:          Mitigation; PTI
     Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp
     Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
     Vulnerability Spectre v2:        Mitigation; Full generic retpoline, IBPB conditional, IBRS_FW, STIBP conditional, RSB filling
     Vulnerability Srbds:             Not affected
     Vulnerability Tsx async abort:   Mitigation; Clear CPU buffers; SMT vulnerable
     Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke md_clear flush_l1d
     
     ***CMake***
     /home/pgali/anaconda3/envs/cudf_dev11/bin/cmake
     cmake version 3.18.2
     
     CMake suite maintained and supported by Kitware (kitware.com/cmake).
     
     ***g++***
     /usr/bin/g++
     g++ (Ubuntu 9.3.0-10ubuntu2) 9.3.0
     Copyright (C) 2019 Free Software Foundation, Inc.
     This is free software; see the source for copying conditions.  There is NO
     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
     
     
     ***nvcc***
     /usr/local/cuda/bin/nvcc
     nvcc: NVIDIA (R) Cuda compiler driver
     Copyright (c) 2005-2020 NVIDIA Corporation
     Built on Wed_Jul_22_19:09:09_PDT_2020
     Cuda compilation tools, release 11.0, V11.0.221
     Build cuda_11.0_bu.TC445_37.28845127_0
     
     ***Python***
     /home/pgali/anaconda3/envs/cudf_dev11/bin/python
     Python 3.7.8
     
     ***Environment Variables***
     PATH                            : /home/pgali/anaconda3/envs/cudf_dev11/bin:/home/pgali/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin
     LD_LIBRARY_PATH                 : :/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64
     NUMBAPRO_NVVM                   :
     NUMBAPRO_LIBDEVICE              :
     CONDA_PREFIX                    : /home/pgali/anaconda3/envs/cudf_dev11
     PYTHON_PATH                     :
     
     ***conda packages***
     /home/pgali/anaconda3/condabin/conda
     # packages in environment at /home/pgali/anaconda3/envs/cudf_dev11:
     #
     # Name                    Version                   Build  Channel
     _libgcc_mutex             0.1                 conda_forge    conda-forge
     _openmp_mutex             4.5                       1_gnu    conda-forge
     abseil-cpp                20200225.2           he1b5a44_2    conda-forge
     alabaster                 0.7.12                     py_0    conda-forge
     appdirs                   1.4.3                      py_1    conda-forge
     argon2-cffi               20.1.0           py37h8f50634_1    conda-forge
     arrow-cpp                 0.17.1          py37h1234567_11_cuda    conda-forge
     arrow-cpp-proc            1.0.1                      cuda    conda-forge
     attrs                     20.1.0             pyh9f0ad1d_0    conda-forge
     aws-sdk-cpp               1.7.164              hba45d7a_2    conda-forge
     babel                     2.8.0                      py_0    conda-forge
     backcall                  0.2.0              pyh9f0ad1d_0    conda-forge
     backports                 1.0                        py_2    conda-forge
     backports.functools_lru_cache 1.6.1                      py_0    conda-forge
     black                     19.10b0                    py_4    conda-forge
     bleach                    3.1.5              pyh9f0ad1d_0    conda-forge
     bokeh                     2.2.1            py37hc8dfbb8_0    conda-forge
     boost-cpp                 1.72.0               h7b93d67_2    conda-forge
     brotli                    1.0.9                he1b5a44_0    conda-forge
     brotlipy                  0.7.0           py37h8f50634_1000    conda-forge
     bzip2                     1.0.8                h516909a_3    conda-forge
     c-ares                    1.16.1               h516909a_3    conda-forge
     ca-certificates           2020.7.22                     0
     certifi                   2020.6.20                py37_0
     cffi                      1.14.1           py37h2b28604_0    conda-forge
     cfgv                      3.2.0                      py_0    conda-forge
     chardet                   3.0.4           py37hc8dfbb8_1006    conda-forge
     clang                     8.0.1                hc9558a2_2    conda-forge
     clang-tools               8.0.1                hc9558a2_2    conda-forge
     clangxx                   8.0.1                         2    conda-forge
     click                     7.1.2              pyh9f0ad1d_0    conda-forge
     cloudpickle               1.6.0                      py_0    conda-forge
     cmake                     3.18.2               h5c55442_0    conda-forge
     cmake_setuptools          0.1.3                      py_0    rapidsai
     commonmark                0.9.1                      py_0    conda-forge
     cryptography              3.1              py37hb09aad4_0    conda-forge
     cudatoolkit               11.0.221             h6bb024c_0    nvidia
     cudf                      0.16.0a0+1513.g8e0d85cfd4          pypi_0    pypi
     cudnn                     8.0.0                cuda11.0_0    nvidia
     cupy                      7.8.0            py37h0ce7dbb_0    rapidsai
     curl                      7.71.1               he644dc0_5    conda-forge
     cython                    0.29.21          py37h3340039_0    conda-forge
     cytoolz                   0.10.1           py37h516909a_0    conda-forge
     dask                      2.25.0+9.g377965ad          pypi_0    pypi
     dask-core                 2.25.0                     py_0
     dask-cudf                 0.16.0a0+1513.g8e0d85cfd4.dirty          pypi_0    pypi
     decorator                 4.4.2                      py_0    conda-forge
     defusedxml                0.6.0                      py_0    conda-forge
     dicttoxml                 1.7.4                    py37_1
     distlib                   0.3.1              pyh9f0ad1d_0    conda-forge
     distributed               2.25.0+6.g73fa9bd1          pypi_0    pypi
     dlpack                    0.3                  he1b5a44_1    conda-forge
     docutils                  0.16             py37hc8dfbb8_1    conda-forge
     double-conversion         3.1.5                he1b5a44_2    conda-forge
     editdistance              0.5.3            py37h3340039_1    conda-forge
     entrypoints               0.3             py37hc8dfbb8_1001    conda-forge
     expat                     2.2.9                he1b5a44_2    conda-forge
     fastavro                  1.0.0.post1      py37h8f50634_0    conda-forge
     fastparquet               0.4.1                    pypi_0    pypi
     fastrlock                 0.5              py37h3340039_0    conda-forge
     filelock                  3.0.12             pyh9f0ad1d_0    conda-forge
     flake8                    3.8.3                      py_1    conda-forge
     flatbuffers               1.12.0               he1b5a44_0    conda-forge
     freetype                  2.10.2               he06d7ca_0    conda-forge
     fsspec                    0.8.0                      py_0    conda-forge
     future                    0.18.2           py37hc8dfbb8_1    conda-forge
     gflags                    2.2.2             he1b5a44_1004    conda-forge
     glog                      0.4.0                h49b9bf7_3    conda-forge
     gmp                       6.2.0                he1b5a44_2    conda-forge
     grpc-cpp                  1.30.2               heedbac9_0    conda-forge
     heapdict                  1.0.1                      py_0    conda-forge
     hypothesis                5.28.0                     py_0    conda-forge
     icu                       67.1                 he1b5a44_0    conda-forge
     identify                  1.4.30             pyh9f0ad1d_0    conda-forge
     idna                      2.10               pyh9f0ad1d_0    conda-forge
     imagesize                 1.2.0                      py_0    conda-forge
     importlib-metadata        1.7.0            py37hc8dfbb8_0    conda-forge
     importlib_metadata        1.7.0                         0    conda-forge
     iniconfig                 1.0.1              pyh9f0ad1d_0    conda-forge
     ipykernel                 5.3.4            py37h43977f1_0    conda-forge
     ipython                   7.18.1           py37hc6149b9_0    conda-forge
     ipython_genutils          0.2.0                      py_1    conda-forge
     isort                     5.0.7            py37hc8dfbb8_0    conda-forge
     jedi                      0.17.2           py37hc8dfbb8_0    conda-forge
     jinja2                    2.11.2             pyh9f0ad1d_0    conda-forge
     jpeg                      9d                   h516909a_0    conda-forge
     jsonschema                3.2.0            py37hc8dfbb8_1    conda-forge
     jupyter_client            6.1.7                      py_0    conda-forge
     jupyter_core              4.6.3            py37hc8dfbb8_1    conda-forge
     krb5                      1.17.1               hfafb76e_3    conda-forge
     lcms2                     2.11                 hbd6801e_0    conda-forge
     ld_impl_linux-64          2.34                 hc38a660_9    conda-forge
     libblas                   3.8.0               17_openblas    conda-forge
     libcblas                  3.8.0               17_openblas    conda-forge
     libcurl                   7.71.1               hcdd3856_5    conda-forge
     libedit                   3.1.20191231         he28a2e2_2    conda-forge
     libev                     4.33                 h516909a_1    conda-forge
     libevent                  2.1.10               hcdb4288_2    conda-forge
     libffi                    3.2.1             he1b5a44_1007    conda-forge
     libgcc-ng                 9.3.0               h24d8f2e_16    conda-forge
     libgfortran-ng            7.5.0               hdf63c60_16    conda-forge
     libgomp                   9.3.0               h24d8f2e_16    conda-forge
     liblapack                 3.8.0               17_openblas    conda-forge
     libllvm10                 10.0.1               he513fc3_3    conda-forge
     libllvm8                  8.0.1                hc9558a2_0    conda-forge
     libnghttp2                1.41.0               h8cfc5f6_2    conda-forge
     libopenblas               0.3.10          pthreads_hb3c22a3_4    conda-forge
     libpng                    1.6.37               hed695b0_2    conda-forge
     libprotobuf               3.12.4               h8b12597_0    conda-forge
     librmm                    0.16.0a200904   cuda11.0_g83f7b35_284    rapidsai-nightly
     libsodium                 1.0.18               h516909a_0    conda-forge
     libssh2                   1.9.0                hab1572f_5    conda-forge
     libstdcxx-ng              9.3.0               hdf63c60_16    conda-forge
     libtiff                   4.1.0                hc7e4089_6    conda-forge
     libuv                     1.39.0               h516909a_0    conda-forge
     libwebp-base              1.1.0                h516909a_3    conda-forge
     llvmlite                  0.34.0           py37h5202443_1    conda-forge
     locket                    0.2.0                      py_2    conda-forge
     lz4-c                     1.9.2                he1b5a44_3    conda-forge
     markdown                  3.2.2                      py_0    conda-forge
     markupsafe                1.1.1            py37h8f50634_1    conda-forge
     mccabe                    0.6.1                      py_1    conda-forge
     mimesis                   4.0.0              pyh9f0ad1d_0    conda-forge
     mistune                   0.8.4           py37h8f50634_1001    conda-forge
     more-itertools            8.5.0                      py_0    conda-forge
     msgpack-python            1.0.0            py37h99015e2_1    conda-forge
     nbconvert                 5.6.1            py37hc8dfbb8_1    conda-forge
     nbformat                  5.0.7                      py_0    conda-forge
     nbsphinx                  0.7.1              pyh9f0ad1d_0    conda-forge
     nccl                      2.7.8.1              h4962215_0    nvidia
     ncurses                   6.2                  he1b5a44_1    conda-forge
     nodeenv                   1.5.0              pyh9f0ad1d_0    conda-forge
     notebook                  6.1.3            py37hc8dfbb8_0    conda-forge
     numba                     0.51.2           py37h9fdb41a_0    conda-forge
     numpy                     1.19.1           py37h7ea13bd_2    conda-forge
     numpydoc                  1.1.0              pyh9f0ad1d_0    conda-forge
     olefile                   0.46                       py_0    conda-forge
     openssl                   1.1.1g               h7b6447c_0
     packaging                 20.4               pyh9f0ad1d_0    conda-forge
     pandas                    1.1.1            py37h3340039_0    conda-forge
     pandoc                    1.19.2                        0    conda-forge
     pandocfilters             1.4.2                      py_1    conda-forge
     parquet-cpp               1.5.1                         2    conda-forge
     parso                     0.7.1              pyh9f0ad1d_0    conda-forge
     partd                     1.1.0                      py_0    conda-forge
     pathspec                  0.8.0              pyh9f0ad1d_0    conda-forge
     pexpect                   4.8.0            py37hc8dfbb8_1    conda-forge
     pickleshare               0.7.5           py37hc8dfbb8_1001    conda-forge
     pillow                    7.2.0            py37h718be6c_1    conda-forge
     pip                       20.2.2                     py_0    conda-forge
     pluggy                    0.13.1           py37hc8dfbb8_2    conda-forge
     pre-commit                2.7.1            py37hc8dfbb8_0    conda-forge
     pre_commit                2.7.1                         0    conda-forge
     prometheus_client         0.8.0              pyh9f0ad1d_0    conda-forge
     prompt-toolkit            3.0.7                      py_0    conda-forge
     psutil                    5.7.2            py37h8f50634_0    conda-forge
     ptyprocess                0.6.0                   py_1001    conda-forge
     py                        1.9.0              pyh9f0ad1d_0    conda-forge
     pyarrow                   0.17.1          py37h1234567_11_cuda    conda-forge
     pycodestyle               2.6.0              pyh9f0ad1d_0    conda-forge
     pycparser                 2.20               pyh9f0ad1d_2    conda-forge
     pyflakes                  2.2.0              pyh9f0ad1d_0    conda-forge
     pygments                  2.6.1                      py_0    conda-forge
     pyopenssl                 19.1.0                     py_1    conda-forge
     pyparsing                 2.4.7              pyh9f0ad1d_0    conda-forge
     pyrsistent                0.16.0           py37h8f50634_0    conda-forge
     pysocks                   1.7.1            py37hc8dfbb8_1    conda-forge
     pytest                    6.0.1            py37hc8dfbb8_0    conda-forge
     python                    3.7.8           h6f2ec95_1_cpython    conda-forge
     python-dateutil           2.8.1                      py_0    conda-forge
     python_abi                3.7                     1_cp37m    conda-forge
     pytz                      2020.1             pyh9f0ad1d_0    conda-forge
     pyyaml                    5.3.1            py37h8f50634_0    conda-forge
     pyzmq                     19.0.2           py37hac76be4_0    conda-forge
     rapidjson                 1.1.0             he1b5a44_1002    conda-forge
     re2                       2020.07.06           he1b5a44_1    conda-forge
     readline                  8.0                  he28a2e2_2    conda-forge
     recommonmark              0.6.0                      py_0    conda-forge
     regex                     2020.7.14        py37h8f50634_0    conda-forge
     requests                  2.24.0             pyh9f0ad1d_0    conda-forge
     rhash                     1.3.6             h14c3975_1001    conda-forge
     rmm                       0.16.0a200904   cuda_11.0_py37_g83f7b35_284    rapidsai-nightly
     send2trash                1.5.0                      py_0    conda-forge
     setuptools                49.6.0           py37hc8dfbb8_0    conda-forge
     six                       1.15.0             pyh9f0ad1d_0    conda-forge
     snappy                    1.1.8                he1b5a44_3    conda-forge
     snowballstemmer           2.0.0                      py_0    conda-forge
     sortedcontainers          2.2.2              pyh9f0ad1d_0    conda-forge
     spdlog                    1.8.0                hc9558a2_0    conda-forge
     sphinx                    3.2.1                      py_0    conda-forge
     sphinx-copybutton         0.3.0              pyh9f0ad1d_0    conda-forge
     sphinx-markdown-tables    0.0.14             pyh9f0ad1d_1    conda-forge
     sphinx_rtd_theme          0.5.0              pyh9f0ad1d_0    conda-forge
     sphinxcontrib-applehelp   1.0.2                      py_0    conda-forge
     sphinxcontrib-devhelp     1.0.2                      py_0    conda-forge
     sphinxcontrib-htmlhelp    1.0.3                      py_0    conda-forge
     sphinxcontrib-jsmath      1.0.1                      py_0    conda-forge
     sphinxcontrib-qthelp      1.0.3                      py_0    conda-forge
     sphinxcontrib-serializinghtml 1.1.4                      py_0    conda-forge
     sphinxcontrib-websupport  1.2.4              pyh9f0ad1d_0    conda-forge
     sqlite                    3.33.0               h4cf870e_0    conda-forge
     streamz                   0.5.5                    pypi_0    pypi
     tblib                     1.6.0                      py_0    conda-forge
     terminado                 0.8.3            py37hc8dfbb8_1    conda-forge
     testpath                  0.4.4                      py_0    conda-forge
     thrift                    0.13.0                   pypi_0    pypi
     thrift-cpp                0.13.0               h62aa4f2_3    conda-forge
     tk                        8.6.10               hed695b0_0    conda-forge
     toml                      0.10.1             pyh9f0ad1d_0    conda-forge
     toolz                     0.10.0                     py_0    conda-forge
     tornado                   6.0.4            py37h8f50634_1    conda-forge
     traitlets                 5.0.3                      py_0    conda-forge
     typed-ast                 1.4.1            py37h516909a_0    conda-forge
     typing_extensions         3.7.4.2                    py_0    conda-forge
     urllib3                   1.25.10                    py_0    conda-forge
     virtualenv                20.0.20          py37hc8dfbb8_1    conda-forge
     wcwidth                   0.2.5              pyh9f0ad1d_1    conda-forge
     webencodings              0.5.1                      py_1    conda-forge
     wheel                     0.35.1             pyh9f0ad1d_0    conda-forge
     xz                        5.2.5                h516909a_1    conda-forge
     yaml                      0.2.5                h516909a_0    conda-forge
     zeromq                    4.3.2                he1b5a44_3    conda-forge
     zict                      2.0.0                      py_0    conda-forge
     zipp                      3.1.0                      py_0    conda-forge
     zlib                      1.2.11            h516909a_1009    conda-forge
     zstd                      1.4.5                h6597ccf_2    conda-forge
     
</pre></details>

**Additional context**
Surfaced while running fuzz tests in #6001 
",2020-09-08T22:16:21Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
58,[FEA] cuio: allow datasource and data_sink to decide how device/host read/write/copies are handled.,"Reader and writer implementations have multiple paths for reading and writing, depending whether data is read-from/written-to the host or the device. This logic could be delegated to `datasource` and `data_sink`, potentially sharing the functionality between all `datasource`s and `data_sink`s.

This would eliminate the need for `supports_device_write` and `supports_device_read`, which could also reduce the surface area of testing.

note: `supports_device_read` is currently unused.

Examples:
https://github.com/rapidsai/cudf/blob/f4735c7f658da4a157dc09391da899b072878305/cpp/src/io/csv/writer_impl.cu#L412-L442
https://github.com/rapidsai/cudf/blob/f4735c7f658da4a157dc09391da899b072878305/cpp/src/io/parquet/writer_impl.cu#L882-L913

Looks like ORC doesn't have this logic at all, which might be a bug?
https://github.com/rapidsai/cudf/blob/f4735c7f658da4a157dc09391da899b072878305/cpp/src/io/orc/writer_impl.cu#L1246

Hypothetical Source/Sink APIs
-
`data_kind` is used to describe the caller-owned buffer.
```C++
// note: data_kind is not used to determine the sink/source buffer's kind.
//       the sink/source buffer kind is an implementation detail of the given sink/source.
enum class data_kind {
  host,  // used when the caller is writing data from host, or reading data to host
  device // used when the caller is writing data from device, or reading data to device
};
```
An API such as this delegates the read/write logic to the source/sink, but gives enough information for the source/sink to determine how data should be copied, and whether or not a sync is necessary to perform the copies. For instance, if the specific source implementation is reading data on device, and the `read(...)` call is made with `data_kind::device`, then the source has enough information to execute a device-to-device copy, without or without syncing the stream (perhaps an API an optional method should be added to ensure sync has taken place).
```C++
class base_source_context {
  base_source_context(cudaStream_t stream) : stream(stream) {}

  virtual size_t read(uint8_t const* data, size_t size, data_kind kind) = 0;

 private:
  cudaStream_t stream;
};

class base_source {
 public:
  virtual unique_ptr<base_source_context> begin_read(cudaStream_t stream) = 0;
};
```
```C++
class base_sink_context {
 public:
  base_sink_context(cudaStream_t stream) : stream(stream) {}

  virtual void write(uint8_t const* data, size_t size, data_kind kind) = 0;

 private:
  cudaStream_t stream;
};

class base_sink {
 public:
  virtual unique_ptr<base_sink_context> begin_write(cudaStream_t stream) = 0;
};
```",2020-09-09T18:28:07Z,0,0,Christopher Harris,Nvidia,True
59,"[BUG] cuio: reader/writer tests do not verify file contents, only isomorphism.","**Describe the bug**
Reader and writer tests have the potential to pass erroneously if the readers/writers change in a way that maintains isomorphism, regardless of whether the written contents contain the correct contents. Unlikely, but possible.

**Expected behavior**
- Reader tests should explicitly test reading file contents and verifying the parsed data is as expected.
- Writer tests should explicitly test writing data and verifying the file contents is as expected.

Examples:
https://github.com/rapidsai/cudf/blob/2bc8eb0edfdc2974a17b396f572d9f16f23caf3c/cpp/tests/io/csv_test.cpp#L902-L911
https://github.com/rapidsai/cudf/blob/2bc8eb0edfdc2974a17b396f572d9f16f23caf3c/cpp/tests/io/orc_test.cpp#L165-L173
https://github.com/rapidsai/cudf/blob/2bc8eb0edfdc2974a17b396f572d9f16f23caf3c/cpp/tests/io/parquet_test.cpp#L191-L200",2020-09-14T14:39:00Z,0,0,Christopher Harris,Nvidia,True
60,[FEA] Ability to query cudf Java jar build details,"**Is your feature request related to a problem? Please describe.**
Some users of the RAPIDS Accelerator plugin for Apache Spark are not matching the correct versions of cudf with the plugin resulting in `NoSuchMethodError` exceptions and other errors.  It would be nice if the plugin could query cudf, either via a properties resource file in the jar or via a formal Java API, to get details about the cudf build such as:
- cudf version
- CUDA runtime version it was compiled against

**Describe the solution you'd like**
A simple properties file containing key/value pairs that can be looked up probably would be sufficient.  We can wrap that properties file with a cudf Java API that returns the properties as a map of key/values to abstract the implementation details a bit if desired.

**Describe alternatives you've considered**
Separate APIs for each build or configuration setting is another way to handle this but can quickly explode the API footprint.
",2020-09-14T21:05:22Z,0,0,Jason Lowe,NVIDIA,True
61,"[BUG] float min, max values are truncated, causing to read again as -inf, +inf","**Describe the bug**
floating point min=-1.7976931348623157e+308 and  max=1.7976931348623157e+308 is written as 
-1.797693135e+308
1.797693135e+308
and so, it is read as -inf and +inf respectively.

**Steps/Code to reproduce bug**
```python
import cudf
import pandas as pd
import numpy as np
pdf = pd.DataFrame({'f': [np.finfo(float).min, np.finfo(float).max]})
gdf = cudf.from_pandas(pdf)
pdf.to_csv('pdf.csv', index=False, header=False)
gdf.to_csv('gdf.csv', index=False, header=False)	# truncates values, so causes error.
g1 = cudf.read_csv('pdf.csv', index_col=None, header=None) # Works right
g2 = cudf.read_csv('gdf.csv', index_col=None, header=None) # Wrong (-inf, +inf)
p1 = pd.read_csv('pdf.csv', index_col=None, header=None) # Read as string (but dtype=[np.float] fails), p1.astype works,
p2 = pd.read_csv('gdf.csv', index_col=None, header=None) # Read as string, p2.astype(np.float) is Wrong (-inf, +inf)

np.equal(pdf.values, gdf.values.get())	# True
np.equal(pdf.values, g1.values.get())	# True
np.equal(pdf.values, g2.values.get())	# False (wrong)
np.equal(pdf.values, p1.astype(np.float).values)	# True
np.equal(pdf.values, p2.astype(np.float).values)	# False (wrong)
```

Pandas parsing also doesn't work. But pandas.astype(np.float) works.
cudf parsing and astype(np.float) both doesn't work because writing truncates and rounds up last number)

**Expected behavior**
cudf.to_csv should write all fractional values of float. It should not truncate.
Content of `gdf.csv` should be same as `pdf.csv`
In the above snippet, following is expected behaviour
```
np.equal(pdf.values, gdf.values.get())	# True
np.equal(pdf.values, g1.values.get())	# True
np.equal(pdf.values, g2.values.get())	# True
np.equal(pdf.values, p1.astype(np.float).values)	# True
np.equal(pdf.values, p2.astype(np.float).values)	# True
```
**Environment overview (please complete the following information)**
 - Environment location: Bare-metal
 - Method of cuDF install: from source

**Environment details**
Please run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details
<details><summary>Click here to see environment details</summary><pre>
     
     **git***
     commit 533b6682e4a9dcf31d73fc3802c55e05a391c6c5 (HEAD -> bug-csv_infinity_support, karthikeyann/bug-csv_infinity_support)
     Author: Karthikeyan Natarajan <karthikeyann@users.noreply.github.com>
     Date:   Tue Sep 15 15:04:57 2020 +0530
     
     changelog entry for PR #6234
     **git submodules***
     
     ***OS Information***
     DISTRIB_ID=Ubuntu
     DISTRIB_RELEASE=18.04
     DISTRIB_CODENAME=bionic
     DISTRIB_DESCRIPTION=""Ubuntu 18.04.3 LTS""
     NAME=""Ubuntu""
     VERSION=""18.04.3 LTS (Bionic Beaver)""
     ID=ubuntu
     ID_LIKE=debian
     PRETTY_NAME=""Ubuntu 18.04.3 LTS""
     VERSION_ID=""18.04""
     HOME_URL=""https://www.ubuntu.com/""
     SUPPORT_URL=""https://help.ubuntu.com/""
     BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
     PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
     VERSION_CODENAME=bionic
     UBUNTU_CODENAME=bionic
     Linux knataraj-ws 4.15.0-112-generic #113-Ubuntu SMP Thu Jul 9 23:41:39 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
     
     ***GPU Information***
     Tue Sep 15 17:18:16 2020
     +-----------------------------------------------------------------------------+
     | NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 10.2     |
     |-------------------------------+----------------------+----------------------+
     | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
     | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
     |===============================+======================+======================|
     |   0  Quadro GV100        On   | 00000000:15:00.0 Off |                  Off |
     | 37%   49C    P2    39W / 250W |    749MiB / 32508MiB |      0%      Default |
     +-------------------------------+----------------------+----------------------+
     |   1  Quadro GV100        On   | 00000000:2D:00.0 Off |                  Off |
     | 42%   56C    P2    40W / 250W |    279MiB / 32498MiB |      0%      Default |
     +-------------------------------+----------------------+----------------------+
     
     +-----------------------------------------------------------------------------+
     | Processes:                                                       GPU Memory |
     |  GPU       PID   Type   Process name                             Usage      |
     |=============================================================================|
     |    0     29543      C   ...yan/miniconda3/envs/cudf_dev/bin/python   737MiB |
     |    1      1554      G   /usr/lib/xorg/Xorg                           137MiB |
     |    1      1759      G   /usr/bin/gnome-shell                         128MiB |
     +-----------------------------------------------------------------------------+
     
     ***CPU***
     Architecture:        x86_64
     CPU op-mode(s):      32-bit, 64-bit
     Byte Order:          Little Endian
     CPU(s):              12
     On-line CPU(s) list: 0-11
     Thread(s) per core:  2
     Core(s) per socket:  6
     Socket(s):           1
     NUMA node(s):        1
     Vendor ID:           GenuineIntel
     CPU family:          6
     Model:               85
     Model name:          Intel(R) Xeon(R) Gold 6128 CPU @ 3.40GHz
     Stepping:            4
     CPU MHz:             1200.197
     CPU max MHz:         3700.0000
     CPU min MHz:         1200.0000
     BogoMIPS:            6800.00
     Virtualization:      VT-x
     L1d cache:           32K
     L1i cache:           32K
     L2 cache:            1024K
     L3 cache:            19712K
     NUMA node0 CPU(s):   0-11
     Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke md_clear flush_l1d
     
     ***CMake***
     /home/karthikeyan/miniconda3/envs/cudf_dev/bin/cmake
     cmake version 3.18.2
     
     CMake suite maintained and supported by Kitware (kitware.com/cmake).
     
     ***g++***
     /usr/local/bin/g++
     g++ (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
     Copyright (C) 2017 Free Software Foundation, Inc.
     This is free software; see the source for copying conditions.  There is NO
     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
     
     
     ***nvcc***
     /usr/local/bin/nvcc
     nvcc: NVIDIA (R) Cuda compiler driver
     Copyright (c) 2005-2019 NVIDIA Corporation
     Built on Wed_Oct_23_19:24:38_PDT_2019
     Cuda compilation tools, release 10.2, V10.2.89
     
     ***Python***
     /home/karthikeyan/miniconda3/envs/cudf_dev/bin/python
     Python 3.7.8
     
     ***Environment Variables***
     PATH                            : /home/karthikeyan/miniconda3/envs/cudf_dev/bin:/usr/local/bin:/usr/local/cuda-10.2/bin:/home/karthikeyan/.vscode-server-insiders/bin/e790b931385d72cf5669fcefc51cdf65990efa5d/bin:/home/karthikeyan/bin:/home/karthikeyan/.local/bin:/home/karthikeyan/miniconda3/condabin:/usr/local/bin:/usr/local/cuda-10.2/bin:/home/karthikeyan/.vscode-server-insiders/bin/e790b931385d72cf5669fcefc51cdf65990efa5d/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin:/home/karthikeyan/miniconda3/bin:/home/karthikeyan/.local/bin:/usr/local/cuda/bin:/home/karthikeyan/miniconda3/bin:/home/karthikeyan/.local/bin
     LD_LIBRARY_PATH                 : /usr/local/cuda-10.2/lib64:/usr/local/cuda-10.2/lib64:/home/karthikeyan/2cudf/cpp/build:/home/karthikeyan/2cudf/cpp/build/release
     NUMBAPRO_NVVM                   :
     NUMBAPRO_LIBDEVICE              :
     CONDA_PREFIX                    : /home/karthikeyan/miniconda3/envs/cudf_dev
     PYTHON_PATH                     :
     
     ***conda packages***
     /home/karthikeyan/miniconda3/condabin/conda
     # packages in environment at /home/karthikeyan/miniconda3/envs/cudf_dev:
     #
     # Name                    Version                   Build  Channel
     _libgcc_mutex             0.1                 conda_forge    conda-forge
     _openmp_mutex             4.5                       1_gnu    conda-forge
     abseil-cpp                20200225.2           he1b5a44_2    conda-forge
     alabaster                 0.7.12                     py_0    conda-forge
     appdirs                   1.4.3                      py_1    conda-forge
     argon2-cffi               20.1.0           py37h8f50634_1    conda-forge
     arrow-cpp                 1.0.1           py37h1234567_1_cuda    conda-forge
     arrow-cpp-proc            1.0.1                      cuda    conda-forge
     attrs                     20.1.0             pyh9f0ad1d_0    conda-forge
     aws-sdk-cpp               1.7.164              hba45d7a_2    conda-forge
     babel                     2.8.0                      py_0    conda-forge
     backcall                  0.2.0              pyh9f0ad1d_0    conda-forge
     backports                 1.0                        py_2    conda-forge
     backports.functools_lru_cache 1.6.1                      py_0    conda-forge
     black                     19.10b0                    py_4    conda-forge
     bleach                    3.1.5              pyh9f0ad1d_0    conda-forge
     bokeh                     2.2.0            py37hc8dfbb8_0    conda-forge
     boost-cpp                 1.72.0               h9359b55_3    conda-forge
     brotli                    1.0.7             he1b5a44_1004    conda-forge
     brotlipy                  0.7.0           py37h8f50634_1000    conda-forge
     bzip2                     1.0.8                h516909a_3    conda-forge
     c-ares                    1.16.1               h516909a_3    conda-forge
     ca-certificates           2020.6.20            hecda079_0    conda-forge
     certifi                   2020.6.20        py37hc8dfbb8_0    conda-forge
     cffi                      1.14.1           py37h2b28604_0    conda-forge
     cfgv                      3.2.0                      py_0    conda-forge
     chardet                   3.0.4           py37hc8dfbb8_1006    conda-forge
     clang                     8.0.1                hc9558a2_2    conda-forge
     clang-tools               8.0.1                hc9558a2_2    conda-forge
     clangxx                   8.0.1                         2    conda-forge
     click                     7.1.2              pyh9f0ad1d_0    conda-forge
     cloudpickle               1.6.0                      py_0    conda-forge
     cmake                     3.18.2               h5c55442_0    conda-forge
     cmake_setuptools          0.1.3                      py_0    rapidsai
     commonmark                0.9.1                      py_0    conda-forge
     cryptography              3.0              py37hb09aad4_0    conda-forge
     cudatoolkit               10.2.89              h6bb024c_0    nvidia
     cudnn                     7.6.5                cuda10.2_0
     cupy                      7.8.0            py37h940342b_0    conda-forge
     curl                      7.71.1               he644dc0_5    conda-forge
     cython                    0.29.21          py37h3340039_0    conda-forge
     cytoolz                   0.10.1           py37h516909a_0    conda-forge
     dask                      2.25.0+15.g3327b2e1          pypi_0    pypi
     decorator                 4.4.2                      py_0    conda-forge
     defusedxml                0.6.0                      py_0    conda-forge
     distlib                   0.3.1              pyh9f0ad1d_0    conda-forge
     distributed               2.25.0+9.g4115f55c          pypi_0    pypi
     dlpack                    0.3                  he1b5a44_1    conda-forge
     docutils                  0.16             py37hc8dfbb8_1    conda-forge
     double-conversion         3.1.5                he1b5a44_2    conda-forge
     editdistance              0.5.3            py37h3340039_1    conda-forge
     entrypoints               0.3             py37hc8dfbb8_1001    conda-forge
     expat                     2.2.9                he1b5a44_2    conda-forge
     fastavro                  1.0.0.post1      py37h8f50634_0    conda-forge
     fastrlock                 0.5              py37h3340039_0    conda-forge
     filelock                  3.0.12             pyh9f0ad1d_0    conda-forge
     flake8                    3.8.3                      py_1    conda-forge
     flatbuffers               1.12.0               he1b5a44_0    conda-forge
     freetype                  2.10.2               he06d7ca_0    conda-forge
     fsspec                    0.8.2                      py_0    conda-forge
     future                    0.18.2           py37hc8dfbb8_1    conda-forge
     gflags                    2.2.2             he1b5a44_1004    conda-forge
     glog                      0.4.0                h49b9bf7_3    conda-forge
     gmp                       6.2.0                he1b5a44_2    conda-forge
     grpc-cpp                  1.30.2               heedbac9_0    conda-forge
     heapdict                  1.0.1                      py_0    conda-forge
     hypothesis                5.28.0                     py_0    conda-forge
     icu                       67.1                 he1b5a44_0    conda-forge
     identify                  1.4.29             pyh9f0ad1d_0    conda-forge
     idna                      2.10               pyh9f0ad1d_0    conda-forge
     imagesize                 1.2.0                      py_0    conda-forge
     importlib-metadata        1.7.0            py37hc8dfbb8_0    conda-forge
     importlib_metadata        1.7.0                         0    conda-forge
     iniconfig                 1.0.1              pyh9f0ad1d_0    conda-forge
     ipykernel                 5.3.4            py37h43977f1_0    conda-forge
     ipython                   7.18.1           py37hc6149b9_0    conda-forge
     ipython_genutils          0.2.0                      py_1    conda-forge
     isort                     5.0.7            py37hc8dfbb8_0    conda-forge
     jedi                      0.17.2           py37hc8dfbb8_0    conda-forge
     jinja2                    2.11.2             pyh9f0ad1d_0    conda-forge
     jpeg                      9d                   h516909a_0    conda-forge
     jsonschema                3.2.0            py37hc8dfbb8_1    conda-forge
     jupyter_client            6.1.7                      py_0    conda-forge
     jupyter_core              4.6.3            py37hc8dfbb8_1    conda-forge
     krb5                      1.17.1               hfafb76e_2    conda-forge
     lcms2                     2.11                 hbd6801e_0    conda-forge
     ld_impl_linux-64          2.34                 hc38a660_9    conda-forge
     libblas                   3.8.0               17_openblas    conda-forge
     libcblas                  3.8.0               17_openblas    conda-forge
     libcurl                   7.71.1               hcdd3856_5    conda-forge
     libedit                   3.1.20191231         he28a2e2_2    conda-forge
     libev                     4.33                 h516909a_0    conda-forge
     libevent                  2.1.10               hcdb4288_2    conda-forge
     libffi                    3.2.1             he1b5a44_1007    conda-forge
     libgcc-ng                 9.3.0               h24d8f2e_16    conda-forge
     libgfortran-ng            7.5.0               hdf63c60_16    conda-forge
     libgomp                   9.3.0               h24d8f2e_16    conda-forge
     liblapack                 3.8.0               17_openblas    conda-forge
     libllvm8                  8.0.1                hc9558a2_0    conda-forge
     libllvm9                  9.0.1                he513fc3_1    conda-forge
     libnghttp2                1.41.0               h8cfc5f6_2    conda-forge
     libopenblas               0.3.10          pthreads_hb3c22a3_4    conda-forge
     libpng                    1.6.37               hed695b0_2    conda-forge
     libprotobuf               3.12.4               h8b12597_0    conda-forge
     librmm                    0.16.0a200826   cuda10.2_g759afdf_267    rapidsai-nightly
     libsodium                 1.0.18               h516909a_0    conda-forge
     libssh2                   1.9.0                hab1572f_5    conda-forge
     libstdcxx-ng              9.3.0               hdf63c60_16    conda-forge
     libtiff                   4.1.0                hc7e4089_6    conda-forge
     libutf8proc               2.5.0                h516909a_2    conda-forge
     libuv                     1.39.0               h516909a_0    conda-forge
     libwebp-base              1.1.0                h516909a_3    conda-forge
     llvmlite                  0.33.0           py37h5202443_1    conda-forge
     locket                    0.2.0                      py_2    conda-forge
     lz4-c                     1.9.2                he1b5a44_3    conda-forge
     markdown                  3.2.2                      py_0    conda-forge
     markupsafe                1.1.1            py37h8f50634_1    conda-forge
     mccabe                    0.6.1                      py_1    conda-forge
     mimesis                   4.0.0              pyh9f0ad1d_0    conda-forge
     mistune                   0.8.4           py37h8f50634_1001    conda-forge
     more-itertools            8.4.0                      py_0    conda-forge
     msgpack-python            1.0.0            py37h99015e2_1    conda-forge
     nbconvert                 5.6.1            py37hc8dfbb8_1    conda-forge
     nbformat                  5.0.7                      py_0    conda-forge
     nbsphinx                  0.7.1              pyh9f0ad1d_0    conda-forge
     nccl                      2.7.8.1              hc6a2c23_0    conda-forge
     ncurses                   6.2                  he1b5a44_1    conda-forge
     nodeenv                   1.5.0              pyh9f0ad1d_0    conda-forge
     notebook                  6.1.4            py37hc8dfbb8_0    conda-forge
     numba                     0.50.1           py37h0da4684_1    conda-forge
     numpy                     1.19.1           py37h7ea13bd_2    conda-forge
     numpydoc                  1.1.0              pyh9f0ad1d_0    conda-forge
     olefile                   0.46                       py_0    conda-forge
     openssl                   1.1.1g               h516909a_1    conda-forge
     packaging                 20.4               pyh9f0ad1d_0    conda-forge
     pandas                    1.1.2            py37h3340039_0    conda-forge
     pandoc                    1.19.2                        0    conda-forge
     pandocfilters             1.4.2                      py_1    conda-forge
     parquet-cpp               1.5.1                         2    conda-forge
     parso                     0.7.1              pyh9f0ad1d_0    conda-forge
     partd                     1.1.0                      py_0    conda-forge
     pathspec                  0.8.0              pyh9f0ad1d_0    conda-forge
     pexpect                   4.8.0            py37hc8dfbb8_1    conda-forge
     pickleshare               0.7.5           py37hc8dfbb8_1001    conda-forge
     pillow                    7.2.0            py37h718be6c_1    conda-forge
     pip                       20.2.3                     py_0    conda-forge
     pluggy                    0.13.1           py37hc8dfbb8_2    conda-forge
     pre-commit                2.7.1            py37hc8dfbb8_0    conda-forge
     pre_commit                2.7.1                         0    conda-forge
     prometheus_client         0.8.0              pyh9f0ad1d_0    conda-forge
     prompt-toolkit            3.0.6                      py_0    conda-forge
     psutil                    5.7.2            py37h8f50634_0    conda-forge
     ptyprocess                0.6.0                   py_1001    conda-forge
     py                        1.9.0              pyh9f0ad1d_0    conda-forge
     pyarrow                   1.0.1           py37h1234567_1_cuda    conda-forge
     pycodestyle               2.6.0              pyh9f0ad1d_0    conda-forge
     pycparser                 2.20               pyh9f0ad1d_2    conda-forge
     pyflakes                  2.2.0              pyh9f0ad1d_0    conda-forge
     pygments                  2.6.1                      py_0    conda-forge
     pyopenssl                 19.1.0                     py_1    conda-forge
     pyparsing                 2.4.7              pyh9f0ad1d_0    conda-forge
     pyrsistent                0.16.0           py37h8f50634_0    conda-forge
     pysocks                   1.7.1            py37hc8dfbb8_1    conda-forge
     pytest                    6.0.1            py37hc8dfbb8_0    conda-forge
     python                    3.7.8           h6f2ec95_1_cpython    conda-forge
     python-dateutil           2.8.1                      py_0    conda-forge
     python_abi                3.7                     1_cp37m    conda-forge
     pytz                      2020.1             pyh9f0ad1d_0    conda-forge
     pyyaml                    5.3.1            py37h8f50634_0    conda-forge
     pyzmq                     19.0.2           py37hac76be4_0    conda-forge
     rapidjson                 1.1.0             he1b5a44_1002    conda-forge
     re2                       2020.08.01           he1b5a44_1    conda-forge
     readline                  8.0                  he28a2e2_2    conda-forge
     recommonmark              0.6.0                      py_0    conda-forge
     regex                     2020.7.14        py37h8f50634_0    conda-forge
     requests                  2.24.0             pyh9f0ad1d_0    conda-forge
     rhash                     1.3.6             h14c3975_1001    conda-forge
     rmm                       0.16.0a200911   cuda_10.2_py37_ge3c55f7_368    rapidsai-nightly
     send2trash                1.5.0                      py_0    conda-forge
     setuptools                49.6.0           py37hc8dfbb8_0    conda-forge
     six                       1.15.0             pyh9f0ad1d_0    conda-forge
     snappy                    1.1.8                he1b5a44_3    conda-forge
     snowballstemmer           2.0.0                      py_0    conda-forge
     sortedcontainers          2.2.2              pyh9f0ad1d_0    conda-forge
     spdlog                    1.7.0                hc9558a2_2    conda-forge
     sphinx                    3.2.1                      py_0    conda-forge
     sphinx-copybutton         0.3.0              pyh9f0ad1d_0    conda-forge
     sphinx-markdown-tables    0.0.14             pyh9f0ad1d_1    conda-forge
     sphinx_rtd_theme          0.5.0              pyh9f0ad1d_0    conda-forge
     sphinxcontrib-applehelp   1.0.2                      py_0    conda-forge
     sphinxcontrib-devhelp     1.0.2                      py_0    conda-forge
     sphinxcontrib-htmlhelp    1.0.3                      py_0    conda-forge
     sphinxcontrib-jsmath      1.0.1                      py_0    conda-forge
     sphinxcontrib-qthelp      1.0.3                      py_0    conda-forge
     sphinxcontrib-serializinghtml 1.1.4                      py_0    conda-forge
     sphinxcontrib-websupport  1.2.4              pyh9f0ad1d_0    conda-forge
     sqlite                    3.33.0               h4cf870e_0    conda-forge
     streamz                   0.5.5                    pypi_0    pypi
     tblib                     1.6.0                      py_0    conda-forge
     terminado                 0.8.3            py37hc8dfbb8_1    conda-forge
     testpath                  0.4.4                      py_0    conda-forge
     thrift-cpp                0.13.0               h62aa4f2_3    conda-forge
     tk                        8.6.10               hed695b0_0    conda-forge
     toml                      0.10.1             pyh9f0ad1d_0    conda-forge
     toolz                     0.10.0                     py_0    conda-forge
     tornado                   6.0.4            py37h8f50634_1    conda-forge
     traitlets                 4.3.3            py37hc8dfbb8_1    conda-forge
     typed-ast                 1.4.1            py37h516909a_0    conda-forge
     typing_extensions         3.7.4.2                    py_0    conda-forge
     urllib3                   1.25.10                    py_0    conda-forge
     virtualenv                20.0.20          py37hc8dfbb8_1    conda-forge
     wcwidth                   0.2.5              pyh9f0ad1d_1    conda-forge
     webencodings              0.5.1                      py_1    conda-forge
     wheel                     0.35.1             pyh9f0ad1d_0    conda-forge
     xz                        5.2.5                h516909a_1    conda-forge
     yaml                      0.2.5                h516909a_0    conda-forge
     zeromq                    4.3.2                he1b5a44_3    conda-forge
     zict                      2.0.0                      py_0    conda-forge
     zipp                      3.1.0                      py_0    conda-forge
     zlib                      1.2.11            h516909a_1007    conda-forge
     zstd                      1.4.5                h6597ccf_2    conda-forge
     
</pre></details>

**Additional context**
",2020-09-15T12:14:46Z,0,0,Karthikeyan,NVIDIA,True
62,[FEA] Improve readability of thread id based branching,"Improve readability of thread id based branches by giving them more descriptive names.

#### e.g.
```c++
if (!t) // is actually a t == 0
```
#### and
https://github.com/rapidsai/cudf/blob/57ef76927373d7260b6a0eda781e59a4c563d36e/cpp/src/io/statistics/column_stats.cu#L285
Is actually a `lane_id == 0`
As demonstrated in https://github.com/rapidsai/cudf/issues/6241#issuecomment-693125331, prefer cooperative groups for this.


#### and
https://github.com/rapidsai/cudf/blob/85cd56dfb3449140f18c7cef3a3be01ac976fd14/cpp/src/io/parquet/page_enc.cu#L1256
is actually ~`t < 32`~ `lane_id == 31`. (~I think this might be an oversight,~ ignore as it might be fixed in #6238 ).",2020-09-15T18:12:23Z,1,0,Devavret Makkar,@VoltronData,False
63,[FEA] Support multiple joins at once,"**Is your feature request related to a problem? Please describe.**

Pandas supports passing multiple dataframes to `join` at once, cudf does not.

ref: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#joining-multiple-dataframes

**Describe the solution you'd like**

To be able to join multiple dataframes efficiently in one call:
```
result = left.join([right, right2])
```

**Describe alternatives you've considered**

Can currently use `cudf.concat` in some cases:
```
df_wide = cudf.concat([...], axis=1)
```
however `concat` does not accept a `join` keyword to specify the kind of join to perform. 

",2020-09-17T19:53:00Z,0,0,Bryan Van de Ven,Nvidia,True
64,[BUG] Rolling window's apply function throws `TypingError`,"On running the apply function for `rolling` and trying to analyze array or any other variable type other than a single value I get the following error:
`TypingError: Failed in nopython mode pipeline (step: nopython frontend)`

Code to reproduce the error:
```
import cudf
import numpy as np
import math
def groll_sort(x):
    t = x.median() #np.median(x.values)
    return t
df = cudf.DataFrame()
df['a'] = (0.25, 0.3, 0.5,1,3,1,-1,3,-2)
rolling = df.rolling(window=3).apply(groll_sort)
print(rolling)
```
Note: I also tried using `t = np.median(x.values)` in the function
On running the above code i get the following error:
```
---------------------------------------------------------------------------
TypingError                               Traceback (most recent call last)
<ipython-input-6-99e3758f2d02> in <module>
      7 df = cudf.DataFrame()
      8 df['a'] = (0.25, 0.3, 0.5,1,3,1,-1,3,-2)
----> 9 rolling = df.rolling(window=3).apply(groll_sort)
     10 print(rolling)

~/miniconda3/envs/branch15/lib/python3.8/site-packages/cudf/core/window/rolling.py in apply(self, func, *args, **kwargs)
    276                 ""Handling UDF with null values is not yet supported""
    277             )
--> 278         return self._apply_agg(func)
    279
    280     def _normalize(self):

~/miniconda3/envs/branch15/lib/python3.8/site-packages/cudf/core/window/rolling.py in _apply_agg(self, agg_name)
    236             return self._apply_agg_series(self.obj, agg_name)
    237         else:
--> 238             return self._apply_agg_dataframe(self.obj, agg_name)
    239
    240     def sum(self):

~/miniconda3/envs/branch15/lib/python3.8/site-packages/cudf/core/window/rolling.py in _apply_agg_dataframe(self, df, agg_name)
    225         result_df = cudf.DataFrame({})
    226         for i, col_name in enumerate(df.columns):
--> 227             result_col = self._apply_agg_series(df[col_name], agg_name)
    228             result_df.insert(i, col_name, result_col)
    229         result_df.index = df.index

~/miniconda3/envs/branch15/lib/python3.8/site-packages/cudf/core/window/rolling.py in _apply_agg_series(self, sr, agg_name)
    201     def _apply_agg_series(self, sr, agg_name):
    202         if isinstance(self.window, int):
--> 203             result_col = libcudf.rolling.rolling(
    204                 sr._column,
    205                 None,

cudf/_lib/rolling.pyx in cudf._lib.rolling.rolling()

cudf/_lib/aggregation.pyx in cudf._lib.aggregation.make_aggregation()

cudf/_lib/aggregation.pyx in cudf._lib.aggregation._AggregationFactory.from_udf()

~/miniconda3/envs/branch15/lib/python3.8/site-packages/cudf/utils/cudautils.py in compile_udf(udf, type_signature)
    287     """"""
    288     decorated_udf = cuda.jit(udf, device=True)
--> 289     compiled = decorated_udf.compile(type_signature)
    290     ptx_code = decorated_udf.inspect_ptx(type_signature).decode(""utf-8"")
    291     output_type = numpy_support.as_dtype(compiled.signature.return_type)

~/miniconda3/envs/branch15/lib/python3.8/site-packages/numba/cuda/compiler.py in compile(self, args)
    162         """"""
    163         if args not in self._compileinfos:
--> 164             cres = compile_cuda(self.py_func, None, args, debug=self.debug,
    165                                 inline=self.inline)
    166             first_definition = not self._compileinfos

~/miniconda3/envs/branch15/lib/python3.8/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs)
     30         def _acquire_compile_lock(*args, **kwargs):
     31             with self:
---> 32                 return func(*args, **kwargs)
     33         return _acquire_compile_lock
     34

~/miniconda3/envs/branch15/lib/python3.8/site-packages/numba/cuda/compiler.py in compile_cuda(pyfunc, return_type, args, debug, inline)
     36         flags.set('forceinline')
     37     # Run compilation pipeline
---> 38     cres = compiler.compile_extra(typingctx=typingctx,
     39                                   targetctx=targetctx,
     40                                   func=pyfunc,

~/miniconda3/envs/branch15/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class)
    601     pipeline = pipeline_class(typingctx, targetctx, library,
    602                               args, return_type, flags, locals)
--> 603     return pipeline.compile_extra(func)
    604
    605

~/miniconda3/envs/branch15/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(self, func)
    337         self.state.lifted = ()
    338         self.state.lifted_from = None
--> 339         return self._compile_bytecode()
    340
    341     def compile_ir(self, func_ir, lifted=(), lifted_from=None):

~/miniconda3/envs/branch15/lib/python3.8/site-packages/numba/core/compiler.py in _compile_bytecode(self)
    399         """"""
    400         assert self.state.func_ir is None
--> 401         return self._compile_core()
    402
    403     def _compile_ir(self):

~/miniconda3/envs/branch15/lib/python3.8/site-packages/numba/core/compiler.py in _compile_core(self)
    379                 self.state.status.fail_reason = e
    380                 if is_final_pipeline:
--> 381                     raise e
    382         else:
    383             raise CompilerError(""All available pipelines exhausted"")

~/miniconda3/envs/branch15/lib/python3.8/site-packages/numba/core/compiler.py in _compile_core(self)
    370             res = None
    371             try:
--> 372                 pm.run(self.state)
    373                 if self.state.cr is not None:
    374                     break

~/miniconda3/envs/branch15/lib/python3.8/site-packages/numba/core/compiler_machinery.py in run(self, state)
    339                     (self.pipeline_name, pass_desc)
    340                 patched_exception = self._patch_error(msg, e)
--> 341                 raise patched_exception
    342
    343     def dependency_analysis(self):

~/miniconda3/envs/branch15/lib/python3.8/site-packages/numba/core/compiler_machinery.py in run(self, state)
    330                 pass_inst = _pass_registry.get(pss).pass_inst
    331                 if isinstance(pass_inst, CompilerPass):
--> 332                     self._runPass(idx, pass_inst, state)
    333                 else:
    334                     raise BaseException(""Legacy pass in use"")

~/miniconda3/envs/branch15/lib/python3.8/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs)
     30         def _acquire_compile_lock(*args, **kwargs):
     31             with self:
---> 32                 return func(*args, **kwargs)
     33         return _acquire_compile_lock
     34

~/miniconda3/envs/branch15/lib/python3.8/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state)
    289             mutated |= check(pss.run_initialization, internal_state)
    290         with SimpleTimer() as pass_time:
--> 291             mutated |= check(pss.run_pass, internal_state)
    292         with SimpleTimer() as finalize_time:
    293             mutated |= check(pss.run_finalizer, internal_state)

~/miniconda3/envs/branch15/lib/python3.8/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state)
    262
    263         def check(func, compiler_state):
--> 264             mangled = func(compiler_state)
    265             if mangled not in (True, False):
    266                 msg = (""CompilerPass implementations should return True/False. ""

~/miniconda3/envs/branch15/lib/python3.8/site-packages/numba/core/typed_passes.py in run_pass(self, state)
     90                               % (state.func_id.func_name,)):
     91             # Type inference
---> 92             typemap, return_type, calltypes = type_inference_stage(
     93                 state.typingctx,
     94                 state.func_ir,

~/miniconda3/envs/branch15/lib/python3.8/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors)
     68
     69         infer.build_constraint()
---> 70         infer.propagate(raise_errors=raise_errors)
     71         typemap, restype, calltypes = infer.unify(raise_errors=raise_errors)
     72

~/miniconda3/envs/branch15/lib/python3.8/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors)
    992                                   if isinstance(e, ForceLiteralArg)]
    993                 if not force_lit_args:
--> 994                     raise errors[0]
    995                 else:
    996                     raise reduce(operator.or_, force_lit_args)

TypingError: Failed in nopython mode pipeline (step: nopython frontend)
Unknown attribute 'median' of type array(float64, 1d, A)

File ""<ipython-input-6-99e3758f2d02>"", line 5:
def groll_sort(x):
    t = x.median() #np.median(x.values)
    ^

During: typing of get attribute at <ipython-input-6-99e3758f2d02> (5)

File ""<ipython-input-6-99e3758f2d02>"", line 5:
def groll_sort(x):
    t = x.median() #np.median(x.values)
```

The same code runs on pandas and gives the following output:
I/P:
```
import pandas
import numpy as np
def groll_sort(x):
    t = x.median()
    return t
df = pandas.DataFrame()
df['a'] = (0.25, 0.3, 0.5,1,3,1,-1,3,-2)
rolling = df.rolling(window=3).apply(groll_sort)
print(rolling)
```
O/P:
```
     a
0  NaN
1  NaN
2  0.3
3  0.5
4  1.0
5  1.0
6  1.0
7  1.0
8 -1.0
```",2020-09-18T20:42:53Z,0,0,,,False
65,[FEA] rolling median(),"Use case: Run a sliding window robust z-score as part of standard EEG preprocessing step. Point is to denoise a non-stationary signal. This is a general outlier detection procedure that could have value outside EEG. 

(There was a previous request last year but seems to have stalled: https://github.com/rapidsai/cudf/issues/2135)

The robust z-score uses the **median** for the first (expectation) and second (variance, standard deviation) moments. This is instead of the average. 

Current operation takes about 30 minutes for 10^7x20 element data frame in pandas. cuDF could bring this down to seconds. 

**Describe the solution you'd like**
Like to have a median() agg added to rolling or UDF for apply. 

For robust z the function is:

$$z_i = \kappa\frac{x_i-median(x)}{median(absolute\{(x_i-median(x))\})}$$
$$\text{where } \kappa\textrm{ := scaling factor}$$
$$x \subseteq X \text{, }X\text{ column vector in DF} $$


So, some possible solution to get the sliding robust z once we have the median working with apply are: 1) run the median twice (once of the original and then again on the median absolute values of the substracted residuals), and 2) right a custom UDF once the median function solution is known. 

**Describe alternatives you've considered**
I have tried writing a UDF but the issue is the **sort()** needed for the median calculation. I tried a numba nopython pandas rolling.apply solution. This works if I copy the windowed array (x2 = x.copy()). It spits out an error when run with cuDF. I believe it's a memory and/or broadcasting issue (I've seen two kinds of errors). If I don't copy then the pandas numba code sorts the original array and propagates this corrupt data back to the original DF.   

**Additional context**
Here is an example of the numba solution that works (this is for example to test on cuDF, of course pandas has a rolling median() agg). 

>code

```
@nb.jit(nopython=True)
def udf_median(x):
  ##version 0
  #mu = np.median(x)
  ##version 1
  x2 = x.copy()
  x2.sort()
  n = len(x2)
  k = int(n/2) 
  if n%2 == 0:
    mu = (x2[k]+x2[k+1])/2
  else:
    mu = x2[k]
  return mu



df = pd.DataFrame()
df['a'] = (-5,-3,-1,0.2,-2)
df['b'] = (5,-3,1,-0.2,-2)
print('original df')
print(df)
rolling = df.rolling(window=3,axis=0)
print(""panda call\nwin=3 rolling median"")
print(rolling.apply(udf_median, engine='numba', raw=True))
print('df after rolling call - if copy is not done, then corrupted original df')
print(df)

print(""\ncuDF call"")
df = cudf.DataFrame()
df['a'] = (-5,-3,-1,0.2,-2)
df['b'] = (5,-3,1,-0.2,-2)
rolling = df.rolling(window=3,axis=0)
print(rolling.apply(udf_median))

```

>output
original df
     a    b
0 -5.0  5.0
1 -3.0 -3.0
2 -1.0  1.0
3  0.2 -0.2
4 -2.0 -2.0
panda call
win=3 rolling median
     a    b
0  NaN  NaN
1  NaN  NaN
2 -3.0  1.0
3 -1.0 -0.2
4 -1.0 -0.2
df after rolling call - if copy is not done, then corrupted original df
(in this case the df is intact due to the copy function, comment that out to see the error)
     a    b
0 -5.0  5.0
1 -3.0 -3.0
2 -1.0  1.0
3  0.2 -0.2
4 -2.0 -2.0

cuDF call

RuntimeError                              Traceback (most recent call last)
/usr/local/lib/python3.6/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs)
    744     try:
--> 745         yield
    746     except NumbaError as e:

40 frames
RuntimeError: NRT required but not enabled

During handling of the above exception, another exception occurred:

LoweringError                             Traceback (most recent call last)
cudf/_lib/rolling.pyx in cudf._lib.rolling.rolling()

cudf/_lib/aggregation.pyx in cudf._lib.aggregation.make_aggregation()

cudf/_lib/aggregation.pyx in cudf._lib.aggregation._AggregationFactory.from_udf()

/usr/local/lib/python3.6/site-packages/numba/core/utils.py in reraise(tp, value, tb)
     79     if value.__traceback__ is not tb:
     80         raise value.with_traceback(tb)
---> 81     raise value
     82 
     83 

LoweringError: Failed in nopython mode pipeline (step: nopython mode backend)
NRT required but not enabled

File ""<ipython-input-16-2c1e7f32fcdd>"", line 6:
def udf_median(x):
    <source elided>
  ##version 1
  x2 = x.copy()
  ^

During: lowering ""$0.3 = call $0.2(func=$0.2, args=[], kws=(), vararg=None)"" at <ipython-input-16-2c1e7f32fcdd> (6)
",2020-09-20T04:50:18Z,0,0,,,False
66,[FEA] Improve escape character and quotation character parsing in Json and CSV reader.,"**Is your feature request related to a problem? Please describe.**
As of now, csv and json reader are post processing occurrences of escape character and quotation character once it parses complete string.
https://github.com/rapidsai/cudf/blob/76e2e155ce6fe2194a2bb41aeca93b48a39a55c2/cpp/src/io/csv/reader_impl.cu#L375

**Describe the solution you'd like**
We might be able to handle skipping/leaving those character while copying the data.",2020-09-23T14:41:02Z,0,0,Ram (Ramakrishna Prabhu),,False
67,[BUG] `cudf.read_csv` should not cast to floating types if there are null entries in csv,"**Describe the bug**
We seem to be reading in an integer column with nulls as a floating column. This is similar to pandas behavior but since we have support for nullable dtypes in all dtypes, should we even type-cast to float?

**Steps/Code to reproduce bug**
```python
In[20]: df = cudf.DataFrame({'a':cudf.Series([1, 2, 3], dtype='uint64')})
In[21]: df.to_csv('temp.csv')
In[22]: cudf.read_csv('temp.csv')
Out[22]: 
   Unnamed: 0  a
0           0  1
1           1  2
2           2  3
In[23]: pd.read_csv('temp.csv')
Out[23]: 
   Unnamed: 0  a
0           0  1
1           1  2
2           2  3
In[24]: df = cudf.DataFrame({'a':cudf.Series([1, 2, 3, None], dtype='uint64')})
In[25]: df.to_csv('temp.csv')
In[26]: cudf.read_csv('temp.csv')
Out[26]: 
   Unnamed: 0     a
0           0   1.0
1           1   2.0
2           2   3.0
3           3  <NA>
In[27]: pd.read_csv('temp.csv')
Out[27]: 
   Unnamed: 0    a
0           0  1.0
1           1  2.0
2           2  3.0
3           3  NaN
```

**Expected behavior**
Expected behavior is to not type-cast to float and only cast to the inferred dtype.

**Environment overview (please complete the following information)**
 - Environment location: Bare-metal
 - Method of cuDF install:  from source

**Environment details**
Please run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details
<details><summary>Click here to see environment details</summary><pre>
     
     **git***
     commit 768b67c37c2e3539f3d80d3c881ff8b1b320e56b (HEAD -> branch-0.16, upstream/branch-0.16)
     Author: GALI PREM SAGAR <sagarprem75@gmail.com>
     Date:   Thu Sep 24 09:18:00 2020 -0500
     
     Implement Fuzz tests for cuIO (#6114)
     
     * add log reporting for failed tests
     
     * add random data generation apis for all dtypes
     
     * make size as mandatory param
     
     * add random dataframe generation api
     
     * add Fuzz tests for parquet reader
     
     * add support for datetime, timedelta and category dtypes datageneration
     
     * add parquet writer tests
     
     * add csv writer and reader tests
     
     * add all dtypes support for random dataframe generation
     
     * add logging for exception
     
     * add all dtypes in CSV and parquet testing
     
     * cleanup
     
     * code cleanup
     
     * typo
     
     * add support for regression run from crash files
     
     * add json reader writer tests
     
     * fix issue related to picking random dtypes
     
     * replace prints with logging
     
     * exclude timedelta dtypes in CSV reader / writer tests
     
     * use seed in generate_input apis
     
     * add max_columns param
     
     * change param file extension to json from xml
     
     * add columns param
     
     * fix issues with regression run
     
     * add Readme for fuzz tests
     
     * add call method to fuzzer
     
     * add readme for fuzz tests
     
     * change main block code
     
     * add url
     
     * fix issue with float dtype data generation
     
     * add tips in readme
     
     * Update CHANGELOG.md
     
     * apply seeds
     
     * change the format of the dtypes_meta for rand_dataframe
     
     * re-organize tests into their respective files
     
     * move common code
     
     * use 3x faster np.radom apis instead of mimesis
     
     * make optimizations in _generate_column and fix lambda scope issues
     
     * remove redundant numpy computation
     
     * moved common code into utils.py
     
     * make max string length generation random
     
     * add todo for list dtype support
     
     * add copyright and use string buffer
     
     * change print to an error
     
     * handle keyboard interrupts and exit
     
     * replace writing to parquet directly from pyarrow
     
     * remove skipping of timedelta types in csv
     
     * give limits to datetime and duration types
     
     * parametrize max_string_length
     
     * move common code into a base class implementation
     
     * push down common logic into IOBase
     
     * use common logic from IOBase
     
     * remove unused variables
     
     * change default max_rows to 100000
     
     * add utility function to translate to pandas nullable dtypes
     
     * rename IOBase to IOFuzz
     
     * copyright
     
     * change default max_rows_size to 100000
     
     * change from saving to file to return buffer
     
     * Update python/cudf/cudf/fuzz_tests/readme.md
     
     Co-authored-by: Christopher Harris <xixonia@gmail.com>
     
     * add comparison of file contents
     
     * handle deafult param for dirs
     
     * remove redundant test code
     
     * change default value for max rows:
     
     Co-authored-by: Christopher Harris <xixonia@gmail.com>
     **git submodules***
     
     ***OS Information***
     DISTRIB_ID=Ubuntu
     DISTRIB_RELEASE=20.04
     DISTRIB_CODENAME=focal
     DISTRIB_DESCRIPTION=""Ubuntu 20.04.1 LTS""
     NAME=""Ubuntu""
     VERSION=""20.04.1 LTS (Focal Fossa)""
     ID=ubuntu
     ID_LIKE=debian
     PRETTY_NAME=""Ubuntu 20.04.1 LTS""
     VERSION_ID=""20.04""
     HOME_URL=""https://www.ubuntu.com/""
     SUPPORT_URL=""https://help.ubuntu.com/""
     BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
     PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
     VERSION_CODENAME=focal
     UBUNTU_CODENAME=focal
     Linux pgali-HP-Z8-G4-Workstation 5.4.0-48-generic #52-Ubuntu SMP Thu Sep 10 10:58:49 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
     
     ***GPU Information***
     Thu Sep 24 15:26:03 2020
     +-----------------------------------------------------------------------------+
     | NVIDIA-SMI 450.66       Driver Version: 450.66       CUDA Version: 11.0     |
     |-------------------------------+----------------------+----------------------+
     | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
     | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
     |                               |                      |               MIG M. |
     |===============================+======================+======================|
     |   0  Quadro RTX 8000     Off  | 00000000:22:00.0 Off |                  Off |
     | 33%   30C    P8     4W / 260W |    874MiB / 48601MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   1  Quadro RTX 8000     Off  | 00000000:2D:00.0  On |                  Off |
     | 33%   35C    P8    20W / 260W |    588MiB / 48592MiB |     18%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     
     +-----------------------------------------------------------------------------+
     | Processes:                                                                  |
     |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
     |        ID   ID                                                   Usage      |
     |=============================================================================|
     |    0   N/A  N/A       960      G   /usr/lib/xorg/Xorg                  4MiB |
     |    0   N/A  N/A      1820      G   /usr/lib/xorg/Xorg                  4MiB |
     |    0   N/A  N/A      5056      C   .../envs/cudf_dev/bin/python      695MiB |
     |    0   N/A  N/A      6256      C   ...ffice/program/soffice.bin      165MiB |
     |    1   N/A  N/A       960      G   /usr/lib/xorg/Xorg                 39MiB |
     |    1   N/A  N/A      1820      G   /usr/lib/xorg/Xorg                287MiB |
     |    1   N/A  N/A      2020      G   /usr/bin/gnome-shell              200MiB |
     |    1   N/A  N/A      5616      G   /usr/lib/firefox/firefox            3MiB |
     |    1   N/A  N/A      5788      G   ...AAAAAAAAA= --shared-files       44MiB |
     +-----------------------------------------------------------------------------+
     
     ***CPU***
     Architecture:                    x86_64
     CPU op-mode(s):                  32-bit, 64-bit
     Byte Order:                      Little Endian
     Address sizes:                   46 bits physical, 48 bits virtual
     CPU(s):                          12
     On-line CPU(s) list:             0-11
     Thread(s) per core:              2
     Core(s) per socket:              6
     Socket(s):                       1
     NUMA node(s):                    1
     Vendor ID:                       GenuineIntel
     CPU family:                      6
     Model:                           85
     Model name:                      Intel(R) Xeon(R) Gold 6128 CPU @ 3.40GHz
     Stepping:                        4
     CPU MHz:                         1200.141
     CPU max MHz:                     3700.0000
     CPU min MHz:                     1200.0000
     BogoMIPS:                        6800.00
     Virtualization:                  VT-x
     L1d cache:                       192 KiB
     L1i cache:                       192 KiB
     L2 cache:                        6 MiB
     L3 cache:                        19.3 MiB
     NUMA node0 CPU(s):               0-11
     Vulnerability Itlb multihit:     KVM: Vulnerable
     Vulnerability L1tf:              Mitigation; PTE Inversion
     Vulnerability Mds:               Mitigation; Clear CPU buffers; SMT vulnerable
     Vulnerability Meltdown:          Mitigation; PTI
     Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp
     Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
     Vulnerability Spectre v2:        Mitigation; Full generic retpoline, IBPB conditional, IBRS_FW, STIBP conditional, RSB filling
     Vulnerability Srbds:             Not affected
     Vulnerability Tsx async abort:   Mitigation; Clear CPU buffers; SMT vulnerable
     Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke md_clear flush_l1d
     
     ***CMake***
     /home/pgali/anaconda3/envs/cudf_dev/bin/cmake
     cmake version 3.18.2
     
     CMake suite maintained and supported by Kitware (kitware.com/cmake).
     
     ***g++***
     /usr/bin/g++
     g++ (Ubuntu 9.3.0-10ubuntu2) 9.3.0
     Copyright (C) 2019 Free Software Foundation, Inc.
     This is free software; see the source for copying conditions.  There is NO
     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
     
     
     ***nvcc***
     /usr/local/cuda/bin/nvcc
     nvcc: NVIDIA (R) Cuda compiler driver
     Copyright (c) 2005-2020 NVIDIA Corporation
     Built on Wed_Jul_22_19:09:09_PDT_2020
     Cuda compilation tools, release 11.0, V11.0.221
     Build cuda_11.0_bu.TC445_37.28845127_0
     
     ***Python***
     /home/pgali/anaconda3/envs/cudf_dev/bin/python
     Python 3.7.8
     
     ***Environment Variables***
     PATH                            : /home/pgali/anaconda3/envs/cudf_dev/bin:/home/pgali/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin
     LD_LIBRARY_PATH                 : :/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64
     NUMBAPRO_NVVM                   :
     NUMBAPRO_LIBDEVICE              :
     CONDA_PREFIX                    : /home/pgali/anaconda3/envs/cudf_dev
     PYTHON_PATH                     :
     
     ***conda packages***
     /home/pgali/anaconda3/condabin/conda
     # packages in environment at /home/pgali/anaconda3/envs/cudf_dev:
     #
     # Name                    Version                   Build  Channel
     _libgcc_mutex             0.1                 conda_forge    conda-forge
     _openmp_mutex             4.5                       1_gnu    conda-forge
     abseil-cpp                20200225.2           he1b5a44_2    conda-forge
     alabaster                 0.7.12                     py_0    conda-forge
     appdirs                   1.4.3                      py_1    conda-forge
     argon2-cffi               20.1.0           py37h8f50634_1    conda-forge
     arrow-cpp                 1.0.1           py37hf00d4d6_5_cuda    conda-forge
     arrow-cpp-proc            1.0.1                      cuda    conda-forge
     async_generator           1.10                       py_0    conda-forge
     attrs                     20.2.0             pyh9f0ad1d_0    conda-forge
     aws-c-common              0.4.57               he1b5a44_0    conda-forge
     aws-c-event-stream        0.1.6                h72b8ae1_3    conda-forge
     aws-checksums             0.1.9                h346380f_0    conda-forge
     aws-sdk-cpp               1.7.164              h69f4914_4    conda-forge
     babel                     2.8.0                      py_0    conda-forge
     backcall                  0.2.0              pyh9f0ad1d_0    conda-forge
     backports                 1.0                        py_2    conda-forge
     backports.functools_lru_cache 1.6.1                      py_0    conda-forge
     black                     19.10b0                    py_4    conda-forge
     bleach                    3.2.1              pyh9f0ad1d_0    conda-forge
     bokeh                     2.2.1            py37hc8dfbb8_0    conda-forge
     boost-cpp                 1.74.0               h9359b55_0    conda-forge
     brotli                    1.0.9                he1b5a44_0    conda-forge
     brotlipy                  0.7.0           py37h8f50634_1000    conda-forge
     bzip2                     1.0.8                h516909a_3    conda-forge
     c-ares                    1.16.1               h516909a_3    conda-forge
     ca-certificates           2020.6.20            hecda079_0    conda-forge
     certifi                   2020.6.20        py37hc8dfbb8_0    conda-forge
     cffi                      1.14.3           py37h2b28604_0    conda-forge
     cfgv                      3.2.0                      py_0    conda-forge
     chardet                   3.0.4           py37hc8dfbb8_1007    conda-forge
     clang                     8.0.1                hc9558a2_2    conda-forge
     clang-tools               8.0.1                hc9558a2_2    conda-forge
     clangxx                   8.0.1                         2    conda-forge
     click                     7.1.2              pyh9f0ad1d_0    conda-forge
     cloudpickle               1.6.0                      py_0    conda-forge
     cmake                     3.18.2               h5c55442_0    conda-forge
     cmake_setuptools          0.1.3                      py_0    rapidsai
     commonmark                0.9.1                      py_0    conda-forge
     cryptography              3.1              py37hb09aad4_0    conda-forge
     csv-diff                  0.6                      pypi_0    pypi
     cudatoolkit               11.0.221             h6bb024c_0    nvidia
     cudf                      0.16.0a0+1960.g848b4266fa          pypi_0    pypi
     cudnn                     8.0.0                cuda11.0_0    nvidia
     cupy                      7.8.0            py37h0ce7dbb_0    rapidsai
     cython                    0.29.21          py37h3340039_0    conda-forge
     cytoolz                   0.10.1           py37h516909a_0    conda-forge
     dask                      2.27.0+5.g5fa77484          pypi_0    pypi
     dask-cudf                 0.16.0a0+1956.g0d50234f35.dirty          pypi_0    pypi
     decorator                 4.4.2                      py_0    conda-forge
     defusedxml                0.6.0                      py_0    conda-forge
     dictdiffer                0.8.1                    pypi_0    pypi
     distlib                   0.3.1              pyh9f0ad1d_0    conda-forge
     distributed               2.27.0+1.g8aefbf36          pypi_0    pypi
     dlpack                    0.3                  he1b5a44_1    conda-forge
     docutils                  0.16             py37hc8dfbb8_1    conda-forge
     double-conversion         3.1.5                he1b5a44_2    conda-forge
     editdistance              0.5.3            py37h3340039_1    conda-forge
     entrypoints               0.3             py37hc8dfbb8_1001    conda-forge
     expat                     2.2.9                he1b5a44_2    conda-forge
     fastavro                  1.0.0.post1      py37h8f50634_0    conda-forge
     fastrlock                 0.5              py37h3340039_0    conda-forge
     filelock                  3.0.12             pyh9f0ad1d_0    conda-forge
     flake8                    3.8.3                      py_1    conda-forge
     flatbuffers               1.12.0               he1b5a44_0    conda-forge
     freetype                  2.10.2               he06d7ca_0    conda-forge
     fsspec                    0.8.2                      py_0    conda-forge
     future                    0.18.2           py37hc8dfbb8_1    conda-forge
     gflags                    2.2.2             he1b5a44_1004    conda-forge
     glog                      0.4.0                h49b9bf7_3    conda-forge
     gmp                       6.2.0                he1b5a44_2    conda-forge
     grpc-cpp                  1.30.2               heedbac9_0    conda-forge
     heapdict                  1.0.1                      py_0    conda-forge
     hypothesis                5.28.0                     py_0    conda-forge
     icu                       67.1                 he1b5a44_0    conda-forge
     identify                  1.5.4              pyh9f0ad1d_0    conda-forge
     idna                      2.10               pyh9f0ad1d_0    conda-forge
     imagesize                 1.2.0                      py_0    conda-forge
     importlib-metadata        1.7.0            py37hc8dfbb8_0    conda-forge
     importlib_metadata        1.7.0                         0    conda-forge
     iniconfig                 1.0.1              pyh9f0ad1d_0    conda-forge
     ipykernel                 5.3.4            py37h43977f1_0    conda-forge
     ipython                   7.18.1           py37hc6149b9_0    conda-forge
     ipython_genutils          0.2.0                      py_1    conda-forge
     isort                     5.0.7            py37hc8dfbb8_0    conda-forge
     jedi                      0.17.2           py37hc8dfbb8_0    conda-forge
     jinja2                    2.11.2             pyh9f0ad1d_0    conda-forge
     jpeg                      9d                   h516909a_0    conda-forge
     jsonschema                3.2.0            py37hc8dfbb8_1    conda-forge
     jupyter_client            6.1.7                      py_0    conda-forge
     jupyter_core              4.6.3            py37hc8dfbb8_1    conda-forge
     jupyterlab_pygments       0.1.1              pyh9f0ad1d_0    conda-forge
     krb5                      1.17.1               hfafb76e_3    conda-forge
     lcms2                     2.11                 hbd6801e_0    conda-forge
     ld_impl_linux-64          2.35                 h769bd43_9    conda-forge
     libblas                   3.8.0               17_openblas    conda-forge
     libcblas                  3.8.0               17_openblas    conda-forge
     libcurl                   7.71.1               hcdd3856_6    conda-forge
     libedit                   3.1.20191231         he28a2e2_2    conda-forge
     libev                     4.33                 h516909a_1    conda-forge
     libevent                  2.1.10               hcdb4288_2    conda-forge
     libffi                    3.2.1             he1b5a44_1007    conda-forge
     libgcc-ng                 9.3.0               h24d8f2e_16    conda-forge
     libgfortran-ng            7.5.0               hdf63c60_16    conda-forge
     libgomp                   9.3.0               h24d8f2e_16    conda-forge
     liblapack                 3.8.0               17_openblas    conda-forge
     libllvm10                 10.0.1               he513fc3_3    conda-forge
     libllvm8                  8.0.1                hc9558a2_0    conda-forge
     libnghttp2                1.41.0               h8cfc5f6_2    conda-forge
     libopenblas               0.3.10          pthreads_hb3c22a3_4    conda-forge
     libpng                    1.6.37               hed695b0_2    conda-forge
     libprotobuf               3.12.4               h8b12597_0    conda-forge
     librmm                    0.16.0a200922   cuda11.0_ge3ce8c5_384    rapidsai-nightly
     libsodium                 1.0.18               h516909a_0    conda-forge
     libssh2                   1.9.0                hab1572f_5    conda-forge
     libstdcxx-ng              9.3.0               hdf63c60_16    conda-forge
     libthrift                 0.13.0               hbe8ec66_6    conda-forge
     libtiff                   4.1.0                hc7e4089_6    conda-forge
     libutf8proc               2.5.0                h516909a_2    conda-forge
     libuv                     1.39.0               h516909a_0    conda-forge
     libwebp-base              1.1.0                h516909a_3    conda-forge
     llvmlite                  0.34.0           py37h5202443_1    conda-forge
     locket                    0.2.0                      py_2    conda-forge
     lz4-c                     1.9.2                he1b5a44_3    conda-forge
     markdown                  3.2.2                      py_0    conda-forge
     markupsafe                1.1.1            py37h8f50634_1    conda-forge
     mccabe                    0.6.1                      py_1    conda-forge
     mimesis                   4.0.0              pyh9f0ad1d_0    conda-forge
     mistune                   0.8.4           py37h8f50634_1001    conda-forge
     more-itertools            8.5.0                      py_0    conda-forge
     msgpack-python            1.0.0            py37h99015e2_1    conda-forge
     nbclient                  0.5.0                      py_0    conda-forge
     nbconvert                 6.0.5            py37hc8dfbb8_0    conda-forge
     nbformat                  5.0.7                      py_0    conda-forge
     nbsphinx                  0.7.1              pyh9f0ad1d_0    conda-forge
     nccl                      2.7.8.1              h4962215_0    nvidia
     ncurses                   6.2                  he1b5a44_1    conda-forge
     nest-asyncio              1.4.0                      py_1    conda-forge
     nodeenv                   1.5.0              pyh9f0ad1d_0    conda-forge
     notebook                  6.1.4            py37hc8dfbb8_0    conda-forge
     numba                     0.51.2           py37h9fdb41a_0    conda-forge
     numpy                     1.19.1           py37h7ea13bd_2    conda-forge
     numpydoc                  1.1.0              pyh9f0ad1d_0    conda-forge
     olefile                   0.46                       py_0    conda-forge
     openssl                   1.1.1h               h516909a_0    conda-forge
     packaging                 20.4               pyh9f0ad1d_0    conda-forge
     pandas                    1.1.2            py37h3340039_0    conda-forge
     pandoc                    1.19.2                        0    conda-forge
     pandocfilters             1.4.2                      py_1    conda-forge
     parquet-cpp               1.5.1                         2    conda-forge
     parso                     0.7.1              pyh9f0ad1d_0    conda-forge
     partd                     1.1.0                      py_0    conda-forge
     pathspec                  0.8.0              pyh9f0ad1d_0    conda-forge
     pexpect                   4.8.0            py37hc8dfbb8_1    conda-forge
     pickleshare               0.7.5           py37hc8dfbb8_1001    conda-forge
     pillow                    7.2.0            py37h718be6c_1    conda-forge
     pip                       20.2.3                     py_0    conda-forge
     pluggy                    0.13.1           py37hc8dfbb8_2    conda-forge
     pre-commit                2.7.1            py37hc8dfbb8_0    conda-forge
     pre_commit                2.7.1                         0    conda-forge
     prometheus_client         0.8.0              pyh9f0ad1d_0    conda-forge
     prompt-toolkit            3.0.7                      py_0    conda-forge
     psutil                    5.7.2            py37h8f50634_0    conda-forge
     ptyprocess                0.6.0                   py_1001    conda-forge
     py                        1.9.0              pyh9f0ad1d_0    conda-forge
     pyarrow                   1.0.1           py37h72578d1_5_cuda    conda-forge
     pycodestyle               2.6.0              pyh9f0ad1d_0    conda-forge
     pycparser                 2.20               pyh9f0ad1d_2    conda-forge
     pyflakes                  2.2.0              pyh9f0ad1d_0    conda-forge
     pygments                  2.7.1                      py_0    conda-forge
     pyopenssl                 19.1.0                     py_1    conda-forge
     pyparsing                 2.4.7              pyh9f0ad1d_0    conda-forge
     pyrsistent                0.17.3           py37h8f50634_0    conda-forge
     pysocks                   1.7.1            py37hc8dfbb8_1    conda-forge
     pytest                    6.0.2            py37hc8dfbb8_0    conda-forge
     python                    3.7.8           h6f2ec95_1_cpython    conda-forge
     python-dateutil           2.8.1                      py_0    conda-forge
     python_abi                3.7                     1_cp37m    conda-forge
     pytz                      2020.1             pyh9f0ad1d_0    conda-forge
     pyyaml                    5.3.1            py37h8f50634_0    conda-forge
     pyzmq                     19.0.2           py37hac76be4_0    conda-forge
     rapidjson                 1.1.0             he1b5a44_1002    conda-forge
     re2                       2020.08.01           he1b5a44_1    conda-forge
     readline                  8.0                  he28a2e2_2    conda-forge
     recommonmark              0.6.0                      py_0    conda-forge
     regex                     2020.7.14        py37h8f50634_0    conda-forge
     requests                  2.24.0             pyh9f0ad1d_0    conda-forge
     rhash                     1.3.6             h14c3975_1001    conda-forge
     rmm                       0.16.0a200922   cuda_11.0_py37_ge3ce8c5_384    rapidsai-nightly
     send2trash                1.5.0                      py_0    conda-forge
     setuptools                49.6.0           py37hc8dfbb8_1    conda-forge
     six                       1.15.0             pyh9f0ad1d_0    conda-forge
     snappy                    1.1.8                he1b5a44_3    conda-forge
     snowballstemmer           2.0.0                      py_0    conda-forge
     sortedcontainers          2.2.2              pyh9f0ad1d_0    conda-forge
     spdlog                    1.7.0                hc9558a2_2    conda-forge
     sphinx                    3.2.1                      py_0    conda-forge
     sphinx-copybutton         0.3.0              pyh9f0ad1d_0    conda-forge
     sphinx-markdown-tables    0.0.14             pyh9f0ad1d_1    conda-forge
     sphinx_rtd_theme          0.5.0              pyh9f0ad1d_0    conda-forge
     sphinxcontrib-applehelp   1.0.2                      py_0    conda-forge
     sphinxcontrib-devhelp     1.0.2                      py_0    conda-forge
     sphinxcontrib-htmlhelp    1.0.3                      py_0    conda-forge
     sphinxcontrib-jsmath      1.0.1                      py_0    conda-forge
     sphinxcontrib-qthelp      1.0.3                      py_0    conda-forge
     sphinxcontrib-serializinghtml 1.1.4                      py_0    conda-forge
     sphinxcontrib-websupport  1.2.4              pyh9f0ad1d_0    conda-forge
     sqlite                    3.33.0               h4cf870e_0    conda-forge
     streamz                   0.5.6                    pypi_0    pypi
     tblib                     1.6.0                      py_0    conda-forge
     terminado                 0.9.1            py37hc8dfbb8_0    conda-forge
     testpath                  0.4.4                      py_0    conda-forge
     thrift-compiler           0.13.0               hbe8ec66_6    conda-forge
     thrift-cpp                0.13.0                        6    conda-forge
     tk                        8.6.10               hed695b0_0    conda-forge
     toml                      0.10.1             pyh9f0ad1d_0    conda-forge
     toolz                     0.10.0                     py_0    conda-forge
     tornado                   6.0.4            py37h8f50634_1    conda-forge
     traitlets                 5.0.4                      py_0    conda-forge
     typed-ast                 1.4.1            py37h516909a_0    conda-forge
     typing_extensions         3.7.4.2                    py_0    conda-forge
     urllib3                   1.25.10                    py_0    conda-forge
     virtualenv                20.0.20          py37hc8dfbb8_1    conda-forge
     wcwidth                   0.2.5              pyh9f0ad1d_1    conda-forge
     webencodings              0.5.1                      py_1    conda-forge
     wheel                     0.35.1             pyh9f0ad1d_0    conda-forge
     xz                        5.2.5                h516909a_1    conda-forge
     yaml                      0.2.5                h516909a_0    conda-forge
     zeromq                    4.3.2                he1b5a44_3    conda-forge
     zict                      2.0.0                      py_0    conda-forge
     zipp                      3.1.0                      py_0    conda-forge
     zlib                      1.2.11            h516909a_1009    conda-forge
     zstd                      1.4.5                h6597ccf_2    conda-forge
     
</pre></details>

**Additional context**
Surfaced while running fuzz tests: #6001 
",2020-09-24T20:27:29Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
68,[BUG] read_avro method fails to handle avro.schema.UnionSchema,"**Describe the bug**

`read_avro` method fails to read avro file with columns that have multiple possible types.

- When `type` of a column is an array, it does not work. Example: schema ( [sample.avsc.txt](https://github.com/rapidsai/cudf/files/5279434/sample.avsc.txt) ), corresponding avro file ( [peters.avro.txt](https://github.com/rapidsai/cudf/files/5279433/peters.avro.txt) )
    ```
    {
        ""name"": ""feature9996"",
        ""type"": [
            ""null"",
            ""boolean""
        ]
    },
    ```
    - The following works though:
        ```
        {
            ""name"": ""feature9996"",
            ""type"": ""boolean""
        },
        ```
       Example schema: [samplew.avsc.txt](https://github.com/rapidsai/cudf/files/5279435/samplew.avsc.txt); Corresponding Avro file: [petersw.avro.txt](https://github.com/rapidsai/cudf/files/5279436/petersw.avro.txt)

**Steps/Code to reproduce bug**

```
import cudf
a = cudf.io.read_avro('peters.avro')
```

results in:
```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-124-330465234c02> in <module>
      1 import cudf
----> 2 a = cudf.io.read_avro('peters.avro')

/opt/conda/envs/rapids/lib/python3.7/site-packages/cudf/io/avro.py in read_avro(filepath_or_buffer, engine, columns, skip_rows, num_rows, **kwargs)
     26         return DataFrame._from_table(
     27             libcudf.avro.read_avro(
---> 28                 filepath_or_buffer, columns, skip_rows, num_rows
     29             )
     30         )

cudf/_lib/avro.pyx in cudf._lib.avro.read_avro()

cudf/_lib/avro.pyx in cudf._lib.avro.read_avro()

RuntimeError: cuDF failure at: /opt/conda/envs/rapids/conda-bld/libcudf_1598487768118/work/cpp/src/io/avro/reader_impl.cu:80: Cannot parse metadata
```



**Expected behavior**

`read_avro` should be able to read the avro file. You can use the following code to validate the avro file:

```
from avro.datafile import DataFileReader, DataFileWriter
from avro.io import DatumReader, DatumWriter

reader = DataFileReader(open(""peters.avro"", ""rb""), DatumReader())
for user in reader:
    print(user)
reader.close()
```

`pip install avro-python3` may be needed to execute the above piece of code.

**Environment overview (please complete the following information)**
 - Environment location: Docker
 - Method of cuDF install: Docker
   - If method of install is [Docker], provide `docker pull` & `docker run` commands used
```
docker pull nvcr.io/nvidia/rapidsai/rapidsai:0.15-cuda11.0-base-ubuntu18.04
docker run --gpus all --rm -it -p 7777:8888 -p 7676:8787 -p 7675:8786 nvcr.io/nvidia/rapidsai/rapidsai:0.15-cuda11.0-base-ubuntu18.04
```

**Environment details**
Please run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details

[env.log](https://github.com/rapidsai/cudf/files/5279465/env.log)

**Additional context**
N/A
",2020-09-24T22:08:07Z,0,0,Akshit Arora,NVIDIA,True
69,[BUG] read_avro method incorrectly reads avro.schema.ArraySchema,"**Describe the bug**

`read_avro` method incorrectly reads avro file with columns that have array data type.

- When `type` of a column is an array, it does not read correctly. Example: schema ( [sample.avsc.txt](https://github.com/rapidsai/cudf/files/5279551/sample.avsc.txt) ), corresponding avro file ( 
[peters.avro.txt](https://github.com/rapidsai/cudf/files/5279548/peters.avro.txt) )
    ```
    {
    ""name"": ""feature81078"",
    ""type"": 
        {
            ""type"": ""array"",
            ""items"": ""string""
        }
  },   
  ```
    - The following works though:
        ```
        {
            ""name"": ""feature9996"",
            ""type"": ""boolean""
        },
        ```
       Example schema: [samplew.avsc.txt](https://github.com/rapidsai/cudf/files/5279435/samplew.avsc.txt); Corresponding Avro file: [petersw.avro.txt](https://github.com/rapidsai/cudf/files/5279436/petersw.avro.txt)

**Steps/Code to reproduce bug**

```
import cudf
a = cudf.io.read_avro('peters.avro')
```

results in:
```
	username	tweet	feature81079	timestamp
0	Sample	Sample	True	0
1	<NA>	<NA>	False	0
2	<NA>	<NA>	False	0
3	<NA>	<NA>	False	0
4	<NA>	<NA>	False	0
5	<NA>	<NA>	False	0
6	<NA>	<NA>	False	0
7	<NA>	<NA>	False	0
8	<NA>	<NA>	False	0
9	<NA>	<NA>	False	0
```


**Expected behavior**

`read_avro` should be able to read the avro file. You can use the following code to validate the avro file:

```
from avro.datafile import DataFileReader, DataFileWriter
from avro.io import DatumReader, DatumWriter

reader = DataFileReader(open(""peters.avro"", ""rb""), DatumReader())
for user in reader:
    print(user)
reader.close()
```

`pip install avro-python3` may be needed to execute the above piece of code.

output: 
```
{'username': 'Sample', 'tweet': 'Sample', 'feature81079': True, 'feature81078': ['Sample', 'Sample', 'Sample', 'Sample', 'Sample'], 'timestamp': 2}
{'username': 'Sample', 'tweet': 'Sample', 'feature81079': True, 'feature81078': ['Sample', 'Sample', 'Sample', 'Sample', 'Sample'], 'timestamp': 2}
{'username': 'Sample', 'tweet': 'Sample', 'feature81079': True, 'feature81078': ['Sample', 'Sample', 'Sample', 'Sample', 'Sample'], 'timestamp': 2}
{'username': 'Sample', 'tweet': 'Sample', 'feature81079': True, 'feature81078': ['Sample', 'Sample', 'Sample', 'Sample', 'Sample'], 'timestamp': 2}
{'username': 'Sample', 'tweet': 'Sample', 'feature81079': True, 'feature81078': ['Sample', 'Sample', 'Sample', 'Sample', 'Sample'], 'timestamp': 2}
{'username': 'Sample', 'tweet': 'Sample', 'feature81079': True, 'feature81078': ['Sample', 'Sample', 'Sample', 'Sample', 'Sample'], 'timestamp': 2}
{'username': 'Sample', 'tweet': 'Sample', 'feature81079': True, 'feature81078': ['Sample', 'Sample', 'Sample', 'Sample', 'Sample'], 'timestamp': 2}
{'username': 'Sample', 'tweet': 'Sample', 'feature81079': True, 'feature81078': ['Sample', 'Sample', 'Sample', 'Sample', 'Sample'], 'timestamp': 2}
{'username': 'Sample', 'tweet': 'Sample', 'feature81079': True, 'feature81078': ['Sample', 'Sample', 'Sample', 'Sample', 'Sample'], 'timestamp': 2}
{'username': 'Sample', 'tweet': 'Sample', 'feature81079': True, 'feature81078': ['Sample', 'Sample', 'Sample', 'Sample', 'Sample'], 'timestamp': 2}
```

**Environment overview (please complete the following information)**
 - Environment location: Docker
 - Method of cuDF install: Docker
   - If method of install is [Docker], provide `docker pull` & `docker run` commands used
```
docker pull nvcr.io/nvidia/rapidsai/rapidsai:0.15-cuda11.0-base-ubuntu18.04
docker run --gpus all --rm -it -p 7777:8888 -p 7676:8787 -p 7675:8786 nvcr.io/nvidia/rapidsai/rapidsai:0.15-cuda11.0-base-ubuntu18.04
```

**Environment details**
Please run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details

[env.log](https://github.com/rapidsai/cudf/files/5279465/env.log)

**Additional context**
N/A",2020-09-24T22:34:02Z,0,0,Akshit Arora,NVIDIA,True
70,[BUG] read_avro method fails to handle avro.schema.MapSchema,"**Describe the bug**

`read_avro` method fails to read avro file with columns that has Map data type.

- When `type` of a column is an array, it does not work. Example: schema ( [sample.avsc.txt](https://github.com/rapidsai/cudf/files/5279666/sample.avsc.txt) ), corresponding avro file ( [peters.avro.txt](https://github.com/rapidsai/cudf/files/5279668/peters.avro.txt) )
    ```
    {
    ""name"": ""feature22183"",
    ""type"": 
        {
            ""type"": ""map"",
            ""values"": ""double""
        }
  },
    ```
    - The following works though:
        ```
        {
            ""name"": ""feature9996"",
            ""type"": ""boolean""
        },
        ```
       Example schema: [samplew.avsc.txt](https://github.com/rapidsai/cudf/files/5279435/samplew.avsc.txt); Corresponding Avro file: [petersw.avro.txt](https://github.com/rapidsai/cudf/files/5279436/petersw.avro.txt)

**Steps/Code to reproduce bug**

```
import cudf
a = cudf.io.read_avro('peters.avro')
```

results in:
```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-167-330465234c02> in <module>
      1 import cudf
----> 2 a = cudf.io.read_avro('peters.avro')

/opt/conda/envs/rapids/lib/python3.7/site-packages/cudf/io/avro.py in read_avro(filepath_or_buffer, engine, columns, skip_rows, num_rows, **kwargs)
     26         return DataFrame._from_table(
     27             libcudf.avro.read_avro(
---> 28                 filepath_or_buffer, columns, skip_rows, num_rows
     29             )
     30         )

cudf/_lib/avro.pyx in cudf._lib.avro.read_avro()

cudf/_lib/avro.pyx in cudf._lib.avro.read_avro()

RuntimeError: cuDF failure at: /opt/conda/envs/rapids/conda-bld/libcudf_1598487768118/work/cpp/src/io/avro/reader_impl.cu:80: Cannot parse metadata
```



**Expected behavior**

`read_avro` should be able to read the avro file. You can use the following code to validate the avro file:

```
from avro.datafile import DataFileReader, DataFileWriter
from avro.io import DatumReader, DatumWriter

reader = DataFileReader(open(""peters.avro"", ""rb""), DatumReader())
for user in reader:
    print(user)
reader.close()
```

`pip install avro-python3` may be needed to execute the above piece of code.

output:
```
{'username': 'Sample', 'tweet': 'Sample', 'feature81079': True, 'feature22183': {'key1': 2.5, 'key2': 2.5, 'key3': 2.5, 'key4': 2.5, 'key5': 2.5, 'key6': 2.5}, 'timestamp': 2}
{'username': 'Sample', 'tweet': 'Sample', 'feature81079': True, 'feature22183': {'key1': 2.5, 'key2': 2.5, 'key3': 2.5, 'key4': 2.5, 'key5': 2.5, 'key6': 2.5}, 'timestamp': 2}
{'username': 'Sample', 'tweet': 'Sample', 'feature81079': True, 'feature22183': {'key1': 2.5, 'key2': 2.5, 'key3': 2.5, 'key4': 2.5, 'key5': 2.5, 'key6': 2.5}, 'timestamp': 2}
{'username': 'Sample', 'tweet': 'Sample', 'feature81079': True, 'feature22183': {'key1': 2.5, 'key2': 2.5, 'key3': 2.5, 'key4': 2.5, 'key5': 2.5, 'key6': 2.5}, 'timestamp': 2}
{'username': 'Sample', 'tweet': 'Sample', 'feature81079': True, 'feature22183': {'key1': 2.5, 'key2': 2.5, 'key3': 2.5, 'key4': 2.5, 'key5': 2.5, 'key6': 2.5}, 'timestamp': 2}
{'username': 'Sample', 'tweet': 'Sample', 'feature81079': True, 'feature22183': {'key1': 2.5, 'key2': 2.5, 'key3': 2.5, 'key4': 2.5, 'key5': 2.5, 'key6': 2.5}, 'timestamp': 2}
{'username': 'Sample', 'tweet': 'Sample', 'feature81079': True, 'feature22183': {'key1': 2.5, 'key2': 2.5, 'key3': 2.5, 'key4': 2.5, 'key5': 2.5, 'key6': 2.5}, 'timestamp': 2}
{'username': 'Sample', 'tweet': 'Sample', 'feature81079': True, 'feature22183': {'key1': 2.5, 'key2': 2.5, 'key3': 2.5, 'key4': 2.5, 'key5': 2.5, 'key6': 2.5}, 'timestamp': 2}
{'username': 'Sample', 'tweet': 'Sample', 'feature81079': True, 'feature22183': {'key1': 2.5, 'key2': 2.5, 'key3': 2.5, 'key4': 2.5, 'key5': 2.5, 'key6': 2.5}, 'timestamp': 2}
{'username': 'Sample', 'tweet': 'Sample', 'feature81079': True, 'feature22183': {'key1': 2.5, 'key2': 2.5, 'key3': 2.5, 'key4': 2.5, 'key5': 2.5, 'key6': 2.5}, 'timestamp': 2}
```

**Environment overview (please complete the following information)**
 - Environment location: Docker
 - Method of cuDF install: Docker
   - If method of install is [Docker], provide `docker pull` & `docker run` commands used
```
docker pull nvcr.io/nvidia/rapidsai/rapidsai:0.15-cuda11.0-base-ubuntu18.04
docker run --gpus all --rm -it -p 7777:8888 -p 7676:8787 -p 7675:8786 nvcr.io/nvidia/rapidsai/rapidsai:0.15-cuda11.0-base-ubuntu18.04
```

**Environment details**
Please run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details

[env.log](https://github.com/rapidsai/cudf/files/5279465/env.log)

**Additional context**
N/A",2020-09-24T23:05:32Z,0,0,Akshit Arora,NVIDIA,True
71,[BUG] `cudf.read_json` is incorrectly parsing TimeStamp typed columns,"**Describe the bug**
`cudf.read_json` is failing to parse DateTime64 typed columns correctly when expected dtype is provided. 

**Steps/Code to reproduce bug**
```
>>> import cudf
>>> import pandas as pd
>>> pdf = pd.DataFrame({""a"":[45461150050, 55414521000, 4544624522000, 4546345758000, 45445254600]}, dtype='datetime64[ms]')
>>> pdf
                        a
0 1970-01-01 00:00:45.461
1 1970-01-01 00:00:55.414
2 1970-01-01 01:15:44.624
3 1970-01-01 01:15:46.345
4 1970-01-01 00:00:45.445
>>> buffer = pdf.to_json(compression='infer', lines=True, orient=""records"")
>>> buffer
'{""a"":45461}\n{""a"":55414}\n{""a"":4544624}\n{""a"":4546345}\n{""a"":45445}'
>>> df = cudf.read_json(buffer, ompression='infer', lines=True, orient=""records"", dtype=['timestamp[ms]'])
>>> df
                        a
0 1969-12-31 23:59:59.999
1 1969-12-31 23:59:59.999
2 1969-12-31 23:59:59.999
3 1969-12-31 23:59:59.999
4 1969-12-31 23:59:59.999
```
If `dtype` isn't specified, and if we cast the resulting int64 column, we get expected result
```
>>> expected_df = cudf.read_json(buffer, ompression='infer', lines=True, orient=""records"")
>>> expected_df['a'] = expected_df['a'].astype('datetime64[ms]')
>>> expected_df
                        a
0 1970-01-01 00:00:45.461
1 1970-01-01 00:00:55.414
2 1970-01-01 01:15:44.624
3 1970-01-01 01:15:46.345
4 1970-01-01 00:00:45.445
>>> 
```

**Expected behavior**
`cudf.read_json` should handle dtype arguement.
```
>>> df = cudf.read_json(buffer, ompression='infer', lines=True, orient=""records"", dtype=['timestamp[ms]'])
>>> df

                        a
0 1970-01-01 00:00:45.461
1 1970-01-01 00:00:55.414
2 1970-01-01 01:15:44.624
3 1970-01-01 01:15:46.345
4 1970-01-01 00:00:45.445
>>> 
```

**Environment overview (please complete the following information)**
 - Environment location: Bare-metal
 - Method of cuDF install: conda",2020-09-30T22:49:03Z,0,0,Ram (Ramakrishna Prabhu),,False
72,[BUG] Incorrect precision of floating values are being written to csv,"**Describe the bug**
While writing a `float64` or `float32` column to csv cudf is truncating or just writing upto precision `9` for both `float64` and `float32`. This is leading to data truncation incase of `float64` and incorrect data representation incase of `float32`.

**Steps/Code to reproduce bug**
For `float64`:
```python
In[55]: pdf = pd.DataFrame({'a':[1.1234567891234564367]})
In[56]: pdf
Out[56]: 
          a
0  1.123457
In[57]: pdf.to_csv()
Out[57]: ',a\n0,1.1234567891234564\n'         # Notice how pandas allows 16 digits after decimal for float64
In[58]: gdf = cudf.DataFrame({'a':[1.1234567891234564367]})
In[59]: gdf
Out[59]: 
          a
0  1.123457
In[60]: gdf.to_csv()
Out[60]: ',a\n0,1.123456789\n'               # cudf seems to be allowing only 9 digits after decimal for float64
In[61]: pdf['a']
Out[61]: 
0    1.123457
Name: a, dtype: float64
In[62]: gdf['a']
Out[62]: 

0    1.123457
Name: a, dtype: float64
```

For `float32`:
```python
In[41]: pdf = pd.DataFrame({'a':pd.Series([1.123456789123456], dtype='float32')})
In[42]: pdf
Out[42]: 
          a
0  1.123457
In[43]: pdf.to_csv()
Out[43]: ',a\n0,1.1234568\n'                # Notice how pandas allows 7 digits after decimal for float32
In[44]: gdf = cudf.DataFrame({'a':cudf.Series([1.123456789123456], dtype='float32')})
In[45]: gdf
Out[45]: 
          a
0  1.123457
In[46]: gdf.to_csv()
Out[46]: ',a\n0,1.123456836\n'         # cudf seems to allow upto 9 digits for float32
```

**Expected behavior**
Ideally we should be matching pandas behavior here as the resolutions of `float32` and `float64` are `1e-06` and `1e-15` respectively:

```python
In[67]: np.finfo('float64')
Out[67]: finfo(resolution=1e-15, min=-1.7976931348623157e+308, max=1.7976931348623157e+308, dtype=float64)
In[68]: np.finfo('float32')
Out[68]: finfo(resolution=1e-06, min=-3.4028235e+38, max=3.4028235e+38, dtype=float32)
```

This difference in precision could lead to inequality while comparing data, for example:


[cudf.txt](https://github.com/rapidsai/cudf/files/5323701/cudf.txt)
[pandas.txt](https://github.com/rapidsai/cudf/files/5323702/pandas.txt)

Diff of these two files: https://www.diffchecker.com/qaAbEXt7

```python
In[69]: df1 = pd.read_csv('cudf.txt')
In[70]: df2 = pd.read_csv('pandas.txt')
In[71]: df1
Out[71]: 
      Unnamed: 0              0
0              0            NaN
1              1  1.454085e+308
2              2 -1.237198e+308
3              3  7.527956e+307
4              4  7.821195e+307
          ...            ...
9576        9576 -1.138957e+308
9577        9577            NaN
9578        9578  5.832193e+307
9579        9579 -9.524882e+307
9580        9580 -6.446848e+307

[9581 rows x 2 columns]
In[72]: df2
Out[72]: 
      Unnamed: 0              0
0              0            NaN
1              1  1.454085e+308
2              2 -1.237198e+308
3              3  7.527956e+307
4              4  7.821195e+307
          ...            ...
9576        9576 -1.138957e+308
9577        9577            NaN
9578        9578  5.832193e+307
9579        9579 -9.524882e+307
9580        9580 -6.446848e+307

[9581 rows x 2 columns]
In[73]: df1.equals(df2)
Out[73]: False
In[74]: df1 == df2
Out[74]: 
      Unnamed: 0      0
0           True  False
1           True  False
2           True  False
3           True  False
4           True  False
          ...    ...
9576        True  False
9577        True  False
9578        True  False
9579        True  False
9580        True  False

[9581 rows x 2 columns]
In[75]: df3 = df2.copy(deep=True)
In[76]: df2 == df3
Out[76]: 
      Unnamed: 0      0
0           True  False
1           True   True
2           True   True
3           True   True
4           True   True
          ...    ...
9576        True   True
9577        True  False
9578        True   True
9579        True   True
9580        True   True

[9581 rows x 2 columns]
```

**Environment overview (please complete the following information)**
 - Environment location: Bare-metal
 - Method of cuDF install:  from source(`0.16`)


**Environment details**
Please run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details
<details><summary>Click here to see environment details</summary><pre>
     
     **git***
     commit fd59ee71ac16a27bc66f7a6ac34f11abf359071e (HEAD -> csv_fuzz_tests)
     Merge: a66c8d02c9 78370a852c
     Author: galipremsagar <sagarprem75@gmail.com>
     Date:   Sun Oct 4 10:39:49 2020 -0500
     
     Merge remote-tracking branch 'upstream/branch-0.16' into csv_fuzz_tests
     **git submodules***
     
     ***OS Information***
     DISTRIB_ID=Ubuntu
     DISTRIB_RELEASE=20.04
     DISTRIB_CODENAME=focal
     DISTRIB_DESCRIPTION=""Ubuntu 20.04.1 LTS""
     NAME=""Ubuntu""
     VERSION=""20.04.1 LTS (Focal Fossa)""
     ID=ubuntu
     ID_LIKE=debian
     PRETTY_NAME=""Ubuntu 20.04.1 LTS""
     VERSION_ID=""20.04""
     HOME_URL=""https://www.ubuntu.com/""
     SUPPORT_URL=""https://help.ubuntu.com/""
     BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
     PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
     VERSION_CODENAME=focal
     UBUNTU_CODENAME=focal
     Linux pgali-HP-Z8-G4-Workstation 5.4.0-48-generic #52-Ubuntu SMP Thu Sep 10 10:58:49 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
     
     ***GPU Information***
     Sun Oct  4 12:04:42 2020
     +-----------------------------------------------------------------------------+
     | NVIDIA-SMI 450.66       Driver Version: 450.66       CUDA Version: 11.0     |
     |-------------------------------+----------------------+----------------------+
     | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
     | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
     |                               |                      |               MIG M. |
     |===============================+======================+======================|
     |   0  Quadro RTX 8000     Off  | 00000000:22:00.0 Off |                  Off |
     | 33%   33C    P8     5W / 260W |   2392MiB / 48601MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   1  Quadro RTX 8000     Off  | 00000000:2D:00.0  On |                  Off |
     | 33%   36C    P8    24W / 260W |    567MiB / 48592MiB |     11%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     
     +-----------------------------------------------------------------------------+
     | Processes:                                                                  |
     |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
     |        ID   ID                                                   Usage      |
     |=============================================================================|
     |    0   N/A  N/A       986      G   /usr/lib/xorg/Xorg                  4MiB |
     |    0   N/A  N/A      1766      G   /usr/lib/xorg/Xorg                  4MiB |
     |    0   N/A  N/A      9601      C   .../envs/cudf_dev/bin/python      597MiB |
     |    0   N/A  N/A     12915      C   python                            595MiB |
     |    0   N/A  N/A     13516      C   python                            595MiB |
     |    0   N/A  N/A     14098      C   python                            591MiB |
     |    1   N/A  N/A       986      G   /usr/lib/xorg/Xorg                 39MiB |
     |    1   N/A  N/A      1766      G   /usr/lib/xorg/Xorg                267MiB |
     |    1   N/A  N/A      1971      G   /usr/bin/gnome-shell              236MiB |
     |    1   N/A  N/A     11515      G   /usr/lib/firefox/firefox            3MiB |
     |    1   N/A  N/A     12283      G   /usr/lib/firefox/firefox            3MiB |
     |    1   N/A  N/A     15310      G   /usr/lib/firefox/firefox            3MiB |
     +-----------------------------------------------------------------------------+
     
     ***CPU***
     Architecture:                    x86_64
     CPU op-mode(s):                  32-bit, 64-bit
     Byte Order:                      Little Endian
     Address sizes:                   46 bits physical, 48 bits virtual
     CPU(s):                          12
     On-line CPU(s) list:             0-11
     Thread(s) per core:              2
     Core(s) per socket:              6
     Socket(s):                       1
     NUMA node(s):                    1
     Vendor ID:                       GenuineIntel
     CPU family:                      6
     Model:                           85
     Model name:                      Intel(R) Xeon(R) Gold 6128 CPU @ 3.40GHz
     Stepping:                        4
     CPU MHz:                         1258.060
     CPU max MHz:                     3700.0000
     CPU min MHz:                     1200.0000
     BogoMIPS:                        6800.00
     Virtualization:                  VT-x
     L1d cache:                       192 KiB
     L1i cache:                       192 KiB
     L2 cache:                        6 MiB
     L3 cache:                        19.3 MiB
     NUMA node0 CPU(s):               0-11
     Vulnerability Itlb multihit:     KVM: Vulnerable
     Vulnerability L1tf:              Mitigation; PTE Inversion
     Vulnerability Mds:               Mitigation; Clear CPU buffers; SMT vulnerable
     Vulnerability Meltdown:          Mitigation; PTI
     Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp
     Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
     Vulnerability Spectre v2:        Mitigation; Full generic retpoline, IBPB conditional, IBRS_FW, STIBP conditional, RSB filling
     Vulnerability Srbds:             Not affected
     Vulnerability Tsx async abort:   Mitigation; Clear CPU buffers; SMT vulnerable
     Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke md_clear flush_l1d
     
     ***CMake***
     /home/pgali/anaconda3/envs/cudf_dev/bin/cmake
     cmake version 3.18.2
     
     CMake suite maintained and supported by Kitware (kitware.com/cmake).
     
     ***g++***
     /usr/bin/g++
     g++ (Ubuntu 9.3.0-10ubuntu2) 9.3.0
     Copyright (C) 2019 Free Software Foundation, Inc.
     This is free software; see the source for copying conditions.  There is NO
     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
     
     
     ***nvcc***
     /usr/local/cuda/bin/nvcc
     nvcc: NVIDIA (R) Cuda compiler driver
     Copyright (c) 2005-2020 NVIDIA Corporation
     Built on Wed_Jul_22_19:09:09_PDT_2020
     Cuda compilation tools, release 11.0, V11.0.221
     Build cuda_11.0_bu.TC445_37.28845127_0
     
     ***Python***
     /home/pgali/anaconda3/envs/cudf_dev/bin/python
     Python 3.7.8
     
     ***Environment Variables***
     PATH                            : /home/pgali/anaconda3/envs/cudf_dev/bin:/home/pgali/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin
     LD_LIBRARY_PATH                 : :/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64
     NUMBAPRO_NVVM                   :
     NUMBAPRO_LIBDEVICE              :
     CONDA_PREFIX                    : /home/pgali/anaconda3/envs/cudf_dev
     PYTHON_PATH                     :
     
     ***conda packages***
     /home/pgali/anaconda3/condabin/conda
     # packages in environment at /home/pgali/anaconda3/envs/cudf_dev:
     #
     # Name                    Version                   Build  Channel
     _libgcc_mutex             0.1                 conda_forge    conda-forge
     _openmp_mutex             4.5                       1_gnu    conda-forge
     abseil-cpp                20200225.2           he1b5a44_2    conda-forge
     alabaster                 0.7.12                     py_0    conda-forge
     appdirs                   1.4.3                      py_1    conda-forge
     argon2-cffi               20.1.0           py37h8f50634_1    conda-forge
     arrow-cpp                 1.0.1           py37hf00d4d6_5_cuda    conda-forge
     arrow-cpp-proc            1.0.1                      cuda    conda-forge
     async_generator           1.10                       py_0    conda-forge
     attrs                     20.2.0             pyh9f0ad1d_0    conda-forge
     aws-c-common              0.4.57               he1b5a44_0    conda-forge
     aws-c-event-stream        0.1.6                h72b8ae1_3    conda-forge
     aws-checksums             0.1.9                h346380f_0    conda-forge
     aws-sdk-cpp               1.7.164              h69f4914_4    conda-forge
     babel                     2.8.0                      py_0    conda-forge
     backcall                  0.2.0              pyh9f0ad1d_0    conda-forge
     backports                 1.0                        py_2    conda-forge
     backports.functools_lru_cache 1.6.1                      py_0    conda-forge
     black                     19.10b0                    py_4    conda-forge
     bleach                    3.2.1              pyh9f0ad1d_0    conda-forge
     bokeh                     2.2.1            py37hc8dfbb8_0    conda-forge
     boost-cpp                 1.74.0               h9359b55_0    conda-forge
     brotli                    1.0.9                he1b5a44_0    conda-forge
     brotlipy                  0.7.0           py37h8f50634_1000    conda-forge
     bzip2                     1.0.8                h516909a_3    conda-forge
     c-ares                    1.16.1               h516909a_3    conda-forge
     ca-certificates           2020.6.20            hecda079_0    conda-forge
     certifi                   2020.6.20        py37hc8dfbb8_0    conda-forge
     cffi                      1.14.3           py37h2b28604_0    conda-forge
     cfgv                      3.2.0                      py_0    conda-forge
     chardet                   3.0.4           py37hc8dfbb8_1007    conda-forge
     clang                     8.0.1                hc9558a2_2    conda-forge
     clang-tools               8.0.1                hc9558a2_2    conda-forge
     clangxx                   8.0.1                         2    conda-forge
     click                     7.1.2              pyh9f0ad1d_0    conda-forge
     cloudpickle               1.6.0                      py_0    conda-forge
     cmake                     3.18.2               h5c55442_0    conda-forge
     cmake_setuptools          0.1.3                      py_0    rapidsai
     commonmark                0.9.1                      py_0    conda-forge
     cryptography              3.1              py37hb09aad4_0    conda-forge
     csv-diff                  0.6                      pypi_0    pypi
     cudatoolkit               11.0.221             h6bb024c_0    nvidia
     cudf                      0.16.0a0+1921.gc436501650          pypi_0    pypi
     cudnn                     8.0.0                cuda11.0_0    nvidia
     cupy                      7.8.0            py37h0ce7dbb_0    rapidsai
     cython                    0.29.21          py37h3340039_0    conda-forge
     cytoolz                   0.10.1           py37h516909a_0    conda-forge
     dask                      2.27.0+5.g5fa77484          pypi_0    pypi
     dask-cudf                 0.16.0a0+1925.ge08b284b76.dirty          pypi_0    pypi
     decorator                 4.4.2                      py_0    conda-forge
     defusedxml                0.6.0                      py_0    conda-forge
     dictdiffer                0.8.1                    pypi_0    pypi
     distlib                   0.3.1              pyh9f0ad1d_0    conda-forge
     distributed               2.27.0+1.g8aefbf36          pypi_0    pypi
     dlpack                    0.3                  he1b5a44_1    conda-forge
     docutils                  0.16             py37hc8dfbb8_1    conda-forge
     double-conversion         3.1.5                he1b5a44_2    conda-forge
     editdistance              0.5.3            py37h3340039_1    conda-forge
     entrypoints               0.3             py37hc8dfbb8_1001    conda-forge
     expat                     2.2.9                he1b5a44_2    conda-forge
     fastavro                  1.0.0.post1      py37h8f50634_0    conda-forge
     fastrlock                 0.5              py37h3340039_0    conda-forge
     filelock                  3.0.12             pyh9f0ad1d_0    conda-forge
     flake8                    3.8.3                      py_1    conda-forge
     flatbuffers               1.12.0               he1b5a44_0    conda-forge
     freetype                  2.10.2               he06d7ca_0    conda-forge
     fsspec                    0.8.2                      py_0    conda-forge
     future                    0.18.2           py37hc8dfbb8_1    conda-forge
     gflags                    2.2.2             he1b5a44_1004    conda-forge
     glog                      0.4.0                h49b9bf7_3    conda-forge
     gmp                       6.2.0                he1b5a44_2    conda-forge
     grpc-cpp                  1.30.2               heedbac9_0    conda-forge
     heapdict                  1.0.1                      py_0    conda-forge
     hypothesis                5.28.0                     py_0    conda-forge
     icu                       67.1                 he1b5a44_0    conda-forge
     identify                  1.5.4              pyh9f0ad1d_0    conda-forge
     idna                      2.10               pyh9f0ad1d_0    conda-forge
     imagesize                 1.2.0                      py_0    conda-forge
     importlib-metadata        1.7.0            py37hc8dfbb8_0    conda-forge
     importlib_metadata        1.7.0                         0    conda-forge
     iniconfig                 1.0.1              pyh9f0ad1d_0    conda-forge
     ipykernel                 5.3.4            py37h43977f1_0    conda-forge
     ipython                   7.18.1           py37hc6149b9_0    conda-forge
     ipython_genutils          0.2.0                      py_1    conda-forge
     isort                     5.0.7            py37hc8dfbb8_0    conda-forge
     jedi                      0.17.2           py37hc8dfbb8_0    conda-forge
     jinja2                    2.11.2             pyh9f0ad1d_0    conda-forge
     jpeg                      9d                   h516909a_0    conda-forge
     jsonschema                3.2.0            py37hc8dfbb8_1    conda-forge
     jupyter_client            6.1.7                      py_0    conda-forge
     jupyter_core              4.6.3            py37hc8dfbb8_1    conda-forge
     jupyterlab_pygments       0.1.1              pyh9f0ad1d_0    conda-forge
     krb5                      1.17.1               hfafb76e_3    conda-forge
     lcms2                     2.11                 hbd6801e_0    conda-forge
     ld_impl_linux-64          2.35                 h769bd43_9    conda-forge
     libblas                   3.8.0               17_openblas    conda-forge
     libcblas                  3.8.0               17_openblas    conda-forge
     libcurl                   7.71.1               hcdd3856_6    conda-forge
     libedit                   3.1.20191231         he28a2e2_2    conda-forge
     libev                     4.33                 h516909a_1    conda-forge
     libevent                  2.1.10               hcdb4288_2    conda-forge
     libffi                    3.2.1             he1b5a44_1007    conda-forge
     libgcc-ng                 9.3.0               h24d8f2e_16    conda-forge
     libgfortran-ng            7.5.0               hdf63c60_16    conda-forge
     libgomp                   9.3.0               h24d8f2e_16    conda-forge
     liblapack                 3.8.0               17_openblas    conda-forge
     libllvm10                 10.0.1               he513fc3_3    conda-forge
     libllvm8                  8.0.1                hc9558a2_0    conda-forge
     libnghttp2                1.41.0               h8cfc5f6_2    conda-forge
     libopenblas               0.3.10          pthreads_hb3c22a3_4    conda-forge
     libpng                    1.6.37               hed695b0_2    conda-forge
     libprotobuf               3.12.4               h8b12597_0    conda-forge
     librmm                    0.16.0a200922   cuda11.0_ge3ce8c5_384    rapidsai-nightly
     libsodium                 1.0.18               h516909a_0    conda-forge
     libssh2                   1.9.0                hab1572f_5    conda-forge
     libstdcxx-ng              9.3.0               hdf63c60_16    conda-forge
     libthrift                 0.13.0               hbe8ec66_6    conda-forge
     libtiff                   4.1.0                hc7e4089_6    conda-forge
     libutf8proc               2.5.0                h516909a_2    conda-forge
     libuv                     1.39.0               h516909a_0    conda-forge
     libwebp-base              1.1.0                h516909a_3    conda-forge
     llvmlite                  0.34.0           py37h5202443_1    conda-forge
     locket                    0.2.0                      py_2    conda-forge
     lz4-c                     1.9.2                he1b5a44_3    conda-forge
     markdown                  3.2.2                      py_0    conda-forge
     markupsafe                1.1.1            py37h8f50634_1    conda-forge
     mccabe                    0.6.1                      py_1    conda-forge
     mimesis                   4.0.0              pyh9f0ad1d_0    conda-forge
     mistune                   0.8.4           py37h8f50634_1001    conda-forge
     more-itertools            8.5.0                      py_0    conda-forge
     msgpack-python            1.0.0            py37h99015e2_1    conda-forge
     nbclient                  0.5.0                      py_0    conda-forge
     nbconvert                 6.0.5            py37hc8dfbb8_0    conda-forge
     nbformat                  5.0.7                      py_0    conda-forge
     nbsphinx                  0.7.1              pyh9f0ad1d_0    conda-forge
     nccl                      2.7.8.1              h4962215_0    nvidia
     ncurses                   6.2                  he1b5a44_1    conda-forge
     nest-asyncio              1.4.0                      py_1    conda-forge
     nodeenv                   1.5.0              pyh9f0ad1d_0    conda-forge
     notebook                  6.1.4            py37hc8dfbb8_0    conda-forge
     numba                     0.51.2           py37h9fdb41a_0    conda-forge
     numpy                     1.19.1           py37h7ea13bd_2    conda-forge
     numpydoc                  1.1.0              pyh9f0ad1d_0    conda-forge
     olefile                   0.46                       py_0    conda-forge
     openssl                   1.1.1h               h516909a_0    conda-forge
     packaging                 20.4               pyh9f0ad1d_0    conda-forge
     pandas                    1.1.2            py37h3340039_0    conda-forge
     pandoc                    1.19.2                        0    conda-forge
     pandocfilters             1.4.2                      py_1    conda-forge
     parquet-cpp               1.5.1                         2    conda-forge
     parso                     0.7.1              pyh9f0ad1d_0    conda-forge
     partd                     1.1.0                      py_0    conda-forge
     pathspec                  0.8.0              pyh9f0ad1d_0    conda-forge
     pexpect                   4.8.0            py37hc8dfbb8_1    conda-forge
     pickleshare               0.7.5           py37hc8dfbb8_1001    conda-forge
     pillow                    7.2.0            py37h718be6c_1    conda-forge
     pip                       20.2.3                     py_0    conda-forge
     pluggy                    0.13.1           py37hc8dfbb8_2    conda-forge
     pre-commit                2.7.1            py37hc8dfbb8_0    conda-forge
     pre_commit                2.7.1                         0    conda-forge
     prometheus_client         0.8.0              pyh9f0ad1d_0    conda-forge
     prompt-toolkit            3.0.7                      py_0    conda-forge
     psutil                    5.7.2            py37h8f50634_0    conda-forge
     ptyprocess                0.6.0                   py_1001    conda-forge
     py                        1.9.0              pyh9f0ad1d_0    conda-forge
     pyarrow                   1.0.1           py37h72578d1_5_cuda    conda-forge
     pycodestyle               2.6.0              pyh9f0ad1d_0    conda-forge
     pycparser                 2.20               pyh9f0ad1d_2    conda-forge
     pyflakes                  2.2.0              pyh9f0ad1d_0    conda-forge
     pygments                  2.7.1                      py_0    conda-forge
     pyopenssl                 19.1.0                     py_1    conda-forge
     pyparsing                 2.4.7              pyh9f0ad1d_0    conda-forge
     pyrsistent                0.17.3           py37h8f50634_0    conda-forge
     pysocks                   1.7.1            py37hc8dfbb8_1    conda-forge
     pytest                    6.0.2            py37hc8dfbb8_0    conda-forge
     python                    3.7.8           h6f2ec95_1_cpython    conda-forge
     python-dateutil           2.8.1                      py_0    conda-forge
     python_abi                3.7                     1_cp37m    conda-forge
     pytz                      2020.1             pyh9f0ad1d_0    conda-forge
     pyyaml                    5.3.1            py37h8f50634_0    conda-forge
     pyzmq                     19.0.2           py37hac76be4_0    conda-forge
     rapidjson                 1.1.0             he1b5a44_1002    conda-forge
     re2                       2020.08.01           he1b5a44_1    conda-forge
     readline                  8.0                  he28a2e2_2    conda-forge
     recommonmark              0.6.0                      py_0    conda-forge
     regex                     2020.7.14        py37h8f50634_0    conda-forge
     requests                  2.24.0             pyh9f0ad1d_0    conda-forge
     rhash                     1.3.6             h14c3975_1001    conda-forge
     rmm                       0.16.0a200922   cuda_11.0_py37_ge3ce8c5_384    rapidsai-nightly
     send2trash                1.5.0                      py_0    conda-forge
     setuptools                49.6.0           py37hc8dfbb8_1    conda-forge
     six                       1.15.0             pyh9f0ad1d_0    conda-forge
     snappy                    1.1.8                he1b5a44_3    conda-forge
     snowballstemmer           2.0.0                      py_0    conda-forge
     sortedcontainers          2.2.2              pyh9f0ad1d_0    conda-forge
     spdlog                    1.7.0                hc9558a2_2    conda-forge
     sphinx                    3.2.1                      py_0    conda-forge
     sphinx-copybutton         0.3.0              pyh9f0ad1d_0    conda-forge
     sphinx-markdown-tables    0.0.14             pyh9f0ad1d_1    conda-forge
     sphinx_rtd_theme          0.5.0              pyh9f0ad1d_0    conda-forge
     sphinxcontrib-applehelp   1.0.2                      py_0    conda-forge
     sphinxcontrib-devhelp     1.0.2                      py_0    conda-forge
     sphinxcontrib-htmlhelp    1.0.3                      py_0    conda-forge
     sphinxcontrib-jsmath      1.0.1                      py_0    conda-forge
     sphinxcontrib-qthelp      1.0.3                      py_0    conda-forge
     sphinxcontrib-serializinghtml 1.1.4                      py_0    conda-forge
     sphinxcontrib-websupport  1.2.4              pyh9f0ad1d_0    conda-forge
     sqlite                    3.33.0               h4cf870e_0    conda-forge
     streamz                   0.5.6                    pypi_0    pypi
     tblib                     1.6.0                      py_0    conda-forge
     terminado                 0.9.1            py37hc8dfbb8_0    conda-forge
     testpath                  0.4.4                      py_0    conda-forge
     thrift-compiler           0.13.0               hbe8ec66_6    conda-forge
     thrift-cpp                0.13.0                        6    conda-forge
     tk                        8.6.10               hed695b0_0    conda-forge
     toml                      0.10.1             pyh9f0ad1d_0    conda-forge
     toolz                     0.10.0                     py_0    conda-forge
     tornado                   6.0.4            py37h8f50634_1    conda-forge
     traitlets                 5.0.4                      py_0    conda-forge
     typed-ast                 1.4.1            py37h516909a_0    conda-forge
     typing_extensions         3.7.4.2                    py_0    conda-forge
     urllib3                   1.25.10                    py_0    conda-forge
     virtualenv                20.0.20          py37hc8dfbb8_1    conda-forge
     wcwidth                   0.2.5              pyh9f0ad1d_1    conda-forge
     webencodings              0.5.1                      py_1    conda-forge
     wheel                     0.35.1             pyh9f0ad1d_0    conda-forge
     xz                        5.2.5                h516909a_1    conda-forge
     yaml                      0.2.5                h516909a_0    conda-forge
     zeromq                    4.3.2                he1b5a44_3    conda-forge
     zict                      2.0.0                      py_0    conda-forge
     zipp                      3.1.0                      py_0    conda-forge
     zlib                      1.2.11            h516909a_1009    conda-forge
     zstd                      1.4.5                h6597ccf_2    conda-forge
     
</pre></details>


**Additional context**
Surfaced while running fuzz tests #6001 
",2020-10-04T17:06:40Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
73,[FEA] Support constructing `datetime64[ms]` series with `pd.NaT`,"**Is your feature request related to a problem? Please describe.**
Currently when I construct a datetime series with `pandas.NaT`:

```python
import cudf as gd
import pandas as pd
gd.Series([""2020-05-01 08:00:00"", pd.NaT], dtype=""<M8[ms]"")
```

It throws:
```
Traceback (most recent call last):
  File ""/home/wangm/dev/rapids/cudf/python/cudf/cudf/core/column/column.py"", line 1772, in as_column
    memoryview(arbitrary), dtype=dtype, nan_as_null=nan_as_null
TypeError: memoryview: a bytes-like object is required, not 'list'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/wangm/dev/rapids/cudf/python/cudf/cudf/core/column/column.py"", line 1801, in as_column
    else nan_as_null,
  File ""pyarrow/array.pxi"", line 269, in pyarrow.lib.array
  File ""pyarrow/array.pxi"", line 38, in pyarrow.lib._sequence_to_array
TypeError: an integer is required (got type str)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/wangm/dev/rapids/cudf/python/cudf/cudf/core/series.py"", line 188, in __init__
    data = column.as_column(data, nan_as_null=nan_as_null, dtype=dtype)
  File ""/home/wangm/dev/rapids/cudf/python/cudf/cudf/core/column/column.py"", line 1823, in as_column
    else np.dtype(native_dtype),
  File ""/home/wangm/dev/rapids/compose/etc/conda/cuda_10.2/envs/rapids/lib/python3.7/site-packages/numpy/core/_asarray.py"", line 83, in asarray
    return array(a, dtype, copy=False, order=order)
ValueError: cannot convert float NaN to integer
```

**Describe the solution you'd like**
`cudf.Series` constructor works with null values, which match with pandas behavior.

**Describe alternatives you've considered**
```python
gd.Series([""2020-05-01 08:00:00"", None], dtype=""<M8[ms]"")
```

**Additional context**
<details><summary>Click here to see environment details</summary><pre>
     
     **git***
     commit 305f1dc6935c5257c018e129dfbc17c90739b647 (HEAD -> i6129, origin/i6129)
     Author: Michael Wang <michelwang0905@icloud.com>
     Date:   Tue Oct 6 20:37:17 2020 -0700
     
     changelog
     **git submodules***
     
     ***OS Information***
     DISTRIB_ID=Ubuntu
     DISTRIB_RELEASE=18.04
     DISTRIB_CODENAME=bionic
     DISTRIB_DESCRIPTION=""Ubuntu 18.04.5 LTS""
     NAME=""Ubuntu""
     VERSION=""18.04.5 LTS (Bionic Beaver)""
     ID=ubuntu
     ID_LIKE=debian
     PRETTY_NAME=""Ubuntu 18.04.5 LTS""
     VERSION_ID=""18.04""
     HOME_URL=""https://www.ubuntu.com/""
     SUPPORT_URL=""https://help.ubuntu.com/""
     BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
     PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
     VERSION_CODENAME=bionic
     UBUNTU_CODENAME=bionic
     Linux wangm-ws 5.4.0-48-generic #52~18.04.1-Ubuntu SMP Thu Sep 10 12:50:22 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
     
     ***GPU Information***
     Tue Oct  6 20:57:30 2020
     +-----------------------------------------------------------------------------+
     | NVIDIA-SMI 450.66       Driver Version: 450.66       CUDA Version: 11.0     |
     |-------------------------------+----------------------+----------------------+
     | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
     | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
     |                               |                      |               MIG M. |
     |===============================+======================+======================|
     |   0  Quadro RTX 8000     Off  | 00000000:15:00.0 Off |                  Off |
     | 33%   40C    P8    28W / 260W |    592MiB / 48601MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     
     +-----------------------------------------------------------------------------+
     | Processes:                                                                  |
     |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
     |        ID   ID                                                   Usage      |
     |=============================================================================|
     +-----------------------------------------------------------------------------+
     
     ***CPU***
     Architecture:        x86_64
     CPU op-mode(s):      32-bit, 64-bit
     Byte Order:          Little Endian
     CPU(s):              12
     On-line CPU(s) list: 0-11
     Thread(s) per core:  2
     Core(s) per socket:  6
     Socket(s):           1
     NUMA node(s):        1
     Vendor ID:           GenuineIntel
     CPU family:          6
     Model:               85
     Model name:          Intel(R) Xeon(R) Gold 6128 CPU @ 3.40GHz
     Stepping:            4
     CPU MHz:             1270.621
     CPU max MHz:         3700.0000
     CPU min MHz:         1200.0000
     BogoMIPS:            6800.00
     Virtualization:      VT-x
     L1d cache:           32K
     L1i cache:           32K
     L2 cache:            1024K
     L3 cache:            19712K
     NUMA node0 CPU(s):   0-11
     Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke md_clear flush_l1d
     
     ***CMake***
     /home/wangm/dev/rapids/compose/etc/conda/cuda_10.2/envs/rapids/bin/cmake
     cmake version 3.17.0
     
     CMake suite maintained and supported by Kitware (kitware.com/cmake).
     
     ***g++***
     /usr/local/bin/g++
     g++ (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
     Copyright (C) 2017 Free Software Foundation, Inc.
     This is free software; see the source for copying conditions.  There is NO
     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
     
     
     ***nvcc***
     /usr/local/bin/nvcc
     nvcc: NVIDIA (R) Cuda compiler driver
     Copyright (c) 2005-2019 NVIDIA Corporation
     Built on Wed_Oct_23_19:24:38_PDT_2019
     Cuda compilation tools, release 10.2, V10.2.89
     
     ***Python***
     /home/wangm/dev/rapids/compose/etc/conda/cuda_10.2/envs/rapids/bin/python
     Python 3.7.8
     
     ***Environment Variables***
     PATH                            : /home/wangm/dev/rapids/compose/etc/conda/cuda_10.2/envs/rapids/bin:/home/wangm/dev/rapids/compose/etc/conda/cuda_10.2/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/cuda-10.2/bin
     LD_LIBRARY_PATH                 : /home/wangm/dev/rapids/compose/etc/conda/cuda_10.2/envs/rapids/lib:/home/wangm/dev/rapids/compose/etc/conda/cuda_10.2/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/i386-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda-10.2/lib64:/usr/local/lib:/home/wangm/dev/rapids/rmm/build/release:/home/wangm/dev/rapids/cudf/cpp/build/release:/home/wangm/dev/rapids/cudf/cpp/build/release:/home/wangm/dev/rapids/cuml/cpp/build/release:/home/wangm/dev/rapids/cugraph/cpp/build/release:/home/wangm/dev/rapids/cuspatial/cpp/build/release
     NUMBAPRO_NVVM                   :
     NUMBAPRO_LIBDEVICE              :
     CONDA_PREFIX                    : /home/wangm/dev/rapids/compose/etc/conda/cuda_10.2/envs/rapids
     PYTHON_PATH                     :
     
     ***conda packages***
     /home/wangm/dev/rapids/compose/etc/conda/cuda_10.2/bin/conda
     # packages in environment at /home/wangm/dev/rapids/compose/etc/conda/cuda_10.2/envs/rapids:
     #
     # Name                    Version                   Build  Channel
     _libgcc_mutex             0.1                 conda_forge    conda-forge
     _openmp_mutex             4.5                       1_gnu    conda-forge
     abseil-cpp                20200225.2           he1b5a44_2    conda-forge
     alabaster                 0.7.12                     py_0    conda-forge
     apipkg                    1.5                      pypi_0    pypi
     appdirs                   1.4.3                      py_1    conda-forge
     argon2-cffi               20.1.0           py37h8f50634_1    conda-forge
     arrow-cpp                 1.0.1           py37hba6904a_7_cuda    conda-forge
     arrow-cpp-proc            1.0.1                      cuda    conda-forge
     asvdb                     0.4.1               gd6cd8f2_36    rapidsai
     async_generator           1.10                       py_0    conda-forge
     attrs                     20.2.0             pyh9f0ad1d_0    conda-forge
     aws-c-common              0.4.57               he1b5a44_1    conda-forge
     aws-c-event-stream        0.1.6                h72b8ae1_3    conda-forge
     aws-checksums             0.1.9                h346380f_0    conda-forge
     aws-sdk-cpp               1.8.54               h69f4914_0    conda-forge
     babel                     2.8.0                      py_0    conda-forge
     backcall                  0.2.0              pyh9f0ad1d_0    conda-forge
     backports                 1.0                        py_2    conda-forge
     backports.functools_lru_cache 1.6.1                      py_0    conda-forge
     black                     19.10b0                  py37_0    conda-forge
     bleach                    3.2.1              pyh9f0ad1d_0    conda-forge
     bokeh                     2.2.1            py37hc8dfbb8_0    conda-forge
     boost                     1.74.0           py37h429e714_0    conda-forge
     boost-cpp                 1.74.0               h9359b55_0    conda-forge
     boto3                     1.15.11            pyh9f0ad1d_0    conda-forge
     botocore                  1.18.11            pyh9f0ad1d_0    conda-forge
     brotli                    1.0.9                he1b5a44_0    conda-forge
     brotlipy                  0.7.0           py37h516909a_1000    conda-forge
     bzip2                     1.0.8                h516909a_3    conda-forge
     c-ares                    1.16.1               h516909a_3    conda-forge
     ca-certificates           2020.6.20            hecda079_0    conda-forge
     cairo                     1.16.0            h3fc0475_1005    conda-forge
     certifi                   2020.6.20        py37hc8dfbb8_0    conda-forge
     cffi                      1.14.3           py37h2b28604_0    conda-forge
     cfgv                      3.2.0                      py_0    conda-forge
     cfitsio                   3.470                hce51eda_6    conda-forge
     chardet                   3.0.4           py37hc8dfbb8_1007    conda-forge
     clang                     8.0.1                hc9558a2_2    conda-forge
     clang-tools               8.0.1                hc9558a2_2    conda-forge
     clangxx                   8.0.1                         2    conda-forge
     click                     7.1.2              pyh9f0ad1d_0    conda-forge
     click-plugins             1.1.1                      py_0    conda-forge
     cligj                     0.5.0                      py_0    conda-forge
     cloudpickle               1.6.0                      py_0    conda-forge
     cmake                     3.17.0               h28c56e5_0    conda-forge
     cmake_setuptools          0.1.3                      py_0    rapidsai
     commonmark                0.9.1                      py_0    conda-forge
     cryptography              3.1.1            py37hb09aad4_0    conda-forge
     cudatoolkit               10.2.89              h6bb024c_0    nvidia
     cudnn                     7.6.5                cuda10.2_0
     cupy                      7.8.0            py37h940342b_1    conda-forge
     curl                      7.71.1               he644dc0_8    conda-forge
     cython                    0.29.21          py37h3340039_0    conda-forge
     cytoolz                   0.11.0           py37h8f50634_0    conda-forge
     dask                      2.29.0                     py_0    conda-forge
     dask-core                 2.29.0                     py_0    conda-forge
     dask-cuda                 0.16.0a201002           py37_93    rapidsai-nightly
     dask-glm                  0.2.0                      py_1    conda-forge
     dask-ml                   1.7.0                      py_0    conda-forge
     decorator                 4.4.2                      py_0    conda-forge
     defusedxml                0.6.0                      py_0    conda-forge
     distlib                   0.3.1              pyh9f0ad1d_0    conda-forge
     distributed               2.29.0           py37hc8dfbb8_0    conda-forge
     dlpack                    0.3                  he1b5a44_1    conda-forge
     docutils                  0.16             py37hc8dfbb8_1    conda-forge
     double-conversion         3.1.5                he1b5a44_2    conda-forge
     doxygen                   1.8.20               h0e019cf_0    conda-forge
     editdistance              0.5.3            py37h3340039_1    conda-forge
     entrypoints               0.3             py37hc8dfbb8_1001    conda-forge
     execnet                   1.7.1                    pypi_0    pypi
     expat                     2.2.9                he1b5a44_2    conda-forge
     faiss-proc                1.0.0                      cuda    rapidsai-nightly
     fastavro                  1.0.0.post1      py37h8f50634_0    conda-forge
     fastrlock                 0.5              py37h3340039_0    conda-forge
     filelock                  3.0.12             pyh9f0ad1d_0    conda-forge
     fiona                     1.8.17           py37ha3d844c_0    conda-forge
     flake8                    3.8.3                      py_1    conda-forge
     flatbuffers               1.12.0               he1b5a44_0    conda-forge
     fontconfig                2.13.1            h1056068_1002    conda-forge
     freetype                  2.10.2               he06d7ca_0    conda-forge
     freexl                    1.0.5             h516909a_1002    conda-forge
     fsspec                    0.8.3                      py_0    conda-forge
     future                    0.18.2           py37hc8dfbb8_1    conda-forge
     gdal                      3.1.2            py37h518339e_1    conda-forge
     geopandas                 0.7.0                      py_1    conda-forge
     geos                      3.8.1                he1b5a44_0    conda-forge
     geotiff                   1.6.0                ha04d9d0_1    conda-forge
     gettext                   0.19.8.1          hc5be6a0_1002    conda-forge
     gflags                    2.2.2             he1b5a44_1004    conda-forge
     giflib                    5.2.1                h516909a_2    conda-forge
     glib                      2.66.1               h680cd38_0    conda-forge
     glog                      0.4.0                h49b9bf7_3    conda-forge
     gmp                       6.2.0                he1b5a44_2    conda-forge
     grpc-cpp                  1.30.2               heedbac9_0    conda-forge
     hdf4                      4.2.13            hf30be14_1003    conda-forge
     hdf5                      1.10.6          nompi_h3c11f04_101    conda-forge
     heapdict                  1.0.1                      py_0    conda-forge
     hypothesis                5.28.0                     py_0    conda-forge
     icu                       67.1                 he1b5a44_0    conda-forge
     identify                  1.5.5              pyh9f0ad1d_0    conda-forge
     idna                      2.10               pyh9f0ad1d_0    conda-forge
     imagesize                 1.2.0                      py_0    conda-forge
     importlib-metadata        1.7.0            py37hc8dfbb8_0    conda-forge
     importlib_metadata        1.7.0                         0    conda-forge
     iniconfig                 1.0.1              pyh9f0ad1d_0    conda-forge
     ipykernel                 5.3.4            py37h43977f1_0    conda-forge
     ipython                   7.18.1           py37hc6149b9_0    conda-forge
     ipython_genutils          0.2.0                      py_1    conda-forge
     isort                     5.0.7            py37hc8dfbb8_0    conda-forge
     jedi                      0.17.2           py37hc8dfbb8_0    conda-forge
     jinja2                    2.11.2             pyh9f0ad1d_0    conda-forge
     jmespath                  0.10.0             pyh9f0ad1d_0    conda-forge
     joblib                    0.17.0                     py_0    conda-forge
     jpeg                      9d                   h516909a_0    conda-forge
     json-c                    0.13.1            hbfbb72e_1002    conda-forge
     jsonschema                3.2.0            py37hc8dfbb8_1    conda-forge
     jupyter_client            6.1.7                      py_0    conda-forge
     jupyter_core              4.6.3            py37hc8dfbb8_1    conda-forge
     jupyterlab_pygments       0.1.2              pyh9f0ad1d_0    conda-forge
     kealib                    1.4.13               h33137a7_1    conda-forge
     krb5                      1.17.1               hfafb76e_3    conda-forge
     lcms2                     2.11                 hbd6801e_0    conda-forge
     ld_impl_linux-64          2.35                 h769bd43_9    conda-forge
     libblas                   3.8.0               17_openblas    conda-forge
     libcblas                  3.8.0               17_openblas    conda-forge
     libcumlprims              0.16.0a200930   cuda10.2_g1c28023_35    rapidsai-nightly
     libcurl                   7.71.1               hcdd3856_8    conda-forge
     libcypher-parser          0.6.2                         1    rapidsai
     libdap4                   3.20.6               h1d1bd15_1    conda-forge
     libedit                   3.1.20191231         he28a2e2_2    conda-forge
     libev                     4.33                 h516909a_1    conda-forge
     libevent                  2.1.10               hcdb4288_2    conda-forge
     libfaiss                  1.6.3           he61ee18_1_cuda    conda-forge
     libffi                    3.2.1             he1b5a44_1007    conda-forge
     libgcc-ng                 9.3.0               h5dbcf3e_17    conda-forge
     libgdal                   3.1.2                hb2a6f5f_1    conda-forge
     libgfortran-ng            7.5.0               hae1eefd_17    conda-forge
     libgfortran4              7.5.0               hae1eefd_17    conda-forge
     libgomp                   9.3.0               h5dbcf3e_17    conda-forge
     libhwloc                  2.3.0                h3c4fd83_0    conda-forge
     libiconv                  1.16                 h516909a_0    conda-forge
     libkml                    1.3.0             h74f7ee3_1012    conda-forge
     liblapack                 3.8.0               17_openblas    conda-forge
     libllvm10                 10.0.1               he513fc3_3    conda-forge
     libllvm8                  8.0.1                hc9558a2_0    conda-forge
     libnetcdf                 4.7.4           nompi_h84807e1_105    conda-forge
     libnghttp2                1.41.0               h8cfc5f6_2    conda-forge
     libopenblas               0.3.10          pthreads_hb3c22a3_4    conda-forge
     libpng                    1.6.37               hed695b0_2    conda-forge
     libpq                     12.3                 h5513abc_0    conda-forge
     libprotobuf               3.12.4               h8b12597_0    conda-forge
     librmm                    0.16.0a201002   cuda10.2_g273f92d_397    rapidsai-nightly
     libsodium                 1.0.18               h516909a_1    conda-forge
     libspatialindex           1.9.3                he1b5a44_3    conda-forge
     libspatialite             4.3.0a            h57f1b35_1039    conda-forge
     libssh2                   1.9.0                hab1572f_5    conda-forge
     libstdcxx-ng              9.3.0               h2ae2ef3_17    conda-forge
     libthrift                 0.13.0               hbe8ec66_6    conda-forge
     libtiff                   4.1.0                hc7e4089_6    conda-forge
     libutf8proc               2.5.0                h516909a_2    conda-forge
     libuuid                   2.32.1            h14c3975_1000    conda-forge
     libuv                     1.40.0               h516909a_0    conda-forge
     libwebp-base              1.1.0                h516909a_3    conda-forge
     libxcb                    1.13              h14c3975_1002    conda-forge
     libxml2                   2.9.10               h68273f3_2    conda-forge
     llvmlite                  0.34.0           py37h5202443_1    conda-forge
     locket                    0.2.0                      py_2    conda-forge
     lz4-c                     1.9.2                he1b5a44_3    conda-forge
     markdown                  3.2.2                      py_0    conda-forge
     markupsafe                1.1.1            py37h8f50634_1    conda-forge
     mccabe                    0.6.1                      py_1    conda-forge
     mimesis                   4.0.0              pyh9f0ad1d_0    conda-forge
     mistune                   0.8.4           py37h8f50634_1001    conda-forge
     more-itertools            8.5.0                      py_0    conda-forge
     msgpack-python            1.0.0            py37h99015e2_1    conda-forge
     multipledispatch          0.6.0                      py_0    conda-forge
     munch                     2.5.0                      py_0    conda-forge
     mypy_extensions           0.4.3            py37hc8dfbb8_1    conda-forge
     nbclient                  0.5.0                      py_0    conda-forge
     nbconvert                 6.0.7            py37hc8dfbb8_0    conda-forge
     nbformat                  5.0.7                      py_0    conda-forge
     nbsphinx                  0.7.1              pyh9f0ad1d_0    conda-forge
     nccl                      2.7.8.1              hc6a2c23_0    conda-forge
     ncurses                   6.2                  he1b5a44_1    conda-forge
     nest-asyncio              1.4.1                      py_0    conda-forge
     networkx                  2.5                        py_0    conda-forge
     nodeenv                   1.5.0              pyh9f0ad1d_0    conda-forge
     notebook                  6.1.4            py37hc8dfbb8_0    conda-forge
     numba                     0.51.2           py37h9fdb41a_0    conda-forge
     numpy                     1.19.1           py37h7ea13bd_2    conda-forge
     numpydoc                  1.1.0                      py_1    conda-forge
     olefile                   0.46                       py_0    conda-forge
     openjpeg                  2.3.1                h981e76c_3    conda-forge
     openssl                   1.1.1h               h516909a_0    conda-forge
     packaging                 20.4               pyh9f0ad1d_0    conda-forge
     pandas                    1.1.2            py37h3340039_0    conda-forge
     pandoc                    1.19.2                        0    conda-forge
     pandocfilters             1.4.2                      py_1    conda-forge
     parquet-cpp               1.5.1                         1    conda-forge
     parso                     0.7.1              pyh9f0ad1d_0    conda-forge
     partd                     1.1.0                      py_0    conda-forge
     pathspec                  0.8.0              pyh9f0ad1d_0    conda-forge
     pcre                      8.44                 he1b5a44_0    conda-forge
     pexpect                   4.8.0            py37hc8dfbb8_1    conda-forge
     pickleshare               0.7.5           py37hc8dfbb8_1001    conda-forge
     pillow                    7.2.0            py37h718be6c_1    conda-forge
     pip                       20.2.3                     py_0    conda-forge
     pixman                    0.38.0            h516909a_1003    conda-forge
     pluggy                    0.13.1           py37hc8dfbb8_2    conda-forge
     poppler                   0.89.0               h4190859_1    conda-forge
     poppler-data              0.4.9                         0    conda-forge
     postgresql                12.3                 h8573dbc_0    conda-forge
     pre-commit                2.7.1            py37hc8dfbb8_0    conda-forge
     pre_commit                2.7.1                         0    conda-forge
     proj                      7.1.0                h966b41f_1    conda-forge
     prometheus_client         0.8.0              pyh9f0ad1d_0    conda-forge
     prompt-toolkit            3.0.7                      py_0    conda-forge
     psutil                    5.7.2            py37h8f50634_0    conda-forge
     pthread-stubs             0.4               h14c3975_1001    conda-forge
     ptvsd                     4.3.2                    pypi_0    pypi
     ptyprocess                0.6.0                 py37_1000    conda-forge
     py                        1.9.0              pyh9f0ad1d_0    conda-forge
     py-cpuinfo                7.0.0              pyh9f0ad1d_0    conda-forge
     pyarrow                   1.0.1           py37h72578d1_7_cuda    conda-forge
     pycodestyle               2.6.0              pyh9f0ad1d_0    conda-forge
     pycparser                 2.20               pyh9f0ad1d_2    conda-forge
     pyflakes                  2.2.0              pyh9f0ad1d_0    conda-forge
     pygal                     2.4.0                      py_0    conda-forge
     pygments                  2.7.1                      py_0    conda-forge
     pynvml                    8.0.4                      py_1    conda-forge
     pyopenssl                 19.1.0                   py37_0    conda-forge
     pyparsing                 2.4.7              pyh9f0ad1d_0    conda-forge
     pyproj                    2.6.1.post1      py37hb5dadc3_1    conda-forge
     pyrsistent                0.17.3           py37h8f50634_0    conda-forge
     pysocks                   1.7.1            py37hc8dfbb8_1    conda-forge
     pytest                    6.1.0            py37hc8dfbb8_0    conda-forge
     pytest-benchmark          3.2.3              pyh9f0ad1d_0    conda-forge
     pytest-forked             1.3.0                    pypi_0    pypi
     pytest-xdist              2.1.0                    pypi_0    pypi
     python                    3.7.8           h425cb1d_1_cpython    conda-forge
     python-dateutil           2.8.1                      py_0    conda-forge
     python-louvain            0.13                       py_0    conda-forge
     python_abi                3.7                     1_cp37m    conda-forge
     pytz                      2020.1             pyh9f0ad1d_0    conda-forge
     pyyaml                    5.3.1            py37h8f50634_0    conda-forge
     pyzmq                     19.0.2           py37hac76be4_0    conda-forge
     rapidjson                 1.1.0             hf484d3e_1002    conda-forge
     rapids-pytest-benchmark   0.0.13                     py_0    rapidsai
     re2                       2020.08.01           he1b5a44_1    conda-forge
     readline                  8.0                  he28a2e2_2    conda-forge
     recommonmark              0.6.0                      py_0    conda-forge
     regex                     2020.9.27        py37h8f50634_0    conda-forge
     requests                  2.24.0             pyh9f0ad1d_0    conda-forge
     rhash                     1.3.6             h14c3975_1001    conda-forge
     rtree                     0.9.4            py37h8526d28_1    conda-forge
     s3transfer                0.3.3            py37hc8dfbb8_1    conda-forge
     scikit-learn              0.23.1           py37h8a51577_0    conda-forge
     scipy                     1.5.2            py37hb14ef9d_0    conda-forge
     send2trash                1.5.0                      py_0    conda-forge
     setuptools                49.6.0           py37hc8dfbb8_1    conda-forge
     shapely                   1.7.1            py37hedb1597_0    conda-forge
     six                       1.15.0             pyh9f0ad1d_0    conda-forge
     snappy                    1.1.8                he1b5a44_3    conda-forge
     snowballstemmer           2.0.0                      py_0    conda-forge
     sortedcontainers          2.2.2              pyh9f0ad1d_0    conda-forge
     spdlog                    1.7.0                hc9558a2_2    conda-forge
     sphinx                    3.2.1                      py_0    conda-forge
     sphinx-copybutton         0.3.0              pyh9f0ad1d_0    conda-forge
     sphinx-markdown-tables    0.0.15                   pypi_0    pypi
     sphinx_rtd_theme          0.5.0              pyh9f0ad1d_0    conda-forge
     sphinxcontrib-applehelp   1.0.2                      py_0    conda-forge
     sphinxcontrib-devhelp     1.0.2                      py_0    conda-forge
     sphinxcontrib-htmlhelp    1.0.3                      py_0    conda-forge
     sphinxcontrib-jsmath      1.0.1                      py_0    conda-forge
     sphinxcontrib-qthelp      1.0.3                      py_0    conda-forge
     sphinxcontrib-serializinghtml 1.1.4                      py_0    conda-forge
     sphinxcontrib-websupport  1.2.4              pyh9f0ad1d_0    conda-forge
     sqlite                    3.33.0               h4cf870e_0    conda-forge
     streamz                   0.6.0                    pypi_0    pypi
     tbb                       2020.2               hc9558a2_0    conda-forge
     tblib                     1.6.0                      py_0    conda-forge
     terminado                 0.9.1            py37hc8dfbb8_0    conda-forge
     testpath                  0.4.4                      py_0    conda-forge
     threadpoolctl             2.1.0              pyh5ca1d4c_0    conda-forge
     tiledb                    2.0.8                h3effe38_1    conda-forge
     tk                        8.6.10               hed695b0_0    conda-forge
     toml                      0.10.1             pyh9f0ad1d_0    conda-forge
     toolz                     0.11.1                     py_0    conda-forge
     tornado                   6.0.4            py37h8f50634_1    conda-forge
     traitlets                 5.0.4                      py_1    conda-forge
     treelite                  0.92             py37h023e13c_2    conda-forge
     treelite-runtime          0.92                     pypi_0    pypi
     typed-ast                 1.4.1            py37h516909a_0    conda-forge
     typing_extensions         3.7.4.2                    py_0    conda-forge
     tzcode                    2020a                h516909a_0    conda-forge
     ucx                       1.8.1+g6b29558       ha5db111_0    rapidsai
     ucx-py                    0.16.0a201003   py37_g6b29558_183    rapidsai-nightly
     umap-learn                0.4.6            py37hc8dfbb8_0    conda-forge
     urllib3                   1.25.10                    py_0    conda-forge
     virtualenv                20.0.20          py37hc8dfbb8_1    conda-forge
     wcwidth                   0.2.5              pyh9f0ad1d_2    conda-forge
     webencodings              0.5.1                      py_1    conda-forge
     wheel                     0.35.1             pyh9f0ad1d_0    conda-forge
     xerces-c                  3.2.3                hfe33f54_1    conda-forge
     xorg-kbproto              1.0.7             h14c3975_1002    conda-forge
     xorg-libice               1.0.10               h516909a_0    conda-forge
     xorg-libsm                1.2.3             h84519dc_1000    conda-forge
     xorg-libx11               1.6.12               h516909a_0    conda-forge
     xorg-libxau               1.0.9                h14c3975_0    conda-forge
     xorg-libxdmcp             1.1.3                h516909a_0    conda-forge
     xorg-libxext              1.3.4                h516909a_0    conda-forge
     xorg-libxrender           0.9.10            h516909a_1002    conda-forge
     xorg-renderproto          0.11.1            h14c3975_1002    conda-forge
     xorg-xextproto            7.3.0             h14c3975_1002    conda-forge
     xorg-xproto               7.0.31            h14c3975_1007    conda-forge
     xz                        5.2.5                h516909a_1    conda-forge
     yaml                      0.2.5                h516909a_0    conda-forge
     zeromq                    4.3.3                he1b5a44_1    conda-forge
     zict                      2.0.0                      py_0    conda-forge
     zipp                      3.3.0                      py_0    conda-forge
     zlib                      1.2.11            h516909a_1009    conda-forge
     zstd                      1.4.5                h6597ccf_2    conda-forge
     
</pre></details>

",2020-10-07T04:01:07Z,0,0,Michael Wang,Nvidia Rapids,True
74,[FEA] Java host string columns should match list columns,"**Is your feature request related to a problem? Please describe.**
The way we construct and layout strings and lists are different but essentially have similar functionality/usage, adding complexity around host side construction/deconstruction. This should be commonized.

**Describe the solution you'd like**
Strings can be represented as lists maybe.
",2020-10-12T13:08:04Z,0,0,Kuhu Shukla,,False
75,"[Discussion] Provide public, templated APIs for libcudf features","## Description

Today, libcudf does not provide function templates in it's public interface. This is because for most users, the input data type is runtime information. Therefore, libcudf APIs operate on type-erased `column_view` objects where the runtime type information is stored in the `data_type` member and internally we dispatch to the appropriately typed code path, e.g., via the `type_dispatcher`.

However, this design lacks the power and flexibility of iterator based interfaces. E.g., there is no way to fuse operations with something like a `thrust::transform_iterator` and instead requires materializing intermediate results. Internally, we side-step this issue with function templates in the `detail::` API that _do_ operate on iterators. We can do this because inside a type-dispatched code path we _know_ the type of the underlying data and can invoke the proper template instantiation (or use device-side dispatch in some cases like with the `indexalator`). This allows greater flexibility and reuse of functionality within the library. 

For example, the public [`cudf::gather`](https://github.com/rapidsai/cudf/blob/421fddb8b52dff8c8152158ca5f379f0f7255b98/cpp/include/cudf/copying.hpp#L64-L68) API expects the `gather_map` to be specified as a `column_view`. The implementation of this public API uses the `indexalator` to convert the type-erased input `column_view` into an iterator and forwards to a [`detail::gather` API](https://github.com/rapidsai/cudf/blob/421fddb8b52dff8c8152158ca5f379f0f7255b98/cpp/include/cudf/detail/gather.cuh#L617-L624) that expects the `gather_map` to be an iterator. 

It would be nice if we could offer external users some of this same power and flexibility, e.g., there may be cases where a user knows that a particular input will always be a certain type (or small set of types) and could invoke a function template with an iterator that fuses some operations to avoid intermediate materializations. 

`gather` is a good example of this where this would be useful. Today, `gather` implements non-standard C++ behavior where a negative index wraps around from the end of the array: https://github.com/rapidsai/cudf/blob/421fddb8b52dff8c8152158ca5f379f0f7255b98/cpp/include/cudf/copying.hpp#L46-L47

This is accomplished internally via a `transform_iterator` over the elements of the `gather_map`. Instead, if there were a public iterator-based `gather` function, users could provide the `transform_iterator` themselves and the `gather` implementation wouldn't be required to do the negative value wrapping. This is desirable because there are situations where we don't want negative values to wrap around, see https://github.com/rapidsai/cudf/issues/6479. 

## Summary

I would like to see libcudf essentially provide two complementary public APIs:

1. A function-template interface operating on iterators as much as possible
2. A non-template interface operating on `column_view`s that just calls the APIs in 1.

To a certain extent, libcudf is already designed this way. Many public APIs call `detail::` APIs that operate on iterators, e.g., `gather`, `apply_boolean_mask`, `copy_if`, etc. This is a convention typically followed in libcudf (though not often enough). We should document and standardize more formally on this pattern (see https://github.com/rapidsai/cudf/issues/6470). 

## Additional Points of Consideration

1. The inability of Cython to define `__device__` functors/lambdas limits the ability of Cython/Python to take advantage of any function template API
    - There may be a solution here long term if we extended Cython to allow `__device__` keyword and then compile the Cython lib with `nvcc`, but there's some amount of work here. 

2. Testing
   - If we bless function template APIs as public, do we need to test those directly in addition to the existing public APIs? That would dramatically increase the surface area of the library. (My personal opinion is that testing the non-template APIs would be sufficient)

3. Outputs
   - Nullable outputs makes output iterators impossible to do efficiently. So outputs would still need to be `column/table`s.

4. Nullable inputs
   - We should standardize nullable input iterators to return a `thrust::optional<T>`, see https://github.com/rapidsai/cudf/issues/6470

## Additional Context

This is an idea I've had bouncing around for quite a while now. I wanted to capture all of my thoughts in a single issue for discussion. It's not something I believe we _must_ do, but we should at least explore it. 


",2020-10-12T17:14:55Z,0,0,Jake Hemstad,@NVIDIA,True
76,[FEA] Jaro-Winkler algorithm for cudf.core.column.string.StringMethods.edit_distance,"**Is your feature request related to a problem? Please describe.**

Add Jaro-Winkler algorithm for `cudf.core.column.string.StringMethods.edit_distance`.

Documentation: https://docs.rapids.ai/api/cudf/stable/api.html?highlight=tokenizer#cudf.core.column.string.StringMethods.edit_distance

**Describe the solution you'd like**

```
def edit_distance(targets, algorithm='levenshtein', **kwargs):
...

Parameters
targets array-like, Sequence or Series or str - The string(s) to measure against each string.
algorithm str - The algorithm - either Levenshtein or Jaro-Winkler.

Returns
Series or Index of int32.
Examples
```

Usage:

```
>>>
import cudf
sr = cudf.Series([""puppy"", ""doggy"", ""kitty""])
targets = cudf.Series([""pup"", ""dogie"", ""kitten""])
sr.str.edit_distance(targets=targets, algorithm='jarowinkler')
0    2
1    2
2    2
dtype: int32
sr.str.edit_distance(""puppy"")
0    0
1    4
2    4
dtype: int32
```

**Describe alternatives you've considered**

cuDF UDFs? Open to ideas.

**Additional context**

cc @beckernick @kkraus14 
",2020-10-12T19:26:58Z,0,0,Paul Hendricks,@NVIDIA,True
77,[BUG] Reading JSON file saved from Series fails,"**Describe the bug**
Reading a JSON file created from a Series `.to_json(path, orient='records', lines=True)` call leads to a `Input data is not a valid JSON file` when trying to read with `.read_json(path, orient='records', lines=True)` (with or without `engine='cudf'` parameter).

**NOTE**: this is only a problem when saving Series in this format -- DataFrame objects are saved properly.

**Steps/Code to reproduce bug**
```
cudf.Series([1,2,3,4,5]).to_json('sample.json', lines=True, orient='records')

cudf.read_json('sample.json', lines=True, orient='records')
```

The output file looks as follows:

```
1
2
3
4
5
```

**Expected behavior**
The file is read back properly and produces a valid `cudf.Series` object. 

**Environment overview (please complete the following information)**
 - Environment location: Docker
 - Method of cuDF install: Docker
   - RAPIDS v 0.16 pull from nightly. ",2020-10-12T22:07:00Z,0,0,Tomek Drabas,Voltron Data,False
78,[FEA] Switch default `chunksize` for `dask_cudf.read_csv` to `2gb` from `256 MiB`,"**Is your feature request related to a problem? Please describe.**
I think we should switch `chunksize` for `dask_cudf.read_csv` increase our `chunksize` from `256 MiB` to something bigger like `1 gb` to `2gb` . 

The code change will go at line : 

https://github.com/rapidsai/cudf/blob/c382989330f97715e4aec3f2c677b0a6d2e99647/python/dask_cudf/dask_cudf/io/csv.py#L14

## Why:

From experience on `gpu-bdb`, workflows and other users,  it seems like current default of  `256 MiB`: 

a. Is often too low to saturate GPU meaningfully especially with wider frames for both io and downstream operations. 

b.  Bigger chunk size often just sidesteps various communication and scheduler bottlenecks for shuffle operations like repartition/merge etc.


CC: @beckernick , @ayushdg , @quasiben , @kkraus14 . ",2020-10-20T00:24:54Z,0,0,Vibhu Jawa,Nvidia,True
79,[FEA] Add java/JNI support for substring() with non-literals parameters,"**Is your feature request related to a problem? Please describe.**
We currently support only literals as params to substring method, using the newly added `slice_strings` we should be able to support non-literals. 

**Describe the solution you'd like**
Requires jni and java side bindings",2020-10-20T13:26:29Z,0,0,Kuhu Shukla,,False
80,"[FEA] support '\n', '\r' and '\r\n' atthe same time as line delimiters for CSV parsing","**Is your feature request related to a problem? Please describe.**
The default setting for Spark when reading CSV for line delimiters is '\r' (Carriage Return), '\n' (Line Feed), and/or '\r\n' (Carriage Return followed by Line Feed)

Currently in the Spark plugin we pre-process the CSV input data before sending it to CUDF for parsing.  The pre-processing handles splits  to match what Spark currently does and also fixing the line delimiters to be a single uniform value.  We have found that with fast storage replacing the line delimiters is a real bottleneck.

We are also concerned about being ready to support GPU Direct Storage where we would not be able to pre-process the data before sending it to cudf.

**Describe the solution you'd like**
We would like an option when parsing CSV to have CUDF recognize  '\r' (Carriage Return), '\n' (Line Feed), and '\r\n' (Carriage Return followed by Line Feed) all as valid line delimiters at the same time.

**Describe alternatives you've considered**
Keep doing what we are doing and be slower than ideal when parsing CSV and not be able to support CSV without config modifications when we do adopt GU Direct Storage.",2020-10-21T12:44:09Z,0,0,Robert (Bobby) Evans,Nvidia,True
81,[BUG] the java DType class has incorrect mappings for CSV parsing,"**Describe the bug**
The java code has incorrect mappings for CSV parsing. For several types.

All of the time deltas are wrong and  we indicate that we support ""list"" and ""struct"" which we do not.

At a minimum the time delta types need to be updated, but ideally we modify the code so the strings are not stored in the enum any more, but are instead stored as a mapping in the CSV reader.",2020-10-21T14:18:15Z,0,0,Robert (Bobby) Evans,Nvidia,True
82,[BUG] Json reader is not correctly inferring a float column with a specific type of scientific notation,"**Describe the bug**
Json reader doesn't seem to be inferring the dtype of a column which has floating values. It appears to be inferred as `object` dtype. The only different I see with resepect to other float columns being inferred correctly is the difference in scientific notation, i.e., `34.3e+09` is being inferred correctly but `34.3e+304` is not.

**Steps/Code to reproduce bug**

Attachment(Just rename the file to `temp.parquet` and load): [temp.parquet.zip](https://github.com/rapidsai/cudf/files/5426479/temp.parquet.zip)

```python
>>> import pandas as pd
>>> import cudf
>>> pdf = pd.read_parquet('temp.parquet')
>>> pdf
              0              1             2              3      4    5             6
0  2.250622e+09 -6.494446e+307 -9.164551e+12  6735604693236 -119.0  1.0 -3.826755e+12
1  2.035766e+09            NaN -2.369047e+12  7574969090271    NaN  NaN -5.743424e+12
2  1.939400e+09 -1.145175e+308  7.534900e+12  3628042478826   28.0  NaN -5.231790e+12
3  1.871836e+09  1.556428e+307  2.206686e+12   767786704906  -62.0  NaN           NaN
4  9.807541e+07            NaN  1.257964e+12 -7919323897153  -96.0  NaN           NaN
5  3.067252e+09 -1.678511e+308  4.968256e+12  5588109998021  -37.0  0.0           NaN
6  3.540629e+09 -1.599954e+307  1.356411e+12  8960674970810    NaN  1.0 -2.679076e+12
7           NaN            NaN           NaN -8605861346425   82.0  NaN  8.262327e+12
8  1.528666e+09            NaN           NaN -3517479000461   40.0  0.0           NaN
9           NaN  9.992153e+307 -5.273377e+12  7975279208357   13.0  1.0 -2.294980e+12
>>> pdf.dtypes
0    float64
1    float64
2    float64
3      int64
4    float64
5    float64
6    float64
dtype: object
>>> pdf.to_json('a', orient='records', lines=True)
>>> new_pdf = pd.read_json('a', orient='records', lines=True)
>>> new_pdf
              0              1             2              3      4    5             6
0  2.250622e+09 -6.494446e+307 -9.164551e+12  6735604693236 -119.0  1.0 -3.826755e+12
1  2.035766e+09            NaN -2.369047e+12  7574969090271    NaN  NaN -5.743424e+12
2  1.939400e+09 -1.145175e+308  7.534900e+12  3628042478826   28.0  NaN -5.231790e+12
3  1.871836e+09  1.556428e+307  2.206686e+12   767786704906  -62.0  NaN           NaN
4  9.807541e+07            NaN  1.257964e+12 -7919323897153  -96.0  NaN           NaN
5  3.067252e+09 -1.678511e+308  4.968256e+12  5588109998021  -37.0  0.0           NaN
6  3.540629e+09 -1.599954e+307  1.356411e+12  8960674970810    NaN  1.0 -2.679076e+12
7           NaN            NaN           NaN -8605861346425   82.0  NaN  8.262327e+12
8  1.528666e+09            NaN           NaN -3517479000461   40.0  0.0           NaN
9           NaN  9.992153e+307 -5.273377e+12  7975279208357   13.0  1.0 -2.294980e+12
>>> new_pdf.dtypes
0    float64
1    float64
2    float64
3      int64
4    float64
5    float64
6    float64
dtype: object
>>> gdf = cudf.read_json('a', engine='cudf', orient='records', lines=True)
>>> gdf
                 0                  1                 2              3       4     5                 6
0  2.250621731e+09  -6.494445507e+307  -9.164551369e+12  6735604693236  -119.0   1.0  -3.826754862e+12
1   2.03576588e+09               <NA>    -2.3690472e+12  7574969090271    <NA>  <NA>  -5.743423727e+12
2  1.939399618e+09  -1.145175127e+308   7.534900233e+12  3628042478826    28.0  <NA>  -5.231789981e+12
3  1.871835886e+09   1.556428026e+307   2.206686314e+12   767786704906   -62.0  <NA>              <NA>
4       98075413.0               <NA>   1.257964087e+12 -7919323897153   -96.0  <NA>              <NA>
5  3.067251544e+09  -1.678511417e+308    4.96825619e+12  5588109998021   -37.0   0.0              <NA>
6  3.540629185e+09  -1.599954352e+307   1.356410998e+12  8960674970810    <NA>   1.0  -2.679076194e+12
7             <NA>               <NA>              <NA> -8605861346425    82.0  <NA>    8.26232733e+12
8  1.528665908e+09               <NA>              <NA> -3517479000461    40.0   0.0              <NA>
9             <NA>   9.992152856e+307  -5.273376883e+12  7975279208357    13.0   1.0  -2.294979586e+12
>>> gdf.dtypes
0    float64
1     object          #<---- This should be float64, See column 1 values & dtypes below
2    float64
3      int64
4    float64
5    float64
6    float64
dtype: object
>>> gdf['1']
0    -6.494445507e+307
1                 <NA>
2    -1.145175127e+308
3     1.556428026e+307
4                 <NA>
5    -1.678511417e+308
6    -1.599954352e+307
7                 <NA>
8                 <NA>
9     9.992152856e+307
Name: 1, dtype: object
>>> new_pdf[1]
0   -6.494446e+307
1              NaN
2   -1.145175e+308
3    1.556428e+307
4              NaN
5   -1.678511e+308
6   -1.599954e+307
7              NaN
8              NaN
9    9.992153e+307
Name: 1, dtype: float64
```

**Expected behavior**
Correctly infer the float dtype.

**Environment overview (please complete the following information)**
 - Environment location: [Bare-metal]
 - Method of cuDF install: [from source]

**Environment details**
Please run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details

<details><summary>Click here to see environment details</summary><pre>
     
     **git***
     commit 0d87f1030d610772477ec7132dc3feb19e89da24 (HEAD -> branch-0.17, upstream/branch-0.17)
     Author: GALI PREM SAGAR <sagarprem75@gmail.com>
     Date:   Thu Oct 22 12:40:57 2020 -0500
     
     Add CSV fuzz tests with varying function parameters (#6384)
     
     * add inital set of params in csv writer
     
     * Fix row_group_size to not select 0.
     
     * Fix imports
     
     * Add utility to convert cudf DataFrame to Pandas Nullable DataFrame
     
     * Fix filename typo in ParquetWriter
     
     * Handle serializations of numpy bool objects.
     
     * Add ability for multi-Parameter runs in CSV Fuzz workers.
     
     * Add multiple parameter combination fuzz tests for CSV reader and writer.
     
     * Update csv.py
     
     * Update CHANGELOG.md
     
     * Add more na_rep values
     
     * Update CHANGELOG.md
     
     * Update csv.py
     
     * Update python/cudf/cudf/_fuzz_testing/utils.py
     
     Co-authored-by: Keith Kraus <kkraus@nvidia.com>
     
     * address review comments
     
     * add todo
     
     Co-authored-by: Keith Kraus <kkraus@nvidia.com>
     **git submodules***
     
     ***OS Information***
     DISTRIB_ID=Ubuntu
     DISTRIB_RELEASE=18.04
     DISTRIB_CODENAME=bionic
     DISTRIB_DESCRIPTION=""Ubuntu 18.04.4 LTS""
     NAME=""Ubuntu""
     VERSION=""18.04.4 LTS (Bionic Beaver)""
     ID=ubuntu
     ID_LIKE=debian
     PRETTY_NAME=""Ubuntu 18.04.4 LTS""
     VERSION_ID=""18.04""
     HOME_URL=""https://www.ubuntu.com/""
     SUPPORT_URL=""https://help.ubuntu.com/""
     BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
     PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
     VERSION_CODENAME=bionic
     UBUNTU_CODENAME=bionic
     Linux dt07 4.15.0-76-generic #86-Ubuntu SMP Fri Jan 17 17:24:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
     
     ***GPU Information***
     Thu Oct 22 16:38:04 2020
     +-----------------------------------------------------------------------------+
     | NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 10.2     |
     |-------------------------------+----------------------+----------------------+
     | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
     | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
     |===============================+======================+======================|
     |   0  Tesla T4            On   | 00000000:3B:00.0 Off |                    0 |
     | N/A   48C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |
     +-------------------------------+----------------------+----------------------+
     |   1  Tesla T4            On   | 00000000:5E:00.0 Off |                    0 |
     | N/A   37C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |
     +-------------------------------+----------------------+----------------------+
     |   2  Tesla T4            On   | 00000000:AF:00.0 Off |                    0 |
     | N/A   32C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |
     +-------------------------------+----------------------+----------------------+
     |   3  Tesla T4            On   | 00000000:D8:00.0 Off |                    0 |
     | N/A   32C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |
     +-------------------------------+----------------------+----------------------+
     
     +-----------------------------------------------------------------------------+
     | Processes:                                                       GPU Memory |
     |  GPU       PID   Type   Process name                             Usage      |
     |=============================================================================|
     |  No running processes found                                                 |
     +-----------------------------------------------------------------------------+
     
     ***CPU***
     Architecture:        x86_64
     CPU op-mode(s):      32-bit, 64-bit
     Byte Order:          Little Endian
     CPU(s):              64
     On-line CPU(s) list: 0-63
     Thread(s) per core:  2
     Core(s) per socket:  16
     Socket(s):           2
     NUMA node(s):        2
     Vendor ID:           GenuineIntel
     CPU family:          6
     Model:               85
     Model name:          Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz
     Stepping:            4
     CPU MHz:             2761.742
     BogoMIPS:            4200.00
     Virtualization:      VT-x
     L1d cache:           32K
     L1i cache:           32K
     L2 cache:            1024K
     L3 cache:            22528K
     NUMA node0 CPU(s):   0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62
     NUMA node1 CPU(s):   1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63
     Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts pku ospke md_clear flush_l1d
     
     ***CMake***
     /nvme/0/pgali/envs/cudfdev/bin/cmake
     cmake version 3.18.2
     
     CMake suite maintained and supported by Kitware (kitware.com/cmake).
     
     ***g++***
     /usr/bin/g++
     g++ (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
     Copyright (C) 2017 Free Software Foundation, Inc.
     This is free software; see the source for copying conditions.  There is NO
     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
     
     
     ***nvcc***
     /usr/local/cuda/bin/nvcc
     nvcc: NVIDIA (R) Cuda compiler driver
     Copyright (c) 2005-2019 NVIDIA Corporation
     Built on Wed_Oct_23_19:24:38_PDT_2019
     Cuda compilation tools, release 10.2, V10.2.89
     
     ***Python***
     /nvme/0/pgali/envs/cudfdev/bin/python
     Python 3.7.8
     
     ***Environment Variables***
     PATH                            : /usr/share/swift/usr/bin:/home/nfs/pgali/bin:/home/nfs/pgali/.local/bin:/nvme/0/pgali/envs/cudfdev/bin:/home/nfs/pgali/anaconda3/bin:/home/nfs/pgali/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/lib/jvm/default-java/bin:/usr/share/sbt-launcher-packaging/bin/sbt-launch.jar/bin:/usr/lib/spark/bin:/usr/lib/spark/sbin:/usr/local/cuda/bin
     LD_LIBRARY_PATH                 : /usr/local/cuda/lib64::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64
     NUMBAPRO_NVVM                   :
     NUMBAPRO_LIBDEVICE              :
     CONDA_PREFIX                    : /nvme/0/pgali/envs/cudfdev
     PYTHON_PATH                     :
     
     ***conda packages***
     /home/nfs/pgali/anaconda3/bin/conda
     # packages in environment at /nvme/0/pgali/envs/cudfdev:
     #
     # Name                    Version                   Build  Channel
     _libgcc_mutex             0.1                 conda_forge    conda-forge
     _openmp_mutex             4.5                       1_gnu    conda-forge
     abseil-cpp                20200225.2           he1b5a44_2    conda-forge
     alabaster                 0.7.12                     py_0    conda-forge
     apipkg                    1.5                      pypi_0    pypi
     appdirs                   1.4.3                      py_1    conda-forge
     argon2-cffi               20.1.0           py37h8f50634_1    conda-forge
     arrow-cpp                 1.0.1           py37heb3366a_13_cuda    conda-forge
     arrow-cpp-proc            1.0.1                      cuda    conda-forge
     async_generator           1.10                       py_0    conda-forge
     attrs                     20.2.0             pyh9f0ad1d_0    conda-forge
     avro                      1.10.0                   pypi_0    pypi
     aws-c-common              0.4.59               he1b5a44_0    conda-forge
     aws-c-event-stream        0.1.6                h84e28f3_5    conda-forge
     aws-checksums             0.1.9                he252421_2    conda-forge
     aws-sdk-cpp               1.8.59               h9b98462_1    conda-forge
     babel                     2.8.0                      py_0    conda-forge
     backcall                  0.2.0              pyh9f0ad1d_0    conda-forge
     backports                 1.0                        py_2    conda-forge
     backports.functools_lru_cache 1.6.1                      py_0    conda-forge
     black                     19.10b0                    py_4    conda-forge
     bleach                    3.2.1              pyh9f0ad1d_0    conda-forge
     bokeh                     2.2.1            py37hc8dfbb8_0    conda-forge
     boost-cpp                 1.74.0               h9359b55_0    conda-forge
     brotli                    1.0.9                he1b5a44_2    conda-forge
     brotlipy                  0.7.0           py37h8f50634_1000    conda-forge
     bzip2                     1.0.8                h516909a_3    conda-forge
     c-ares                    1.16.1               h516909a_3    conda-forge
     ca-certificates           2020.6.20            hecda079_0    conda-forge
     certifi                   2020.6.20        py37he5f6b98_2    conda-forge
     cffi                      1.14.3           py37h2b28604_0    conda-forge
     cfgv                      3.2.0                      py_0    conda-forge
     chardet                   3.0.4           py37he5f6b98_1008    conda-forge
     clang                     8.0.1                hc9558a2_2    conda-forge
     clang-tools               8.0.1                hc9558a2_2    conda-forge
     clangxx                   8.0.1                         2    conda-forge
     click                     7.1.2              pyh9f0ad1d_0    conda-forge
     cloudpickle               1.6.0                      py_0    conda-forge
     cmake                     3.18.2               h5c55442_0    conda-forge
     cmake_setuptools          0.1.3                      py_0    rapidsai
     commonmark                0.9.1                      py_0    conda-forge
     cryptography              3.1.1            py37hb09aad4_0    conda-forge
     cudatoolkit               10.2.89              h6bb024c_0    nvidia
     cudf                      0.17.0a0+87.ge25d1ca9d2          pypi_0    pypi
     cudnn                     7.6.5                cuda10.2_0
     cupy                      8.0.0            py37hd9eba22_1    conda-forge
     cython                    0.29.21          py37hb892b2f_1    conda-forge
     cytoolz                   0.11.0           py37h8f50634_1    conda-forge
     dask                      2.30.0+7.gdcbdbe63          pypi_0    pypi
     dask-cudf                 0.17.0a0+91.gbdd4e9cf52          pypi_0    pypi
     decorator                 4.4.2                      py_0    conda-forge
     defusedxml                0.6.0                      py_0    conda-forge
     distlib                   0.3.1              pyh9f0ad1d_0    conda-forge
     distributed               2.30.0+3.g946c6a46          pypi_0    pypi
     dlpack                    0.3                  he1b5a44_1    conda-forge
     docutils                  0.16             py37hc8dfbb8_2    conda-forge
     double-conversion         3.1.5                he1b5a44_2    conda-forge
     editdistance              0.5.3            py37h3340039_2    conda-forge
     entrypoints               0.3             py37hc8dfbb8_1001    conda-forge
     execnet                   1.7.1                    pypi_0    pypi
     expat                     2.2.9                he1b5a44_2    conda-forge
     fastavro                  1.0.0.post1      py37h8f50634_0    conda-forge
     fastrlock                 0.5              py37h3340039_1    conda-forge
     filelock                  3.0.12             pyh9f0ad1d_0    conda-forge
     flake8                    3.8.3                      py_1    conda-forge
     flatbuffers               1.12.0               he1b5a44_0    conda-forge
     freetype                  2.10.3               he06d7ca_0    conda-forge
     fsspec                    0.8.3                      py_0    conda-forge
     future                    0.18.2           py37hc8dfbb8_1    conda-forge
     gflags                    2.2.2             he1b5a44_1004    conda-forge
     glog                      0.4.0                h49b9bf7_3    conda-forge
     gmp                       6.2.0                he1b5a44_3    conda-forge
     grpc-cpp                  1.32.0               h7997a97_1    conda-forge
     heapdict                  1.0.1                      py_0    conda-forge
     hypothesis                5.37.1                     py_0    conda-forge
     icu                       67.1                 he1b5a44_0    conda-forge
     identify                  1.5.6              pyh9f0ad1d_0    conda-forge
     idna                      2.10               pyh9f0ad1d_0    conda-forge
     imagesize                 1.2.0                      py_0    conda-forge
     importlib-metadata        2.0.0                      py_1    conda-forge
     importlib_metadata        2.0.0                         1    conda-forge
     iniconfig                 1.0.1              pyh9f0ad1d_0    conda-forge
     ipykernel                 5.3.4            py37h43977f1_0    conda-forge
     ipython                   7.18.1           py37hc6149b9_0    conda-forge
     ipython_genutils          0.2.0                      py_1    conda-forge
     isort                     5.0.7            py37hc8dfbb8_0    conda-forge
     jedi                      0.17.2           py37hc8dfbb8_1    conda-forge
     jinja2                    2.11.2             pyh9f0ad1d_0    conda-forge
     jpeg                      9d                   h516909a_0    conda-forge
     jsonschema                3.2.0            py37hc8dfbb8_1    conda-forge
     jupyter_client            6.1.7                      py_0    conda-forge
     jupyter_core              4.6.3            py37hc8dfbb8_1    conda-forge
     jupyterlab_pygments       0.1.2              pyh9f0ad1d_0    conda-forge
     krb5                      1.17.1               hfafb76e_3    conda-forge
     lcms2                     2.11                 hbd6801e_0    conda-forge
     ld_impl_linux-64          2.35                 h769bd43_9    conda-forge
     libblas                   3.8.0               17_openblas    conda-forge
     libcblas                  3.8.0               17_openblas    conda-forge
     libcurl                   7.71.1               hcdd3856_8    conda-forge
     libedit                   3.1.20191231         he28a2e2_2    conda-forge
     libev                     4.33                 h516909a_1    conda-forge
     libevent                  2.1.10               hcdb4288_2    conda-forge
     libffi                    3.2.1             he1b5a44_1007    conda-forge
     libgcc-ng                 9.3.0               h5dbcf3e_17    conda-forge
     libgfortran-ng            9.3.0               he4bcb1c_17    conda-forge
     libgfortran5              9.3.0               he4bcb1c_17    conda-forge
     libgomp                   9.3.0               h5dbcf3e_17    conda-forge
     liblapack                 3.8.0               17_openblas    conda-forge
     libllvm10                 10.0.1               he513fc3_3    conda-forge
     libllvm8                  8.0.1                hc9558a2_0    conda-forge
     libnghttp2                1.41.0               h8cfc5f6_2    conda-forge
     libopenblas               0.3.10          pthreads_h4812303_5    conda-forge
     libpng                    1.6.37               hed695b0_2    conda-forge
     libprotobuf               3.13.0.1             h8b12597_0    conda-forge
     librmm                    0.17.0a201011   cuda10.2_g2752e28_4    rapidsai-nightly
     libsodium                 1.0.18               h516909a_1    conda-forge
     libssh2                   1.9.0                hab1572f_5    conda-forge
     libstdcxx-ng              9.3.0               h2ae2ef3_17    conda-forge
     libthrift                 0.13.0               h5aa387f_6    conda-forge
     libtiff                   4.1.0                hc7e4089_6    conda-forge
     libutf8proc               2.5.0                h516909a_2    conda-forge
     libuv                     1.40.0               h516909a_0    conda-forge
     libwebp-base              1.1.0                h516909a_3    conda-forge
     llvmlite                  0.34.0           py37h5202443_2    conda-forge
     locket                    0.2.0                      py_2    conda-forge
     lz4-c                     1.9.2                he1b5a44_3    conda-forge
     markdown                  3.3                pyh9f0ad1d_0    conda-forge
     markupsafe                1.1.1            py37hb5d75c8_2    conda-forge
     mccabe                    0.6.1                      py_1    conda-forge
     mimesis                   4.0.0              pyh9f0ad1d_0    conda-forge
     mistune                   0.8.4           py37h8f50634_1002    conda-forge
     more-itertools            8.5.0                      py_0    conda-forge
     msgpack-python            1.0.0            py37h99015e2_2    conda-forge
     nbclient                  0.5.0                      py_0    conda-forge
     nbconvert                 6.0.7            py37hc8dfbb8_0    conda-forge
     nbformat                  5.0.7                      py_0    conda-forge
     nbsphinx                  0.7.1              pyh9f0ad1d_0    conda-forge
     nccl                      2.7.8.1              hc6a2c23_1    conda-forge
     ncurses                   6.2                  he1b5a44_1    conda-forge
     nest-asyncio              1.4.1                      py_0    conda-forge
     nodeenv                   1.5.0              pyh9f0ad1d_0    conda-forge
     notebook                  6.1.4            py37hc8dfbb8_0    conda-forge
     numba                     0.51.2           py37h9fdb41a_0    conda-forge
     numpy                     1.19.2           py37h7ea13bd_1    conda-forge
     numpydoc                  1.1.0                      py_1    conda-forge
     nvtx                      0.2.1            py37h8f50634_1    conda-forge
     olefile                   0.46                       py_0    conda-forge
     openssl                   1.1.1h               h516909a_0    conda-forge
     orc                       1.6.5                hd3605a7_0    conda-forge
     packaging                 20.4               pyh9f0ad1d_0    conda-forge
     pandas                    1.1.3            py37h9fdb41a_1    conda-forge
     pandavro                  1.5.2                    pypi_0    pypi
     pandoc                    1.19.2                        0    conda-forge
     pandocfilters             1.4.2                      py_1    conda-forge
     parquet-cpp               1.5.1                         2    conda-forge
     parso                     0.7.1              pyh9f0ad1d_0    conda-forge
     partd                     1.1.0                      py_0    conda-forge
     pathspec                  0.8.0              pyh9f0ad1d_0    conda-forge
     pexpect                   4.8.0            py37hc8dfbb8_1    conda-forge
     pickleshare               0.7.5           py37hc8dfbb8_1001    conda-forge
     pillow                    7.2.0            py37h718be6c_1    conda-forge
     pip                       20.2.3                     py_0    conda-forge
     pluggy                    0.13.1           py37hc8dfbb8_3    conda-forge
     pre-commit                2.7.1            py37hc8dfbb8_0    conda-forge
     pre_commit                2.7.1                         0    conda-forge
     prometheus_client         0.8.0              pyh9f0ad1d_0    conda-forge
     prompt-toolkit            3.0.7                      py_0    conda-forge
     psutil                    5.7.2            py37hb5d75c8_1    conda-forge
     ptyprocess                0.6.0                   py_1001    conda-forge
     py                        1.9.0              pyh9f0ad1d_0    conda-forge
     py-cpuinfo                7.0.0              pyh9f0ad1d_0    conda-forge
     pyarrow                   1.0.1           py37h72578d1_13_cuda    conda-forge
     pybind11                  2.5.0                    pypi_0    pypi
     pycodestyle               2.6.0              pyh9f0ad1d_0    conda-forge
     pycparser                 2.20               pyh9f0ad1d_2    conda-forge
     pyflakes                  2.2.0              pyh9f0ad1d_0    conda-forge
     pygments                  2.7.1                      py_0    conda-forge
     pyopenssl                 19.1.0                     py_1    conda-forge
     pyorc                     0.3.0                    pypi_0    pypi
     pyparsing                 2.4.7              pyh9f0ad1d_0    conda-forge
     pyrsistent                0.17.3           py37h8f50634_1    conda-forge
     pysocks                   1.7.1            py37he5f6b98_2    conda-forge
     pytest                    6.1.1            py37hc8dfbb8_1    conda-forge
     pytest-benchmark          3.2.3              pyh9f0ad1d_0    conda-forge
     pytest-forked             1.3.0                    pypi_0    pypi
     pytest-xdist              2.1.0                    pypi_0    pypi
     python                    3.7.8           h6f2ec95_1_cpython    conda-forge
     python-dateutil           2.8.1                      py_0    conda-forge
     python_abi                3.7                     1_cp37m    conda-forge
     pytz                      2020.1             pyh9f0ad1d_0    conda-forge
     pyyaml                    5.3.1            py37hb5d75c8_1    conda-forge
     pyzmq                     19.0.2           py37hac76be4_2    conda-forge
     rapidjson                 1.1.0             he1b5a44_1002    conda-forge
     re2                       2020.10.01           he1b5a44_0    conda-forge
     readline                  8.0                  he28a2e2_2    conda-forge
     recommonmark              0.6.0                      py_0    conda-forge
     regex                     2020.10.11       py37h8f50634_0    conda-forge
     requests                  2.24.0             pyh9f0ad1d_0    conda-forge
     rhash                     1.3.6             h14c3975_1001    conda-forge
     rmm                       0.17.0a201011   cuda_10.2_py37_g2752e28_4    rapidsai-nightly
     send2trash                1.5.0                      py_0    conda-forge
     setuptools                49.6.0           py37he5f6b98_2    conda-forge
     six                       1.15.0             pyh9f0ad1d_0    conda-forge
     snappy                    1.1.8                he1b5a44_3    conda-forge
     snowballstemmer           2.0.0                      py_0    conda-forge
     sortedcontainers          2.2.2              pyh9f0ad1d_0    conda-forge
     spdlog                    1.7.0                hc9558a2_2    conda-forge
     sphinx                    3.2.1                      py_0    conda-forge
     sphinx-copybutton         0.3.0              pyh9f0ad1d_0    conda-forge
     sphinx-markdown-tables    0.0.14             pyh9f0ad1d_1    conda-forge
     sphinx_rtd_theme          0.5.0              pyh9f0ad1d_0    conda-forge
     sphinxcontrib-applehelp   1.0.2                      py_0    conda-forge
     sphinxcontrib-devhelp     1.0.2                      py_0    conda-forge
     sphinxcontrib-htmlhelp    1.0.3                      py_0    conda-forge
     sphinxcontrib-jsmath      1.0.1                      py_0    conda-forge
     sphinxcontrib-qthelp      1.0.3                      py_0    conda-forge
     sphinxcontrib-serializinghtml 1.1.4                      py_0    conda-forge
     sphinxcontrib-websupport  1.2.4              pyh9f0ad1d_0    conda-forge
     sqlite                    3.33.0               h4cf870e_1    conda-forge
     streamz                   0.6.0                    pypi_0    pypi
     tblib                     1.6.0                      py_0    conda-forge
     terminado                 0.9.1            py37hc8dfbb8_0    conda-forge
     testpath                  0.4.4                      py_0    conda-forge
     tk                        8.6.10               hed695b0_1    conda-forge
     toml                      0.10.1             pyh9f0ad1d_0    conda-forge
     toolz                     0.11.1                     py_0    conda-forge
     tornado                   6.0.4            py37h8f50634_1    conda-forge
     traitlets                 5.0.4                      py_1    conda-forge
     typed-ast                 1.4.1            py37h516909a_0    conda-forge
     typing_extensions         3.7.4.2                    py_0    conda-forge
     urllib3                   1.25.10                    py_0    conda-forge
     virtualenv                20.0.33          py37hc8dfbb8_1    conda-forge
     wcwidth                   0.2.5              pyh9f0ad1d_2    conda-forge
     webencodings              0.5.1                      py_1    conda-forge
     wheel                     0.35.1             pyh9f0ad1d_0    conda-forge
     xz                        5.2.5                h516909a_1    conda-forge
     yaml                      0.2.5                h516909a_0    conda-forge
     zeromq                    4.3.3                he1b5a44_2    conda-forge
     zict                      2.0.0                      py_0    conda-forge
     zipp                      3.3.0                      py_0    conda-forge
     zlib                      1.2.11            h516909a_1009    conda-forge
     zstd                      1.4.5                h6597ccf_2    conda-forge
     
</pre></details>

**Additional context**
Surfaced while running fuzz tests: #6001 
",2020-10-22T23:45:09Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
83,[BUG] RuntimeError while reading avro file with duration types,"**Describe the bug**
Avro supports duration types through `logicalTypes`: https://avro.apache.org/docs/current/spec.html#Logical+Types
While reading an avro file with duration types, there is a RuntimeError.

**Steps/Code to reproduce bug**
Writing a duration column to avro file:

```python
from fastavro import json_writer, parse_schema

schema = {
    'doc': 'A Sample doc',
    'name': 'Sample name',
    'namespace': 'test',
    'type': 'record',
    'fields': [
        {'name': 'duration_column', ""type"": {""type"": ""long"", ""logicalType"": ""duration""}},
    ],
}
parsed_schema = parse_schema(schema)

records = [
    {'duration_column': 1433269388},
    {'duration_column': 464375847},
    {'duration_column': 498375874},
]

with open('temp_file', 'w') as out:
    json_writer(out, parsed_schema, records)
```

Reading the same file into cudf:
```python
In[32]: cudf.read_avro('temp_file')
Traceback (most recent call last):
  File ""/home/pgali/anaconda3/envs/cudf_dev/lib/python3.7/site-packages/IPython/core/interactiveshell.py"", line 3417, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-32-318dacb58dbd>"", line 1, in <module>
    cudf.read_avro('/home/pgali/Desktop/cudf/temp_file')
  File ""/home/pgali/anaconda3/envs/cudf_dev/lib/python3.7/site-packages/cudf/io/avro.py"", line 28, in read_avro
    filepath_or_buffer, columns, skiprows, num_rows
  File ""cudf/_lib/avro.pyx"", line 18, in cudf._lib.avro.read_avro
  File ""cudf/_lib/avro.pyx"", line 52, in cudf._lib.avro.read_avro
RuntimeError: cuDF failure at: /home/pgali/Desktop/cudf/cpp/src/io/avro/reader_impl.cu:80: Cannot parse metadata
```

**Expected behavior**
we should be able to infer dtype via logicalTypes.

**Environment overview (please complete the following information)**
 - Environment location: [Bare-metal]
 - Method of cuDF install: [from source]


**Environment details**
Please run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details
<details><summary>Click here to see environment details</summary><pre>
     
     **git***
     commit 935dca4ff72d1b9fe8911275497e279314d583f9 (HEAD -> avro_fuzz_tests)
     Merge: 843da29147 4b4d962651
     Author: galipremsagar <sagarprem75@gmail.com>
     Date:   Tue Oct 27 10:31:06 2020 -0500
     
     Merge remote-tracking branch 'upstream/branch-0.17' into avro_fuzz_tests
     **git submodules***
     
     ***OS Information***
     DISTRIB_ID=Ubuntu
     DISTRIB_RELEASE=20.04
     DISTRIB_CODENAME=focal
     DISTRIB_DESCRIPTION=""Ubuntu 20.04.1 LTS""
     NAME=""Ubuntu""
     VERSION=""20.04.1 LTS (Focal Fossa)""
     ID=ubuntu
     ID_LIKE=debian
     PRETTY_NAME=""Ubuntu 20.04.1 LTS""
     VERSION_ID=""20.04""
     HOME_URL=""https://www.ubuntu.com/""
     SUPPORT_URL=""https://help.ubuntu.com/""
     BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
     PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
     VERSION_CODENAME=focal
     UBUNTU_CODENAME=focal
     Linux pgali-HP-Z8-G4-Workstation 5.4.0-52-generic #57-Ubuntu SMP Thu Oct 15 10:57:00 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
     
     ***GPU Information***
     Tue Oct 27 12:08:06 2020
     +-----------------------------------------------------------------------------+
     | NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |
     |-------------------------------+----------------------+----------------------+
     | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
     | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
     |                               |                      |               MIG M. |
     |===============================+======================+======================|
     |   0  Quadro RTX 8000     Off  | 00000000:22:00.0 Off |                  Off |
     | 33%   31C    P8     4W / 260W |    539MiB / 48601MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   1  Quadro RTX 8000     Off  | 00000000:2D:00.0  On |                  Off |
     | 33%   35C    P8    20W / 260W |    387MiB / 48592MiB |      9%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     
     +-----------------------------------------------------------------------------+
     | Processes:                                                                  |
     |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
     |        ID   ID                                                   Usage      |
     |=============================================================================|
     |    0   N/A  N/A       978      G   /usr/lib/xorg/Xorg                  4MiB |
     |    0   N/A  N/A      1940      G   /usr/lib/xorg/Xorg                  4MiB |
     |    0   N/A  N/A     12815      C   .../envs/cudf_dev/bin/python      525MiB |
     |    1   N/A  N/A       978      G   /usr/lib/xorg/Xorg                 39MiB |
     |    1   N/A  N/A      1940      G   /usr/lib/xorg/Xorg                159MiB |
     |    1   N/A  N/A      2068      G   /usr/bin/gnome-shell              176MiB |
     +-----------------------------------------------------------------------------+
     
     ***CPU***
     Architecture:                    x86_64
     CPU op-mode(s):                  32-bit, 64-bit
     Byte Order:                      Little Endian
     Address sizes:                   46 bits physical, 48 bits virtual
     CPU(s):                          12
     On-line CPU(s) list:             0-11
     Thread(s) per core:              2
     Core(s) per socket:              6
     Socket(s):                       1
     NUMA node(s):                    1
     Vendor ID:                       GenuineIntel
     CPU family:                      6
     Model:                           85
     Model name:                      Intel(R) Xeon(R) Gold 6128 CPU @ 3.40GHz
     Stepping:                        4
     CPU MHz:                         1200.854
     CPU max MHz:                     3700.0000
     CPU min MHz:                     1200.0000
     BogoMIPS:                        6800.00
     Virtualization:                  VT-x
     L1d cache:                       192 KiB
     L1i cache:                       192 KiB
     L2 cache:                        6 MiB
     L3 cache:                        19.3 MiB
     NUMA node0 CPU(s):               0-11
     Vulnerability Itlb multihit:     KVM: Vulnerable
     Vulnerability L1tf:              Mitigation; PTE Inversion
     Vulnerability Mds:               Mitigation; Clear CPU buffers; SMT vulnerable
     Vulnerability Meltdown:          Mitigation; PTI
     Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp
     Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
     Vulnerability Spectre v2:        Mitigation; Full generic retpoline, IBPB conditional, IBRS_FW, STIBP conditional, RSB filling
     Vulnerability Srbds:             Not affected
     Vulnerability Tsx async abort:   Mitigation; Clear CPU buffers; SMT vulnerable
     Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke md_clear flush_l1d
     
     ***CMake***
     /home/pgali/anaconda3/envs/cudf_dev/bin/cmake
     cmake version 3.18.2
     
     CMake suite maintained and supported by Kitware (kitware.com/cmake).
     
     ***g++***
     /usr/bin/g++
     g++ (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
     Copyright (C) 2019 Free Software Foundation, Inc.
     This is free software; see the source for copying conditions.  There is NO
     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
     
     
     ***nvcc***
     /usr/local/cuda/bin/nvcc
     nvcc: NVIDIA (R) Cuda compiler driver
     Copyright (c) 2005-2020 NVIDIA Corporation
     Built on Wed_Jul_22_19:09:09_PDT_2020
     Cuda compilation tools, release 11.0, V11.0.221
     Build cuda_11.0_bu.TC445_37.28845127_0
     
     ***Python***
     /home/pgali/anaconda3/envs/cudf_dev/bin/python
     Python 3.7.8
     
     ***Environment Variables***
     PATH                            : /home/pgali/anaconda3/envs/cudf_dev/bin:/home/pgali/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin
     LD_LIBRARY_PATH                 : :/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64
     NUMBAPRO_NVVM                   :
     NUMBAPRO_LIBDEVICE              :
     CONDA_PREFIX                    : /home/pgali/anaconda3/envs/cudf_dev
     PYTHON_PATH                     :
     
     ***conda packages***
     /home/pgali/anaconda3/condabin/conda
     # packages in environment at /home/pgali/anaconda3/envs/cudf_dev:
     #
     # Name                    Version                   Build  Channel
     _libgcc_mutex             0.1                 conda_forge    conda-forge
     _openmp_mutex             4.5                       1_gnu    conda-forge
     abseil-cpp                20200225.2           he1b5a44_2    conda-forge
     alabaster                 0.7.12                     py_0    conda-forge
     apipkg                    1.5                      pypi_0    pypi
     appdirs                   1.4.3                      py_1    conda-forge
     argon2-cffi               20.1.0           py37h8f50634_2    conda-forge
     arrow-cpp                 1.0.1           py37hd1d2865_14_cuda    conda-forge
     arrow-cpp-proc            1.0.1                      cuda    conda-forge
     async_generator           1.10                       py_0    conda-forge
     attrs                     20.2.0             pyh9f0ad1d_0    conda-forge
     aws-c-common              0.4.59               he1b5a44_0    conda-forge
     aws-c-event-stream        0.1.6                h84e28f3_5    conda-forge
     aws-checksums             0.1.9                he252421_2    conda-forge
     aws-sdk-cpp               1.8.63               h9b98462_0    conda-forge
     babel                     2.8.0                      py_0    conda-forge
     backcall                  0.2.0              pyh9f0ad1d_0    conda-forge
     backports                 1.0                        py_2    conda-forge
     backports.functools_lru_cache 1.6.1                      py_0    conda-forge
     black                     19.10b0                    py_4    conda-forge
     bleach                    3.2.1              pyh9f0ad1d_0    conda-forge
     bokeh                     2.2.2            py37hc8dfbb8_0    conda-forge
     boost-cpp                 1.74.0               h9359b55_0    conda-forge
     brotli                    1.0.9                he1b5a44_2    conda-forge
     brotlipy                  0.7.0           py37hb5d75c8_1001    conda-forge
     bzip2                     1.0.8                h516909a_3    conda-forge
     c-ares                    1.16.1               h516909a_3    conda-forge
     ca-certificates           2020.6.20            hecda079_0    conda-forge
     certifi                   2020.6.20        py37he5f6b98_2    conda-forge
     cffi                      1.14.3           py37h00ebd2e_1    conda-forge
     cfgv                      3.2.0                      py_0    conda-forge
     chardet                   3.0.4           py37he5f6b98_1008    conda-forge
     clang                     8.0.1                hc9558a2_2    conda-forge
     clang-tools               8.0.1                hc9558a2_2    conda-forge
     clangxx                   8.0.1                         2    conda-forge
     click                     7.1.2              pyh9f0ad1d_0    conda-forge
     cloudpickle               1.6.0                      py_0    conda-forge
     cmake                     3.18.2               h3f3948e_0    conda-forge
     cmake_setuptools          0.1.3                      py_0    rapidsai
     commonmark                0.9.1                      py_0    conda-forge
     cryptography              3.1.1            py37hff6837a_1    conda-forge
     cudatoolkit               11.0.221             h6bb024c_0    nvidia
     cudf                      0.17.0a0+97.ge33bf4297d          pypi_0    pypi
     cudnn                     8.0.0                cuda11.0_0    nvidia
     cupy                      7.8.0            py37h0ce7dbb_0    rapidsai
     cython                    0.29.21          py37hb892b2f_1    conda-forge
     cytoolz                   0.11.0           py37h8f50634_1    conda-forge
     dask                      2.30.0+16.gfbe51746          pypi_0    pypi
     dask-cudf                 0.17.0a0+112.g63facc645d.dirty          pypi_0    pypi
     decorator                 4.4.2                      py_0    conda-forge
     defusedxml                0.6.0                      py_0    conda-forge
     distlib                   0.3.1              pyh9f0ad1d_0    conda-forge
     distributed               2.30.0+7.gdcb46d07          pypi_0    pypi
     dlpack                    0.3                  he1b5a44_1    conda-forge
     docutils                  0.16             py37he5f6b98_2    conda-forge
     double-conversion         3.1.5                he1b5a44_2    conda-forge
     editdistance              0.5.3            py37h3340039_2    conda-forge
     entrypoints               0.3             py37hc8dfbb8_1002    conda-forge
     execnet                   1.7.1                    pypi_0    pypi
     expat                     2.2.9                he1b5a44_2    conda-forge
     fastavro                  1.0.0.post1      py37h8f50634_1    conda-forge
     fastrlock                 0.5              py37h3340039_1    conda-forge
     filelock                  3.0.12             pyh9f0ad1d_0    conda-forge
     flake8                    3.8.3                      py_1    conda-forge
     flatbuffers               1.12.0               he1b5a44_0    conda-forge
     freetype                  2.10.3               he06d7ca_0    conda-forge
     fsspec                    0.8.4                      py_0    conda-forge
     future                    0.18.2           py37hc8dfbb8_2    conda-forge
     gflags                    2.2.2             he1b5a44_1004    conda-forge
     glog                      0.4.0                h49b9bf7_3    conda-forge
     gmp                       6.2.0                he1b5a44_3    conda-forge
     grpc-cpp                  1.32.0               h7997a97_1    conda-forge
     heapdict                  1.0.1                      py_0    conda-forge
     hypothesis                5.37.3                     py_0    conda-forge
     icu                       67.1                 he1b5a44_0    conda-forge
     identify                  1.5.6              pyh9f0ad1d_0    conda-forge
     idna                      2.10               pyh9f0ad1d_0    conda-forge
     imagesize                 1.2.0                      py_0    conda-forge
     importlib-metadata        2.0.0                      py_1    conda-forge
     importlib_metadata        2.0.0                         1    conda-forge
     iniconfig                 1.1.0              pyh9f0ad1d_0    conda-forge
     ipykernel                 5.3.4            py37hc6149b9_1    conda-forge
     ipython                   7.18.1           py37hc6149b9_1    conda-forge
     ipython_genutils          0.2.0                      py_1    conda-forge
     isort                     5.0.7            py37hc8dfbb8_0    conda-forge
     jedi                      0.17.2           py37hc8dfbb8_1    conda-forge
     jinja2                    2.11.2             pyh9f0ad1d_0    conda-forge
     jpeg                      9d                   h516909a_0    conda-forge
     jsonschema                3.2.0                      py_2    conda-forge
     jupyter_client            6.1.7                      py_0    conda-forge
     jupyter_core              4.6.3            py37hc8dfbb8_2    conda-forge
     jupyterlab_pygments       0.1.2              pyh9f0ad1d_0    conda-forge
     krb5                      1.17.1               hfafb76e_3    conda-forge
     lcms2                     2.11                 hbd6801e_0    conda-forge
     ld_impl_linux-64          2.35                 h769bd43_9    conda-forge
     libblas                   3.8.0               17_openblas    conda-forge
     libcblas                  3.8.0               17_openblas    conda-forge
     libcurl                   7.71.1               hcdd3856_8    conda-forge
     libedit                   3.1.20191231         he28a2e2_2    conda-forge
     libev                     4.33                 h516909a_1    conda-forge
     libevent                  2.1.10               hcdb4288_3    conda-forge
     libffi                    3.2.1             he1b5a44_1007    conda-forge
     libgcc-ng                 9.3.0               h5dbcf3e_17    conda-forge
     libgfortran-ng            9.3.0               he4bcb1c_17    conda-forge
     libgfortran5              9.3.0               he4bcb1c_17    conda-forge
     libgomp                   9.3.0               h5dbcf3e_17    conda-forge
     liblapack                 3.8.0               17_openblas    conda-forge
     libllvm10                 10.0.1               he513fc3_3    conda-forge
     libllvm8                  8.0.1                hc9558a2_0    conda-forge
     libnghttp2                1.41.0               h8cfc5f6_2    conda-forge
     libopenblas               0.3.10          pthreads_h4812303_5    conda-forge
     libpng                    1.6.37               hed695b0_2    conda-forge
     libprotobuf               3.13.0.1             h8b12597_0    conda-forge
     librmm                    0.17.0a201018   cuda11.0_g8cdd176_21    rapidsai-nightly
     libsodium                 1.0.18               h516909a_1    conda-forge
     libssh2                   1.9.0                hab1572f_5    conda-forge
     libstdcxx-ng              9.3.0               h2ae2ef3_17    conda-forge
     libthrift                 0.13.0               h5aa387f_6    conda-forge
     libtiff                   4.1.0                hc7e4089_6    conda-forge
     libutf8proc               2.5.0                h516909a_2    conda-forge
     libuv                     1.40.0               hd18ef5c_0    conda-forge
     libwebp-base              1.1.0                h516909a_3    conda-forge
     llvmlite                  0.34.0           py37h5202443_2    conda-forge
     locket                    0.2.0                      py_2    conda-forge
     lz4-c                     1.9.2                he1b5a44_3    conda-forge
     markdown                  3.3.1              pyh9f0ad1d_0    conda-forge
     markupsafe                1.1.1            py37hb5d75c8_2    conda-forge
     mccabe                    0.6.1                      py_1    conda-forge
     mimesis                   4.0.0              pyh9f0ad1d_0    conda-forge
     mistune                   0.8.4           py37h8f50634_1002    conda-forge
     more-itertools            8.5.0                      py_0    conda-forge
     msgpack-python            1.0.0            py37h99015e2_2    conda-forge
     nbclient                  0.5.1                      py_0    conda-forge
     nbconvert                 6.0.7            py37hc8dfbb8_1    conda-forge
     nbformat                  5.0.8                      py_0    conda-forge
     nbsphinx                  0.7.1              pyh9f0ad1d_0    conda-forge
     nccl                      2.7.8.1            h4962215_100    nvidia
     ncurses                   6.2                  he1b5a44_2    conda-forge
     nest-asyncio              1.4.1                      py_0    conda-forge
     nodeenv                   1.5.0              pyh9f0ad1d_0    conda-forge
     notebook                  6.1.4            py37hc8dfbb8_1    conda-forge
     numba                     0.51.2           py37h9fdb41a_0    conda-forge
     numpy                     1.19.2           py37h7ea13bd_1    conda-forge
     numpydoc                  1.1.0                      py_1    conda-forge
     nvtx                      0.2.1            py37h8f50634_2    conda-forge
     olefile                   0.46               pyh9f0ad1d_1    conda-forge
     openssl                   1.1.1h               h516909a_0    conda-forge
     orc                       1.6.5                hd3605a7_0    conda-forge
     packaging                 20.4               pyh9f0ad1d_0    conda-forge
     pandas                    1.1.3            py37h9fdb41a_2    conda-forge
     pandavro                  1.5.2                    pypi_0    pypi
     pandoc                    1.19.2                        0    conda-forge
     pandocfilters             1.4.2                      py_1    conda-forge
     parquet-cpp               1.5.1                         2    conda-forge
     parso                     0.7.1              pyh9f0ad1d_0    conda-forge
     partd                     1.1.0                      py_0    conda-forge
     pathspec                  0.8.0              pyh9f0ad1d_0    conda-forge
     pexpect                   4.8.0              pyh9f0ad1d_2    conda-forge
     pickleshare               0.7.5                   py_1003    conda-forge
     pillow                    8.0.0            py37h718be6c_0    conda-forge
     pip                       20.2.4                     py_0    conda-forge
     pluggy                    0.13.1           py37hc8dfbb8_3    conda-forge
     pre-commit                2.7.1            py37hc8dfbb8_1    conda-forge
     pre_commit                2.7.1                         1    conda-forge
     prometheus_client         0.8.0              pyh9f0ad1d_0    conda-forge
     prompt-toolkit            3.0.8                      py_0    conda-forge
     psutil                    5.7.2            py37hb5d75c8_1    conda-forge
     ptyprocess                0.6.0                   py_1001    conda-forge
     py                        1.9.0              pyh9f0ad1d_0    conda-forge
     py-cpuinfo                7.0.0              pyh9f0ad1d_0    conda-forge
     pyarrow                   1.0.1           py37h5b20ac3_14_cuda    conda-forge
     pybind11                  2.5.0                    pypi_0    pypi
     pycodestyle               2.6.0              pyh9f0ad1d_0    conda-forge
     pycparser                 2.20               pyh9f0ad1d_2    conda-forge
     pyflakes                  2.2.0              pyh9f0ad1d_0    conda-forge
     pygments                  2.7.1                      py_0    conda-forge
     pyopenssl                 19.1.0                     py_1    conda-forge
     pyorc                     0.3.0                    pypi_0    pypi
     pyparsing                 2.4.7              pyh9f0ad1d_0    conda-forge
     pyrsistent                0.17.3           py37h8f50634_1    conda-forge
     pysocks                   1.7.1            py37he5f6b98_2    conda-forge
     pytest                    6.1.1            py37hc8dfbb8_1    conda-forge
     pytest-benchmark          3.2.3              pyh9f0ad1d_0    conda-forge
     pytest-forked             1.3.0                    pypi_0    pypi
     pytest-xdist              2.1.0                    pypi_0    pypi
     python                    3.7.8           h6f2ec95_1_cpython    conda-forge
     python-dateutil           2.8.1                      py_0    conda-forge
     python_abi                3.7                     1_cp37m    conda-forge
     pytz                      2020.1             pyh9f0ad1d_0    conda-forge
     pyyaml                    5.3.1            py37hb5d75c8_1    conda-forge
     pyzmq                     19.0.2           py37hac76be4_2    conda-forge
     rapidjson                 1.1.0             he1b5a44_1002    conda-forge
     re2                       2020.10.01           he1b5a44_0    conda-forge
     readline                  8.0                  he28a2e2_2    conda-forge
     recommonmark              0.6.0                      py_0    conda-forge
     regex                     2020.10.15       py37h8f50634_0    conda-forge
     requests                  2.24.0             pyh9f0ad1d_0    conda-forge
     rhash                     1.3.6             h516909a_1001    conda-forge
     rmm                       0.17.0a201017   cuda_11.0_py37_g8cdd176_21    rapidsai-nightly
     send2trash                1.5.0                      py_0    conda-forge
     setuptools                49.6.0           py37he5f6b98_2    conda-forge
     six                       1.15.0             pyh9f0ad1d_0    conda-forge
     snappy                    1.1.8                he1b5a44_3    conda-forge
     snowballstemmer           2.0.0                      py_0    conda-forge
     sortedcontainers          2.2.2              pyh9f0ad1d_0    conda-forge
     spdlog                    1.7.0                hc9558a2_2    conda-forge
     sphinx                    3.2.1                      py_0    conda-forge
     sphinx-copybutton         0.3.0              pyh9f0ad1d_0    conda-forge
     sphinx-markdown-tables    0.0.14             pyh9f0ad1d_1    conda-forge
     sphinx_rtd_theme          0.5.0              pyh9f0ad1d_0    conda-forge
     sphinxcontrib-applehelp   1.0.2                      py_0    conda-forge
     sphinxcontrib-devhelp     1.0.2                      py_0    conda-forge
     sphinxcontrib-htmlhelp    1.0.3                      py_0    conda-forge
     sphinxcontrib-jsmath      1.0.1                      py_0    conda-forge
     sphinxcontrib-qthelp      1.0.3                      py_0    conda-forge
     sphinxcontrib-serializinghtml 1.1.4                      py_0    conda-forge
     sphinxcontrib-websupport  1.2.4              pyh9f0ad1d_0    conda-forge
     sqlite                    3.33.0               h4cf870e_1    conda-forge
     streamz                   0.6.0                    pypi_0    pypi
     tblib                     1.6.0                      py_0    conda-forge
     terminado                 0.9.1            py37hc8dfbb8_1    conda-forge
     testpath                  0.4.4                      py_0    conda-forge
     tk                        8.6.10               hed695b0_1    conda-forge
     toml                      0.10.1             pyh9f0ad1d_0    conda-forge
     toolz                     0.11.1                     py_0    conda-forge
     tornado                   6.0.4            py37h8f50634_2    conda-forge
     traitlets                 5.0.5                      py_0    conda-forge
     typed-ast                 1.4.1            py37h516909a_0    conda-forge
     typing_extensions         3.7.4.3                    py_0    conda-forge
     urllib3                   1.25.10                    py_0    conda-forge
     virtualenv                20.0.35          py37hc8dfbb8_0    conda-forge
     wcwidth                   0.2.5              pyh9f0ad1d_2    conda-forge
     webencodings              0.5.1                      py_1    conda-forge
     wheel                     0.35.1             pyh9f0ad1d_0    conda-forge
     xz                        5.2.5                h516909a_1    conda-forge
     yaml                      0.2.5                h516909a_0    conda-forge
     zeromq                    4.3.3                he1b5a44_2    conda-forge
     zict                      2.0.0                      py_0    conda-forge
     zipp                      3.3.1                      py_0    conda-forge
     zlib                      1.2.11            h516909a_1010    conda-forge
     zstd                      1.4.5                h6597ccf_2    conda-forge
     
</pre></details>


**Additional context**
Surfaced while fuzz testing: #6001 
",2020-10-27T17:08:57Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
84,[FEA] Java ColumnVector should support async copyToHost,"**Is your feature request related to a problem? Please describe.**
During Spark's shuffle table columns are copied back to the host via `copyToHost`.  Each operation is synchronous with the default stream.  This causes unnecessary synchronization when there are many columns to copy, since only a single synchronization at the end of all copy transfers is necessary.  Also the copy interface does not allow a stream to be specified.

**Describe the solution you'd like**
`ColumnVector` should provide a `copyToHostAsync` method that can specify an optional stream to use for the copy operations.  The Javadoc for the method should make it clear that the caller must synchronize with the stream before using the resulting `HostColumnVector` instance.

**Describe alternatives you've considered**
This can be accomplished today via reaching into the `ColumnVector` and accessing the individual `DeviceMemoryBuffer` instances (e.g.: data, validity, offsets), but this is complicated for the caller and especially so when child columns are involved for nested types.
",2020-10-30T18:05:03Z,0,0,Jason Lowe,NVIDIA,True
85,[FEA] Implement skipinitialspace read_csv parameter,"This parameter is already a part of the API, but it is not used internally.

From experiments with Pandas, it looks like this parameter is only relevant when determining if a given field has a special value (True, False, NaN). With `skipinitialspace=True`, leading spaces should be ignored and files with content like `"" NaN\n   n/a\n""` can then be parsed correctly.",2020-11-03T23:38:40Z,0,0,Vukasin Milovanovic,NVIDIA,True
86,[BUG] csv writer returning full-subsecond data incase of duration types,"**Describe the bug**
When there is a column with `timedelta64` dtype and there is no sub-second data for a particular value, pandas doesn't output sub-second data as part of csv string output. But, whereas in cudf we are currently converting all values of the column with sub-second data.

**Steps/Code to reproduce bug**
```python
In[2]: import pandas as pd
In[3]: pdf = pd.DataFrame()
In[4]: pdf['a'] = pd.Series([432343244342, 34223423423, 32432423432432], dtype='timedelta64[ns]')
In[5]: pdf
Out[5]: 
                          a
0 0 days 00:07:12.343244342
1 0 days 00:00:34.223423423
2 0 days 09:00:32.423432432
In[6]: pdf.to_csv()
Out[6]: ',a\n0,0 days 00:07:12.343244342\n1,0 days 00:00:34.223423423\n2,0 days 09:00:32.423432432\n'
In[7]: pdf['a'][0] = pd.Timedelta('106751 days 23:07:15')
In[8]: pdf
Out[8]: 
                          a
0      106751 days 23:07:15
1 0 days 00:00:34.223423423
2 0 days 09:00:32.423432432

# The value at 0-th index doesn't have any sub-second data, thus all the 9digits of sub-second data is not returned in csv
In[9]: pdf.to_csv()
Out[9]: ',a\n0,106751 days 23:07:15\n1,0 days 00:00:34.223423423\n2,0 days 09:00:32.423432432\n'
In[10]: import cudf
In[11]: gdf = cudf.from_pandas(pdf)
In[12]: gdf
Out[12]: 
                          a
0      106751 days 23:07:15
1 0 days 00:00:34.223423423
2 0 days 09:00:32.423432432

# Whereas incase of cudf, we are returning all 9digits of sub-second data irrespective of sub-second data being present or not.
In[13]: gdf.to_csv()
Out[13]: ',a\n0,106751 days 23:07:15.000000000\n1,0 days 00:00:34.223423423\n2,0 days 09:00:32.423432432\n'

# If there is any sub-second data pandas returns full sub-second resolution upto nano-seconds.

In[14]: pdf['a'][1] = pd.Timedelta('106751 days 23:07:15.12')
In[15]: pdf
Out[15]: 
                            a
0        106751 days 23:07:15
1 106751 days 23:07:15.120000
2   0 days 09:00:32.423432432
In[16]: pdf.to_csv()
Out[16]: ',a\n0,106751 days 23:07:15\n1,106751 days 23:07:15.120000\n2,0 days 09:00:32.423432432\n'

```


**Expected behavior**
I think we should be matching the pandas behavior here, because when when we read back the csv file cudf or pandas infer it as a string column and will ultimately result in storing extra values where not necessary.

**Environment overview (please complete the following information)**
 - Environment location: [Bare-metal]
 - Method of cuDF install: [from source]


**Environment details**
Please run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details

<details><summary>Click here to see environment details</summary><pre>
     
     **git***
     commit 7bea2645ca4bfd9db344e7f84bc61438749770e3 (HEAD -> branch-0.17, upstream/branch-0.17)
     Author: Conor Hoekstra <36027403+codereport@users.noreply.github.com>
     Date:   Tue Nov 3 03:56:49 2020 -0500
     
     Implement `cudf::round` floating point and integer types (`HALF_UP`) (#6562)
     **git submodules***
     
     ***OS Information***
     DISTRIB_ID=Ubuntu
     DISTRIB_RELEASE=20.04
     DISTRIB_CODENAME=focal
     DISTRIB_DESCRIPTION=""Ubuntu 20.04.1 LTS""
     NAME=""Ubuntu""
     VERSION=""20.04.1 LTS (Focal Fossa)""
     ID=ubuntu
     ID_LIKE=debian
     PRETTY_NAME=""Ubuntu 20.04.1 LTS""
     VERSION_ID=""20.04""
     HOME_URL=""https://www.ubuntu.com/""
     SUPPORT_URL=""https://help.ubuntu.com/""
     BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
     PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
     VERSION_CODENAME=focal
     UBUNTU_CODENAME=focal
     Linux pgali-HP-Z8-G4-Workstation 5.4.0-52-generic #57-Ubuntu SMP Thu Oct 15 10:57:00 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
     
     ***GPU Information***
     Tue Nov  3 18:02:45 2020
     +-----------------------------------------------------------------------------+
     | NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |
     |-------------------------------+----------------------+----------------------+
     | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
     | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
     |                               |                      |               MIG M. |
     |===============================+======================+======================|
     |   0  Quadro RTX 8000     Off  | 00000000:22:00.0 Off |                  Off |
     | 33%   30C    P8     4W / 260W |   2132MiB / 48601MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   1  Quadro RTX 8000     Off  | 00000000:2D:00.0  On |                  Off |
     | 33%   34C    P8    22W / 260W |    351MiB / 48592MiB |     18%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     
     +-----------------------------------------------------------------------------+
     | Processes:                                                                  |
     |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
     |        ID   ID                                                   Usage      |
     |=============================================================================|
     |    0   N/A  N/A       963      G   /usr/lib/xorg/Xorg                  4MiB |
     |    0   N/A  N/A      1833      G   /usr/lib/xorg/Xorg                  4MiB |
     |    0   N/A  N/A      5362      C   python                            531MiB |
     |    0   N/A  N/A      6150      C   python                            529MiB |
     |    0   N/A  N/A      7208      C   python                            529MiB |
     |    0   N/A  N/A      7343      C   .../envs/cudf_dev/bin/python      529MiB |
     |    1   N/A  N/A       963      G   /usr/lib/xorg/Xorg                 39MiB |
     |    1   N/A  N/A      1833      G   /usr/lib/xorg/Xorg                153MiB |
     |    1   N/A  N/A      1960      G   /usr/bin/gnome-shell              146MiB |
     +-----------------------------------------------------------------------------+
     
     ***CPU***
     Architecture:                    x86_64
     CPU op-mode(s):                  32-bit, 64-bit
     Byte Order:                      Little Endian
     Address sizes:                   46 bits physical, 48 bits virtual
     CPU(s):                          12
     On-line CPU(s) list:             0-11
     Thread(s) per core:              2
     Core(s) per socket:              6
     Socket(s):                       1
     NUMA node(s):                    1
     Vendor ID:                       GenuineIntel
     CPU family:                      6
     Model:                           85
     Model name:                      Intel(R) Xeon(R) Gold 6128 CPU @ 3.40GHz
     Stepping:                        4
     CPU MHz:                         1200.477
     CPU max MHz:                     3700.0000
     CPU min MHz:                     1200.0000
     BogoMIPS:                        6800.00
     Virtualization:                  VT-x
     L1d cache:                       192 KiB
     L1i cache:                       192 KiB
     L2 cache:                        6 MiB
     L3 cache:                        19.3 MiB
     NUMA node0 CPU(s):               0-11
     Vulnerability Itlb multihit:     KVM: Vulnerable
     Vulnerability L1tf:              Mitigation; PTE Inversion
     Vulnerability Mds:               Mitigation; Clear CPU buffers; SMT vulnerable
     Vulnerability Meltdown:          Mitigation; PTI
     Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp
     Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
     Vulnerability Spectre v2:        Mitigation; Full generic retpoline, IBPB conditional, IBRS_FW, STIBP conditional, RSB filling
     Vulnerability Srbds:             Not affected
     Vulnerability Tsx async abort:   Mitigation; Clear CPU buffers; SMT vulnerable
     Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke md_clear flush_l1d
     
     ***CMake***
     /home/pgali/anaconda3/envs/cudf_dev/bin/cmake
     cmake version 3.18.2
     
     CMake suite maintained and supported by Kitware (kitware.com/cmake).
     
     ***g++***
     /usr/bin/g++
     g++ (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
     Copyright (C) 2019 Free Software Foundation, Inc.
     This is free software; see the source for copying conditions.  There is NO
     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
     
     
     ***nvcc***
     /usr/local/cuda/bin/nvcc
     nvcc: NVIDIA (R) Cuda compiler driver
     Copyright (c) 2005-2020 NVIDIA Corporation
     Built on Wed_Jul_22_19:09:09_PDT_2020
     Cuda compilation tools, release 11.0, V11.0.221
     Build cuda_11.0_bu.TC445_37.28845127_0
     
     ***Python***
     /home/pgali/anaconda3/envs/cudf_dev/bin/python
     Python 3.7.8
     
     ***Environment Variables***
     PATH                            : /home/pgali/anaconda3/envs/cudf_dev/bin:/home/pgali/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin
     LD_LIBRARY_PATH                 : :/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64
     NUMBAPRO_NVVM                   :
     NUMBAPRO_LIBDEVICE              :
     CONDA_PREFIX                    : /home/pgali/anaconda3/envs/cudf_dev
     PYTHON_PATH                     :
     
     ***conda packages***
     /home/pgali/anaconda3/condabin/conda
     # packages in environment at /home/pgali/anaconda3/envs/cudf_dev:
     #
     # Name                    Version                   Build  Channel
     _libgcc_mutex             0.1                 conda_forge    conda-forge
     _openmp_mutex             4.5                       1_gnu    conda-forge
     abseil-cpp                20200225.2           he1b5a44_2    conda-forge
     alabaster                 0.7.12                     py_0    conda-forge
     apipkg                    1.5                      pypi_0    pypi
     appdirs                   1.4.3                      py_1    conda-forge
     argon2-cffi               20.1.0           py37h8f50634_2    conda-forge
     arrow-cpp                 1.0.1           py37hd1d2865_14_cuda    conda-forge
     arrow-cpp-proc            1.0.1                      cuda    conda-forge
     async_generator           1.10                       py_0    conda-forge
     attrs                     20.2.0             pyh9f0ad1d_0    conda-forge
     aws-c-common              0.4.59               he1b5a44_0    conda-forge
     aws-c-event-stream        0.1.6                h84e28f3_5    conda-forge
     aws-checksums             0.1.9                he252421_2    conda-forge
     aws-sdk-cpp               1.8.63               h9b98462_0    conda-forge
     babel                     2.8.0                      py_0    conda-forge
     backcall                  0.2.0              pyh9f0ad1d_0    conda-forge
     backports                 1.0                        py_2    conda-forge
     backports.functools_lru_cache 1.6.1                      py_0    conda-forge
     black                     19.10b0                    py_4    conda-forge
     bleach                    3.2.1              pyh9f0ad1d_0    conda-forge
     bokeh                     2.2.2            py37hc8dfbb8_0    conda-forge
     boost-cpp                 1.74.0               h9359b55_0    conda-forge
     brotli                    1.0.9                he1b5a44_2    conda-forge
     brotlipy                  0.7.0           py37hb5d75c8_1001    conda-forge
     bzip2                     1.0.8                h516909a_3    conda-forge
     c-ares                    1.16.1               h516909a_3    conda-forge
     ca-certificates           2020.6.20            hecda079_0    conda-forge
     certifi                   2020.6.20        py37he5f6b98_2    conda-forge
     cffi                      1.14.3           py37h00ebd2e_1    conda-forge
     cfgv                      3.2.0                      py_0    conda-forge
     chardet                   3.0.4           py37he5f6b98_1008    conda-forge
     clang                     8.0.1                hc9558a2_2    conda-forge
     clang-tools               8.0.1                hc9558a2_2    conda-forge
     clangxx                   8.0.1                         2    conda-forge
     click                     7.1.2              pyh9f0ad1d_0    conda-forge
     cloudpickle               1.6.0                      py_0    conda-forge
     cmake                     3.18.2               h3f3948e_0    conda-forge
     cmake_setuptools          0.1.3                      py_0    rapidsai
     commonmark                0.9.1                      py_0    conda-forge
     cryptography              3.1.1            py37hff6837a_1    conda-forge
     cudatoolkit               11.0.221             h6bb024c_0    nvidia
     cudf                      0.17.0a0+92.gf552adaeb3          pypi_0    pypi
     cudnn                     8.0.0                cuda11.0_0    nvidia
     cupy                      7.8.0            py37h0ce7dbb_0    rapidsai
     cython                    0.29.21          py37hb892b2f_1    conda-forge
     cytoolz                   0.11.0           py37h8f50634_1    conda-forge
     dask                      2.30.0+16.gfbe51746          pypi_0    pypi
     dask-cudf                 0.17.0a0+249.g979dc78c98.dirty          pypi_0    pypi
     decorator                 4.4.2                      py_0    conda-forge
     defusedxml                0.6.0                      py_0    conda-forge
     distlib                   0.3.1              pyh9f0ad1d_0    conda-forge
     distributed               2.30.0+7.gdcb46d07          pypi_0    pypi
     dlpack                    0.3                  he1b5a44_1    conda-forge
     docutils                  0.16             py37he5f6b98_2    conda-forge
     double-conversion         3.1.5                he1b5a44_2    conda-forge
     editdistance              0.5.3            py37h3340039_2    conda-forge
     entrypoints               0.3             py37hc8dfbb8_1002    conda-forge
     execnet                   1.7.1                    pypi_0    pypi
     expat                     2.2.9                he1b5a44_2    conda-forge
     fastavro                  1.0.0.post1      py37h8f50634_1    conda-forge
     fastrlock                 0.5              py37h3340039_1    conda-forge
     filelock                  3.0.12             pyh9f0ad1d_0    conda-forge
     flake8                    3.8.3                      py_1    conda-forge
     flatbuffers               1.12.0               he1b5a44_0    conda-forge
     freetype                  2.10.3               he06d7ca_0    conda-forge
     fsspec                    0.8.4                      py_0    conda-forge
     future                    0.18.2           py37hc8dfbb8_2    conda-forge
     gflags                    2.2.2             he1b5a44_1004    conda-forge
     glog                      0.4.0                h49b9bf7_3    conda-forge
     gmp                       6.2.0                he1b5a44_3    conda-forge
     grpc-cpp                  1.32.0               h7997a97_1    conda-forge
     heapdict                  1.0.1                      py_0    conda-forge
     hypothesis                5.37.3                     py_0    conda-forge
     icu                       67.1                 he1b5a44_0    conda-forge
     identify                  1.5.6              pyh9f0ad1d_0    conda-forge
     idna                      2.10               pyh9f0ad1d_0    conda-forge
     imagesize                 1.2.0                      py_0    conda-forge
     importlib-metadata        2.0.0                      py_1    conda-forge
     importlib_metadata        2.0.0                         1    conda-forge
     iniconfig                 1.1.0              pyh9f0ad1d_0    conda-forge
     ipykernel                 5.3.4            py37hc6149b9_1    conda-forge
     ipython                   7.18.1           py37hc6149b9_1    conda-forge
     ipython_genutils          0.2.0                      py_1    conda-forge
     isort                     5.0.7            py37hc8dfbb8_0    conda-forge
     jedi                      0.17.2           py37hc8dfbb8_1    conda-forge
     jinja2                    2.11.2             pyh9f0ad1d_0    conda-forge
     jpeg                      9d                   h516909a_0    conda-forge
     jsonschema                3.2.0                      py_2    conda-forge
     jupyter_client            6.1.7                      py_0    conda-forge
     jupyter_core              4.6.3            py37hc8dfbb8_2    conda-forge
     jupyterlab_pygments       0.1.2              pyh9f0ad1d_0    conda-forge
     krb5                      1.17.1               hfafb76e_3    conda-forge
     lcms2                     2.11                 hbd6801e_0    conda-forge
     ld_impl_linux-64          2.35                 h769bd43_9    conda-forge
     libblas                   3.8.0               17_openblas    conda-forge
     libcblas                  3.8.0               17_openblas    conda-forge
     libcurl                   7.71.1               hcdd3856_8    conda-forge
     libedit                   3.1.20191231         he28a2e2_2    conda-forge
     libev                     4.33                 h516909a_1    conda-forge
     libevent                  2.1.10               hcdb4288_3    conda-forge
     libffi                    3.2.1             he1b5a44_1007    conda-forge
     libgcc-ng                 9.3.0               h5dbcf3e_17    conda-forge
     libgfortran-ng            9.3.0               he4bcb1c_17    conda-forge
     libgfortran5              9.3.0               he4bcb1c_17    conda-forge
     libgomp                   9.3.0               h5dbcf3e_17    conda-forge
     liblapack                 3.8.0               17_openblas    conda-forge
     libllvm10                 10.0.1               he513fc3_3    conda-forge
     libllvm8                  8.0.1                hc9558a2_0    conda-forge
     libnghttp2                1.41.0               h8cfc5f6_2    conda-forge
     libopenblas               0.3.10          pthreads_h4812303_5    conda-forge
     libpng                    1.6.37               hed695b0_2    conda-forge
     libprotobuf               3.13.0.1             h8b12597_0    conda-forge
     librmm                    0.17.0a201018   cuda11.0_g8cdd176_21    rapidsai-nightly
     libsodium                 1.0.18               h516909a_1    conda-forge
     libssh2                   1.9.0                hab1572f_5    conda-forge
     libstdcxx-ng              9.3.0               h2ae2ef3_17    conda-forge
     libthrift                 0.13.0               h5aa387f_6    conda-forge
     libtiff                   4.1.0                hc7e4089_6    conda-forge
     libutf8proc               2.5.0                h516909a_2    conda-forge
     libuv                     1.40.0               hd18ef5c_0    conda-forge
     libwebp-base              1.1.0                h516909a_3    conda-forge
     llvmlite                  0.34.0           py37h5202443_2    conda-forge
     locket                    0.2.0                      py_2    conda-forge
     lz4-c                     1.9.2                he1b5a44_3    conda-forge
     markdown                  3.3.1              pyh9f0ad1d_0    conda-forge
     markupsafe                1.1.1            py37hb5d75c8_2    conda-forge
     mccabe                    0.6.1                      py_1    conda-forge
     mimesis                   4.0.0              pyh9f0ad1d_0    conda-forge
     mistune                   0.8.4           py37h8f50634_1002    conda-forge
     more-itertools            8.5.0                      py_0    conda-forge
     msgpack-python            1.0.0            py37h99015e2_2    conda-forge
     nbclient                  0.5.1                      py_0    conda-forge
     nbconvert                 6.0.7            py37hc8dfbb8_1    conda-forge
     nbformat                  5.0.8                      py_0    conda-forge
     nbsphinx                  0.7.1              pyh9f0ad1d_0    conda-forge
     nccl                      2.7.8.1            h4962215_100    nvidia
     ncurses                   6.2                  he1b5a44_2    conda-forge
     nest-asyncio              1.4.1                      py_0    conda-forge
     nodeenv                   1.5.0              pyh9f0ad1d_0    conda-forge
     notebook                  6.1.4            py37hc8dfbb8_1    conda-forge
     numba                     0.51.2           py37h9fdb41a_0    conda-forge
     numpy                     1.19.2           py37h7ea13bd_1    conda-forge
     numpydoc                  1.1.0                      py_1    conda-forge
     nvtx                      0.2.1            py37h8f50634_2    conda-forge
     olefile                   0.46               pyh9f0ad1d_1    conda-forge
     openssl                   1.1.1h               h516909a_0    conda-forge
     orc                       1.6.5                hd3605a7_0    conda-forge
     packaging                 20.4               pyh9f0ad1d_0    conda-forge
     pandas                    1.1.3            py37h9fdb41a_2    conda-forge
     pandavro                  1.5.2                    pypi_0    pypi
     pandoc                    1.19.2                        0    conda-forge
     pandocfilters             1.4.2                      py_1    conda-forge
     parquet-cpp               1.5.1                         2    conda-forge
     parso                     0.7.1              pyh9f0ad1d_0    conda-forge
     partd                     1.1.0                      py_0    conda-forge
     pathspec                  0.8.0              pyh9f0ad1d_0    conda-forge
     pexpect                   4.8.0              pyh9f0ad1d_2    conda-forge
     pickleshare               0.7.5                   py_1003    conda-forge
     pillow                    8.0.0            py37h718be6c_0    conda-forge
     pip                       20.2.4                     py_0    conda-forge
     pluggy                    0.13.1           py37hc8dfbb8_3    conda-forge
     pre-commit                2.7.1            py37hc8dfbb8_1    conda-forge
     pre_commit                2.7.1                         1    conda-forge
     prometheus_client         0.8.0              pyh9f0ad1d_0    conda-forge
     prompt-toolkit            3.0.8                      py_0    conda-forge
     psutil                    5.7.2            py37hb5d75c8_1    conda-forge
     ptyprocess                0.6.0                   py_1001    conda-forge
     py                        1.9.0              pyh9f0ad1d_0    conda-forge
     py-cpuinfo                7.0.0              pyh9f0ad1d_0    conda-forge
     pyarrow                   1.0.1           py37h5b20ac3_14_cuda    conda-forge
     pybind11                  2.5.0                    pypi_0    pypi
     pycodestyle               2.6.0              pyh9f0ad1d_0    conda-forge
     pycparser                 2.20               pyh9f0ad1d_2    conda-forge
     pyflakes                  2.2.0              pyh9f0ad1d_0    conda-forge
     pygments                  2.7.1                      py_0    conda-forge
     pyopenssl                 19.1.0                     py_1    conda-forge
     pyorc                     0.3.0                    pypi_0    pypi
     pyparsing                 2.4.7              pyh9f0ad1d_0    conda-forge
     pyrsistent                0.17.3           py37h8f50634_1    conda-forge
     pysocks                   1.7.1            py37he5f6b98_2    conda-forge
     pytest                    6.1.1            py37hc8dfbb8_1    conda-forge
     pytest-benchmark          3.2.3              pyh9f0ad1d_0    conda-forge
     pytest-forked             1.3.0                    pypi_0    pypi
     pytest-xdist              2.1.0                    pypi_0    pypi
     python                    3.7.8           h6f2ec95_1_cpython    conda-forge
     python-dateutil           2.8.1                      py_0    conda-forge
     python_abi                3.7                     1_cp37m    conda-forge
     pytz                      2020.1             pyh9f0ad1d_0    conda-forge
     pyyaml                    5.3.1            py37hb5d75c8_1    conda-forge
     pyzmq                     19.0.2           py37hac76be4_2    conda-forge
     rapidjson                 1.1.0             he1b5a44_1002    conda-forge
     re2                       2020.10.01           he1b5a44_0    conda-forge
     readline                  8.0                  he28a2e2_2    conda-forge
     recommonmark              0.6.0                      py_0    conda-forge
     regex                     2020.10.15       py37h8f50634_0    conda-forge
     requests                  2.24.0             pyh9f0ad1d_0    conda-forge
     rhash                     1.3.6             h516909a_1001    conda-forge
     rmm                       0.17.0a201017   cuda_11.0_py37_g8cdd176_21    rapidsai-nightly
     send2trash                1.5.0                      py_0    conda-forge
     setuptools                49.6.0           py37he5f6b98_2    conda-forge
     six                       1.15.0             pyh9f0ad1d_0    conda-forge
     snappy                    1.1.8                he1b5a44_3    conda-forge
     snowballstemmer           2.0.0                      py_0    conda-forge
     sortedcontainers          2.2.2              pyh9f0ad1d_0    conda-forge
     spdlog                    1.7.0                hc9558a2_2    conda-forge
     sphinx                    3.2.1                      py_0    conda-forge
     sphinx-copybutton         0.3.0              pyh9f0ad1d_0    conda-forge
     sphinx-markdown-tables    0.0.14             pyh9f0ad1d_1    conda-forge
     sphinx_rtd_theme          0.5.0              pyh9f0ad1d_0    conda-forge
     sphinxcontrib-applehelp   1.0.2                      py_0    conda-forge
     sphinxcontrib-devhelp     1.0.2                      py_0    conda-forge
     sphinxcontrib-htmlhelp    1.0.3                      py_0    conda-forge
     sphinxcontrib-jsmath      1.0.1                      py_0    conda-forge
     sphinxcontrib-qthelp      1.0.3                      py_0    conda-forge
     sphinxcontrib-serializinghtml 1.1.4                      py_0    conda-forge
     sphinxcontrib-websupport  1.2.4              pyh9f0ad1d_0    conda-forge
     sqlite                    3.33.0               h4cf870e_1    conda-forge
     streamz                   0.6.0                    pypi_0    pypi
     tblib                     1.6.0                      py_0    conda-forge
     terminado                 0.9.1            py37hc8dfbb8_1    conda-forge
     testpath                  0.4.4                      py_0    conda-forge
     tk                        8.6.10               hed695b0_1    conda-forge
     toml                      0.10.1             pyh9f0ad1d_0    conda-forge
     toolz                     0.11.1                     py_0    conda-forge
     tornado                   6.0.4            py37h8f50634_2    conda-forge
     traitlets                 5.0.5                      py_0    conda-forge
     typed-ast                 1.4.1            py37h516909a_0    conda-forge
     typing_extensions         3.7.4.3                    py_0    conda-forge
     urllib3                   1.25.10                    py_0    conda-forge
     virtualenv                20.0.35          py37hc8dfbb8_0    conda-forge
     wcwidth                   0.2.5              pyh9f0ad1d_2    conda-forge
     webencodings              0.5.1                      py_1    conda-forge
     wheel                     0.35.1             pyh9f0ad1d_0    conda-forge
     xz                        5.2.5                h516909a_1    conda-forge
     yaml                      0.2.5                h516909a_0    conda-forge
     zeromq                    4.3.3                he1b5a44_2    conda-forge
     zict                      2.0.0                      py_0    conda-forge
     zipp                      3.3.1                      py_0    conda-forge
     zlib                      1.2.11            h516909a_1010    conda-forge
     zstd                      1.4.5                h6597ccf_2    conda-forge
     
</pre></details>

**Additional context**
Surfaced while running fuzz tests: #6001 
",2020-11-04T00:04:07Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
87,[FEA] java HostMemoryBuffer should mirror copy functions in BaseDeviceMemoryBuffer,"We have some places where we would like a host buffer to be able to copy:
1. Given a source/destination offset
2. Given a stream
3. From host or from device buffers.

Currently `HostMemoryBuffer` does allow wholesale copies of device buffers, and even using a stream, but there is no way I can find to do this at an offset. I can think of ways of hacking around it, but it would be best if the APIs were similar for both buffer types. ",2020-11-06T17:51:36Z,0,0,Alessandro Bellina,NVIDIA,True
88,[FEA] Support objects with default values in Series.map,"**Is your feature request related to a problem? Please describe.**
I'd like for cuDF to support 'defaultdict' like objects in the Series.map method. There is currently a NotImplementedError which contrasts from the Pandas implementation of Series.map

**Describe the solution you'd like**
An implementation of Series.map that converts values that are not found in the dict to a default value, if the dict has a default value (e.g. defaultdict).

**Additional context**
With Pandas an example of this looks like
```
>>>p1 = pd.Series(['cat', 'dog', np.nan, 'rabbit'])
>>>from collections import defaultdict
>>>t = defaultdict(lambda: 'bird')
>>> t['cat'] = 'kitten'
>>> t['dog'] = 'puppy'
>>> p1.map(t)
0    kitten
1     puppy
2      bird
3      bird
dtype: object
```
With cuDF currently we get the following
```
>>> s = cudf.Series(['cat', 'dog', np.nan, 'rabbit'])
>>>s.map(t)

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/nfs/mmhangami/marlene/rapids/cudf/python/cudf/cudf/core/series.py"", line 878, in map
    ""default values in dicts are currently not supported.""
NotImplementedError: default values in dicts are currently not supported.
```

for additional context see [pandas.Series.map](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html) and PR #6459
",2020-11-10T11:44:49Z,0,0,Marlene ,Microsoft,False
89,"[FEA]Coalesce(), find the first non-null value, no equivalent function in RAPIDS","I want to create a new column, which is the first non-null value of several columns, I used the function [coalesce()](https://www.rdocumentation.org/packages/dplyr/versions/0.7.8/topics/coalesce) in dplyr to achieve that by simply run ` data %>% mutate(d = coalesce(a, b, c))`, in Dask CPU, I run below code to achieve that
```
d = {'a': [1, 2, 3, None, 5, 1, None, None, None, None, 3, None, 5, 6, 7], \
     'b': [1, 2, 3, 200, 5, 1, 200, None, 400, None, 3, None, 5, 6, 7], \
     'c': [1, 2, 3, None, 5, 1, 300, 300, None, 600, 3, 300, 5, 6, 7]} 

df2 = pd.DataFrame(d)

ddf2 = dd.from_pandas(df2,npartitions=50)
ddf2['d'] = ddf2['a'].copy().fillna(ddf2['b']).fillna(ddf2['c'])
ddf2.compute()
```
However, in dask cuDF, when I am trying to use the fillna() to acheive my intent, I got the error message
```
d = {'id': ['a', 'a','a','a','a','b','b','b','b','b','c','c','c','c','c'], \
     'time': ['1', '2', '4', '3', '5', '1', '2', '3', '4', '5','1', '2', '3', '4', '5'], \
     'a': [1, 2, 3, None, 5, 1, None, None, None, None, 3, None, 5, 6, 7], \
     'b': [1, 2, 3, 200, 5, 1, 200, None, 400, None, 3, None, 5, 6, 7], \
     'c': [1, 2, 3, None, 5, 1, 300, 300, None, 600, 3, 300, 5, 6, 7]} 

df = pd.DataFrame(data=d)
gdf = cudf.DataFrame.from_pandas(df)

ddf = dask_cudf.from_cudf(gdf, npartitions=4)

ddf['d'] = ddf['a'].copy().fillna(ddf['b']).fillna(ddf['c'])
ddf.compute()
```
error message: `RuntimeError: cuDF failure at: /opt/conda/envs/rapids/conda-bld/libcudf_1598487636199/work/cpp/src/replace/replace.cu:804: Column size mismatch`

So I am wondering if there are functions that allows me to get the first non-null value from several columns in cuDF or if we can add that.",2020-11-12T21:16:22Z,0,0,,,False
90,[BUG] Bools written by cuIO ORC writer don't match when read by pyarrow/pyorc,"When writing a large dataframe with bool column using cuIO ORC writer, the result of reading the file back using pyarrow does not match the input dataframe. However when reading back from cudf's ORC reader it matches.

```python
import pandas as pd
import numpy as np
import pyarrow as pa
import pyorc
import cudf

np.random.seed(0)
from cudf._lib.null_mask import bitmask_allocation_size_bytes

def random_bitmask(size):
    sz = bitmask_allocation_size_bytes(size)
    data = np.random.randint(0, 255, dtype=""u1"", size=sz)
    return data.view(""i1"")

size = 6000000
arr = np.random.randint(low=0, high=2, size=size).astype(np.bool)
s = cudf.Series.from_masked_array(arr, random_bitmask(size))
gdf = cudf.DataFrame({""col_bool"": s})

# write with cuIO
fname = ""brokenbool.orc""
gdf.to_orc(fname)

# read with pyarrow
pdf = pa.orc.ORCFile(fname).read().to_pandas()

# the sum doesn't match
print(gdf.col_bool.sum(), pdf.col_bool.sum())

# read with pyorc
file = open(fname, 'rb')
data = pyorc.Reader(file).read()
pdf = pd.DataFrame(data, columns=[""col_bool""])

# sum matches pyarrow but not original df
print(gdf.col_bool.sum(), pdf.col_bool.sum())

# reading with cuIO gives the correct result
print(gdf.col_bool.sum(), cudf.read_orc(fname).col_bool.sum())
```

Note that this doesn't occur when there are no nulls in the input.",2020-11-13T08:17:43Z,0,0,Devavret Makkar,@VoltronData,False
91,[FEA] Decimal constructor for boxed unscaled values,"As mentioned in conversation of #6770, it is good to have a decimal constructor for boxed unscaled values, which working with `ColumnVector.build`.",2020-11-18T01:54:43Z,0,0,Alfred Xu,,False
92,[FEA] Add factory methods for ColumnVector creation in cudf java,"As a follow on to https://github.com/rapidsai/cudf/pull/6751, we plan to move towards factory methods to create ColumnVector instances.",2020-11-18T15:22:04Z,0,0,Kuhu Shukla,,False
93,[BUG]Groupby agg with np.nan in aggregating column leads to incorrect results ,"**Describe the bug**

If we have nulls in the column to run aggregate on we, seem to get incorrect results if there are `np.nan` in the column we are aggregating on.    


**Steps/Code to reproduce bug**
```python

>>> import numpy as np
>>> import cudf
>>> df = cudf.DataFrame({'key':[0,0,0],'val':[None,1.1,None]})
>>> df['val']=df.val.fillna(np.nan)
>>> df.groupby(by='key').val.sum()
key
0   NaN
Name: val, dtype: float64
```

**Expected behavior**

```python
>>> df.to_pandas().groupby(by='key').val.sum()
key
0    1.1
Name: val, dtype: float64
```

**Environment details**
```python
cudf                      0.17.0a201119   cuda_10.2_py37_g1a80df96c4_285    rapidsai-nightly
libcudf                   0.17.0a201119   cuda10.2_g1a80df96c4_285    rapidsai-nightly
```

**Additional context**
As a workaround below does the trick. 

```python
df.nans_to_nulls().groupby(by='key').agg({'val':['sum']})
```

CC: @beckernick ",2020-11-19T21:09:51Z,0,0,Vibhu Jawa,Nvidia,True
94,[BUG] arrow data: Categorical categories must be unique,"**Describe the bug**

I switched from using csv format to feather format and getting and error for the same dataset. Error is being raised during printing. Sorry for not having minimal example but python is not my native language.

**Steps/Code to reproduce bug**

Generate data in csv and feather
```sh
Rscript -e 'install.packages(c(""data.table"",""arrow""))'
wget https://raw.githubusercontent.com/h2oai/db-benchmark/629755352248c9538fbd924e56356e5592f268be/_data/groupby-datagen.R
Rscript groupby-datagen.R 1e7 1e2 0 0
Rscript -e 'arrow::write_feather(data.table::fread(""G1_1e7_1e2_0_0.csv"", stringsAsFactors=TRUE, data.table=FALSE), ""G1_1e7_1e2_0_0.feather"")'
```

cudf using csv
```py
import cudf as cu
x = cu.read_csv(""G1_1e7_1e2_0_0.csv"", header=0, dtype=['str','str','str','int32','int32','int32','int32','int32','float64'])
x['id1'] = x['id1'].astype('category')
x['id2'] = x['id2'].astype('category')
x['id3'] = x['id3'].astype('category')
ans = x.groupby(['id1'],as_index=False).agg({'v1':'sum'})
print(ans.head(3), flush=True)
```
```
     id1      v1
0  id001  299542
1  id002  300933
2  id003  301968
```

cudf using feather
```py
import cudf as cu
x = cu.io.feather.read_feather(""data/G1_1e7_1e2_0_0.feather"")
#/home/jan/anaconda3/envs/cudf/lib/python3.8/site-packages/cudf/io/feather.py:15: UserWarning: Using CPU via PyArrow to read #feather 
#dataset, this may be GPU accelerated in the future
#  warnings.warn(
ans = x.groupby(['id1'],as_index=False).agg({'v1':'sum'})
print(ans.head(3), flush=True)
```
```
Traceback (most recent call last):
  File ""./cudf/groupby-cudf.py"", line 58, in <module>
    print(ans.head(3), flush=True)
  File ""/home/jan/anaconda3/envs/cudf/lib/python3.8/site-packages/cudf/core/dataframe.py"", line 1025, in __str__
    return self.to_string()
  File ""/home/jan/anaconda3/envs/cudf/lib/python3.8/site-packages/cudf/core/dataframe.py"", line 1022, in to_string
    return self.__repr__()
  File ""/home/jan/anaconda3/envs/cudf/lib/python3.8/site-packages/cudf/core/dataframe.py"", line 1272, in __repr__
    return self._clean_renderable_dataframe(output)
  File ""/home/jan/anaconda3/envs/cudf/lib/python3.8/site-packages/cudf/core/dataframe.py"", line 1151, in _clean_renderable_dataframe
    output = output.to_pandas().to_string(
  File ""/home/jan/anaconda3/envs/cudf/lib/python3.8/site-packages/cudf/core/dataframe.py"", line 4861, in to_pandas
    out_data[i] = self._data[col_key].to_pandas(index=out_index)
  File ""/home/jan/anaconda3/envs/cudf/lib/python3.8/site-packages/cudf/core/column/categorical.py"", line 935, in to_pandas
    data = pd.Categorical.from_codes(
  File ""/home/jan/anaconda3/envs/cudf/lib/python3.8/site-packages/pandas/core/arrays/categorical.py"", line 606, in from_codes
    dtype = CategoricalDtype._from_values_or_dtype(
  File ""/home/jan/anaconda3/envs/cudf/lib/python3.8/site-packages/pandas/core/dtypes/dtypes.py"", line 277, in _from_values_or_dtype
    dtype = CategoricalDtype(categories, ordered)
  File ""/home/jan/anaconda3/envs/cudf/lib/python3.8/site-packages/pandas/core/dtypes/dtypes.py"", line 164, in __init__
    self._finalize(categories, ordered, fastpath=False)
  File ""/home/jan/anaconda3/envs/cudf/lib/python3.8/site-packages/pandas/core/dtypes/dtypes.py"", line 318, in _finalize
    categories = self.validate_categories(categories, fastpath=fastpath)
  File ""/home/jan/anaconda3/envs/cudf/lib/python3.8/site-packages/pandas/core/dtypes/dtypes.py"", line 495, in validate_categories
    raise ValueError(""Categorical categories must be unique"")
ValueError: Categorical categories must be unique
```

**Expected behavior**

Printing `ans` should not raise error for feather source data, same as it doesn't for csv data.

**Environment overview (please complete the following information)**

bare-metal
cudf installed from conda

**Environment details**

```
cudf/print_env.sh
#Traceback (most recent call last):
#  File ""<stdin>"", line 1, in <module>
#NameError: name 'cudf' is not defined
```

cudf 0.16
arrow 2.0.0
",2020-11-21T09:00:14Z,0,0,Jan Gorecki,,False
95,[FEA] `aggregation` should be more opaque,"**Is your feature request related to a problem? Please describe.**

The `aggregation` class is used for specifying details of what kind of aggregation a user wants from APIs like `groupby`. User's should not need to interact with this class other than using the `make_*_aggregation` APIs to get an `aggregation` object. 

Originally this type was opaque, but if I remember correctly, that broke Cython. So we had to make it non-opaque. However, this has lead to issues of users using methods of `aggregation` that they should not. 

**Describe the solution you'd like**
`aggregation` should be made more opaque. The PIMPL pattern is probably the right call here such that the type's definition is still visible to appease Cython, but it's members aren't available. 
",2020-12-03T22:46:05Z,0,0,Jake Hemstad,@NVIDIA,True
96,"[FEA] Add function ""to_julian_date()""  to cuDF","'DatetimeIndex' object has no attribute 'to_julian_date', would be good if it could do as pandas:
julianday = pd.DatetimeIndex(df['mydatetime']).to_julian_date()",2020-12-06T21:16:16Z,0,0,,,False
97,[FEA] Support `use_na_sentinel` in factorize,"**Is your feature request related to a problem? Please describe.**

**EDITED 12/16/2022**
As of pandas 1.5, [the `na_sentinel` parameter is deprecated in favor of a boolean flag `use_na_sentinel`](https://github.com/pandas-dev/pandas/issues/46910). We need to update cudf to match the new behavior to retain compatibility with pandas 2.0.

The rest of this issue's text is preserved for the record, but is no longer relevant.

**ORIGINAL ISSUE INTENT**

When using `cudf.Series.factorize` with `na_sentinel=None` an error is encountered, whereas pandas supports this option.  

```
  File ""/cudf/python/cudf/cudf/core/series.py"", line 2517, in label_encoding                                                                                                                 
    dtype = min_scalar_type(max(len(cats), na_sentinel), 8)                                                                                                                                                          
TypeError: '>' not supported between instances of 'NoneType' and 'int'
```

**Describe the solution you'd like**
As described [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.factorize.html) When `None` is passed, any nans in the input data should result in `nan` being part of the set of uniques returned by the function. In cuDF this likely reads the same except substitute true `<NA>`. 


**Describe alternatives you've considered**

**Additional context**
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.factorize.html",2020-12-08T21:20:36Z,0,0,,NVIDIA,True
98,`CompactProtocolWriter` and `ProtobufWriter` API provide no encapsulation of the output buffer,"**Problem:**
The protocol writer classes take a pointer to a vector and use it as the output buffer. Writes change the size of this vector. This vector is also modified (including size changes) outside of the writers. The ORC/Parquet writers have a `std::vector` data member that is reused for protocol writes and manually reset between uses. ORC writer also reuses the `ProtobufWriter` object. In addition, Parquet writer reuses the output buffer to output data unrelated to `CompactProtocolWriter`. All this makes the use error-prone.

**Solution proposal:**
Modify the protocol writer API to use an internal output buffer and only provide getters for it. Also, protocol writer objects should not be reused (and cannot, with the proposed API). There shouldn't be a buffer data member in `xyz::writer::impl`.
These changes would limit the scope of the state to functions instead of the lifetime of `xyz::writer::impl` objects.",2020-12-15T22:20:23Z,0,0,Vukasin Milovanovic,NVIDIA,True
99,[BUG] Overflow ints with nulls is being inferred as float column in CSV reader,"**Describe the bug**
A csv files contains values greater than max(int64) and some null values. In this case, cudf CSV reader is inferring the column type to be `float` column. This will lead to loss of data partially. 

**Steps/Code to reproduce bug**
`a.csv`: [a.csv.zip](https://github.com/rapidsai/cudf/files/5777669/a.csv.zip)(Rename this to `a.csv`)
```python
>>> import cudf
>>> import pandas as pd
>>> cudf.read_csv('a.csv')['3']
0                 <NA>
1       1.77585123e+19
2                 <NA>
3      3.374168268e+18
4                 <NA>
            ...       
365               <NA>
366               <NA>
367               <NA>
368    1.430186644e+19
369               <NA>
Name: 3, Length: 370, dtype: float64
>>> pd.read_csv('a.csv')['3']
0                          
1      17758512297797920768
2                          
3       3374168267804635136
4                          
               ...         
365                        
366                        
367                        
368    14301866441110444032
369                        
Name: 3, Length: 370, dtype: object
```


**Expected behavior**
If the values overflow `int` and fit into `uint`, they should be inferred as `uint`.
If the values overflow `int` & `uint` types, they should be inferred as `string` column.

**Environment overview (please complete the following information)**
 - Environment location: [Bare-metal]
 - Method of cuDF install: [from source]

**Environment details**
Please run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details
<details><summary>Click here to see environment details</summary><pre>
     
     **git***
     commit 402614e6acb25fad9230a6ab78e96e1c4d769230 (HEAD -> 6263)
     Merge: 1433bebda5 8787a647a4
     Author: galipremsagar <sagarprem75@gmail.com>
     Date:   Wed Jan 6 09:18:35 2021 -0800
     
     Merge remote-tracking branch 'upstream/branch-0.18' into 6263
     **git submodules***
     
     ***OS Information***
     DISTRIB_ID=Ubuntu
     DISTRIB_RELEASE=18.04
     DISTRIB_CODENAME=bionic
     DISTRIB_DESCRIPTION=""Ubuntu 18.04.4 LTS""
     NAME=""Ubuntu""
     VERSION=""18.04.4 LTS (Bionic Beaver)""
     ID=ubuntu
     ID_LIKE=debian
     PRETTY_NAME=""Ubuntu 18.04.4 LTS""
     VERSION_ID=""18.04""
     HOME_URL=""https://www.ubuntu.com/""
     SUPPORT_URL=""https://help.ubuntu.com/""
     BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
     PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
     VERSION_CODENAME=bionic
     UBUNTU_CODENAME=bionic
     Linux dt07 4.15.0-76-generic #86-Ubuntu SMP Fri Jan 17 17:24:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
     
     ***GPU Information***
     Wed Jan  6 09:39:24 2021
     +-----------------------------------------------------------------------------+
     | NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 10.2     |
     |-------------------------------+----------------------+----------------------+
     | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
     | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
     |===============================+======================+======================|
     |   0  Tesla T4            On   | 00000000:3B:00.0 Off |                    0 |
     | N/A   48C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |
     +-------------------------------+----------------------+----------------------+
     |   1  Tesla T4            On   | 00000000:5E:00.0 Off |                    0 |
     | N/A   37C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |
     +-------------------------------+----------------------+----------------------+
     |   2  Tesla T4            On   | 00000000:AF:00.0 Off |                    0 |
     | N/A   33C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |
     +-------------------------------+----------------------+----------------------+
     |   3  Tesla T4            On   | 00000000:D8:00.0 Off |                    0 |
     | N/A   32C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |
     +-------------------------------+----------------------+----------------------+
     
     +-----------------------------------------------------------------------------+
     | Processes:                                                       GPU Memory |
     |  GPU       PID   Type   Process name                             Usage      |
     |=============================================================================|
     |  No running processes found                                                 |
     +-----------------------------------------------------------------------------+
     
     ***CPU***
     Architecture:        x86_64
     CPU op-mode(s):      32-bit, 64-bit
     Byte Order:          Little Endian
     CPU(s):              64
     On-line CPU(s) list: 0-63
     Thread(s) per core:  2
     Core(s) per socket:  16
     Socket(s):           2
     NUMA node(s):        2
     Vendor ID:           GenuineIntel
     CPU family:          6
     Model:               85
     Model name:          Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz
     Stepping:            4
     CPU MHz:             1405.100
     BogoMIPS:            4200.00
     Virtualization:      VT-x
     L1d cache:           32K
     L1i cache:           32K
     L2 cache:            1024K
     L3 cache:            22528K
     NUMA node0 CPU(s):   0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62
     NUMA node1 CPU(s):   1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63
     Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts pku ospke md_clear flush_l1d
     
     ***CMake***
     /nvme/0/pgali/envs/cudfdev/bin/cmake
     cmake version 3.19.2
     
     CMake suite maintained and supported by Kitware (kitware.com/cmake).
     
     ***g++***
     /usr/bin/g++
     g++ (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
     Copyright (C) 2017 Free Software Foundation, Inc.
     This is free software; see the source for copying conditions.  There is NO
     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
     
     
     ***nvcc***
     /usr/local/cuda/bin/nvcc
     nvcc: NVIDIA (R) Cuda compiler driver
     Copyright (c) 2005-2019 NVIDIA Corporation
     Built on Wed_Oct_23_19:24:38_PDT_2019
     Cuda compilation tools, release 10.2, V10.2.89
     
     ***Python***
     /nvme/0/pgali/envs/cudfdev/bin/python
     Python 3.7.9
     
     ***Environment Variables***
     PATH                            : /nvme/0/pgali/envs/cudfdev/bin:/usr/share/swift/usr/bin:/home/nfs/pgali/bin:/home/nfs/pgali/.local/bin:/home/nfs/pgali/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/lib/jvm/default-java/bin:/usr/share/sbt-launcher-packaging/bin/sbt-launch.jar/bin:/usr/lib/spark/bin:/usr/lib/spark/sbin:/usr/local/cuda/bin:/nvme/0/pgali/envs/cudfdev/bin:/usr/local/cuda/bin:/nvme/0/pgali/envs/cudfdev/bin
     LD_LIBRARY_PATH                 : /usr/local/cuda/lib64:/usr/local/cuda/lib64::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64
     NUMBAPRO_NVVM                   :
     NUMBAPRO_LIBDEVICE              :
     CONDA_PREFIX                    : /nvme/0/pgali/envs/cudfdev
     PYTHON_PATH                     :
     
     ***conda packages***
     /home/nfs/pgali/anaconda3/condabin/conda
     # packages in environment at /nvme/0/pgali/envs/cudfdev:
     #
     # Name                    Version                   Build  Channel
     _libgcc_mutex             0.1                 conda_forge    conda-forge
     _openmp_mutex             4.5                       1_gnu    conda-forge
     abseil-cpp                20200923.2           h9c3ff4c_1    conda-forge
     alabaster                 0.7.12                     py_0    conda-forge
     apipkg                    1.5                        py_0    conda-forge
     appdirs                   1.4.4              pyh9f0ad1d_0    conda-forge
     argon2-cffi               20.1.0           py37h4abf009_2    conda-forge
     arrow-cpp                 1.0.1           py37h271258c_19_cuda    conda-forge
     arrow-cpp-proc            2.0.0                      cuda    conda-forge
     async_generator           1.10                       py_0    conda-forge
     attrs                     20.3.0             pyhd3deb0d_0    conda-forge
     aws-c-common              0.4.59               h36c2ea0_1    conda-forge
     aws-c-event-stream        0.1.6                had2084c_6    conda-forge
     aws-checksums             0.1.10               h4e93380_0    conda-forge
     aws-sdk-cpp               1.8.70               h57dc084_1    conda-forge
     babel                     2.9.0              pyhd3deb0d_0    conda-forge
     backcall                  0.2.0              pyh9f0ad1d_0    conda-forge
     backports                 1.0                        py_2    conda-forge
     backports.functools_lru_cache 1.6.1                      py_0    conda-forge
     black                     19.10b0                    py_4    conda-forge
     bleach                    3.2.1              pyh9f0ad1d_0    conda-forge
     bokeh                     2.2.3            py37h89c1867_0    conda-forge
     boost-cpp                 1.74.0               h9d3c048_1    conda-forge
     brotli                    1.0.9                he1b5a44_3    conda-forge
     brotlipy                  0.7.0           py37hb5d75c8_1001    conda-forge
     bzip2                     1.0.8                h7f98852_4    conda-forge
     c-ares                    1.17.1               h36c2ea0_0    conda-forge
     ca-certificates           2020.12.5            ha878542_0    conda-forge
     certifi                   2020.12.5        py37h89c1867_0    conda-forge
     cffi                      1.14.4           py37hc58025e_1    conda-forge
     cfgv                      3.2.0                      py_0    conda-forge
     chardet                   4.0.0            py37h89c1867_0    conda-forge
     clang                     8.0.1                hc9558a2_2    conda-forge
     clang-tools               8.0.1                hc9558a2_2    conda-forge
     clangxx                   8.0.1                         2    conda-forge
     click                     7.1.2              pyh9f0ad1d_0    conda-forge
     cloudpickle               1.6.0                      py_0    conda-forge
     cmake                     3.19.2               h4547794_0    conda-forge
     cmake_setuptools          0.1.3                      py_0    rapidsai
     colorama                  0.4.4              pyh9f0ad1d_0    conda-forge
     commonmark                0.9.1                      py_0    conda-forge
     cryptography              3.3.1            py37h7f0c10b_0    conda-forge
     cudatoolkit               10.2.89              h6bb024c_0    nvidia
     cudf                      0.18.0a0+155.g402614e6ac.dirty          pypi_0    pypi
     cudnn                     7.6.5.32             h01f27c4_1    conda-forge
     cupy                      8.3.0            py37h3e89ef8_0    conda-forge
     cython                    0.29.21          py37hb892b2f_1    conda-forge
     cytoolz                   0.11.0           py37h4abf009_1    conda-forge
     dask                      2020.12.0+24.g18804f20          pypi_0    pypi
     dask-cudf                 0.18.0a0+153.g56230780e9          pypi_0    pypi
     decorator                 4.4.2                      py_0    conda-forge
     defusedxml                0.6.0                      py_0    conda-forge
     distlib                   0.3.1              pyh9f0ad1d_0    conda-forge
     distributed               2020.12.0+14.ga07fc398          pypi_0    pypi
     dlpack                    0.3                  he1b5a44_1    conda-forge
     docutils                  0.16             py37h89c1867_2    conda-forge
     double-conversion         3.1.5                he1b5a44_2    conda-forge
     editdistance              0.5.3            py37hcd2ae1e_2    conda-forge
     entrypoints               0.3             pyhd8ed1ab_1003    conda-forge
     execnet                   1.7.1                      py_0    conda-forge
     expat                     2.2.9                he1b5a44_2    conda-forge
     fastavro                  1.2.3            py37h5e8e339_0    conda-forge
     fastrlock                 0.5              py37h3340039_1    conda-forge
     filelock                  3.0.12             pyh9f0ad1d_0    conda-forge
     flake8                    3.8.3                      py_1    conda-forge
     flatbuffers               1.12.0               h58526e2_0    conda-forge
     freetype                  2.10.4               h7ca028e_0    conda-forge
     fsspec                    0.8.5              pyhd8ed1ab_0    conda-forge
     future                    0.18.2           py37h89c1867_2    conda-forge
     gflags                    2.2.2             he1b5a44_1004    conda-forge
     glog                      0.4.0                h49b9bf7_3    conda-forge
     gmp                       6.2.1                h58526e2_0    conda-forge
     grpc-cpp                  1.33.2               hf41bbd9_2    conda-forge
     heapdict                  1.0.1                      py_0    conda-forge
     hypothesis                5.48.0             pyhd8ed1ab_0    conda-forge
     icu                       68.1                 h58526e2_0    conda-forge
     identify                  1.5.11             pyhd3deb0d_0    conda-forge
     idna                      2.10               pyh9f0ad1d_0    conda-forge
     imagesize                 1.2.0                      py_0    conda-forge
     importlib-metadata        2.0.0                      py_1    conda-forge
     importlib_metadata        2.0.0                         1    conda-forge
     iniconfig                 1.1.1              pyh9f0ad1d_0    conda-forge
     ipykernel                 5.4.2            py37h888b3d9_0    conda-forge
     ipython                   7.19.0           py37h888b3d9_1    conda-forge
     ipython_genutils          0.2.0                      py_1    conda-forge
     isort                     5.0.7            py37hc8dfbb8_0    conda-forge
     jedi                      0.17.2           py37h89c1867_1    conda-forge
     jinja2                    2.11.2             pyh9f0ad1d_0    conda-forge
     jpeg                      9d                   h36c2ea0_0    conda-forge
     jsonschema                3.2.0                      py_2    conda-forge
     jupyter_client            6.1.7                      py_0    conda-forge
     jupyter_core              4.7.0            py37h89c1867_0    conda-forge
     jupyterlab_pygments       0.1.2              pyh9f0ad1d_0    conda-forge
     krb5                      1.17.2               h926e7f8_0    conda-forge
     lcms2                     2.11                 hcbb858e_1    conda-forge
     ld_impl_linux-64          2.35.1               hea4e1c9_1    conda-forge
     libblas                   3.9.0                6_openblas    conda-forge
     libcblas                  3.9.0                6_openblas    conda-forge
     libcurl                   7.71.1               hcdd3856_8    conda-forge
     libedit                   3.1.20191231         he28a2e2_2    conda-forge
     libev                     4.33                 h516909a_1    conda-forge
     libevent                  2.1.10               hcdb4288_3    conda-forge
     libffi                    3.3                  h58526e2_2    conda-forge
     libgcc-ng                 9.3.0               h5dbcf3e_17    conda-forge
     libgfortran-ng            9.3.0               he4bcb1c_17    conda-forge
     libgfortran5              9.3.0               he4bcb1c_17    conda-forge
     libgomp                   9.3.0               h5dbcf3e_17    conda-forge
     liblapack                 3.9.0                6_openblas    conda-forge
     libllvm10                 10.0.1               he513fc3_3    conda-forge
     libllvm8                  8.0.1                hc9558a2_0    conda-forge
     libnghttp2                1.41.0               h8cfc5f6_2    conda-forge
     libopenblas               0.3.12          pthreads_h4812303_1    conda-forge
     libpng                    1.6.37               h21135ba_2    conda-forge
     libprotobuf               3.14.0               h780b84a_0    conda-forge
     librmm                    0.18.0a210106   cuda10.2_g9fed212_19    rapidsai-nightly
     libsodium                 1.0.18               h36c2ea0_1    conda-forge
     libssh2                   1.9.0                hab1572f_5    conda-forge
     libstdcxx-ng              9.3.0               h2ae2ef3_17    conda-forge
     libthrift                 0.13.0               h5aa387f_6    conda-forge
     libtiff                   4.2.0                hdc55705_0    conda-forge
     libutf8proc               2.6.1                h7f98852_0    conda-forge
     libuv                     1.40.0               h7f98852_0    conda-forge
     libwebp-base              1.1.0                h36c2ea0_3    conda-forge
     llvmlite                  0.35.0           py37h9d7f4d0_0    conda-forge
     locket                    0.2.0                      py_2    conda-forge
     lz4-c                     1.9.3                h9c3ff4c_0    conda-forge
     markdown                  3.3.3              pyh9f0ad1d_0    conda-forge
     markupsafe                1.1.1            py37hb5d75c8_2    conda-forge
     mccabe                    0.6.1                      py_1    conda-forge
     mimesis                   4.0.0              pyh9f0ad1d_0    conda-forge
     mistune                   0.8.4           py37h4abf009_1002    conda-forge
     more-itertools            8.6.0              pyhd8ed1ab_0    conda-forge
     msgpack-python            1.0.2            py37h2527ec5_0    conda-forge
     nbclient                  0.5.1                      py_0    conda-forge
     nbconvert                 6.0.7            py37h89c1867_3    conda-forge
     nbformat                  5.0.8                      py_0    conda-forge
     nbsphinx                  0.7.1              pyh9f0ad1d_0    conda-forge
     nccl                      2.8.3.1              h1a5f58c_0    conda-forge
     ncurses                   6.2                  h58526e2_4    conda-forge
     nest-asyncio              1.4.3              pyhd8ed1ab_0    conda-forge
     nodeenv                   1.5.0              pyh9f0ad1d_0    conda-forge
     notebook                  6.1.6            py37h89c1867_0    conda-forge
     numba                     0.52.0           py37hdc94413_0    conda-forge
     numpy                     1.19.5           py37haa41c4c_0    conda-forge
     numpydoc                  1.1.0                      py_1    conda-forge
     nvtx                      0.2.1            py37h8f50634_2    conda-forge
     olefile                   0.46               pyh9f0ad1d_1    conda-forge
     openssl                   1.1.1i               h7f98852_0    conda-forge
     orc                       1.6.6                h7950760_1    conda-forge
     packaging                 20.8               pyhd3deb0d_0    conda-forge
     pandas                    1.1.5            py37hdc94413_0    conda-forge
     pandoc                    1.19.2                        0    conda-forge
     pandocfilters             1.4.2                      py_1    conda-forge
     parquet-cpp               1.5.1                         2    conda-forge
     parso                     0.7.1              pyh9f0ad1d_0    conda-forge
     partd                     1.1.0                      py_0    conda-forge
     pathspec                  0.8.1              pyhd3deb0d_0    conda-forge
     pexpect                   4.8.0              pyh9f0ad1d_2    conda-forge
     pickleshare               0.7.5                   py_1003    conda-forge
     pillow                    8.1.0            py37he6b4880_0    conda-forge
     pip                       20.3.3             pyhd8ed1ab_0    conda-forge
     pluggy                    0.13.1           py37he5f6b98_3    conda-forge
     pre-commit                2.9.3            py37h89c1867_0    conda-forge
     pre_commit                2.9.3                hd8ed1ab_0    conda-forge
     prometheus_client         0.9.0              pyhd3deb0d_0    conda-forge
     prompt-toolkit            3.0.9              pyha770c72_0    conda-forge
     protobuf                  3.14.0           py37hcd2ae1e_0    conda-forge
     psutil                    5.8.0            py37h5e8e339_0    conda-forge
     ptyprocess                0.7.0              pyhd3deb0d_0    conda-forge
     py                        1.10.0             pyhd3deb0d_0    conda-forge
     py-cpuinfo                7.0.0              pyh9f0ad1d_0    conda-forge
     pyarrow                   1.0.1           py37h9267296_19_cuda    conda-forge
     pybind11                  2.6.1                    pypi_0    pypi
     pycodestyle               2.6.0              pyh9f0ad1d_0    conda-forge
     pycparser                 2.20               pyh9f0ad1d_2    conda-forge
     pyflakes                  2.2.0              pyh9f0ad1d_0    conda-forge
     pygments                  2.7.3              pyhd8ed1ab_0    conda-forge
     pyopenssl                 20.0.1             pyhd8ed1ab_0    conda-forge
     pyorc                     0.3.0                    pypi_0    pypi
     pyparsing                 2.4.7              pyh9f0ad1d_0    conda-forge
     pyrsistent                0.17.3           py37h4abf009_1    conda-forge
     pysocks                   1.7.1            py37he5f6b98_2    conda-forge
     pytest                    6.2.1            py37h89c1867_0    conda-forge
     pytest-benchmark          3.2.3              pyh9f0ad1d_0    conda-forge
     pytest-forked             1.2.0              pyh9f0ad1d_0    conda-forge
     pytest-xdist              2.2.0              pyhd8ed1ab_0    conda-forge
     python                    3.7.9           hffdb5ce_0_cpython    conda-forge
     python-dateutil           2.8.1                      py_0    conda-forge
     python_abi                3.7                     1_cp37m    conda-forge
     pytz                      2020.5             pyhd8ed1ab_0    conda-forge
     pyyaml                    5.3.1            py37hb5d75c8_1    conda-forge
     pyzmq                     20.0.0           py37h5a562af_1    conda-forge
     rapidjson                 1.1.0             he1b5a44_1002    conda-forge
     re2                       2020.11.01           h58526e2_0    conda-forge
     readline                  8.0                  he28a2e2_2    conda-forge
     recommonmark              0.7.1              pyhd8ed1ab_0    conda-forge
     regex                     2020.11.13       py37h5e8e339_0    conda-forge
     requests                  2.25.1             pyhd3deb0d_0    conda-forge
     rhash                     1.3.6             h516909a_1001    conda-forge
     rmm                       0.18.0a210106   cuda_10.2_py37_g9fed212_19    rapidsai-nightly
     send2trash                1.5.0                      py_0    conda-forge
     setuptools                49.6.0           py37he5f6b98_2    conda-forge
     six                       1.15.0             pyh9f0ad1d_0    conda-forge
     snappy                    1.1.8                he1b5a44_3    conda-forge
     snowballstemmer           2.0.0                      py_0    conda-forge
     sortedcontainers          2.3.0              pyhd8ed1ab_0    conda-forge
     spdlog                    1.7.0                hc9558a2_2    conda-forge
     sphinx                    3.4.2              pyhd8ed1ab_0    conda-forge
     sphinx-copybutton         0.3.1              pyhd8ed1ab_0    conda-forge
     sphinx-markdown-tables    0.0.15             pyhd3deb0d_0    conda-forge
     sphinx_rtd_theme          0.5.1              pyhd3deb0d_0    conda-forge
     sphinxcontrib-applehelp   1.0.2                      py_0    conda-forge
     sphinxcontrib-devhelp     1.0.2                      py_0    conda-forge
     sphinxcontrib-htmlhelp    1.0.3                      py_0    conda-forge
     sphinxcontrib-jsmath      1.0.1                      py_0    conda-forge
     sphinxcontrib-qthelp      1.0.3                      py_0    conda-forge
     sphinxcontrib-serializinghtml 1.1.4                      py_0    conda-forge
     sphinxcontrib-websupport  1.2.4              pyh9f0ad1d_0    conda-forge
     sqlite                    3.34.0               h74cdb3f_0    conda-forge
     streamz                   0.6.1              pyhd3deb0d_1    conda-forge
     tblib                     1.6.0                      py_0    conda-forge
     terminado                 0.9.2            py37h89c1867_0    conda-forge
     testpath                  0.4.4                      py_0    conda-forge
     tk                        8.6.10               h21135ba_1    conda-forge
     toml                      0.10.2             pyhd8ed1ab_0    conda-forge
     toolz                     0.11.1                     py_0    conda-forge
     tornado                   6.1              py37h4abf009_0    conda-forge
     traitlets                 5.0.5                      py_0    conda-forge
     typed-ast                 1.4.2            py37h5e8e339_0    conda-forge
     typing_extensions         3.7.4.3                    py_0    conda-forge
     urllib3                   1.26.2             pyhd8ed1ab_0    conda-forge
     virtualenv                20.2.2           py37h89c1867_0    conda-forge
     wcwidth                   0.2.5              pyh9f0ad1d_2    conda-forge
     webencodings              0.5.1                      py_1    conda-forge
     wheel                     0.36.2             pyhd3deb0d_0    conda-forge
     xz                        5.2.5                h516909a_1    conda-forge
     yaml                      0.2.5                h516909a_0    conda-forge
     zeromq                    4.3.3                h58526e2_3    conda-forge
     zict                      2.0.0                      py_0    conda-forge
     zipp                      3.4.0                      py_0    conda-forge
     zlib                      1.2.11            h516909a_1010    conda-forge
     zstd                      1.4.8                ha95c52a_1    conda-forge
     
</pre></details>

**Additional context**
Surfaced in fuzz testing: #6001 
",2021-01-06T19:07:26Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
100,[QST] Changing `cudf::rolling_window` API to have exclusive `preceding` parameter,"**Should we change `cudf::rolling_window` API so that the `preceding` parameter is exclusive?**

This is the current API for `cudf::rolling_window`:
```cpp
std::unique_ptr<column> rolling_window(
  column_view const& input,
  size_type preceding_window,
  size_type following_window,
  size_type min_periods,
  std::unique_ptr<aggregation> const& agg,
  rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());
```

The question is whether `preceding` should be exclusive or inclusive. Current examples of behaviour:
```cpp
// auto col = {1, 2, 3, 4, 5};
cudf::rolling_window(col, 2, 1, 1, ???); // gets you [1, 2], [1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5]
cudf::rolling_window(col, 2, 1, 1, min); //            1,        1,         2,         3,       4
cudf::rolling_window(col, 2, 1, 1, max); //            2,        3,         4,         5,       5 
cudf::rolling_window(col, 2, 1, 1, sum); //            3,        6,         9,        12,       9

// auto col = {1, 2, 3, 4, 5};
cudf::rolling_window(col, 1, 1, 1, ???); // gets you [1, 2], [2, 3], [3, 4], [4, 5], [5]
cudf::rolling_window(col, 1, 1, 1, min); //            1,      2,      3,      4,     5
cudf::rolling_window(col, 1, 1, 1, max); //            2,      3,      4,      5,     5
cudf::rolling_window(col, 1, 1, 1, sum); //            3,      5,      7,      9,     5

// auto col = {1, 2, 3, 4, 5};
cudf::rolling_window(col, 1, 0, 1, ???); // gets you [1], [2], [3], [4], [5]
cudf::rolling_window(col, 1, 0, 1, min); //           1,   2,   3,   4,   5
cudf::rolling_window(col, 1, 0, 1, max); //           1,   2,   3,   4,   5
cudf::rolling_window(col, 1, 0, 1, sum); //           1,   2,   3,   4,   5
```
Recommendation would be that all of the `preceding` parameters we be ""reduce by 1"" and the current index would be included by default. Therefore window length would always be `preceding` + `following` + 1. However, need to consider the API that takes a column of window sizes and also the future changes @mythrocks will make.

**Previous discussion:**
* https://github.com/rapidsai/cudf/pull/3305#discussion_r352659894
* https://github.com/rapidsai/cudf/pull/3305#discussion_r353878021 

**Relevant SQL Links:**
* https://docs.microsoft.com/en-us/sql/t-sql/queries/select-over-clause-transact-sql?view=sql-server-ver15
* https://www.red-gate.com/simple-talk/sql/learn-sql-server/window-functions-in-sql-server-part-2-the-frame/",2021-01-07T01:13:03Z,0,0,Conor Hoekstra,NVIDIA,True
101,[QST] Changing `cudf::rolling_window` API to have exclusive `preceding` parameter,"**Should we change `cudf::rolling_window` API so that the `preceding` parameter is exclusive?**

This is the current API for `cudf::rolling_window`:
```cpp
std::unique_ptr<column> rolling_window(
  column_view const& input,
  size_type preceding_window,
  size_type following_window,
  size_type min_periods,
  std::unique_ptr<aggregation> const& agg,
  rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());
```

The question is whether `preceding` should be exclusive or inclusive. Current examples of behaviour:
```cpp
// auto col = {1, 2, 3, 4, 5};
cudf::rolling_window(col, 2, 1, 1, ???); // gets you [1, 2], [1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5]
cudf::rolling_window(col, 2, 1, 1, min); //            1,        1,         2,         3,       4
cudf::rolling_window(col, 2, 1, 1, max); //            2,        3,         4,         5,       5 
cudf::rolling_window(col, 2, 1, 1, sum); //            3,        6,         9,        12,       9

// auto col = {1, 2, 3, 4, 5};
cudf::rolling_window(col, 1, 1, 1, ???); // gets you [1, 2], [2, 3], [3, 4], [4, 5], [5]
cudf::rolling_window(col, 1, 1, 1, min); //            1,      2,      3,      4,     5
cudf::rolling_window(col, 1, 1, 1, max); //            2,      3,      4,      5,     5
cudf::rolling_window(col, 1, 1, 1, sum); //            3,      5,      7,      9,     5

// auto col = {1, 2, 3, 4, 5};
cudf::rolling_window(col, 1, 0, 1, ???); // gets you [1], [2], [3], [4], [5]
cudf::rolling_window(col, 1, 0, 1, min); //           1,   2,   3,   4,   5
cudf::rolling_window(col, 1, 0, 1, max); //           1,   2,   3,   4,   5
cudf::rolling_window(col, 1, 0, 1, sum); //           1,   2,   3,   4,   5
```
Recommendation would be that all of the `preceding` parameters we be ""reduce by 1"" and the current index would be included by default. Therefore window length would always be `preceding` + `following` + 1. However, need to consider the API that takes a column of window sizes and also the future changes @mythrocks will make.

**Previous discussion:**
* https://github.com/rapidsai/cudf/pull/3305#discussion_r352659894
* https://github.com/rapidsai/cudf/pull/3305#discussion_r353878021 

**Relevant SQL Links:**
* https://docs.microsoft.com/en-us/sql/t-sql/queries/select-over-clause-transact-sql?view=sql-server-ver15
* https://www.red-gate.com/simple-talk/sql/learn-sql-server/window-functions-in-sql-server-part-2-the-frame/",2021-01-07T01:13:03Z,0,0,Conor Hoekstra,NVIDIA,True
102,Feature request: add additional flags to to_csv() function to avoid csv writer adding additional quotation marks ,"**Is your feature request related to a problem? Please describe.**
When using `to_csv` function for a string column, the csv writer automatically adds ""quotation"" signs to complete possibly missing signs. This can potentially add undesirable quotation marks.
For example the string
`""abcd`
will be written in the CSV as
`""""""abcd""`

**Describe the solution you'd like**
Ideally, `to_csv` should duplicate the quotation functionality found in `read_csv`
This documentation can be found in the following [link](https://docs.rapids.ai/api/cudf/stable/api.html?highlight=read_csv#strings).

quoting: str or int, default 0
Controls quoting behavior. Set to one of 0 (csv.QUOTE_MINIMAL), 1 (csv.QUOTE_ALL), 2 (csv.QUOTE_NONNUMERIC) or 3 (csv.QUOTE_NONE). Quoting is enabled with all values except 3.


CCing @vuule .",2021-01-09T00:45:48Z,0,0,Oded Green,NVIDIA & Georgia Institute of Technology,True
103,[BUG] Leading space handling with delim_whitespace,"When using `read_csv` the `delim_whitespace` option does not handle leading spaces like pandas and instead adds an extra column.

Steps to reproduce:
```
import cudf
import pandas as pd
from io import StringIO

gdf = cudf.read_csv(StringIO(' 1    2  3\n4  5 6\n'), delim_whitespace=True)
df = pd.read_csv(StringIO(' 1    2  3\n4  5 6\n'), delim_whitespace=True)
print(gdf.shape)
print(df.shape)
```
Output:
```
(1, 4)
(1, 3)
```",2021-01-12T22:33:11Z,0,0,Hugo Linsenmaier,@NVIDIA,True
104,[FEA] Groupby Skew,"**Is your feature request related to a problem? Please describe.**
I would like to have aggregations for skew with cudf. 
```python
import cudf
df = cudf.DataFrame({'col_1':[0,0,0,0,1,1,1,1],
                   'col_2':[0,10,20,20,30,40,10,20]})
df.groupby(['col_1']).skew()
```
```python
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-3-02b14a669537> in <module>
      2 df = cudf.DataFrame({'col_1':[0,0,0,0,1,1,1,1],
      3                    'col_2':[0,10,20,20,30,40,10,20]})
----> 4 df.groupby(['col_1']).skew()

/nvme/0/vjawa/vjawa_cudf/cudf/python/cudf/cudf/core/groupby/groupby.py in __getattribute__(self, key)
    686     def __getattribute__(self, key):
    687         try:
--> 688             return super().__getattribute__(key)
    689         except AttributeError:
    690             if key in self.obj:

/nvme/0/vjawa/vjawa_cudf/cudf/python/cudf/cudf/core/groupby/groupby.py in __getattribute__(self, key)
     61     def __getattribute__(self, key):
     62         try:
---> 63             return super().__getattribute__(key)
     64         except AttributeError:
     65             if key in libgroupby._GROUPBY_AGGS:

AttributeError: 'DataFrameGroupBy' object has no attribute 'skew'
```


**Describe the solution you'd like**
```python
import pandas as pd
df = pd.DataFrame({'col_1':[0,0,0,0,1,1,1,1],
                   'col_2':[0,10,20,20,30,40,10,20]})
df.groupby(['col_1']).skew()

```
```python
	col_2
col_1	
0	-0.854563
1	0.000000
```

",2021-01-21T16:37:50Z,0,0,Vibhu Jawa,Nvidia,True
105,[FEA]Groupby Mode Aggregation,"**Is your feature request related to a problem? Please describe.**
I would like to do group by  mode aggregation. 

**Describe the solution you'd like**
```python

import pandas as pd
import numpy as np
df =cudf.DataFrame({'col_1':[0,0,0,0,1,1,1,1],
                   'col_2':[0,10,20,20,30,40,10,20]})
df.groupby(['col_1']).agg({'col_2':'mode')})
```

**Additional context**
In pandas we achieve this by doing below:
```python
import pandas as pd
import numpy as np
df = pd.DataFrame({'col_1':[0,0,0,0,1,1,1,1],
                   'col_2':[0,10,20,20,30,40,10,20]})
df.groupby(['col_1']).agg({'col_2':lambda s:s.mode().get(0)})
```
```python
	col_2
col_1	
0	20
1	10
```


If we do the same in cudf we get:

```python
import cudf
import numpy as np
df = cudf.DataFrame({'col_1':[0,0,0,0,1,1,1,1],
                   'col_2':[0,10,20,20,30,40,10,20]})
df.groupby(['col_1']).agg({'col_2':lambda s:s.mode().get(0)})
```
```
AttributeError                            Traceback (most recent call last)
<ipython-input-4-88da2c242150> in <module>
      3 df = cudf.DataFrame({'col_1':[0,0,0,0,1,1,1,1],
      4                    'col_2':[0,10,20,20,30,40,10,20]})
----> 5 df.groupby(['col_1']).agg({'col_2':lambda s:s.mode().get(0)})

/nvme/0/vjawa/conda/envs/cudf_dev_encoding_branch/lib/python3.7/contextlib.py in inner(*args, **kwds)
     72         def inner(*args, **kwds):
     73             with self._recreate_cm():
---> 74                 return func(*args, **kwds)
     75         return inner
     76 

/nvme/0/vjawa/vjawa_cudf/cudf/python/cudf/cudf/core/groupby/groupby.py in agg(self, func)
    168         # a Float64Index, while Pandas returns an Int64Index
    169         # (GH: 6945)
--> 170         result = self._groupby.aggregate(self.obj, normalized_aggs)
    171 
    172         result = cudf.DataFrame._from_table(result)

/nvme/0/vjawa/vjawa_cudf/cudf/python/cudf/cudf/_lib/groupby.pyx in cudf._lib.groupby.GroupBy.aggregate()

/nvme/0/vjawa/vjawa_cudf/cudf/python/cudf/cudf/_lib/aggregation.pyx in cudf._lib.aggregation.make_aggregation()

<ipython-input-4-88da2c242150> in <lambda>(s)
      3 df = cudf.DataFrame({'col_1':[0,0,0,0,1,1,1,1],
      4                    'col_2':[0,10,20,20,30,40,10,20]})
----> 5 df.groupby(['col_1']).agg({'col_2':lambda s:s.mode().get(0)})

AttributeError: type object 'cudf._lib.aggregation._AggregationFactory' has no attribute 'mode'
```
",2021-01-21T16:50:41Z,0,0,Vibhu Jawa,Nvidia,True
106,[FEA] rolling_window() does not support struct_view for most operations,"It appears that all the `*rolling_window()` functions do not currently support any aggregations on `struct_view` columns. There is no case in [`is_rolling_supported()`](https://github.com/rapidsai/cudf/blob/6c116e382f8fa85d9207babbcc425a18684f06b3/cpp/src/rolling/rolling_detail.hpp#L69) that handles `struct_view`.

In the ideal case, we should support `COUNT_VALID`, `COUNT_ALL`, `ROW_NUMBER`, `LEAD`, `LAG`, and (soon) `COLLECT` on `struct_view`.

(`COLLECT` support should be added when #7133 is addressed.)",2021-01-22T04:05:04Z,0,0,MithunR,NVIDIA,True
107,[FEA] Support casting operations on nested types,"**Is your feature request related to a problem? Please describe.**
Currently we support `castTo` on primitive data types. This request is to extend this functionality/operator to nested types like lists, structs, and list of structs.

**Describe the solution you'd like**
One should be able to call a cudf function to cast an `array(floats)` for example to an `array(doubles)`. We should also allow other valid casts that we support today to nested type category.
",2021-01-27T14:31:06Z,0,0,Kuhu Shukla,,False
108,[FEA] Support FIRST_VALUE and LAST_VALUE in grouped_rolling_window,"**Is your feature request related to a problem? Please describe.**
Like ROW_NUMBER, which supported in PR 4881, FIRST_VALUE is widely used window function in sql. So, we want to provide it for BlazingSQL. FIRST_VALUE and LAST_VALUE are fairly trivial where there are no partitions to worry about, but if you have multiple groups or partitions, for the case of `grouped_rolling_window`, then its not so trivial, and we would want native support for that aggregator in cudf.

**Describe the solution you'd like**
Implementation of the aggregation for FIRST_VALUE and LAST_VALUE, especially for `grouped_rolling_window`
",2021-01-27T21:56:45Z,0,0,William Malpica,Voltron Data,False
109,[FEA] Agg groupby.filter support,"**Is your feature request related to a problem? Please describe.**
Hi!

While porting some existing code from Pandas to cuDF, I have just noticed that `groupby.filter` method is not supported.

The code I am porting is the following:

> import pandas as pd
> df1 = pd.read_csv('./data/ml-20m/ratings.csv')
> df1 = df1.groupby(USER_COLUMN).filter(lambda x: len(x) >= MIN_RATINGS)

I have already found a workaround for it, but I think it would be great to add support to `groupby.filter` method.

**Describe the solution you'd like**
Having `groupby.filter` method available in cudf.

Thanks!

Miguel",2021-02-09T17:44:16Z,0,0,Miguel Martínez,NVIDIA,True
110,"[FEA] have cudf::merge or similar API be stable, with the order of the tables passed in","**Is your feature request related to a problem? Please describe.**
Spark's sort is a stable sort.  Most of the time that does not mater much, but there are a few cases where it does make a difference in the output (some window functions in a single process).

`cudf::merge` currently provides no way to ensure that if batch 1 came before batch 2 in read order that the output would be preserved.

**Describe the solution you'd like**
It would be great in the case of ties in cudf::merge if the order of the rows could match the order of the tables passed in. If this is going to cause a performance difference with the existing merge then a separate API is fine too.

**Describe alternatives you've considered**
Add in a separate row of longs with all of the data that is a sequence to get a total ordering of the data read in so I can then use it as the final key in the sort, but that adds a lot of extra memory for something that is a really rare use case.

**Additional context**
This is not super critical, as I said it is for a few corner cases. But from looking at the code it looks like it is simple enough that I could do it. The underlying `trust::merge` says that it is stable for what we want in the docs. That just leaves the order in which the tables are merged. It looks like right now there is a priority queue used to try and reduce the amount of data transferred and merges done by merging the two smallest tables at a time. I think if we instead merged them in waves, we could still reduce the amount of intermediate data transferred and maintain stable ordering.  Not as clean as the existing code but the following pseudo code would probably do what we want.

```
queue input(all the input tables)
queue output()
while (input.size > 1) {
  while (input.size > 1) {
    left = input.pop
    right = input.pop
    output.add(merge(left, right))
  }
  if (input.size == 1) {
    output.add(input.pop)
  }
  input = output
  output.reset()
}
return input.pop
```",2021-02-12T13:41:57Z,0,0,Robert (Bobby) Evans,Nvidia,True
111,Inconsistency in cudf directory and file structures,"Currently, there has been a number of inconsistency issues in `cudf` directory and file structures. Below are some examples:

 * The current directory structures of `cpp/include`, `cpp/src`, `cpp/tests` and `cpp/benchmarks` are inconsistent. For instance, there are tests in `tests/grouped_rolling` which test `grouped_rolling_window` corresponding to `src/rolling`; or unit tests in `tests/collect_list` and `tests/lead_lag` that are all tests for `cpp/aggregation`.
 * There are `tests/utilities` and `tests/utilities_test` which have confusing names. 
 * There are tests in `tests/column/colum_view_test.cpp` which are actually testing `logical_cast` (or `bit_cast`), so they should be put together with the tests in `tests/unary/cast_tests.cpp`.
 * File name/extensions are inconsistent. For example, many unit tests in the `tests/<some test name>` are written in both `.cu` and `.cpp` extensions. Of course, `.cu` file may be required here, but having the tests written in two file extensions at the same time make things look messy.

And many other similar issues that are not mentioned here. I would recommend to refactor and enforce more consistency.",2021-02-25T02:34:16Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
112,[BUG] `Series.__setitem__` fails with tuple keys on a multiindex,"**Describe the bug**
When assigning to a row of a series with multiindex, cudf raises exception.

**Steps/Code to reproduce bug**
```python3
>>> idx= cudf.MultiIndex.from_product([(1, 2), (3, 4)])
>>> gs = cudf.Series([1, 2, 3, 4], index=idx)
>>> gs[(1, 3)] = 101
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-1-598f378d5431> in <module>
      4 gs3 = cudf.Series([1, 2, 3, 4], index=gmidx2)
      5 
----> 6 gs3[(1, 3)] = 101
      7 gs3

~/cudf/python/cudf/cudf/core/series.py in __setitem__(self, key, value)
    892             self.iloc[key] = value
    893         else:
--> 894             self.loc[key] = value
    895 
    896     def take(self, indices, keep_index=True):

~/cudf/python/cudf/cudf/core/indexing.py in __setitem__(self, key, value)
    145     def __setitem__(self, key, value):
    146         try:
--> 147             key = self._loc_to_iloc(key)
    148         except KeyError as e:
    149             if (

~/cudf/python/cudf/cudf/core/indexing.py in _loc_to_iloc(self, arg)
    190                 return arg
    191             else:
--> 192                 indices = indices_from_labels(self._sr, arg)
    193                 if indices.null_count > 0:
    194                     raise KeyError(""label scalar is out of bound"")

~/cudf/python/cudf/cudf/core/indexing.py in indices_from_labels(obj, labels)
     28         labels = column.as_column(labels)
     29 
---> 30         if is_categorical_dtype(obj.index):
     31             labels = labels.astype(""category"")
     32             codes = labels.codes.astype(obj.index._values.codes.dtype)

~/cudf/python/cudf/cudf/utils/dtypes.py in is_categorical_dtype(obj)
    213         ),
    214     ):
--> 215         return is_categorical_dtype(obj.dtype)
    216     if hasattr(obj, ""type""):
    217         if obj.type is CategoricalDtypeType:

AttributeError: 'MultiIndex' object has no attribute 'dtype'
```
**Expected behavior**
```python3
>>> idx = pd.MultiIndex.from_product([(1, 2), (3, 4)])
>>> s = pd.Series([1, 2, 3, 4], index=idx)
>>> s[(1, 3)] = 101
```

**Environment overview (please complete the following information)**
 - Environment location: Docker
 - Method of cuDF install: conda

**Environment details**
<details><summary>Click here to see environment details</summary><pre>
     
     **git***
     commit 7d613f5c4b4a5d75cd8f83383251e4e8b0572bfb (HEAD -> 7290, origin/7290)
     Author: Michael Wang <isVoid@users.noreply.github.com>
     Date:   Wed Feb 24 16:43:25 2021 -0800
     
     Update Docstrings
     **git submodules***
     
     ***OS Information***
     DISTRIB_ID=Ubuntu
     DISTRIB_RELEASE=18.04
     DISTRIB_CODENAME=bionic
     DISTRIB_DESCRIPTION=""Ubuntu 18.04.5 LTS""
     NAME=""Ubuntu""
     VERSION=""18.04.5 LTS (Bionic Beaver)""
     ID=ubuntu
     ID_LIKE=debian
     PRETTY_NAME=""Ubuntu 18.04.5 LTS""
     VERSION_ID=""18.04""
     HOME_URL=""https://www.ubuntu.com/""
     SUPPORT_URL=""https://help.ubuntu.com/""
     BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
     PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
     VERSION_CODENAME=bionic
     UBUNTU_CODENAME=bionic
     Linux dgx06 4.15.0-76-generic #86-Ubuntu SMP Fri Jan 17 17:24:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
     
     ***GPU Information***
     Wed Feb 24 18:33:21 2021
     +-----------------------------------------------------------------------------+
     | NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 10.2     |
     |-------------------------------+----------------------+----------------------+
     | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
     | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
     |===============================+======================+======================|
     |   0  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |
     | N/A   34C    P0    57W / 300W |   1498MiB / 32510MiB |      0%      Default |
     +-------------------------------+----------------------+----------------------+
     
     +-----------------------------------------------------------------------------+
     | Processes:                                                       GPU Memory |
     |  GPU       PID   Type   Process name                             Usage      |
     |=============================================================================|
     +-----------------------------------------------------------------------------+
     
     ***CPU***
     Architecture:        x86_64
     CPU op-mode(s):      32-bit, 64-bit
     Byte Order:          Little Endian
     CPU(s):              80
     On-line CPU(s) list: 0-79
     Thread(s) per core:  2
     Core(s) per socket:  20
     Socket(s):           2
     NUMA node(s):        2
     Vendor ID:           GenuineIntel
     CPU family:          6
     Model:               79
     Model name:          Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz
     Stepping:            1
     CPU MHz:             1456.910
     CPU max MHz:         3600.0000
     CPU min MHz:         1200.0000
     BogoMIPS:            4390.16
     Virtualization:      VT-x
     L1d cache:           32K
     L1i cache:           32K
     L2 cache:            256K
     L3 cache:            51200K
     NUMA node0 CPU(s):   0-19,40-59
     NUMA node1 CPU(s):   20-39,60-79
     Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap intel_pt xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts md_clear flush_l1d
     
     ***CMake***
     /raid/wangm/dev/rapids/compose/etc/conda/cuda_10.2/envs/rapids/bin/cmake
     cmake version 3.18.5
     
     CMake suite maintained and supported by Kitware (kitware.com/cmake).
     
     ***g++***
     /usr/local/bin/g++
     g++ (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
     Copyright (C) 2017 Free Software Foundation, Inc.
     This is free software; see the source for copying conditions.  There is NO
     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
     
     
     ***nvcc***
     /usr/local/bin/nvcc
     nvcc: NVIDIA (R) Cuda compiler driver
     Copyright (c) 2005-2019 NVIDIA Corporation
     Built on Wed_Oct_23_19:24:38_PDT_2019
     Cuda compilation tools, release 10.2, V10.2.89
     
     ***Python***
     /raid/wangm/dev/rapids/compose/etc/conda/cuda_10.2/envs/rapids/bin/python
     Python 3.7.10
     
     ***Environment Variables***
     PATH                            : /raid/wangm/dev/rapids/compose/etc/conda/cuda_10.2/envs/rapids/bin:/raid/wangm/dev/rapids/compose/etc/conda/cuda_10.2/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/cuda-10.2/bin
     LD_LIBRARY_PATH                 : /raid/wangm/dev/rapids/compose/etc/conda/cuda_10.2/envs/rapids/lib:/raid/wangm/dev/rapids/compose/etc/conda/cuda_10.2/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/i386-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda-10.2/lib64:/usr/local/lib:/raid/wangm/dev/rapids/rmm/build/release:/raid/wangm/dev/rapids/cudf/cpp/build/release:/raid/wangm/dev/rapids/cudf/cpp/build/release:/raid/wangm/dev/rapids/cuml/cpp/build/release:/raid/wangm/dev/rapids/cugraph/cpp/build/release:/raid/wangm/dev/rapids/cuspatial/cpp/build/release
     NUMBAPRO_NVVM                   :
     NUMBAPRO_LIBDEVICE              :
     CONDA_PREFIX                    : /raid/wangm/dev/rapids/compose/etc/conda/cuda_10.2/envs/rapids
     PYTHON_PATH                     :
     
     ***conda packages***
     /raid/wangm/dev/rapids/compose/etc/conda/cuda_10.2/bin/conda
     # packages in environment at /raid/wangm/dev/rapids/compose/etc/conda/cuda_10.2/envs/rapids:
     #
     # Name                    Version                   Build  Channel
     _libgcc_mutex             0.1                 conda_forge    conda-forge
     _openmp_mutex             4.5                       1_gnu    conda-forge
     abseil-cpp                20200923.3           h9c3ff4c_0    conda-forge
     alabaster                 0.7.12                     py_0    conda-forge
     apipkg                    1.5                        py_0    conda-forge
     appdirs                   1.4.4              pyh9f0ad1d_0    conda-forge
     argon2-cffi               20.1.0           py37h8f50634_2    conda-forge
     arrow-cpp                 1.0.1           py37hb84ec65_31_cuda    conda-forge
     arrow-cpp-proc            3.0.0                      cuda    conda-forge
     async_generator           1.10                       py_0    conda-forge
     attrs                     20.3.0             pyhd3deb0d_0    conda-forge
     aws-c-cal                 0.4.5                h2ff795d_6    conda-forge
     aws-c-common              0.4.67               h7f98852_0    conda-forge
     aws-c-event-stream        0.2.6                h4285e0c_4    conda-forge
     aws-c-io                  0.8.3                h3b39b8c_1    conda-forge
     aws-checksums             0.1.11               h3b39b8c_1    conda-forge
     aws-sdk-cpp               1.8.138              h9e0957a_1    conda-forge
     babel                     2.9.0              pyhd3deb0d_0    conda-forge
     backcall                  0.2.0              pyh9f0ad1d_0    conda-forge
     backports                 1.0                        py_2    conda-forge
     backports.functools_lru_cache 1.6.1                      py_0    conda-forge
     black                     19.10b0                  py37_0    conda-forge
     bleach                    3.3.0              pyh44b312d_0    conda-forge
     bokeh                     2.2.3            py37hc8dfbb8_0    conda-forge
     boost-cpp                 1.75.0               hc6e9bd1_0    conda-forge
     brotli                    1.0.9                h9c3ff4c_4    conda-forge
     brotlipy                  0.7.0           py37hb5d75c8_1001    conda-forge
     bzip2                     1.0.8                h7f98852_4    conda-forge
     c-ares                    1.17.1               h36c2ea0_0    conda-forge
     ca-certificates           2021.1.19            h06a4308_0
     certifi                   2020.12.5        py37h89c1867_1    conda-forge
     cffi                      1.14.5           py37hc58025e_0    conda-forge
     cfgv                      3.2.0                      py_0    conda-forge
     chardet                   4.0.0            py37h89c1867_1    conda-forge
     clang                     8.0.1                hc9558a2_2    conda-forge
     clang-tools               8.0.1                hc9558a2_2    conda-forge
     clangxx                   8.0.1                         2    conda-forge
     click                     7.1.2              pyh9f0ad1d_0    conda-forge
     cloudpickle               1.6.0                      py_0    conda-forge
     cmake                     3.18.5               h1f3970d_0    rapidsai-nightly
     cmake-format              0.6.11             pyh9f0ad1d_0    conda-forge
     cmake_setuptools          0.1.3                      py_0    rapidsai
     colorama                  0.4.4              pyh9f0ad1d_0    conda-forge
     commonmark                0.9.1                      py_0    conda-forge
     cryptography              3.4.4            py37hf1a17b8_0    conda-forge
     cudatoolkit               10.2.89              h6bb024c_0    nvidia
     cudnn                     7.6.5.32             h01f27c4_1    conda-forge
     cupy                      8.4.0            py37hca4e673_1    conda-forge
     cython                    0.29.22          py37hcd2ae1e_0    conda-forge
     cytoolz                   0.11.0           py37h5e8e339_3    conda-forge
     dask                      2021.2.0+31.g895c9541          pypi_0    pypi
     dask-core                 2021.2.0           pyhd8ed1ab_0    conda-forge
     decorator                 4.4.2                      py_0    conda-forge
     defusedxml                0.6.0                      py_0    conda-forge
     distlib                   0.3.1              pyh9f0ad1d_0    conda-forge
     distributed               2021.2.0+12.gd24d62f1          pypi_0    pypi
     dlpack                    0.3                  he1b5a44_1    conda-forge
     docutils                  0.16             py37h89c1867_3    conda-forge
     double-conversion         3.1.5                he1b5a44_2    conda-forge
     editdistance              0.5.3            py37hcd2ae1e_3    conda-forge
     entrypoints               0.3             py37hc8dfbb8_1002    conda-forge
     execnet                   1.8.0              pyh44b312d_0    conda-forge
     expat                     2.2.10               h9c3ff4c_0    conda-forge
     fastavro                  1.3.2            py37h5e8e339_0    conda-forge
     fastrlock                 0.5              py37hcd2ae1e_2    conda-forge
     filelock                  3.0.12             pyh9f0ad1d_0    conda-forge
     flake8                    3.8.3                      py_1    conda-forge
     flatbuffers               1.12.0               he1b5a44_0    conda-forge
     freetype                  2.10.4               h0708190_1    conda-forge
     fsspec                    0.8.5              pyhd8ed1ab_0    conda-forge
     future                    0.18.2           py37h89c1867_3    conda-forge
     gflags                    2.2.2             he1b5a44_1004    conda-forge
     glog                      0.4.0                h49b9bf7_3    conda-forge
     gmp                       6.2.1                h58526e2_0    conda-forge
     grpc-cpp                  1.35.0               h7919d58_1    conda-forge
     heapdict                  1.0.1                      py_0    conda-forge
     hypothesis                6.3.0              pyhd8ed1ab_0    conda-forge
     icu                       68.1                 h58526e2_0    conda-forge
     identify                  1.5.14             pyh44b312d_0    conda-forge
     idna                      2.10               pyh9f0ad1d_0    conda-forge
     imagesize                 1.2.0                      py_0    conda-forge
     importlib-metadata        3.4.0            py37h89c1867_0    conda-forge
     importlib_metadata        3.4.0                hd8ed1ab_0    conda-forge
     iniconfig                 1.1.1              pyh9f0ad1d_0    conda-forge
     ipykernel                 5.5.0            py37h888b3d9_1    conda-forge
     ipython                   7.20.0           py37h888b3d9_2    conda-forge
     ipython_genutils          0.2.0                      py_1    conda-forge
     isort                     5.0.7            py37hc8dfbb8_0    conda-forge
     jedi                      0.18.0           py37h89c1867_2    conda-forge
     jinja2                    2.11.3             pyh44b312d_0    conda-forge
     jpeg                      9d                   h516909a_0    conda-forge
     jsonschema                3.2.0            py37hc8dfbb8_1    conda-forge
     jupyter_client            6.1.11             pyhd8ed1ab_1    conda-forge
     jupyter_core              4.7.1            py37h89c1867_0    conda-forge
     jupyterlab_pygments       0.1.2              pyh9f0ad1d_0    conda-forge
     krb5                      1.17.2               h926e7f8_0    conda-forge
     lcms2                     2.12                 hddcbb42_0    conda-forge
     ld_impl_linux-64          2.35.1               hea4e1c9_2    conda-forge
     libblas                   3.9.0                8_openblas    conda-forge
     libcblas                  3.9.0                8_openblas    conda-forge
     libcurl                   7.71.1               hcdd3856_8    conda-forge
     libedit                   3.1.20191231         he28a2e2_2    conda-forge
     libev                     4.33                 h516909a_1    conda-forge
     libevent                  2.1.10               hcdb4288_3    conda-forge
     libffi                    3.3                  h58526e2_2    conda-forge
     libgcc-ng                 9.3.0               h2828fa1_18    conda-forge
     libgfortran-ng            7.5.0               h14aa051_18    conda-forge
     libgfortran4              7.5.0               h14aa051_18    conda-forge
     libgomp                   9.3.0               h2828fa1_18    conda-forge
     liblapack                 3.9.0                8_openblas    conda-forge
     libllvm10                 10.0.1               he513fc3_3    conda-forge
     libllvm8                  8.0.1                hc9558a2_0    conda-forge
     libnghttp2                1.43.0               h812cca2_0    conda-forge
     libopenblas               0.3.12          pthreads_hb3c22a3_1    conda-forge
     libpng                    1.6.37               hed695b0_2    conda-forge
     libprotobuf               3.15.1               h780b84a_0    conda-forge
     libsodium                 1.0.18               h516909a_1    conda-forge
     libssh2                   1.9.0                hab1572f_5    conda-forge
     libstdcxx-ng              9.3.0               h6de172a_18    conda-forge
     libthrift                 0.13.0               hbe8ec66_6    conda-forge
     libtiff                   4.2.0                hdc55705_0    conda-forge
     libutf8proc               2.6.1                h7f98852_0    conda-forge
     libuv                     1.41.0               h7f98852_0    conda-forge
     libwebp-base              1.2.0                h7f98852_0    conda-forge
     llvmlite                  0.35.0           py37h9d7f4d0_1    conda-forge
     locket                    0.2.1            py37h06a4308_1
     lz4-c                     1.9.3                h9c3ff4c_0    conda-forge
     markdown                  3.3.3              pyh9f0ad1d_0    conda-forge
     markupsafe                1.1.1            py37h5e8e339_3    conda-forge
     mccabe                    0.6.1                      py_1    conda-forge
     mimesis                   4.0.0              pyh9f0ad1d_0    conda-forge
     mistune                   0.8.4           py37h5e8e339_1003    conda-forge
     more-itertools            8.7.0              pyhd8ed1ab_0    conda-forge
     msgpack-python            1.0.2            py37h2527ec5_1    conda-forge
     mypy                      0.782                      py_0    conda-forge
     mypy_extensions           0.4.3            py37h89c1867_3    conda-forge
     nbclient                  0.5.2              pyhd8ed1ab_0    conda-forge
     nbconvert                 6.0.7            py37h89c1867_3    conda-forge
     nbformat                  5.1.2              pyhd8ed1ab_1    conda-forge
     nbsphinx                  0.8.1              pyh44b312d_0    conda-forge
     nccl                      2.8.4.1              h1a5f58c_1    conda-forge
     ncurses                   6.2                  h58526e2_4    conda-forge
     nest-asyncio              1.5.1              pyhd3eb1b0_0
     nodeenv                   1.5.0              pyh9f0ad1d_0    conda-forge
     notebook                  6.2.0            py37h89c1867_0    conda-forge
     numba                     0.52.0           py37hdc94413_0    conda-forge
     numpy                     1.19.5           py37haa41c4c_1    conda-forge
     numpydoc                  1.1.0                      py_1    conda-forge
     nvtx                      0.2.3            py37h5e8e339_0    conda-forge
     olefile                   0.46               pyh9f0ad1d_1    conda-forge
     openssl                   1.1.1j               h7f98852_0    conda-forge
     orc                       1.6.7                heec2584_1    conda-forge
     packaging                 20.9               pyh44b312d_0    conda-forge
     pandas                    1.1.5            py37hdc94413_0    conda-forge
     pandoc                    1.19.2.1             hea2e7c5_1
     pandocfilters             1.4.3            py37h06a4308_1
     parquet-cpp               1.5.1                         1    conda-forge
     parso                     0.8.1              pyhd8ed1ab_0    conda-forge
     partd                     1.1.0                      py_0    conda-forge
     pathspec                  0.8.1              pyhd3deb0d_0    conda-forge
     pexpect                   4.8.0            py37hc8dfbb8_1    conda-forge
     pickleshare               0.7.5           py37hc8dfbb8_1002    conda-forge
     pillow                    8.1.0            py37h4600e1f_2    conda-forge
     pip                       21.0.1             pyhd8ed1ab_0    conda-forge
     pluggy                    0.13.1           py37h89c1867_4    conda-forge
     pre-commit                2.10.1           py37h89c1867_0    conda-forge
     pre_commit                2.10.1               hd8ed1ab_0    conda-forge
     prometheus_client         0.9.0              pyhd3deb0d_0    conda-forge
     prompt-toolkit            3.0.16             pyha770c72_0    conda-forge
     protobuf                  3.15.1           py37hcd2ae1e_0    conda-forge
     psutil                    5.8.0            py37h5e8e339_1    conda-forge
     ptvsd                     4.3.2                    pypi_0    pypi
     ptyprocess                0.7.0              pyhd3deb0d_0    conda-forge
     py                        1.10.0             pyhd3deb0d_0    conda-forge
     py-cpuinfo                7.0.0              pyh9f0ad1d_0    conda-forge
     pyarrow                   1.0.1           py37h3dc597d_31_cuda    conda-forge
     pycodestyle               2.6.0              pyh9f0ad1d_0    conda-forge
     pycparser                 2.20               pyh9f0ad1d_2    conda-forge
     pyflakes                  2.2.0              pyh9f0ad1d_0    conda-forge
     pygments                  2.8.0              pyhd8ed1ab_0    conda-forge
     pynvml                    8.0.4                    pypi_0    pypi
     pyopenssl                 20.0.1             pyhd8ed1ab_0    conda-forge
     pyorc                     0.4.0                    pypi_0    pypi
     pyparsing                 2.4.7              pyh9f0ad1d_0    conda-forge
     pyrsistent                0.17.3           py37h5e8e339_2    conda-forge
     pysocks                   1.7.1            py37h89c1867_3    conda-forge
     pytest                    6.2.2            py37h89c1867_0    conda-forge
     pytest-benchmark          3.2.3              pyh9f0ad1d_0    conda-forge
     pytest-forked             1.3.0              pyhd3deb0d_0    conda-forge
     pytest-xdist              2.2.1              pyhd8ed1ab_0    conda-forge
     python                    3.7.10          hffdb5ce_100_cpython    conda-forge
     python-dateutil           2.8.1                      py_0    conda-forge
     python_abi                3.7                     1_cp37m    conda-forge
     pytz                      2021.1             pyhd8ed1ab_0    conda-forge
     pyyaml                    5.4.1            py37h5e8e339_0    conda-forge
     pyzmq                     22.0.3           py37h499b945_0    conda-forge
     rapidjson                 1.1.0             hf484d3e_1002    conda-forge
     re2                       2020.11.01           h58526e2_0    conda-forge
     readline                  8.1                  h27cfd23_0
     recommonmark              0.7.1              pyhd8ed1ab_0    conda-forge
     regex                     2020.11.13       py37h5e8e339_1    conda-forge
     requests                  2.25.1             pyhd3deb0d_0    conda-forge
     rhash                     1.4.1                h7f98852_0    conda-forge
     s2n                       0.10.26              h9b69904_0    conda-forge
     send2trash                1.5.0                      py_0    conda-forge
     setuptools                52.0.0           py37h06a4308_0
     six                       1.15.0             pyh9f0ad1d_0    conda-forge
     snappy                    1.1.8                he1b5a44_3    conda-forge
     snowballstemmer           2.1.0              pyhd8ed1ab_0    conda-forge
     sortedcontainers          2.3.0              pyhd8ed1ab_0    conda-forge
     spdlog                    1.7.0                hc9558a2_2    conda-forge
     sphinx                    3.5.1              pyhd8ed1ab_0    conda-forge
     sphinx-copybutton         0.3.1              pyhd8ed1ab_0    conda-forge
     sphinx-markdown-tables    0.0.15             pyhd3deb0d_0    conda-forge
     sphinx_rtd_theme          0.5.1              pyhd3deb0d_0    conda-forge
     sphinxcontrib-applehelp   1.0.2                      py_0    conda-forge
     sphinxcontrib-devhelp     1.0.2                      py_0    conda-forge
     sphinxcontrib-htmlhelp    1.0.3                      py_0    conda-forge
     sphinxcontrib-jsmath      1.0.1                      py_0    conda-forge
     sphinxcontrib-qthelp      1.0.3                      py_0    conda-forge
     sphinxcontrib-serializinghtml 1.1.4                      py_0    conda-forge
     sphinxcontrib-websupport  1.2.4              pyh9f0ad1d_0    conda-forge
     sqlite                    3.34.0               h74cdb3f_0    conda-forge
     streamz                   0.6.2              pyh44b312d_0    conda-forge
     tblib                     1.7.0                      py_0
     terminado                 0.9.2            py37h89c1867_0    conda-forge
     testpath                  0.4.4                      py_0    conda-forge
     tk                        8.6.10               hed695b0_1    conda-forge
     toml                      0.10.2             pyhd8ed1ab_0    conda-forge
     toolz                     0.11.1                     py_0    conda-forge
     tornado                   6.1              py37h5e8e339_1    conda-forge
     traitlets                 5.0.5                      py_0    conda-forge
     typed-ast                 1.4.2            py37h5e8e339_0    conda-forge
     typing_extensions         3.7.4.3                    py_0    conda-forge
     urllib3                   1.26.3             pyhd8ed1ab_0    conda-forge
     virtualenv                20.4.2           py37h89c1867_0    conda-forge
     wcwidth                   0.2.5              pyh9f0ad1d_2    conda-forge
     webencodings              0.5.1                      py_1    conda-forge
     wheel                     0.36.2             pyhd3deb0d_0    conda-forge
     xz                        5.2.5                h516909a_1    conda-forge
     yaml                      0.2.5                h516909a_0    conda-forge
     zeromq                    4.3.4                h9c3ff4c_0    conda-forge
     zict                      2.0.0                      py_0    conda-forge
     zipp                      3.4.0                      py_0    conda-forge
     zlib                      1.2.11            h516909a_1010    conda-forge
     zstd                      1.4.8                ha95c52a_1    conda-forge
     
</pre></details>",2021-02-25T02:34:17Z,0,0,Michael Wang,Nvidia Rapids,True
113,[BUG] Behavior of __pow__ differs from Pandas for special values,"**Describe the bug**
In pandas, `1**<NA> == 1`, whereas in cuDF, `1**<NA> == <NA>`. Furthermore, in pandas, `<NA> ** 0 == 1` whereas in cuDF, `<NA> ** 0 == <NA>`. 


**Steps/Code to reproduce bug**


First issue:
```
>>> psr = pd.Series([1,2,3], dtype='int64')
>>> gsr = cudf.Series([1,2,3], dtype='int64')
>>> psr ** pd.NA
0       1
1    <NA>
2    <NA>
dtype: object
>>> gsr ** cudf.NA
0    <NA>
1    <NA>
2    <NA>
dtype: int64
```

```
>>> psr = pd.Series([None], dtype='Int64')
>>> gsr = cudf.Series([None], dtype='int64')
>>> psr
0    <NA>
dtype: Int64
>>> gsr
0    <NA>
dtype: int64
>>> psr ** 0
0    1
dtype: Int64
>>> gsr ** 0
0    <NA>
dtype: int64
```

**Expected behavior**
I believe we should match pandas here. Since the behavior we expose here is the behavior of libcudf, we might have to run a few extra kernels to explicitly solve this case.

I believe this is worth doing. It's a tradeoff between extra work that we need to do on the GPU which of course will impact performance, vs the possibility of users running the same data through the same sequence of mathematical operations between pandas and cuDF and getting a different number. IMO the second possibility is more likely to lead to issues on the user side than the first. 


**Environment overview (please complete the following information)**
 - Environment location: Bare Metal
 - Method of cuDF install: Source

**Environment details**
Please run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details

**Additional context**
Add any other context about the problem here.
",2021-03-01T21:46:13Z,0,0,,NVIDIA,True
114,[FEA] Improve performance of Java pinned memory pool at scale,"**Is your feature request related to a problem? Please describe.**
We have seen cases where allocating many thousands of pinned memory buffers causes very poor performance in the Java `PinnedMemoryPool` allocator.  That allocator currently performs a linear coalesce scan on free which performs decently when there aren't many allocations but does not scale well.

**Describe the solution you'd like**
The pinned memory pool should minimally use a logarithmic, heap-like algorithm for managing the address space, or possibly a bucketing technique with hashing.
",2021-03-10T15:56:45Z,0,0,Jason Lowe,NVIDIA,True
115,[FEA] Propagate nulls through `isin` ,"**Is your feature request related to a problem? Please describe.**
In pandas, we can check if the values of a series or dataframe are contained within some other container, like a list or dataframe, by using `isin`. Currently, this doesn't work correctly for nulls. On branch-0.19, if the dataframe or series we're checking contains an `<NA>`, we get a `False`:

```
>>> values = cudf.Series([1,2,3])
>>> df = cudf.DataFrame({'a':[1,2,None]})
>>> df
      a
0     1
1     2
2  <NA>
>>> df.isin(values)
       a
0   True
1   True
2  False
```

Where we should get just another `<NA>` there, like in pandas, using nullable dtypes: 

```
>>> values = pd.Series([1,2,3], dtype='Int64')
>>> df = pd.DataFrame({'a':pd.Series([1,2,None], dtype='Int64')})
>>> df
      a
0     1
1     2
2  <NA>
>>> df.isin(values)
      a
0  True
1  True
2  <NA>
```

While the `fillna` that causes us to get `False` is being removed in PR https://github.com/rapidsai/cudf/pull/7490, we'll need to rework how we're testing this functionality and change it to test against nullable types. It just so happens that when using non nullable pandas types, we get `False` as well - hence our results lining up so far. 


**Describe the solution you'd like**
We should get an `<NA>` everywhere the series or dataframe in question already has an `<NA>` and our tests should be updated to reflect that. 


**Describe alternatives you've considered**
We could change it as part of PR https://github.com/rapidsai/cudf/pull/7490 but it would be somewhat tangential to the point. 


**Additional context**
Add any other context, code examples, or references to existing implementations about the feature request here.
",2021-03-10T20:22:41Z,0,0,,NVIDIA,True
116,[BUG] Missing test for memory corruption in range window queries on null timestamps,"#7568 fixed the null-bounds calculation for time range window functions, for cases where the timestamp column might contain nulls. No tests were added as part of the fix, because the crash is elusive. There already are tests for grouped time-range rolling windows that exercise this code path, but do not crash on account of the out-of-bounds access. 

It would be good to add a test that reproduces the error condition.",2021-03-12T23:45:45Z,0,0,MithunR,NVIDIA,True
117,[BUG] dask_cudf generates files it cannot read back,"**Describe the bug**

Somewhere between `dgdf = dask_cudf.read_csv(..)` and `dgdf.to_parquet()`, the generated files are written in a way that `cudf.read_parquet` and `dask_cudf.read_parquet` will fail to read the data back.

Exception:  `cuDF failure at: /opt/conda/envs/rapids/conda-bld/libcudf_1607621803079/work/cpp/src/io/parquet/reader_impl.cu:371: All sources must have the same schemas`



**Steps/Code to reproduce bug**

May need to fix paths:

Download:

```bash
curl -O /tmp/logs.csv.gz ""https://s3.amazonaws.com/botsdataset/botsv1/csv-by-sourcetype/botsv1.WinEventLog%3ASecurity.csv.gz""
(cd /tmp && gunzip logs.csv.gz)
```

Convert:
```python
with dask.distributed.Client(ADDRESS):
  dgdf = dask_cudf.read_csv('/tmp/logs.csv')
  dgdf.to_parquet(
       '/tmp/logs.parquet',
        compression='snappy',
        write_index=False,
        index=False)
```

Test: Unexpectedly throws exn
```python
cudf.read_parquet('/tmp/logs.parquet')
```

**Expected behavior**
The converted file to read back with matching dtypes... but throws an exn

**Environment overview (please complete the following information)**
RAPIDS 0.18 (conda) in docker (ubuntu); A100's

**Additional context**

* Variants where we set `schema`, `dtypes`,  and `use_pandas_metadata` also fail
* Also seeing failures when doing dask_cudf.read_parquet, and doing an intermediate repartition
",2021-03-15T21:02:56Z,0,0,,Graphistry,False
118,[FEA][INTERNALS] A `ColumnMeta` type to represent the column metadata of a `Frame`,"When we roundtrip a `Frame` between Python and libcudf, we potentially lose a bunch of metadata:

1. Names of columns
2. Whether the columns have multiple levels (i.e., the Frame has a MultiIndex as its columns(
3. The level names

## The problem

libcudf functions return a `unique_ptr<cudf::table>`, we  convert that `table` into a  `Frame` in the function [from_unique_ptr](https://github.com/rapidsai/cudf/blob/ec5364c2fc0a3c63583e648ec90efa8d3b5675bc/python/cudf/cudf/_lib/table.pyx#L82). Here, we pass the column names (1), but not the multiindex (2) or level_names (3) metadata.

This can lead to surprising behaviour in many situations. For example, consider the `loc` call below where we lose the `multiindex` part of our metadata:

```python
In [10]: df
Out[10]:
    a       b
  sum min max min
a
2   4   2   5   4
1   3   1   3   1

In [11]: df.loc[[2, 2, 1, 1], :]
Out[11]:
   (a, sum)  (a, min)  (b, max)  (b, min)
a
2         4         2         5         4
2         4         2         5         4
1         3         1         3         1
1         3         1         3         1

In [12]: df.to_pandas().loc[[2, 2, 1, 1], :]
Out[12]:
    a       b
  sum min max min
a
2   4   2   5   4
2   4   2   5   4
1   3   1   3   1
1   3   1   3   1
```

## Proposed solution

We could introduce an internal `ColumnMeta` type:

```python
class ColumnMeta:
    names: Tuple[Any]
    multiindex: bool
    level_names: Optional[Tuple[Any]]
```

which could be a property of `Frame` objects for convenience:

```python
class Frame:
    @cached_property
    def _column_meta(self):
         ...
```

Now, instead of passing just the column names and index names to `from_unique_ptr`, we could pass the full metadata for both:

```python
cdef Table from_unique_ptr(
    unique_ptr[table] c_tbl,
    ColumnMeta data_meta,
    ColumnMeta index_meta=None
):
```

and it would construct the resulting `Frame` with the correct column metadata.

--

With this, a typical Python wrapper around a libcudf API would be:

```python
def py_func(Table foo, ...):
    cdef table_view c_input = foo.view()
    cdef unique_ptr[table] c_result
    with nogil:
        c_result = cpp_func(c_input)
    return Table.from_unique_ptr(c_result, foo._column_meta, foo.index._column_meta)
```",2021-03-18T22:07:35Z,0,0,Ashwin Srinath,Voltron Data,False
119,[BUG] new parquet writer code checks for `nullable` not `has_nulls`,"**Describe the bug**
New parquet code was added to support writing nested types.  This is great, but it broke the java build.  As a part of fixing the java build I found that the new code checks for `nullable` on all of the columns to see if it matches what was set when the writer was initially configured.  But Spark can tell that validity is not needed in some cases where cudf apparently cannot, and cudf will add in a validity column in some cases when it is not needed.  because `nullable` only checks to see if there is a column, and not if there are actually any nulls we can run into a situation where spark tells us that there will be no nulls, but cudf blows up because it thinks that there might be.
",2021-03-19T17:18:52Z,0,0,Robert (Bobby) Evans,Nvidia,True
120,[FEA] rolling correlation,"**What is your question?**
How to calculate rolling correlation between two cuDF columns?",2021-03-22T14:11:38Z,0,0,,,False
121,[FEA] Mixed precision Decimal math support in cudf Python,"Using recent cudf nightly conda package (0.19.0a+250.g8632ca0da3):

**Int & Decimal Addition**:
```
import cudf
from cudf.core.dtypes import Decimal64Dtype

df = cudf.DataFrame({'val': [0.01, 0.02, 0.03]})

df['dec_val'] = df['val'].astype(Decimal64Dtype(7,2))
df['dec_val'] + 1
```
**Result**:
```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-8-d4f1761193b6> in <module>
----> 1 df['val'] + 1

/conda/lib/python3.8/site-packages/cudf/core/series.py in __add__(self, other)
   1600 
   1601     def __add__(self, other):
-> 1602         return self._binaryop(other, ""add"")
   1603 
   1604     def radd(self, other, fill_value=None, axis=0):

/conda/lib/python3.8/contextlib.py in inner(*args, **kwds)
     73         def inner(*args, **kwds):
     74             with self._recreate_cm():
---> 75                 return func(*args, **kwds)
     76         return inner
     77 

/conda/lib/python3.8/site-packages/cudf/core/series.py in _binaryop(self, other, fn, fill_value, reflect, can_reindex)
   1515         else:
   1516             lhs, rhs = self, other
-> 1517         rhs = self._normalize_binop_value(rhs)
   1518 
   1519         if fn == ""truediv"":

/conda/lib/python3.8/site-packages/cudf/core/series.py in _normalize_binop_value(self, other)
   2307             return cudf.Scalar(other, dtype=self.dtype)
   2308         else:
-> 2309             return self._column.normalize_binop_value(other)
   2310 
   2311     def eq(self, other, fill_value=None, axis=0):

AttributeError: 'DecimalColumn' object has no attribute 'normalize_binop_value'
```

**Workaround**:
```
import cudf
from cudf.core.dtypes import Decimal64Dtype

df = cudf.DataFrame({'val': [0.01, 0.02, 0.03]})

df['dec_val'] = df['val'].astype(Decimal64Dtype(7,2))
df['ones'] = 1.00
df['dec_val'] + df['ones'].astype(Decimal64Dtype(7,0))
```
```
0    1.01
1    1.02
2    1.03
dtype: decimal
```

**Decimal & Float Multiplication**:
```
import cudf
from cudf.core.dtypes import Decimal64Dtype

df = cudf.DataFrame({'val': [0.01, 0.02, 0.03]})

df['dec_val'] = df['val'].astype(Decimal64Dtype(7,2))
df['val'] * df['dec_val']
```
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-13-4680a31be74b> in <module>
----> 1 df['val'] * df['dec_val']

/conda/lib/python3.8/site-packages/cudf/core/series.py in __mul__(self, other)
   1799 
   1800     def __mul__(self, other):
-> 1801         return self._binaryop(other, ""mul"")
   1802 
   1803     def rmul(self, other, fill_value=None, axis=0):

/conda/lib/python3.8/contextlib.py in inner(*args, **kwds)
     73         def inner(*args, **kwds):
     74             with self._recreate_cm():
---> 75                 return func(*args, **kwds)
     76         return inner
     77 

/conda/lib/python3.8/site-packages/cudf/core/series.py in _binaryop(self, other, fn, fill_value, reflect, can_reindex)
   1542                     rhs = rhs.fillna(fill_value)
   1543 
-> 1544         outcol = lhs._column.binary_operator(fn, rhs, reflect=reflect)
   1545         result = lhs._copy_construct(data=outcol, name=result_name)
   1546         return result

/conda/lib/python3.8/site-packages/cudf/core/column/numerical.py in binary_operator(self, binop, rhs, reflect)
    108             ):
    109                 msg = ""{!r} operator not supported between {} and {}""
--> 110                 raise TypeError(msg.format(binop, type(self), type(rhs)))
    111             out_dtype = np.result_type(self.dtype, rhs.dtype)
    112             if binop in [""mod"", ""floordiv""]:

TypeError: 'mul' operator not supported between <class 'cudf.core.column.numerical.NumericalColumn'> and <class 'cudf.core.column.decimal.DecimalColumn'
```

**Workaround**:
```
import cudf
from cudf.core.dtypes import Decimal64Dtype

df = cudf.DataFrame({'val': [0.01, 0.02, 0.03]})

df['dec_val'] = df['val'].astype(Decimal64Dtype(7,2))
df['dec_val'] * df['val'].astype(Decimal64Dtype(7, 2))
```
```
0    0.0001
1    0.0004
2    0.0009
dtype: decimal
```",2021-03-23T14:36:45Z,0,0,Randy Gelhausen,,False
122,[FEA] Improve libcudf hashing tests,"**Is your feature request related to a problem? Please describe.**
Currently the hashing tests use a relatively small number of fixed values and some tests should be parameterized to test all possible input types.

**Describe the solution you'd like**
Ideally the tests should use procedurally generated input with the ability to generate a controllable percentage of nulls and corner case values (e.g.: min/max values, zero, -0.0/NaN,+/-Inf for floating point types, etc.).  The results should then be compared against a reference CPU implementation that computes on the same input.

There should also be negative tests that verify any unsupported input types throw appropriate exceptions.
",2021-03-24T14:00:05Z,0,0,Jason Lowe,NVIDIA,True
123,"[FEA] Support list types in ""to_csv""","**Example**:
```
import cudf

df = cudf.DataFrame({'id': [0, 1], 'list_col': [[0, 0], [1, 1]]})
df.to_csv('test.csv')
```
**Result**:
```
---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
<ipython-input-11-781fe2a14f27> in <module>
      1 df = cudf.DataFrame({'id': [0, 1], 'list_col': [[0, 0], [1, 1]]})
----> 2 df.to_csv('test.csv')

/conda/lib/python3.8/site-packages/cudf/core/dataframe.py in to_csv(self, path_or_buf, sep, na_rep, columns, header, index, line_terminator, chunksize, encoding, compression, **kwargs)
   7366         from cudf.io import csv as csv
   7367 
-> 7368         return csv.to_csv(
   7369             self,
   7370             path_or_buf=path_or_buf,

/conda/lib/python3.8/contextlib.py in inner(*args, **kwds)
     73         def inner(*args, **kwds):
     74             with self._recreate_cm():
---> 75                 return func(*args, **kwds)
     76         return inner
     77 

/conda/lib/python3.8/site-packages/cudf/io/csv.py in to_csv(df, path_or_buf, sep, na_rep, columns, header, index, line_terminator, chunksize, encoding, compression, **kwargs)
    157     for col in df._data.columns:
    158         if isinstance(col, cudf.core.column.ListColumn):
--> 159             raise NotImplementedError(
    160                 ""Writing to csv format is not yet supported with ""
    161                 ""list columns.""

NotImplementedError: Writing to csv format is not yet supported with list columns.
```

Pandas does this by wrapping a stringified representation of each row's list `quotechar`:
```
import pandas as pd

df = pd.DataFrame({'id': [0, 1], 'list_col': [[0, 0], [1, 1]]})
df.to_csv('test.csv')
```
test.csv:
```
,id,list_col
0,0,""[0, 0]""
1,1,""[1, 1]""
```",2021-03-24T15:55:18Z,0,0,Randy Gelhausen,,False
124,[FEA] dask_cudf cross-partition type coercions,"**Is your feature request related to a problem? Please describe.**

It's been frustrating adapting cudf -> dask_cudf kernels in two basic areas around cross-partition type mismatches:

* ingest: loading json, csv, etc. that vary in column types across partitions: existence, nans, int vs float, etc. When the code writer isn't the user -- so a library, piece of software, a UI, this is common and you can't just workaround by specifying dtypes ahead of time

* compute: when doing data cleaning (ex: date inference) or some algs, it's unclear what `meta` should be ahead of time, only after you actually do the calc. dask will sample the first df... which is often wrong

**Describe the solution you'd like**

dask_cudf ingest operators: an auto-coercion flag (""when columns are in conflict across partitions, coerce to the closest common type, like float or str"")

dask_cudf map, concat, etc: same thing


**Describe alternatives you've considered**

It may also be possible to make each operator smarter via sampling or other tricks. dask core and some cudf io seems to be experimenting here.

I like explicit flags b/c of their predictability/reliability, and uniformity... but ultimately, whatever work :)


**Additional context**

By default, I'm guessing this issue will be ignored & deprioritized ;-)

Before doing that, it may be worth polling dask_cudf users -- not devs -- how they feel about this ;-) my bet is people spend a surprising % of their time on a few issues around here, well before actual perf
",2021-03-26T21:18:23Z,0,0,,Graphistry,False
125,[FEA]: Match pandas ordering convention for idxmin/idxmax,"**Is your feature request related to a problem? Please describe.**
In pandas, the `idxmin` and `idxmax` aggregations will always break ties in favor of the lower index. For instance, 
```
>>> import pandas as pd
>>> df = pd.DataFrame({'idx': [0, 0], 'x': [1, 1]})
>>> df.groupby('idx').agg('idxmin')
     x
idx
0    0
```
can be relied upon to always return `0`. `cudf` does not make this promise, although it frequently gives the same result in practice. I have a dataset where I'm able to reproduce this discrepancy, but I haven't been able to generate a truly ""minimal"" example yet because various attempt to systematically reduce my column of size 100 have failed to consistently reproduce the problem. I can give this another shot if someone is interested.

**Describe the solution you'd like**
The reason for this discrepancy is likely that (as pointed out by @shwina) `cudf` utilizes hash maps in the groupby->aggregation, which likely destroys any stability guarantees with respect to the input ordering for equal elements. Allowing calling code to specify an algorithm would be a poor API choice for `libcudf`, but it would be reasonable to expose an API to request stability in any internal sorting. This would allow `cudf` to match the `pandas` behavior.

**Additional context**
The [pandas documentation of idxmin](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html) (and idxmax) does not actually promise this behavior. I suspect that the behavior is chosen to match that of `numpy`, which _does_ [promise this behavior](https://numpy.org/doc/stable/reference/generated/numpy.argmin.html) (see the Notes section). That being the case, we could simply choose not to support this in `cudf`, which would save us from having to reimplement alternative algorithms in `libcudf`. This could lead to problems for `pandas` conversions down the line, though.
",2021-03-30T00:16:23Z,0,0,Vyas Ramasubramani,@rapidsai,True
126,[FEA] Consider disabling `--expt-relaxed-constexpr`,"**Is your feature request related to a problem? Please describe.**

`--expt-relaxed-constexpr` is a convenient way to reuse existing `constexpr` host code, e.g., things like `std::max`.

However, it can lead to some pretty surprising behavior. Consider:

```
constexpr int bar(int j){
    if(j<0){
        throw;
    }
    return 42;
}
__global__ void kernel(int * i){
    *i = bar(-1);
}
```

https://godbolt.org/z/frb8c6cd7

One might expect this to fail to compile as `throw` is not valid in device code. However, not only does it happily compile, but it just stores the value 42. 

This example looks pretty harmless:
```
int foo(int i){
    return i * 2;
}
constexpr int bar(int j){
    if(j<0){
        return foo(j);
    }
    return 42;
}
__global__ void kernel( int * i){
    *i = bar(-1);
}
```
But this too results in an ill-formed program without a diagnostic.

https://godbolt.org/z/aTzGaMrGd

**Describe the solution you'd like**

We should think pretty hard about if we want to risk such egregious undefined behavior in libcudf. 

As such, we may want to consider moving towards disabling `--expt-relaxed-constexpr`. At the very least, we should be preferring `CUDA_HOST_DEVICE_CALLABLE` whenever possible (for functions that need be called from both host and device). 

**Additional Context**

The only place it is 100% safe to use a `constexpr` function in device code with `--expt-relaxed-constexpr` is when used in a context that requires constant evaluation. Then it will fail to compile if the `constexpr` function contains things that would result in an ill-formed program: https://godbolt.org/z/47qfnPnc9",2021-03-31T20:14:16Z,0,0,Jake Hemstad,@NVIDIA,True
127,[FEA] Improve cudf tests for hash_partition,"**Is your feature request related to a problem? Please describe.**

As `cudf::hash_partition` now can use a hash function other than Murmur3 or take a custom seed value, I think the tests should be improved for better coverage.

**Describe the solution you'd like**

As described in [this comment](https://github.com/rapidsai/cudf/pull/7771#issuecomment-811692022), two key properties of `cudf::hash_partition` are

1. Output partitions are disjoint; and
2. Rows having the same key are always assigned the same partition id, even when they appear in different dataframes (when the number of partitions is fixed).

We need to check these properties for some combinations of hash function types and seed values. Property 1 is checked in [this Python test](https://github.com/rapidsai/cudf/blob/b9415cae362882380e14b5baa6af49275021356e/python/cudf/cudf/tests/test_dataframe.py#L1106-L1135), but only with the default hash function and seed value, and I believe there are no tests for property 2 yet.

Additionally, we should remove [this test](https://github.com/rapidsai/cudf/blob/branch-0.19/python/cudf/cudf/tests/test_dataframe.py#L1181-L1200). The test is comparing a hard-coded value with the result of `hash_partition` for a test input, but this is over-constraining, as `hash_partition` in general has no guarantee on the order in which the keys appear in the output partitions. If we later decide to change the `row_hasher` in any way, this test will start to fail.",2021-04-01T18:05:31Z,0,0,Wonchan Lee,NVIDIA,True
128,[FEA] Groupby support with Decimal column as key column,"```
import cudf
from cudf.core.dtypes import Decimal64Dtype

df = cudf.DataFrame({'id': [0, 1, 2]})

df['id_dec'] = df['id'].astype(Decimal64Dtype(7,2))

#works
df.groupby('id').count()

# fails
df.groupby('id_dec').count()
```
```
---------------------------------------------------------------------------
RecursionError                            Traceback (most recent call last)
<ipython-input-6-a16bdd01391b> in <module>
----> 1 df.groupby('id_dec').count()

/conda/lib/python3.8/site-packages/cudf/core/groupby/groupby.py in _agg_func_name_with_args(self, func_name, *args, **kwargs)
    605 
    606     func.__name__ = func_name
--> 607     return self.agg(func)
    608 
    609 

/conda/lib/python3.8/contextlib.py in inner(*args, **kwds)
     73         def inner(*args, **kwds):
     74             with self._recreate_cm():
---> 75                 return func(*args, **kwds)
     76         return inner
     77 

/conda/lib/python3.8/site-packages/cudf/core/groupby/groupby.py in agg(self, func)
    163         # a Float64Index, while Pandas returns an Int64Index
    164         # (GH: 6945)
--> 165         result = self._groupby.aggregate(self.obj, normalized_aggs)
    166 
    167         result = cudf.DataFrame._from_table(result)

/conda/lib/python3.8/site-packages/cudf/utils/utils.py in __get__(self, instance, cls)
    276             return self
    277         else:
--> 278             value = self.func(instance)
    279             setattr(instance, self.func.__name__, value)
    280             return value

/conda/lib/python3.8/site-packages/cudf/core/groupby/groupby.py in _groupby(self)
    102     @cached_property
    103     def _groupby(self):
--> 104         return libgroupby.GroupBy(self.grouping.keys, dropna=self._dropna)
    105 
    106     @annotate(""GROUPBY_AGG"", domain=""cudf_python"")

/conda/lib/python3.8/site-packages/cudf/core/groupby/groupby.py in keys(self)
    887             )
    888         else:
--> 889             return cudf.core.index.as_index(
    890                 self._key_columns[0], name=self.names[0]
    891             )

/conda/lib/python3.8/site-packages/cudf/core/index.py in as_index(arbitrary, **kwargs)
   2866     elif isinstance(arbitrary, range):
   2867         return RangeIndex(arbitrary, **kwargs)
-> 2868     return as_index(
   2869         column.as_column(arbitrary, dtype=kwargs.get(""dtype"", None)), **kwargs
   2870     )

... last 1 frames repeated, from the frame below ...

/conda/lib/python3.8/site-packages/cudf/core/index.py in as_index(arbitrary, **kwargs)
   2866     elif isinstance(arbitrary, range):
   2867         return RangeIndex(arbitrary, **kwargs)
-> 2868     return as_index(
   2869         column.as_column(arbitrary, dtype=kwargs.get(""dtype"", None)), **kwargs
   2870     )

RecursionError: maximum recursion depth exceeded in comparison
```",2021-04-01T22:08:37Z,0,0,Randy Gelhausen,,False
129,[BUG] Scalar.astype is ill-defined,"## What is the expected behavior of `Scalar.astype`?

During implementing https://github.com/rapidsai/cudf/pull/7182, I realized `cudf.Scalar.astype`'s behavior is not well-defined. Since cudf Scalar has a host-side and a device-side, which one `astype` should operate on and what is the expected behavior? In addition, device side scalar is not really operable since libcudf does not cast scalars types. How to resolve for the discrepancy?

cc @kkraus14 @brandon-b-miller ",2021-04-02T04:28:31Z,0,0,Michael Wang,Nvidia Rapids,True
130,[DOC] Rolling window apply with constant parameters.,"## Report incorrect documentation

**Location of incorrect documentation**
 https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.window.rolling.Rolling.apply.html?highlight=apply#pandas.core.[…]olling.apply

**Describe the problems or issues found in the documentation**
Is the rolling apply fully implemented? `args` and `kwargs` seem to be ignored, or I am doing it wrong. I searched far and wide for examples of how to add a constant parameter for this function (like decay rate) but I have not found it. 

Looking at the implementation in Github https://github.com/rapidsai/cudf/blob/c929ba1fe85c152d6e8b4c868cd36f0802dafa51/python/cudf/cudf/core/window/rolling.py#L254 it also does not become directly clear how to use it. `args` and `kwargs` are not referenced.

**Steps taken to verify documentation is incorrect**
List any steps you have taken:
Looked at the source code and examples.

**Suggested fix for documentation**
Detail proposed changes to fix the documentation if you have any.
Give an example usage of the rolling apply with a constant parameter (for example, decay rate).
---

## Report needed documentation

**Report needed documentation**
A lot of rolling functions will have some kind of configuration parameters that are required. It should be possible to pass them.

**Describe the documentation you'd like**
Either documentation giving examples for this use case should be added, or if the feature is forgotten, the feature should be completed.

**Steps taken to search for needed documentation**
List any steps you have taken:
- Google
- https://pandas.pydata.org/pandas-docs/stable/user_guide/window.html#rolling-window
- https://docs.rapids.ai/api/cudf/stable/api.html?highlight=apply#cudf.core.window.Rolling.apply
",2021-04-04T12:48:14Z,1,0,Disper,,False
131,[FEA] Support string concatenation of a Series and something array-like into a Series ,"I’d like for cuDF to support concatenating a series and something array-like into a series similarly to how Pandas functions.

# Example:
```
cudfArray = cudf.concat([cudfSeriesB, cudfSeries], axis=1)
cudfSeries
cudfArray
cudfSeries.str.cat(cudfArray, na_rep=""-"")
```

# Result:
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/opt/conda/envs/rapids/lib/python3.7/site-packages/cudf/core/column/column.py in as_column(arbitrary, nan_as_null, dtype, length)
   1950             data = as_column(
-> 1951                 memoryview(arbitrary), dtype=dtype, nan_as_null=nan_as_null
   1952             )

TypeError: memoryview: a bytes-like object is required, not 'DataFrame'

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
/opt/conda/envs/rapids/lib/python3.7/site-packages/cudf/core/column/column.py in as_column(arbitrary, nan_as_null, dtype, length)
   1987                         if nan_as_null is None
-> 1988                         else nan_as_null,
   1989                     ),

/opt/conda/envs/rapids/lib/python3.7/site-packages/pyarrow/array.pxi in pyarrow.lib.array()

/opt/conda/envs/rapids/lib/python3.7/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()

/opt/conda/envs/rapids/lib/python3.7/site-packages/cudf/core/dataframe.py in __arrow_array__(self, type)
   1018         raise TypeError(
-> 1019             ""Implicit conversion to a host PyArrow Table via __arrow_array__ ""
   1020             ""is not allowed, To explicitly construct a PyArrow Table, ""

TypeError: Implicit conversion to a host PyArrow Table via __arrow_array__ is not allowed, To explicitly construct a PyArrow Table, consider using .to_arrow()

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
<ipython-input-64-9046cb089696> in <module>
      2 cudfSeries
      3 cudfArray
----> 4 cudfSeries.str.cat(cudfArray, na_rep=""-"")

/opt/conda/envs/rapids/lib/python3.7/site-packages/cudf/core/column/string.py in cat(self, others, sep, na_rep)
    441             )
    442         else:
--> 443             other_cols = _get_cols_list(self._parent, others)
    444             all_cols = [self._column] + other_cols
    445             data = cpp_concatenate(

/opt/conda/envs/rapids/lib/python3.7/site-packages/cudf/core/column/string.py in _get_cols_list(parent_obj, others)
   5198             others = others.reindex(parent_index)
   5199 
-> 5200         return [column.as_column(others, dtype=""str"")]
   5201     else:
   5202         raise TypeError(

/opt/conda/envs/rapids/lib/python3.7/site-packages/cudf/core/column/column.py in as_column(arbitrary, nan_as_null, dtype, length)
   1996                     data = as_column(sr, nan_as_null=nan_as_null, dtype=dtype)
   1997                 elif np_type == np.str_:
-> 1998                     sr = pd.Series(arbitrary, dtype=""str"")
   1999                     data = as_column(sr, nan_as_null=nan_as_null)
   2000                 else:

/opt/conda/envs/rapids/lib/python3.7/site-packages/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath)
    325                     data = data.copy()
    326             else:
--> 327                 data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True)
    328 
    329                 data = SingleBlockManager.from_array(data, index)

/opt/conda/envs/rapids/lib/python3.7/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure)
    461         subarr = construct_1d_arraylike_from_scalar(data, len(index), dtype)
    462     else:
--> 463         subarr = _try_cast(data, dtype, copy, raise_cast_failure)
    464 
    465     # scalar like, GH

/opt/conda/envs/rapids/lib/python3.7/site-packages/pandas/core/construction.py in _try_cast(arr, dtype, copy, raise_cast_failure)
    566             subarr = construct_1d_object_array_from_listlike(subarr)
    567         elif not is_extension_array_dtype(subarr):
--> 568             subarr = construct_1d_ndarray_preserving_na(subarr, dtype, copy=copy)
    569     except OutOfBoundsDatetime:
    570         # in case of out of bound datetime64 -> always raise

/opt/conda/envs/rapids/lib/python3.7/site-packages/pandas/core/dtypes/cast.py in construct_1d_ndarray_preserving_na(values, dtype, copy)
   1621 
   1622     if dtype is not None and dtype.kind == ""U"":
-> 1623         subarr = lib.ensure_string_array(values, convert_na_value=False, copy=copy)
   1624     else:
   1625         subarr = np.array(values, dtype=dtype, copy=copy)

pandas/_libs/lib.pyx in pandas._libs.lib.ensure_string_array()

pandas/_libs/lib.pyx in pandas._libs.lib.ensure_string_array()

/opt/conda/envs/rapids/lib/python3.7/site-packages/numpy/core/_asarray.py in asarray(a, dtype, order)
     81 
     82     """"""
---> 83     return array(a, dtype, copy=False, order=order)
     84 
     85 

/opt/conda/envs/rapids/lib/python3.7/site-packages/cudf/core/dataframe.py in __array__(self, dtype)
   1009     def __array__(self, dtype=None):
   1010         raise TypeError(
-> 1011             ""Implicit conversion to a host NumPy array via __array__ is not ""
   1012             ""allowed, To explicitly construct a GPU matrix, consider using ""
   1013             "".as_gpu_matrix()\nTo explicitly construct a host ""

TypeError: Implicit conversion to a host NumPy array via __array__ is not allowed, To explicitly construct a GPU matrix, consider using .as_gpu_matrix()
To explicitly construct a host matrix, consider using .as_matrix()
```

**Pandas does this by Concatenating a series and something array-like (dataframe) into a series.:**
 
```
pandasArray = pd.concat([pandasSeriesB, pandasSeries], axis=1)
pandasSeries
pandasArray
pandasSeries.str.cat(pandasArray, na_rep=""-"")

```
**A cudf workaround was found doing the following:**

```
cudfArray = cudf.concat([cudfSeriesB, cudfSeries], axis=1)
print(cudfSeries)
print(cudfArray)
print(cudfSeriesB)
#cudfArray = cudfArray.as_matrix()
cudfArray[1].str.cat(cudfArray[0], na_rep=""-"").str.cat(cudfSeries, na_rep=""-"")
```

",2021-04-08T01:26:40Z,0,0,,,False
132,[FEA] Support an “extractall” method,"I’d like for cuDF to support an “extractall” method similarly to how Pandas functions

# Example:

```
cudfSeries = cudf.Series([""a1a2"", ""b1"", ""c1""], index=[""A"", ""B"", ""C""], dtype=""str"")
cudfSeries
cudf_two_groups = ""(?P<letter>[a-z])(?P<digit>[0-9])""
cudfSeries.str.extract(cudf_two_groups, expand=True)
cudfSeries.str.extractall(cudf_two_groups)
```
 
# Result:
```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-76-e32113055d25> in <module>
----> 1 cudfSeries.str.extractall(cudf_two_groups)
 
AttributeError: 'StringMethods' object has no attribute 'extractall'
```
 

**Pandas does this by extracting using ‘extractall’ is always a DataFrame with a MultiIndex on its rows. The last level of the MultiIndex is named match and indicates the order in the subject.:**
 
pandasSeries = pd.Series([""a1a2"", ""b1"", ""c1""], index=[""A"", ""B"", ""C""], dtype=""string"")
pandasSeries
pandas_two_groups = ""(?P<letter>[a-z])(?P<digit>[0-9])""
pandasSeries.str.extract(pandas_two_groups, expand=True)
pandasSeries.str.extractall(pandas_two_groups)

# Output

```
	   letter   digit
    match		
_________________________
A	0	a	1
1	a	2
B	0	b	1
C	0	c	1
```

",2021-04-08T07:30:21Z,0,0,,,False
133,[FEA] Support `cudf::replace_nulls` on structs,"**Is your feature request related to a problem? Please describe.**
I am working on supporting join on StructType in spark-rapids.  And I found an unsupported error while testing FullOuterJoin, which caused by `cudf::replace_nulls` on StructType.  Here is the [link](https://github.com/rapidsai/cudf/blob/branch-0.20/java/src/main/native/src/TableJni.cpp#L1696) of error code piece.",2021-04-13T14:18:04Z,0,0,Alfred Xu,,False
134,[FEA] Java bindings should connect RMM logger to slf4j,"**Is your feature request related to a problem? Please describe.**
RMM can create log output which defaults to output in `rmm_log.txt`.  There currently isn't a way to connect this output to the other logging output from cudf which is currently funneled through the slf4j API.

**Describe the solution you'd like**
The RMM spdlog sink should be replaced immediately after startup with a sink that can log to slf4j.  This allows the user to customize at runtime how the RMM logs should be handled, whether that's sent to a file, stderr, or some other, custom logging handler.  See https://github.com/rapidsai/rmm/issues/564#issuecomment-822964167 for details on how the RMM spdlog sink can be updated.

**Describe alternatives you've considered**
We could simply emit these error messages to stderr, but using slf4j would be more flexible for applications.
",2021-04-20T13:29:13Z,0,0,Jason Lowe,NVIDIA,True
135,[FEA] aggregation each list in a column to a single value using a user supplied function,"**Is your feature request related to a problem? Please describe.**
Spark supports the aggregate function in SQL (Not really standard but we have customers who use it)

https://spark.apache.org/docs/latest/api/sql/index.html#aggregate

It takes 4 arguments.
  * **argument** an array/list column to do the aggregation on
  * **initial** an initial value for accumulation
  * **merge** a higher order function that takes two arguments an accumulation value and the current value in the list
  * **finish** an optional higher order function that takes the output of merge and transforms it into a final value
 
A higher order function is a function that is written in SQL like `(a, b) -> a + b` to add two things together.

Even though **finish** is a higher order function we don't have to treat it that way so we can ignore it for now.

**merge** however is something new that CUDF has not really supported before. It allows the user to specify how they want an aggregation to happen instead of having a declarative aggregation like most SQL does.  So for example if I wanted to do the equivalent of SUM it would look something like

```
SELECT aggregate(list_of_int_column, 0, (acc, x) -> acc + x) as sum_of_ints_in_list
```

For each list in the column it would do essentially the equivalent of

```
MERGE_OUTPUT_TYPE acc = initial_value;
for (ELEMENT_TYPE & elem : list_data) {
  acc = merge(acc, elem);
}
```

The problem we are running into is that our customers have rather complicated operations, where the higher order function can reach out to other columns in the same row.

```
(acc, x) -> (
  CASE WHEN other_column - x.struct_sub_column >= acc
                         AND other_column - x.struct_sub_column < 100
                         AND x.struct_string_column = 'FOO'
                         AND yet_another_column <> x.third_struct_sub_column
             THEN other_column - x.struct_sub_column
             ELSE acc
  END))
```

**Describe the solution you'd like**

I would love something where we could build up an AST tree that represents the higher order function and have cudf provide a list_aggregation function that would do what we need/want. But we know that there are potentially issues with the AST in terms or performance when there are too many operators so this is all open to discussion.

**Describe alternatives you've considered**

We have thought about trying to do pattern matching to decompose the higher order function into something more manageable for CUDF to support.

i.e.

```
(acc, x) -> acc + x
```
could be translated into an SUM aggregation across the values in the list, or 

```
(acc, x) -> CASE WHEN x > acc THEN x ELSE acc END
```

could be translated into a MAX aggregation across the values in the list.

We could even use pattern matching for things like 

```
(acc, x) -> acc + x.first - x.second
```

To translate it into first doing a `x.first - x.second` for all of the struct values within the list, and then doing a SUM aggregation on that resulting list.  But things get much more difficult when we try to support pulling in other columns, and struct columns, etc.

```
(acc, x) -> acc + x.first + foo
```

In this case we would have to do essentially an `explode` on `foo` and the list column so we could execute `x.first + foo` and then finally do the SUM aggregation. This is a bit problematic because of potential memory issues that explode can cause.

So if the AST is not a workable solution we would like to request a generic list aggregation operation instead.

```
cudf::column list_aggregate(cudf::lists_column_view list, std::unique_ptr<aggregation> & aggregation);
```

With at a minimum supporting MAX, SUM, and MIN aggregations initially.

It would probably be ideal to expand it out to multiple aggregations at once like with `groupby`, but it is not a requirement.

I also need to add that null handling would have to be a bit different than other aggregations. If the list itself is a null, then the output should be a null, but if a value in the list is a null, then the output should also be a null.

```
scala> spark.sql(""SELECT aggregate(array(1, 2, 3), 0, (acc, x) -> acc + x) as A"").show
+---+
|  A|
+---+
|  6|
+---+


scala> spark.sql(""SELECT aggregate(array(1, 2, 3, null), 0, (acc, x) -> acc + x) as A"").show
+----+
|   A|
+----+
|null|
+----+
```

Ideally we would also love to have some kind of explode that would not make a copy of the array we are exploding on, but instead just do the explode on the columns that need it.
",2021-04-21T17:31:59Z,0,0,Robert (Bobby) Evans,Nvidia,True
136,[FEA] Support lists as groupby keys,"I'd like to be able to use lists as keys in a groupby:
```
import cudf

df = cudf.DataFrame({
    'id': [0, 1],
    'id_lst': [[0, 0], [1, 1]],
    'val': [0, 1]
})

df.groupby(['id', 'id_lst']).val.sum()
```
Result:
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
~/conda/envs/rapids/lib/python3.8/site-packages/pandas/core/arrays/categorical.py in __init__(self, values, categories, ordered, dtype, fastpath)
    339             try:
--> 340                 codes, categories = factorize(values, sort=True)
    341             except TypeError as err:

~/conda/envs/rapids/lib/python3.8/site-packages/pandas/core/algorithms.py in factorize(values, sort, na_sentinel, size_hint)
    721 
--> 722         codes, uniques = factorize_array(
    723             values, na_sentinel=na_sentinel, size_hint=size_hint, na_value=na_value

~/conda/envs/rapids/lib/python3.8/site-packages/pandas/core/algorithms.py in factorize_array(values, na_sentinel, size_hint, na_value, mask)
    527     table = hash_klass(size_hint or len(values))
--> 528     uniques, codes = table.factorize(
    529         values, na_sentinel=na_sentinel, na_value=na_value, mask=mask

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.factorize()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable._unique()

TypeError: unhashable type: 'numpy.ndarray'
```

One workaround is once [string list concatenation](https://github.com/rapidsai/cudf/pull/7929) merges, converting `id_lst` to a tokenized string and using the string representation as the grouping key.",2021-04-22T18:26:08Z,0,0,Randy Gelhausen,,False
137,[FEA] `cudf::scalar` cache `is_valid` to reduce amount of stream syncs,"**Is your feature request related to a problem? Please describe.**
As discovered in https://github.com/rapidsai/cudf/pull/8004 the `is_valid` call in `cudf::scalar` is not cached locally, and therefore a stream sync / `cudaMemcpyAsync` will be executed for each call.

If algorithms such as `clamp` this cost is very high and has a impact on total runtime. 

**Describe the solution you'd like**
Cache the value of `is_valid` on initial construction and any modification.

",2021-04-26T20:41:42Z,0,0,Robert Maynard,NVIDIA,True
138,[FEA] Move template class implementation to source file where possible,"Currently, there are a lot of template classes in cudf. They are mostly put in the header files (`.cuh`), and those headers are included in many other source files. As a result, compiling those source files will be very slow due to the implicit instantiation of template classes. In addition, whenever the template classes were changed, all the source files containing the headers containing those template classes will be recompiled, in a significant amount of time.

**Solution:** Move the implementation of the template classes to separate source files whenever possible, and explicitly instantiate those template classes in their source files. By doing so:
* The headers now only contain (template) class declaration, which can reduce much of compile time, and
* The source files including those headers will not be recompiled when the template classes changed their implementation.

Classes with template argument that is a cudf type can be refactored to apply this principle very easily. For classes that have template argument is an iterator, we can also separate their implementation and explicitly instantiate them with some common iterator types. For classes that have template argument is a functor, we can't do anything, unfortunately 😞 ",2021-05-03T21:33:47Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
139,[BUG] Series unique should return an array rather than a dataframe,"Pandas's unique method returns an array, while ours returns a Series.

```python
import cudf
print(cudf.__version__)
​
df = cudf.DataFrame({
    ""a"": [0,1,2,0,1,2,0]
})
type(df['a'].unique()), type(df['a'].to_pandas().unique())
0.20.0a+248.g7623f3978b
(cudf.core.series.Series, numpy.ndarray)
```",2021-05-06T16:08:54Z,0,0,Nick Becker,@NVIDIA,True
140,[FEA] Move the implementation of building an empty nested column to the native,"Currently building an empty nested column is done in Java layer, the performance is not good, and it creates a lot of unnecessary Java objects.
We should move it to the native after figuring out a way to pass the nested data type to the native.

This requirement is from https://github.com/rapidsai/cudf/pull/8173#discussion_r627530793",2021-05-07T01:14:25Z,0,0,Liangcai Li,@Nvidia,True
141,Concatenated rows exceeds size_type range after merge operation[BUG],"**Describe the bug**
Hi Guys,  I got an error about `Total number of concatenated rows exceeds size_type range` after  doing an`inner join` on two `dask_cudf` dfs. It seems that some partitions contains a large number of rows. However, the code works good when I `concat` these two dfs.

**Steps/Code to reproduce bug**
In my understanding, the rows in q2 should be larger than q1 .
```
# setup
c = LocalCUDACluster( device_memory_limit=0.8, rmm_managed_memory=True, jit_unspill=True)
c = Client(c)

# 40G data
a = dask_cudf.read_orc('a/*.orc')
# 4G data
b = dask_cudf.read_orc('b/*.orc')

# Works good
q2 = dask_cudf.concat([a,b])
q2.map_partitions(len).compute()

0        307200
1        291840
2        337920
3        261120
4        256000
          ...  
21529    100525
21530     94142
21531     86762
21532     94782
21533     12502
Length: 21534, dtype: int64

# Errors with concatenated rows exceeds size_type range
q1 = a.merge(b, on=['a'],how='inner' )
length_partition = q1.map_partitions(len)
length_partition.compute()

---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-8-efedf1b4ef97> in <module>
      1 length_partition = q1.map_partitions(len)
----> 2 length_partition.compute()

/conda/envs/rapids/lib/python3.7/site-packages/dask/base.py in compute(self, **kwargs)
    282         dask.base.compute
    283         """"""
--> 284         (result,) = compute(self, traverse=False, **kwargs)
    285         return result
    286 

/conda/envs/rapids/lib/python3.7/site-packages/dask/base.py in compute(*args, **kwargs)
    564         postcomputes.append(x.__dask_postcompute__())
    565 
--> 566     results = schedule(dsk, keys, **kwargs)
    567     return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])
    568 

/conda/envs/rapids/lib/python3.7/site-packages/distributed/client.py in get(self, dsk, keys, workers, allow_other_workers, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)
   2664                     should_rejoin = False
   2665             try:
-> 2666                 results = self.gather(packed, asynchronous=asynchronous, direct=direct)
   2667             finally:
   2668                 for f in futures.values():

/conda/envs/rapids/lib/python3.7/site-packages/distributed/client.py in gather(self, futures, errors, direct, asynchronous)
   1979                 direct=direct,
   1980                 local_worker=local_worker,
-> 1981                 asynchronous=asynchronous,
   1982             )
   1983 

/conda/envs/rapids/lib/python3.7/site-packages/distributed/client.py in sync(self, func, asynchronous, callback_timeout, *args, **kwargs)
    842         else:
    843             return sync(
--> 844                 self.loop, func, *args, callback_timeout=callback_timeout, **kwargs
    845             )
    846 

/conda/envs/rapids/lib/python3.7/site-packages/distributed/utils.py in sync(loop, func, callback_timeout, *args, **kwargs)
    351     if error[0]:
    352         typ, exc, tb = error[0]
--> 353         raise exc.with_traceback(tb)
    354     else:
    355         return result[0]

/conda/envs/rapids/lib/python3.7/site-packages/distributed/utils.py in f()
    334             if callback_timeout is not None:
    335                 future = asyncio.wait_for(future, callback_timeout)
--> 336             result[0] = yield future
    337         except Exception as exc:
    338             error[0] = sys.exc_info()

/conda/envs/rapids/lib/python3.7/site-packages/tornado/gen.py in run(self)
    760 
    761                     try:
--> 762                         value = future.result()
    763                     except Exception:
    764                         exc_info = sys.exc_info()

/conda/envs/rapids/lib/python3.7/site-packages/distributed/client.py in _gather(self, futures, errors, direct, local_worker)
   1838                             exc = CancelledError(key)
   1839                         else:
-> 1840                             raise exception.with_traceback(traceback)
   1841                         raise exc
   1842                     if errors == ""skip"":

/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/core.py in _concat()
    101         args[0]
    102         if not args2
--> 103         else methods.concat(args2, uniform=True, ignore_index=ignore_index)
    104     )
    105 

/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/methods.py in concat()
    434             filter_warning=filter_warning,
    435             ignore_index=ignore_index,
--> 436             **kwargs
    437         )
    438 

/conda/envs/rapids/lib/python3.7/site-packages/dask_cuda/proxy_object.py in wrapper()
    708         args = [unproxy(d) for d in args]
    709         kwargs = {k: unproxy(v) for k, v in kwargs.items()}
--> 710         return func(*args, **kwargs)
    711 
    712     return wrapper

/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/methods.py in concat()
    434             filter_warning=filter_warning,
    435             ignore_index=ignore_index,
--> 436             **kwargs
    437         )
    438 

/conda/envs/rapids/lib/python3.7/site-packages/dask_cudf/backends.py in concat_cudf()
    223         )
    224 
--> 225     return cudf.concat(dfs, axis=axis, ignore_index=ignore_index)
    226 
    227 

/conda/envs/rapids/lib/python3.7/site-packages/cudf/core/reshape.py in concat()
    370                 ignore_index=ignore_index,
    371                 # Explicitly cast rather than relying on None being falsy.
--> 372                 sort=bool(sort),
    373             )
    374         return result

/conda/envs/rapids/lib/python3.7/contextlib.py in inner()
     72         def inner(*args, **kwds):
     73             with self._recreate_cm():
---> 74                 return func(*args, **kwds)
     75         return inner
     76 

/conda/envs/rapids/lib/python3.7/site-packages/cudf/core/frame.py in _concat()
    454         # Concatenate the Tables
    455         out = cls._from_table(
--> 456             libcudf.concat.concat_tables(tables, ignore_index=ignore_index)
    457         )
    458 

cudf/_lib/concat.pyx in cudf._lib.concat.concat_tables()

cudf/_lib/concat.pyx in cudf._lib.concat.concat_tables()

RuntimeError: cuDF failure at: ../src/copying/concatenate.cu:364: Total number of concatenated rows exceeds size_type range



```


**Environment overview (please complete the following information)**
 - Environment location: Docker
 - Method of cuDF install: conda
",2021-05-07T15:42:19Z,0,0,Cg Lai,,False
142,[FEA] Return a GPU scalar from `__getitem__` calls,"**Is your feature request related to a problem? Please describe.**
Up until now we've always returned host side numpy scalars when a user pulls an item out of a cuDF series or index using `the_object[idx]`, `loc` or similar. This was originally to match pandas. However now that we have structured types the concern becomes that this incurs a potentially expensive device to host copy if the data in a column with a structured dtype is sufficiently wide, such as in the case of long lists. We should experiment with returning device side values, noting however that we'll probably need to do the same for all our dtypes. 


**Describe the solution you'd like**
`cudf.Series([42])[0] == cudf.Scalar(42)`

**Describe alternatives you've considered**
Currently the approach is to copy everything to host.


**Additional context**
",2021-05-07T18:45:15Z,0,0,,NVIDIA,True
143,[BUG] map_partitions() potentially resulting in unexpected index,"**Describe the bug**
When using map_partitions() on a `dask_cudf` dataframe we can potentially get an unexpected index. This does not affect `.compute()` by default as it reconstructs a global index.

**Steps/Code to reproduce bug**
```
import cudf
import dask_cudf
s = cudf.Series([""ab"", ""cd"", ""ef"", ""gh"", ""ij""])
ds = dask_cudf.from_cudf(s, 2)
print(ds.compute())
print(ds.map_partitions(lambda x: x.str.character_tokenize(), meta=ds._meta).compute())
```
This prints
```
0    ab
1    cd
2    ef
3    gh
4    ij
dtype: object
0    a
1    b
2    c
3    d
4    e
5    f
0    g
1    h
2    i
3    j
dtype: object
```

**Expected behavior**
Would love to discuss what the expected output would be, but below is what I would expect.
```
0    ab
1    cd
2    ef
3    gh
4    ij
dtype: object
0    a
1    b
2    c
3    d
4    e
5    f
6    g
7    h
8    i
9    j
dtype: object
```
**Environment details**
<details><summary>Click here to see environment details</summary><pre>
     
     **git***
     commit 512e485c41f0b9aa2261430ba426fa8c05c98c2a (HEAD -> list-accessor, origin/list-accessor)
     Merge: bf68778b78 5f9dade58a
     Author: Shane Ding <shane200195@gmail.com>
     Date:   Fri May 7 14:33:32 2021 +0000
     
     Merge branch 'branch-0.20' of https://github.com/rapidsai/cudf into list-accessor
     **git submodules***
     
     ***OS Information***
     DGX_NAME=""DGX Server""
     DGX_PRETTY_NAME=""NVIDIA DGX Server""
     DGX_SWBUILD_DATE=""2019-12-02""
     DGX_SWBUILD_VERSION=""4.3.0""
     DGX_COMMIT_ID=""3015363""
     DGX_PLATFORM=""DGX Server for DGX-1""
     DGX_SERIAL_NUMBER=""QTFCOU9270061""
     DISTRIB_ID=Ubuntu
     DISTRIB_RELEASE=18.04
     DISTRIB_CODENAME=bionic
     DISTRIB_DESCRIPTION=""Ubuntu 18.04.3 LTS""
     NAME=""Ubuntu""
     VERSION=""18.04.3 LTS (Bionic Beaver)""
     ID=ubuntu
     ID_LIKE=debian
     PRETTY_NAME=""Ubuntu 18.04.3 LTS""
     VERSION_ID=""18.04""
     HOME_URL=""https://www.ubuntu.com/""
     SUPPORT_URL=""https://help.ubuntu.com/""
     BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
     PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
     VERSION_CODENAME=bionic
     UBUNTU_CODENAME=bionic
     Linux rl-dgx-r11-u30-rapids-dgx103 4.15.0-55-generic #60-Ubuntu SMP Tue Jul 2 18:22:20 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux
     
     ***GPU Information***
     Mon May 10 17:20:28 2021
     +-----------------------------------------------------------------------------+
     | NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |
     |-------------------------------+----------------------+----------------------+
     | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
     | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
     |                               |                      |               MIG M. |
     |===============================+======================+======================|
     |   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |
     | N/A   35C    P0    48W / 163W |    718MiB / 32510MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |
     | N/A   33C    P0    42W / 163W |      3MiB / 32510MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |
     | N/A   34C    P0    44W / 163W |      3MiB / 32510MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |
     | N/A   31C    P0    43W / 163W |      3MiB / 32510MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |
     | N/A   36C    P0    43W / 163W |      3MiB / 32510MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |
     | N/A   35C    P0    43W / 163W |      3MiB / 32510MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |
     | N/A   37C    P0    41W / 163W |      3MiB / 32510MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |
     | N/A   34C    P0    42W / 163W |      3MiB / 32510MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     
     +-----------------------------------------------------------------------------+
     | Processes:                                                                  |
     |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
     |        ID   ID                                                   Usage      |
     |=============================================================================|
     |    0   N/A  N/A     44033      C   .../envs/cudf_dev/bin/python      715MiB |
     +-----------------------------------------------------------------------------+
     
     ***CPU***
     Architecture:        x86_64
     CPU op-mode(s):      32-bit, 64-bit
     Byte Order:          Little Endian
     CPU(s):              80
     On-line CPU(s) list: 0-79
     Thread(s) per core:  2
     Core(s) per socket:  20
     Socket(s):           2
     NUMA node(s):        2
     Vendor ID:           GenuineIntel
     CPU family:          6
     Model:               79
     Model name:          Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz
     Stepping:            1
     CPU MHz:             1967.306
     CPU max MHz:         3600.0000
     CPU min MHz:         1200.0000
     BogoMIPS:            4390.35
     Virtualization:      VT-x
     L1d cache:           32K
     L1i cache:           32K
     L2 cache:            256K
     L3 cache:            51200K
     NUMA node0 CPU(s):   0-19,40-59
     NUMA node1 CPU(s):   20-39,60-79
     Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap intel_pt xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts md_clear flush_l1d
     
     ***CMake***
     /raid/sding/miniconda3/envs/cudf_dev/bin/cmake
     cmake version 3.18.5
     
     CMake suite maintained and supported by Kitware (kitware.com/cmake).
     
     ***g++***
     /usr/bin/g++
     g++ (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
     Copyright (C) 2017 Free Software Foundation, Inc.
     This is free software; see the source for copying conditions.  There is NO
     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
     
     
     ***nvcc***
     /usr/local/cuda/bin/nvcc
     nvcc: NVIDIA (R) Cuda compiler driver
     Copyright (c) 2005-2021 NVIDIA Corporation
     Built on Sun_Feb_14_21:12:58_PST_2021
     Cuda compilation tools, release 11.2, V11.2.152
     Build cuda_11.2.r11.2/compiler.29618528_0
     
     ***Python***
     /raid/sding/miniconda3/envs/cudf_dev/bin/python
     Python 3.8.8
     
     ***Environment Variables***
     PATH                            : /home/u00u97shnnb9gHdDUD357/.vscode-server/bin/3c4e3df9e89829dce27b7b5c24508306b151f30d/bin:/raid/sding/miniconda3/envs/cudf_dev/bin:/raid/sding/miniconda3/condabin:/home/u00u97shnnb9gHdDUD357/.vscode-server/bin/3c4e3df9e89829dce27b7b5c24508306b151f30d/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin:/usr/local/cuda/bin
     LD_LIBRARY_PATH                 :
     NUMBAPRO_NVVM                   :
     NUMBAPRO_LIBDEVICE              :
     CONDA_PREFIX                    : /raid/sding/miniconda3/envs/cudf_dev
     PYTHON_PATH                     :
     
     ***conda packages***
     /raid/sding/miniconda3/condabin/conda
     # packages in environment at /raid/sding/miniconda3/envs/cudf_dev:
     #
     # Name                    Version                   Build  Channel
     _libgcc_mutex             0.1                 conda_forge    conda-forge
     _openmp_mutex             4.5                       1_gnu    conda-forge
     abseil-cpp                20210324.0           h9c3ff4c_0    conda-forge
     alabaster                 0.7.12                     py_0    conda-forge
     apipkg                    1.5                        py_0    conda-forge
     appdirs                   1.4.4              pyh9f0ad1d_0    conda-forge
     argon2-cffi               20.1.0           py38h497a2fe_2    conda-forge
     arrow-cpp                 1.0.1           py38h27527b3_37_cuda    conda-forge
     arrow-cpp-proc            3.0.0                      cuda    conda-forge
     async_generator           1.10                       py_0    conda-forge
     attrs                     20.3.0             pyhd3deb0d_0    conda-forge
     aws-c-cal                 0.5.6                hd8e7a0d_1    conda-forge
     aws-c-common              0.5.10               h7f98852_0    conda-forge
     aws-c-event-stream        0.2.7                he3525c2_3    conda-forge
     aws-c-io                  0.9.11               hd6868ff_1    conda-forge
     aws-checksums             0.1.11               h8a473d3_5    conda-forge
     aws-sdk-cpp               1.8.186              h2f913a8_1    conda-forge
     babel                     2.9.1              pyh44b312d_0    conda-forge
     backcall                  0.2.0              pyh9f0ad1d_0    conda-forge
     backports                 1.0                        py_2    conda-forge
     backports.functools_lru_cache 1.6.4              pyhd8ed1ab_0    conda-forge
     black                     19.10b0                    py_4    conda-forge
     bleach                    3.3.0              pyh44b312d_0    conda-forge
     bokeh                     2.3.1            py38h578d9bd_0    conda-forge
     boost-cpp                 1.76.0               hc6e9bd1_0    conda-forge
     brotli                    1.0.9                h9c3ff4c_4    conda-forge
     brotlipy                  0.7.0           py38h497a2fe_1001    conda-forge
     bzip2                     1.0.8                h7f98852_4    conda-forge
     c-ares                    1.17.1               h7f98852_1    conda-forge
     ca-certificates           2020.12.5            ha878542_0    conda-forge
     cachetools                4.2.2              pyhd8ed1ab_0    conda-forge
     certifi                   2020.12.5        py38h578d9bd_1    conda-forge
     cffi                      1.14.5           py38ha65f79e_0    conda-forge
     cfgv                      3.2.0                      py_0    conda-forge
     chardet                   4.0.0            py38h578d9bd_1    conda-forge
     clang                     8.0.1                hc9558a2_2    conda-forge
     clang-tools               8.0.1                hc9558a2_2    conda-forge
     clangxx                   8.0.1                         2    conda-forge
     click                     7.1.2              pyh9f0ad1d_0    conda-forge
     cloudpickle               1.6.0                      py_0    conda-forge
     cmake                     3.18.5               h1f3970d_0    rapidsai-nightly
     cmake_setuptools          0.1.3                      py_0    rapidsai
     colorama                  0.4.4              pyh9f0ad1d_0    conda-forge
     commonmark                0.9.1                      py_0    conda-forge
     cryptography              3.4.7            py38ha5dfef3_0    conda-forge
     cudatoolkit               11.0.221             h6bb024c_0    nvidia
     cudf                      0.20.0a0+260.gd56428abfc          pypi_0    pypi
     cudnn                     8.0.0                cuda11.0_0    nvidia
     cupy                      8.0.0            py38hb7c6141_0    rapidsai
     cython                    0.29.23          py38h709712a_0    conda-forge
     cytoolz                   0.11.0           py38h497a2fe_3    conda-forge
     dask                      2021.4.1+17.gc3993fd9          pypi_0    pypi
     dask-cudf                 0.20.0a0+280.g611cabd5ba.dirty          pypi_0    pypi
     dataclasses               0.8                pyhc8e2a94_1    conda-forge
     decorator                 5.0.7              pyhd8ed1ab_0    conda-forge
     defusedxml                0.7.1              pyhd8ed1ab_0    conda-forge
     distlib                   0.3.1              pyh9f0ad1d_0    conda-forge
     distributed               2021.4.1+12.g1ee22c84          pypi_0    pypi
     dlpack                    0.3                  he1b5a44_1    conda-forge
     docutils                  0.16             py38h578d9bd_3    conda-forge
     double-conversion         3.1.5                h9c3ff4c_2    conda-forge
     editdistance-s            1.0.0            py38h1fd1430_1    conda-forge
     entrypoints               0.3             pyhd8ed1ab_1003    conda-forge
     execnet                   1.8.0              pyh44b312d_0    conda-forge
     expat                     2.2.10               h9c3ff4c_0    conda-forge
     fastavro                  1.4.0            py38h497a2fe_0    conda-forge
     fastrlock                 0.6              py38h709712a_0    conda-forge
     filelock                  3.0.12             pyh9f0ad1d_0    conda-forge
     flake8                    3.8.3                      py_1    conda-forge
     flatbuffers               1.12.0               h58526e2_0    conda-forge
     freetype                  2.10.4               h0708190_1    conda-forge
     fsspec                    2021.4.0           pyhd8ed1ab_0    conda-forge
     future                    0.18.2           py38h578d9bd_3    conda-forge
     gflags                    2.2.2             he1b5a44_1004    conda-forge
     glog                      0.4.0                h49b9bf7_3    conda-forge
     gmp                       6.2.1                h58526e2_0    conda-forge
     grpc-cpp                  1.37.1               h36de60a_0    conda-forge
     heapdict                  1.0.1                      py_0    conda-forge
     hypothesis                6.10.1             pyhd8ed1ab_0    conda-forge
     icu                       68.1                 h58526e2_0    conda-forge
     identify                  2.2.4              pyhd8ed1ab_0    conda-forge
     idna                      2.10               pyh9f0ad1d_0    conda-forge
     imagesize                 1.2.0                      py_0    conda-forge
     importlib-metadata        4.0.1            py38h578d9bd_0    conda-forge
     iniconfig                 1.1.1              pyh9f0ad1d_0    conda-forge
     ipykernel                 5.5.3            py38hd0cf306_0    conda-forge
     ipython                   7.23.0           py38hd0cf306_0    conda-forge
     ipython_genutils          0.2.0                      py_1    conda-forge
     isort                     5.0.7            py38h32f6830_0    conda-forge
     jedi                      0.18.0           py38h578d9bd_2    conda-forge
     jinja2                    2.11.3             pyh44b312d_0    conda-forge
     joblib                    1.0.1              pyhd8ed1ab_0    conda-forge
     jpeg                      9d                   h36c2ea0_0    conda-forge
     jsonschema                3.2.0              pyhd8ed1ab_3    conda-forge
     jupyter_client            6.1.12             pyhd8ed1ab_0    conda-forge
     jupyter_core              4.7.1            py38h578d9bd_0    conda-forge
     jupyterlab_pygments       0.1.2              pyh9f0ad1d_0    conda-forge
     krb5                      1.17.2               h926e7f8_0    conda-forge
     lcms2                     2.12                 hddcbb42_0    conda-forge
     ld_impl_linux-64          2.35.1               hea4e1c9_2    conda-forge
     libblas                   3.9.0                9_openblas    conda-forge
     libcblas                  3.9.0                9_openblas    conda-forge
     libcurl                   7.76.1               hc4aaa36_1    conda-forge
     libedit                   3.1.20191231         he28a2e2_2    conda-forge
     libev                     4.33                 h516909a_1    conda-forge
     libevent                  2.1.10               hcdb4288_3    conda-forge
     libffi                    3.3                  h58526e2_2    conda-forge
     libgcc-ng                 9.3.0               h2828fa1_19    conda-forge
     libgfortran-ng            9.3.0               hff62375_19    conda-forge
     libgfortran5              9.3.0               hff62375_19    conda-forge
     libgomp                   9.3.0               h2828fa1_19    conda-forge
     liblapack                 3.9.0                9_openblas    conda-forge
     libllvm10                 10.0.1               he513fc3_3    conda-forge
     libllvm8                  8.0.1                hc9558a2_0    conda-forge
     libnghttp2                1.43.0               h812cca2_0    conda-forge
     libopenblas               0.3.15          pthreads_h8fe5266_0    conda-forge
     libpng                    1.6.37               h21135ba_2    conda-forge
     libprotobuf               3.15.8               h780b84a_0    conda-forge
     librmm                    0.20.0a210505   cuda11.0_g9aaa7bc_26    rapidsai-nightly
     libsodium                 1.0.18               h36c2ea0_1    conda-forge
     libssh2                   1.9.0                ha56f1ee_6    conda-forge
     libstdcxx-ng              9.3.0               h6de172a_19    conda-forge
     libthrift                 0.14.1               he6d91bd_1    conda-forge
     libtiff                   4.2.0                hdc55705_1    conda-forge
     libutf8proc               2.6.1                h7f98852_0    conda-forge
     libuv                     1.41.0               h7f98852_0    conda-forge
     libwebp-base              1.2.0                h7f98852_2    conda-forge
     llvmlite                  0.36.0           py38h4630a5e_0    conda-forge
     locket                    0.2.0                      py_2    conda-forge
     lz4-c                     1.9.3                h9c3ff4c_0    conda-forge
     markdown                  3.3.4              pyhd8ed1ab_0    conda-forge
     markupsafe                1.1.1            py38h497a2fe_3    conda-forge
     matplotlib-inline         0.1.2              pyhd8ed1ab_2    conda-forge
     mccabe                    0.6.1                      py_1    conda-forge
     mimesis                   4.0.0              pyh9f0ad1d_0    conda-forge
     mistune                   0.8.4           py38h497a2fe_1003    conda-forge
     more-itertools            8.7.0              pyhd8ed1ab_1    conda-forge
     msgpack-python            1.0.2            py38h1fd1430_1    conda-forge
     mypy                      0.782                      py_0    conda-forge
     mypy_extensions           0.4.3            py38h578d9bd_3    conda-forge
     nbclient                  0.5.3              pyhd8ed1ab_0    conda-forge
     nbconvert                 6.0.7            py38h578d9bd_3    conda-forge
     nbformat                  5.1.3              pyhd8ed1ab_0    conda-forge
     nbsphinx                  0.8.4              pyhd8ed1ab_0    conda-forge
     nccl                      2.7.8.1            h4962215_100    nvidia
     ncurses                   6.2                  h58526e2_4    conda-forge
     nest-asyncio              1.5.1              pyhd8ed1ab_0    conda-forge
     nodeenv                   1.6.0              pyhd8ed1ab_0    conda-forge
     notebook                  6.3.0              pyha770c72_1    conda-forge
     numba                     0.53.1           py38h0e12cce_0    conda-forge
     numpy                     1.20.2           py38h9894fe3_0    conda-forge
     numpydoc                  1.1.0                      py_1    conda-forge
     nvtx                      0.2.3            py38h497a2fe_0    conda-forge
     olefile                   0.46               pyh9f0ad1d_1    conda-forge
     openjpeg                  2.4.0                hf7af979_0    conda-forge
     openssl                   1.1.1k               h7f98852_0    conda-forge
     orc                       1.6.7                heec2584_1    conda-forge
     packaging                 20.9               pyh44b312d_0    conda-forge
     pandas                    1.2.4            py38h1abd341_0    conda-forge
     pandoc                    1.19.2                        0    conda-forge
     pandocfilters             1.4.2                      py_1    conda-forge
     parquet-cpp               1.5.1                         2    conda-forge
     parso                     0.8.2              pyhd8ed1ab_0    conda-forge
     partd                     1.2.0              pyhd8ed1ab_0    conda-forge
     pathspec                  0.8.1              pyhd3deb0d_0    conda-forge
     pexpect                   4.8.0              pyh9f0ad1d_2    conda-forge
     pickleshare               0.7.5                   py_1003    conda-forge
     pillow                    8.1.2            py38ha0e1e83_1    conda-forge
     pip                       21.1.1             pyhd8ed1ab_0    conda-forge
     pluggy                    0.13.1           py38h578d9bd_4    conda-forge
     pre-commit                2.12.1           py38h578d9bd_0    conda-forge
     pre_commit                2.12.1               hd8ed1ab_0    conda-forge
     prometheus_client         0.10.1             pyhd8ed1ab_0    conda-forge
     prompt-toolkit            3.0.18             pyha770c72_0    conda-forge
     protobuf                  3.15.8           py38h709712a_0    conda-forge
     psutil                    5.8.0            py38h497a2fe_1    conda-forge
     ptyprocess                0.7.0              pyhd3deb0d_0    conda-forge
     py                        1.10.0             pyhd3deb0d_0    conda-forge
     py-cpuinfo                8.0.0              pyhd8ed1ab_0    conda-forge
     pyarrow                   1.0.1           py38hb53058b_37_cuda    conda-forge
     pycodestyle               2.6.0              pyh9f0ad1d_0    conda-forge
     pycparser                 2.20               pyh9f0ad1d_2    conda-forge
     pyflakes                  2.2.0              pyh9f0ad1d_0    conda-forge
     pygments                  2.8.1              pyhd8ed1ab_0    conda-forge
     pyopenssl                 20.0.1             pyhd8ed1ab_0    conda-forge
     pyorc                     0.4.0                    pypi_0    pypi
     pyparsing                 2.4.7              pyh9f0ad1d_0    conda-forge
     pyrsistent                0.17.3           py38h497a2fe_2    conda-forge
     pysocks                   1.7.1            py38h578d9bd_3    conda-forge
     pytest                    6.2.4            py38h578d9bd_0    conda-forge
     pytest-benchmark          3.4.1              pyhd8ed1ab_0    conda-forge
     pytest-forked             1.3.0              pyhd3deb0d_0    conda-forge
     pytest-xdist              2.2.1              pyhd8ed1ab_0    conda-forge
     python                    3.8.8           hffdb5ce_0_cpython    conda-forge
     python-dateutil           2.8.1                      py_0    conda-forge
     python_abi                3.8                      1_cp38    conda-forge
     pytz                      2021.1             pyhd8ed1ab_0    conda-forge
     pyyaml                    5.4.1            py38h497a2fe_0    conda-forge
     pyzmq                     22.0.3           py38h2035c66_1    conda-forge
     rapidjson                 1.1.0             he1b5a44_1002    conda-forge
     re2                       2021.04.01           h9c3ff4c_0    conda-forge
     readline                  8.1                  h46c0cb4_0    conda-forge
     recommonmark              0.7.1              pyhd8ed1ab_0    conda-forge
     regex                     2021.4.4         py38h497a2fe_0    conda-forge
     requests                  2.25.1             pyhd3deb0d_0    conda-forge
     rhash                     1.4.1                h7f98852_0    conda-forge
     rmm                       0.20.0a210505   cuda_11.0_py38_g9aaa7bc_26    rapidsai-nightly
     s2n                       1.0.5                h9b69904_0    conda-forge
     sacremoses                0.0.43             pyh9f0ad1d_0    conda-forge
     send2trash                1.5.0                      py_0    conda-forge
     setuptools                49.6.0           py38h578d9bd_3    conda-forge
     six                       1.15.0             pyh9f0ad1d_0    conda-forge
     snappy                    1.1.8                he1b5a44_3    conda-forge
     snowballstemmer           2.1.0              pyhd8ed1ab_0    conda-forge
     sortedcontainers          2.3.0              pyhd8ed1ab_0    conda-forge
     spdlog                    1.7.0                hc9558a2_2    conda-forge
     sphinx                    3.5.4              pyh44b312d_0    conda-forge
     sphinx-copybutton         0.3.1              pyhd8ed1ab_0    conda-forge
     sphinx-markdown-tables    0.0.15             pyhd3deb0d_0    conda-forge
     sphinx_rtd_theme          0.5.2              pyhd8ed1ab_1    conda-forge
     sphinxcontrib-applehelp   1.0.2                      py_0    conda-forge
     sphinxcontrib-devhelp     1.0.2                      py_0    conda-forge
     sphinxcontrib-htmlhelp    1.0.3                      py_0    conda-forge
     sphinxcontrib-jsmath      1.0.1                      py_0    conda-forge
     sphinxcontrib-qthelp      1.0.3                      py_0    conda-forge
     sphinxcontrib-serializinghtml 1.1.4                      py_0    conda-forge
     sphinxcontrib-websupport  1.2.4              pyh9f0ad1d_0    conda-forge
     sqlite                    3.35.5               h74cdb3f_0    conda-forge
     streamz                   0.6.2              pyh44b312d_0    conda-forge
     tblib                     1.7.0              pyhd8ed1ab_0    conda-forge
     terminado                 0.9.4            py38h578d9bd_0    conda-forge
     testpath                  0.4.4                      py_0    conda-forge
     tk                        8.6.10               h21135ba_1    conda-forge
     tokenizers                0.10.1           py38hb63a372_0    conda-forge
     toml                      0.10.2             pyhd8ed1ab_0    conda-forge
     toolz                     0.11.1                     py_0    conda-forge
     tornado                   6.1              py38h497a2fe_1    conda-forge
     tqdm                      4.60.0             pyhd8ed1ab_0    conda-forge
     traitlets                 5.0.5                      py_0    conda-forge
     transformers              4.5.1              pyhd8ed1ab_1    conda-forge
     typed-ast                 1.4.3            py38h497a2fe_0    conda-forge
     typing_extensions         3.7.4.3                    py_0    conda-forge
     unknown                   0.0.0                     dev_0    <develop>
     urllib3                   1.26.4             pyhd8ed1ab_0    conda-forge
     virtualenv                20.4.4           py38h578d9bd_0    conda-forge
     wcwidth                   0.2.5              pyh9f0ad1d_2    conda-forge
     webencodings              0.5.1                      py_1    conda-forge
     wheel                     0.36.2             pyhd3deb0d_0    conda-forge
     xz                        5.2.5                h516909a_1    conda-forge
     yaml                      0.2.5                h516909a_0    conda-forge
     zeromq                    4.3.4                h9c3ff4c_0    conda-forge
     zict                      2.0.0                      py_0    conda-forge
     zipp                      3.4.1              pyhd8ed1ab_0    conda-forge
     zlib                      1.2.11            h516909a_1010    conda-forge
     zstd                      1.4.9                ha95c52a_0    conda-forge
",2021-05-10T17:27:36Z,0,0,,,False
144,[FEA] Leak tracking for Java Scalar instances,"**Is your feature request related to a problem? Please describe.**
The Java bindings require explicit `close()` calls by the caller to prevent GPU OOM errors if garbage collection doesn't kick in often enough to free unreferenced Java objects wrapping GPU resources in a timely manner.  We currently have leaked object tracking via weak reference collection events for most GPU resources, but that does not currently cover `Scalar` instances which wrap `cudf::scalar`.

**Describe the solution you'd like**
The same leak tracking and debugging done for `ColumnVector` should be applied to `Scalar`.  Although `Scalar` instances are not very large in practice (excluding the corner cases of nested type scalars), they can fragment the GPU memory pool drastically if there are enough of these tiny allocations leaked, causing GPU OOM errors much earlier than one would expect.
",2021-05-12T18:29:35Z,0,0,Jason Lowe,NVIDIA,True
145,Remove `volatile` usage in `rolling_detail.cuh` where not required,"In the device code in `rolling_detail.cuh`, there are variables declared as `volatile` to work around compiler optimizations in CUDA 10.x. E.g.:
```c++
    // declare this as volatile to avoid some compiler optimizations that lead to incorrect results
    // for CUDA 10.0 and below (fixed in CUDA 10.1)
    volatile cudf::size_type count = 0;
```
Given that CUDF 0.20 supports CUDA 11.0+, it should now be safe to remove this comment, and the `volatile` attributes.

If not handled in #8158, this can be addressed after #8158 is merged.",2021-05-12T22:20:57Z,0,0,MithunR,NVIDIA,True
146,[BUG] Implement custom dtype handling utilities,"**Describe the bug**
We currently make use of certain numpy and pandas utilities for dtype checking in various parts of our code base. These utilities are not reliable for us because they do not necessarily work on our extension dtypes, but currently these issues are being handled on a case-by-case basis, leaving us open to various bugs. We should instead write our own utilities that handle our own dtypes in addition to the common dtypes from other projects.

**Steps/Code to reproduce bug**
Here's one example for `np.issubdtype`.
```
>>> np.issubdtype(cudf.Decimal64Dtype(10, 2), np.int32)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/nfs/vyasr/local/rapids/compose/etc/conda/cuda_11.2.0/envs/rapids/lib/python3.7/site-packages/numpy/core/numerictypes.py"", line 419, in issubdtype
    arg1 = dtype(arg1).type
TypeError: Cannot interpret 'Decimal64Dtype(precision=10, scale=2)' as a data type
```

**Expected behavior**
Returns True or False

**Additional context**
Here's a not necessarily exhaustive list of functions that may need to be replaced (some have equivalents that simply aren't used everywhere):
- [ ] `np.issubdtype`
- [ ] `np.can_cast`
- [ ] Anything from `pd.api.types`
- [ ] `np.find_common_type`",2021-05-14T16:52:41Z,1,0,Vyas Ramasubramani,@rapidsai,True
147,[FEA] Add support for limit to Series.fillna,"**Is your feature request related to a problem? Please describe.**
It is not currently possible to limit the number of missing values filled consecutively using `fillna`.

**Describe the solution you'd like**
Support for a limit argument similar to how pandas does it.

**Describe alternatives you've considered**
Writing a clunkier version of fillna that keeps track of the number of missing values filled.

**Additional context**
Example:
```python
import cudf
sr = cudf.Series([1, 2, None, None, 5])
sr.fillna(method='pad', limit=1)
```
Should return:
```
0    1
1    2
2    2
3    <NA>
4    5
dtype: int64
```
But throws a NotImplementedError instead",2021-05-20T21:55:04Z,0,0,,,False
148,[FEA] Add decimal support to Dask cuDF parquet reader,"**Is your feature request related to a problem? Please describe.**
I wish I could use the Dask cuDF parquet reader to read parquet files with decimal columns.

**Describe the solution you'd like**
Currently
```
import cudf
import dask_cudf
from cudf.core.dtypes import Decimal64Dtype
from decimal import Decimal

df = cudf.DataFrame({""val"":[Decimal(""3.5""), Decimal(""4.3"")]}, dtype=Decimal64Dtype(5,2))
df.to_parquet(""test.parquet"")

dask_df = dask_cudf.read_parquet(""test.parquet"")
dask_df.dtypes
```
returns
```
val    object
dtype: object
```
Whereas using cudf
```
df = cudf.read_parquet(""test.parquet"")
df.dtypes
```
returns
```
val    decimal
dtype: object
```

",2021-05-21T04:54:30Z,0,0,,,False
149,Identify appropriate return type for empty aggregation results with UDF aggregations,"In #8274 the output of aggregations returning empty data sets was changed so that the output type of nested columns would match the expected type even when it is empty. However, this led to a separate edge case, which is determining what data type to return when performing a UDF aggregation that returns an empty column. A numba-compiled Python UDF provides enough information to infer the output type, but we need to assess the extent to which UDFs returning nested are/should be supported in cudf Python to determine whether/how to pass this information down to `libcudf` to ensure that the appropriate types are returned. As of #8274 such aggregations simply mimic the input data type for the returned column.",2021-05-27T21:26:57Z,0,0,Vyas Ramasubramani,@rapidsai,True
150,[FEA] Better handling for nullable columns that do not have any null element,"Currently, many cudf's APIs (such as `copy_bitmask` or `valid_if` and many others) always generate an output bitmask if the input columns are nullable. However, the input columns may not have any null element. Thus, it may be a waste of time and memory to generate such a bitmask containing all valid bit. So, a good practice is to avoid generating a bitmask if the input does not contain any null element. Some APIs have already followed this practice, but not all.

However, if not generating a bitmask for the output, there may be potential issues emerging from the inconsistency between the input and output. Consider an example that an algorithm processing data through several stages. One of the intermediate stages cuts out the bitmask from its output, but all the subsequent stages continue to use the nullable information from the first input. In such cases, the system may crash, or some stages throw an exception, or the final result is undefined. 

So, the question here is, should we keep the consistency between the input and output column? (i.e., if the input column has a bitmask, should we always generate a bitmask for the output column even if it doesn't have any null?)

---------
I have observed a lot of bugs that emerged due to mixing `nullable()` and `null_count()` in the current cudf implementation. Thus, I think a good way to avoid (maybe) all potential issues is to totally remove the `nullable()` API and use `null_count()` all the time. Maybe this is a little bit more expensive for the users to execute the code, but it could definitely eliminate all the headaches from having unexpected bugs and make developers/users much happier.",2021-06-07T21:45:24Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
151,[FEA] add value methods to fixed_point_scalar_device_view ,"**Is your feature request related to a problem? Please describe.**

libcudf `fixed_point_scalar_device_view` has `set_value(rep_type)`.
It doesn't have `T value()` and `set_value(T)`.
These are needed for usage of `fixed_point_scalar_device_view` in device code.

**Describe the solution you'd like**
Add  `T value()` and `set_value(T)` methods.

**Additional context**
This issue was found while developing scalar support for compiled binary ops #8192.",2021-06-08T05:23:48Z,0,0,Karthikeyan,NVIDIA,True
152,[FEA] Support groupby collect agg on struct columns,"**Is your feature request related to a problem? Please describe.**
Ability to create a list of struct columns using the `collect` agg via groupby

**Describe the solution you'd like**
```python
df = cudf.DataFrame(
    {
        'a':['aa','aa','cc'],
        'd':[{""b"": '1', ""c"": ""one""}, {""b"": '2', ""c"": ""two""}, {""b"": '3', ""c"": ""one""}]
     }
)

df.groupby('a').collect()

	d
a	
aa	[{'b': '1', 'c': 'one'}, {'b': '2', 'c': 'two'}]
cc	[{'b': '3', 'c': 'one'}]
```

**Describe alternatives you've considered**
N/A 

**Additional context**
Add any other context, code examples, or references to existing implementations about the feature request here.
",2021-06-15T16:41:12Z,0,0,Ayush Dattagupta,Nvidia,True
153,[FEA] Use libcudf Dictionary type for CategoricalColumn in Python,cuDF Python would like to back the CategoricalColumn with the Dictionary type. Work has been initiated toward this goal in https://github.com/rapidsai/cudf/pull/8567,2021-06-21T16:21:54Z,0,0,Nick Becker,@NVIDIA,True
154,[BUG] MultiIndex loc expects an iterable when passed Timestamp,"**Describe the bug**
cuDF DataFrames indexed by a Timestamp range can be accessed using `.loc[]` without any problem. However, if the cuDF DataFrame is indexed with a MultiIndex with timestamps as the first key, `.loc[]` fails, when doing so causes no issue with pandas.

**Steps/Code to reproduce bug**
The [following gist](https://gist.github.com/pbruneau/689242cf5c79ce9185aa7fa3bb1f2e89) holds a self-contained example. The last line of the code fails with error: `TypeError: 'Timestamp' object is not iterable`

**Expected behavior**
I would expect the pandas and cuDF snippets to behave similarly. 

**Environment overview (please complete the following information)**
 - Environment location: Docker
 - Method of cuDF install: Docker
   - docker pull rapidsai/rapidsai:0.18-cuda10.1-runtime-ubuntu18.04-py3.7
   - docker run -d -p 10000:8888 -p 10001:8787 -p 10002:8786 --privileged=true --gpus all --name test -t test

**Environment details**
<details><summary>Click here to see environment details</summary><pre>
     
     **git***
     commit 2cda39b34197c60614186ec51106d8254e5f7b05 (grafted, HEAD, origin/branch-0.16)
     Author: Ray Douglass <3107146+raydouglass@users.noreply.github.com>
     Date:   Wed Oct 21 10:31:49 2020 -0400
     
     Update CHANGELOG.md
     **git submodules***
     
     ***OS Information***
     DISTRIB_ID=Ubuntu
     DISTRIB_RELEASE=18.04
     DISTRIB_CODENAME=bionic
     DISTRIB_DESCRIPTION=""Ubuntu 18.04.5 LTS""
     NAME=""Ubuntu""
     VERSION=""18.04.5 LTS (Bionic Beaver)""
     ID=ubuntu
     ID_LIKE=debian
     PRETTY_NAME=""Ubuntu 18.04.5 LTS""
     VERSION_ID=""18.04""
     HOME_URL=""https://www.ubuntu.com/""
     SUPPORT_URL=""https://help.ubuntu.com/""
     BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
     PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
     VERSION_CODENAME=bionic
     UBUNTU_CODENAME=bionic
     Linux fe1b5c84b917 4.15.0-143-generic #147-Ubuntu SMP Wed Apr 14 16:10:11 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux
     
     ***GPU Information***
     Tue Jun 22 15:01:51 2021
     +-----------------------------------------------------------------------------+
     | NVIDIA-SMI 455.23.05    Driver Version: 455.23.05    CUDA Version: 11.1     |
     |-------------------------------+----------------------+----------------------+
     | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
     | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
     |                               |                      |               MIG M. |
     |===============================+======================+======================|
     |   0  GeForce GTX 1080    On   | 00000000:05:00.0 Off |                  N/A |
     | 28%   43C    P8     7W / 180W |   1504MiB /  8114MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     
     +-----------------------------------------------------------------------------+
     | Processes:                                                                  |
     |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
     |        ID   ID                                                   Usage      |
     |=============================================================================|
     +-----------------------------------------------------------------------------+
     
     ***CPU***
     Architecture:        x86_64
     CPU op-mode(s):      32-bit, 64-bit
     Byte Order:          Little Endian
     CPU(s):              12
     On-line CPU(s) list: 0-11
     Thread(s) per core:  2
     Core(s) per socket:  6
     Socket(s):           1
     NUMA node(s):        1
     Vendor ID:           GenuineIntel
     CPU family:          6
     Model:               79
     Model name:          Intel(R) Core(TM) i7-6800K CPU @ 3.40GHz
     Stepping:            1
     CPU MHz:             1200.861
     CPU max MHz:         3800.0000
     CPU min MHz:         1200.0000
     BogoMIPS:            6800.53
     Virtualization:      VT-x
     L1d cache:           32K
     L1i cache:           32K
     L2 cache:            256K
     L3 cache:            15360K
     NUMA node0 CPU(s):   0-11
     Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap intel_pt xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts md_clear flush_l1d
     
     ***CMake***
     
     ***g++***
     
     ***nvcc***
     
     ***Python***
     /opt/conda/envs/rapids/bin/python
     Python 3.7.10
     
     ***Environment Variables***
     PATH                            : /opt/conda/envs/rapids/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
     LD_LIBRARY_PATH                 : /usr/local/nvidia/lib:/usr/local/nvidia/lib64
     NUMBAPRO_NVVM                   :
     NUMBAPRO_LIBDEVICE              :
     CONDA_PREFIX                    : /opt/conda/envs/rapids
     PYTHON_PATH                     :
     
     ***conda packages***
     /opt/conda/condabin/conda
     # packages in environment at /opt/conda/envs/rapids:
     #
     # Name                    Version                   Build  Channel
     _libgcc_mutex             0.1                 conda_forge    conda-forge
     _openmp_mutex             4.5                       1_gnu    conda-forge
     abseil-cpp                20200225.2           he1b5a44_2    conda-forge
     aiobotocore               1.2.1              pyhd8ed1ab_0    conda-forge
     aiohttp                   3.7.4            py37h5e8e339_0    conda-forge
     aioitertools              0.7.1              pyhd8ed1ab_0    conda-forge
     appdirs                   1.4.4              pyh9f0ad1d_0    conda-forge
     argon2-cffi               20.1.0           py37h5e8e339_2    conda-forge
     arrow-cpp                 1.0.1           py37h2318771_14_cuda    conda-forge
     arrow-cpp-proc            3.0.0                      cuda    conda-forge
     async-timeout             3.0.1                   py_1000    conda-forge
     async_generator           1.10                       py_0    conda-forge
     attrs                     20.3.0             pyhd3deb0d_0    conda-forge
     aws-c-common              0.4.59               h36c2ea0_1    conda-forge
     aws-c-event-stream        0.1.6                had2084c_6    conda-forge
     aws-checksums             0.1.10               h4e93380_0    conda-forge
     aws-sdk-cpp               1.8.63               h9b98462_0    conda-forge
     backcall                  0.2.0              pyh9f0ad1d_0    conda-forge
     backports                 1.0                        py_2    conda-forge
     backports.functools_lru_cache 1.6.1                      py_0    conda-forge
     blas                      2.14                   openblas    conda-forge
     blazingsql                0.18.0                   pypi_0    pypi
     bleach                    3.3.0              pyh44b312d_0    conda-forge
     bokeh                     2.2.3            py37h89c1867_0    conda-forge
     boost                     1.72.0           py37h48f8a5e_1    conda-forge
     boost-cpp                 1.72.0               h9d3c048_4    conda-forge
     botocore                  1.19.52            pyhd8ed1ab_0    conda-forge
     brotli                    1.0.9                h9c3ff4c_4    conda-forge
     brotlipy                  0.7.0           py37h5e8e339_1001    conda-forge
     bzip2                     1.0.8                h7f98852_4    conda-forge
     c-ares                    1.17.1               h36c2ea0_0    conda-forge
     ca-certificates           2020.12.5            ha878542_0    conda-forge
     cairo                     1.16.0            h6cf1ce9_1008    conda-forge
     certifi                   2020.12.5        py37h89c1867_1    conda-forge
     cffi                      1.14.5           py37hc58025e_0    conda-forge
     cfitsio                   3.470                h2e3daa1_7    conda-forge
     cftime                    1.5.0                    pypi_0    pypi
     chardet                   4.0.0            py37h89c1867_1    conda-forge
     click                     7.1.2              pyh9f0ad1d_0    conda-forge
     click-plugins             1.1.1                      py_0    conda-forge
     cligj                     0.7.1              pyhd8ed1ab_0    conda-forge
     cloudpickle               1.6.0                      py_0    conda-forge
     colorcet                  2.0.6              pyhd8ed1ab_0    conda-forge
     convertdate               2.3.2                    pypi_0    pypi
     cryptography              3.4.4            py37hf1a17b8_0    conda-forge
     cudatoolkit               10.1.243             h036e899_8    nvidia
     cudf                      0.18.0          cuda_10.1_py37_g20778e5ddb_0    rapidsai
     cudf_kafka                0.18.0          py37_g20778e5ddb_0    rapidsai
     cudnn                     7.6.0                cuda10.1_0    nvidia
     cugraph                   0.18.0          py37_g65ec965f_0    rapidsai
     cuml                      0.18.0          cuda10.1_py37_gb5f59e005_0    rapidsai
     cupy                      8.0.0            py37h0632833_0    conda-forge
     curl                      7.71.1               he644dc0_8    conda-forge
     cusignal                  0.18.0          py38_g42899d2_0    rapidsai
     cuspatial                 0.18.0a210212   py37_g3045c48_21    rapidsai-nightly
     custreamz                 0.18.0          py37_g20778e5ddb_0    rapidsai
     cuxfilter                 0.18.0          py37_gac6f488_0    rapidsai
     cycler                    0.10.0                     py_2    conda-forge
     cyrus-sasl                2.1.27               h3274739_1    conda-forge
     cython                    0.29.22          py37hcd2ae1e_0    conda-forge
     cytoolz                   0.11.0           py37h5e8e339_3    conda-forge
     dask                      2021.2.0           pyhd8ed1ab_0    conda-forge
     dask-core                 2021.2.0           pyhd8ed1ab_0    conda-forge
     dask-cuda                 0.18.0                   py37_0    rapidsai
     dask-cudf                 0.18.0          py37_g20778e5ddb_0    rapidsai
     dask-glm                  0.2.0                      py_1    conda-forge
     dask-labextension         4.0.1              pyhd8ed1ab_0    conda-forge
     dask-ml                   1.8.0              pyhd8ed1ab_0    conda-forge
     datashader                0.11.1             pyh9f0ad1d_0    conda-forge
     datashape                 0.5.4                      py_1    conda-forge
     decorator                 4.4.2                      py_0    conda-forge
     defusedxml                0.6.0                      py_0    conda-forge
     distlib                   0.3.2                    pypi_0    pypi
     distributed               2021.2.0         py37h89c1867_0    conda-forge
     dlpack                    0.3                  he1b5a44_1    conda-forge
     ecmwf-api-client          1.6.1                    pypi_0    pypi
     entrypoints               0.3             pyhd8ed1ab_1003    conda-forge
     ephem                     4.0.0.2                  pypi_0    pypi
     expat                     2.2.10               h9c3ff4c_0    conda-forge
     fa2                       0.3.5            py37h8f50634_0    conda-forge
     faiss-proc                1.0.0                      cuda    conda-forge
     fastavro                  1.3.4            py37h5e8e339_0    conda-forge
     fastrlock                 0.5              py37hcd2ae1e_2    conda-forge
     filelock                  3.0.12                   pypi_0    pypi
     filterpy                  1.4.5                      py_1    conda-forge
     fiona                     1.8.18           py37h527b4ca_0    conda-forge
     flask                     2.0.1                    pypi_0    pypi
     flask-wtf                 0.15.1                   pypi_0    pypi
     fontconfig                2.13.1            hba837de_1004    conda-forge
     freetype                  2.10.4               h0708190_1    conda-forge
     freexl                    1.0.6                h7f98852_0    conda-forge
     fsspec                    0.8.7              pyhd8ed1ab_0    conda-forge
     future                    0.18.2           py37h89c1867_3    conda-forge
     gdal                      3.1.4            py37h2ec2946_2    conda-forge
     geopandas                 0.8.1                      py_0    conda-forge
     geos                      3.8.1                he1b5a44_0    conda-forge
     geotiff                   1.6.0                h5d11630_3    conda-forge
     gettext                   0.19.8.1          h0b5b191_1005    conda-forge
     gflags                    2.2.2             he1b5a44_1004    conda-forge
     giflib                    5.2.1                h36c2ea0_2    conda-forge
     git                       2.30.1          pl5320h6697202_1    conda-forge
     glog                      0.4.0                h49b9bf7_3    conda-forge
     gluonts                   0.7.6                    pypi_0    pypi
     google-cloud-cpp          1.16.0               he4a878c_2    conda-forge
     google-cloud-cpp-common   0.25.0               he83eced_7    conda-forge
     googleapis-cpp            0.10.0               h6b1abdc_4    conda-forge
     gpuci-tools               0.3.1                         0    gpuci
     greenlet                  1.0.0            py37hcd2ae1e_0    conda-forge
     grpc-cpp                  1.32.0               h7997a97_1    conda-forge
     gunicorn                  20.1.0                   pypi_0    pypi
     hdf4                      4.2.13            h10796ff_1004    conda-forge
     hdf5                      1.10.6          nompi_h7c3c948_1111    conda-forge
     heapdict                  1.0.1                      py_0    conda-forge
     hijri-converter           2.1.2                    pypi_0    pypi
     holidays                  0.11.1                   pypi_0    pypi
     holoviews                 1.14.2             pyhd8ed1ab_0    conda-forge
     icu                       68.1                 h58526e2_0    conda-forge
     idna                      2.10               pyh9f0ad1d_0    conda-forge
     importlib-metadata        3.7.0            py37h89c1867_0    conda-forge
     importlib_metadata        3.7.0                hd8ed1ab_0    conda-forge
     iniconfig                 1.1.1              pyh9f0ad1d_0    conda-forge
     ipykernel                 5.5.0            py37h888b3d9_1    conda-forge
     ipython                   7.15.0           py37hc8dfbb8_0    conda-forge
     ipython_genutils          0.2.0                      py_1    conda-forge
     ipywidgets                7.6.3              pyhd3deb0d_0    conda-forge
     itsdangerous              2.0.1                    pypi_0    pypi
     jedi                      0.17.2           py37h89c1867_1    conda-forge
     jinja2                    3.0.1                    pypi_0    pypi
     jmespath                  0.10.0             pyh9f0ad1d_0    conda-forge
     joblib                    1.0.1              pyhd8ed1ab_0    conda-forge
     jpeg                      9d                   h36c2ea0_0    conda-forge
     jpype1                    1.2.1            py37h2527ec5_0    conda-forge
     json-c                    0.13.1            hbfbb72e_1002    conda-forge
     json5                     0.9.5              pyh9f0ad1d_0    conda-forge
     jsonschema                3.2.0                      py_2    conda-forge
     jupyter-server-proxy      1.6.0              pyhd8ed1ab_0    conda-forge
     jupyter_client            6.1.11             pyhd8ed1ab_1    conda-forge
     jupyter_core              4.7.1            py37h89c1867_0    conda-forge
     jupyterlab                2.1.5                      py_0    conda-forge
     jupyterlab-nvdashboard    0.1.11200212              py_12    rapidsai-nightly
     jupyterlab_pygments       0.1.2              pyh9f0ad1d_0    conda-forge
     jupyterlab_server         1.2.0                      py_0    conda-forge
     jupyterlab_widgets        1.0.0              pyhd8ed1ab_1    conda-forge
     kealib                    1.4.14               hcc255d8_2    conda-forge
     kiwisolver                1.3.1            py37h2527ec5_1    conda-forge
     korean-lunar-calendar     0.2.1                    pypi_0    pypi
     krb5                      1.17.2               h926e7f8_0    conda-forge
     lcms2                     2.12                 hddcbb42_0    conda-forge
     ld_impl_linux-64          2.35.1               hea4e1c9_2    conda-forge
     libblas                   3.8.0               14_openblas    conda-forge
     libcblas                  3.8.0               14_openblas    conda-forge
     libcrc32c                 1.1.1                h9c3ff4c_2    conda-forge
     libcudf                   0.18.1          cuda10.1_g999be56c80_0    rapidsai
     libcudf_kafka             0.18.0a210226   g1544474166_254    rapidsai-nightly
     libcugraph                0.18.0          cuda10.1_g65ec965f_0    rapidsai
     libcuml                   0.18.0          cuda10.1_gb5f59e005_0    rapidsai
     libcumlprims              0.18.0a210211   cuda10.1_gff080f3_0    rapidsai-nightly
     libcurl                   7.71.1               hcdd3856_8    conda-forge
     libcuspatial              0.18.0          cuda10.1_gf4da460_0    rapidsai
     libdap4                   3.20.6               hd7c4107_1    conda-forge
     libedit                   3.1.20191231         he28a2e2_2    conda-forge
     libev                     4.33                 h516909a_1    conda-forge
     libevent                  2.1.10               hcdb4288_3    conda-forge
     libfaiss                  1.6.3           he68dc02_3_cuda    conda-forge
     libffi                    3.3                  h58526e2_2    conda-forge
     libgcc-ng                 9.3.0               h2828fa1_18    conda-forge
     libgcrypt                 1.9.2                h7f98852_0    conda-forge
     libgdal                   3.1.4                h02eeb80_2    conda-forge
     libgfortran-ng            7.5.0               h14aa051_18    conda-forge
     libgfortran4              7.5.0               h14aa051_18    conda-forge
     libglib                   2.68.0               h3e27bee_2    conda-forge
     libgomp                   9.3.0               h2828fa1_18    conda-forge
     libgpg-error              1.42                 h9c3ff4c_0    conda-forge
     libgsasl                  1.8.0                         2    conda-forge
     libhwloc                  2.3.0                h5e5b7d1_1    conda-forge
     libiconv                  1.16                 h516909a_0    conda-forge
     libkml                    1.3.0             hd79254b_1012    conda-forge
     liblapack                 3.8.0               14_openblas    conda-forge
     liblapacke                3.8.0               14_openblas    conda-forge
     libllvm10                 10.0.1               he513fc3_3    conda-forge
     libnetcdf                 4.7.4           nompi_h56d31a8_107    conda-forge
     libnghttp2                1.43.0               h812cca2_0    conda-forge
     libntlm                   1.4               h7f98852_1002    conda-forge
     libopenblas               0.3.7                h5ec1e0e_6    conda-forge
     libpng                    1.6.37               h21135ba_2    conda-forge
     libpq                     12.3                 h255efa7_3    conda-forge
     libprotobuf               3.13.0.1             h8b12597_0    conda-forge
     librdkafka                1.5.3                h54cafa9_0    conda-forge
     librmm                    0.18.0          cuda10.1_ga4ee6b7_0    rapidsai
     librttopo                 1.1.0                hb271727_4    conda-forge
     libsodium                 1.0.18               h36c2ea0_1    conda-forge
     libspatialindex           1.9.3                h9c3ff4c_3    conda-forge
     libspatialite             5.0.1                h6ec7341_0    conda-forge
     libssh2                   1.9.0                hab1572f_5    conda-forge
     libstdcxx-ng              9.3.0               h6de172a_18    conda-forge
     libthrift                 0.13.0               h5aa387f_6    conda-forge
     libtiff                   4.2.0                hdc55705_0    conda-forge
     libutf8proc               2.6.1                h7f98852_0    conda-forge
     libuuid                   2.32.1            h7f98852_1000    conda-forge
     libuv                     1.41.0               h7f98852_0    conda-forge
     libwebp                   1.2.0                h3452ae3_0    conda-forge
     libwebp-base              1.2.0                h7f98852_0    conda-forge
     libxcb                    1.13              h7f98852_1003    conda-forge
     libxgboost                1.3.3dev.rapidsai0.18      cuda10.1_0    rapidsai-nightly
     libxml2                   2.9.10               h72842e0_3    conda-forge
     line-profiler             3.3.0                    pypi_0    pypi
     llvmlite                  0.35.0           py37h9d7f4d0_1    conda-forge
     locket                    0.2.0                      py_2    conda-forge
     lz4-c                     1.9.2                he1b5a44_3    conda-forge
     markdown                  3.3.4              pyhd8ed1ab_0    conda-forge
     markupsafe                2.0.1                    pypi_0    pypi
     matplotlib-base           3.3.4            py37h0c9df89_0    conda-forge
     mistune                   0.8.4           py37h5e8e339_1003    conda-forge
     more-itertools            8.7.0              pyhd8ed1ab_0    conda-forge
     msgpack-python            1.0.2            py37h2527ec5_1    conda-forge
     multidict                 5.1.0            py37h5e8e339_1    conda-forge
     multipledispatch          0.6.0                      py_0    conda-forge
     munch                     2.5.0                      py_0    conda-forge
     mxnet-cu101               1.8.0                    pypi_0    pypi
     nbclient                  0.5.3              pyhd8ed1ab_0    conda-forge
     nbconvert                 6.0.7            py37h89c1867_3    conda-forge
     nbformat                  5.1.2              pyhd8ed1ab_1    conda-forge
     nccl                      2.8.4.1              h8b44402_3    conda-forge
     ncurses                   6.2                  h58526e2_4    conda-forge
     nest-asyncio              1.4.3              pyhd8ed1ab_0    conda-forge
     netcdf4                   1.5.6                    pypi_0    pypi
     netifaces                 0.10.9          py37h5e8e339_1003    conda-forge
     networkx                  2.5                        py_0    conda-forge
     nodejs                    14.15.4              h92b4a50_1    conda-forge
     notebook                  6.2.0            py37h89c1867_0    conda-forge
     numba                     0.52.0           py37hdc94413_0    conda-forge
     numpy                     1.19.5           py37haa41c4c_1    conda-forge
     nvtx                      0.2.3            py37h5e8e339_0    conda-forge
     olefile                   0.46               pyh9f0ad1d_1    conda-forge
     openjdk                   11.0.1            h516909a_1016    conda-forge
     openjpeg                  2.4.0                hf7af979_0    conda-forge
     openssl                   1.1.1k               h7f98852_0    conda-forge
     orc                       1.6.5                hd3605a7_0    conda-forge
     packaging                 20.9               pyh44b312d_0    conda-forge
     pandas                    1.1.5            py37hdc94413_0    conda-forge
     pandoc                    2.11.4               h7f98852_0    conda-forge
     pandocfilters             1.4.2                      py_1    conda-forge
     panel                     0.10.3             pyhd8ed1ab_0    conda-forge
     param                     1.10.1             pyhd3deb0d_0    conda-forge
     parquet-cpp               1.5.1                         2    conda-forge
     parso                     0.7.1              pyh9f0ad1d_0    conda-forge
     partd                     1.1.0                      py_0    conda-forge
     patsy                     0.5.1                      py_0    conda-forge
     pcre                      8.44                 he1b5a44_0    conda-forge
     perl                      5.32.0               h36c2ea0_0    conda-forge
     pexpect                   4.8.0              pyh9f0ad1d_2    conda-forge
     pickle5                   0.0.11           py37h8f50634_0    conda-forge
     pickleshare               0.7.5                   py_1003    conda-forge
     pillow                    8.1.1            py37h4600e1f_0    conda-forge
     pip                       21.0.1             pyhd8ed1ab_0    conda-forge
     pipenv                    2021.5.29                pypi_0    pypi
     pixman                    0.40.0               h36c2ea0_0    conda-forge
     pluggy                    0.13.1           py37h89c1867_4    conda-forge
     poppler                   0.89.0               h2de54a5_5    conda-forge
     poppler-data              0.4.10                        0    conda-forge
     postgresql                12.3                 hc2f5b80_3    conda-forge
     proj                      7.1.1                h966b41f_3    conda-forge
     prometheus_client         0.9.0              pyhd3deb0d_0    conda-forge
     prompt-toolkit            3.0.16             pyha770c72_0    conda-forge
     protobuf                  3.13.0.1         py37h745909e_1    conda-forge
     psutil                    5.8.0            py37h5e8e339_1    conda-forge
     pthread-stubs             0.4               h36c2ea0_1001    conda-forge
     ptyprocess                0.7.0              pyhd3deb0d_0    conda-forge
     pvlib                     0.8.1                    pypi_0    pypi
     py                        1.10.0             pyhd3deb0d_0    conda-forge
     py-xgboost                1.3.3dev.rapidsai0.18  cuda10.1py37_0    rapidsai-nightly
     pyarrow                   1.0.1           py37hbeecfa9_14_cuda    conda-forge
     pycparser                 2.20               pyh9f0ad1d_2    conda-forge
     pyct                      0.4.6                      py_0    conda-forge
     pyct-core                 0.4.6                      py_0    conda-forge
     pydantic                  1.8.2                    pypi_0    pypi
     pydeck                    0.5.0              pyh9f0ad1d_0    conda-forge
     pyee                      7.0.4              pyh9f0ad1d_0    conda-forge
     pyephem                   9.99                     pypi_0    pypi
     pygments                  2.8.0              pyhd8ed1ab_0    conda-forge
     pyhive                    0.6.3              pyhd3deb0d_0    conda-forge
     pymeeus                   0.5.11                   pypi_0    pypi
     pynndescent               0.5.2              pyh44b312d_0    conda-forge
     pynvml                    8.0.4                      py_1    conda-forge
     pyopenssl                 20.0.1             pyhd8ed1ab_0    conda-forge
     pyparsing                 2.4.7              pyh9f0ad1d_0    conda-forge
     pyppeteer                 0.2.2                      py_1    conda-forge
     pyproj                    2.6.1.post1      py37h6415a23_3    conda-forge
     pyrsistent                0.17.3           py37h5e8e339_2    conda-forge
     pysocks                   1.7.1            py37h89c1867_3    conda-forge
     pytest                    6.2.2            py37h89c1867_0    conda-forge
     python                    3.7.10          hffdb5ce_100_cpython    conda-forge
     python-confluent-kafka    1.5.0            py37h8f50634_0    conda-forge
     python-dateutil           2.8.1                      py_0    conda-forge
     python-graphviz           0.8.4                    pypi_0    pypi
     python_abi                3.7                     1_cp37m    conda-forge
     pytz                      2021.1             pyhd8ed1ab_0    conda-forge
     pyviz_comms               2.0.1              pyhd3deb0d_0    conda-forge
     pyyaml                    5.4.1            py37h5e8e339_0    conda-forge
     pyzmq                     22.0.3           py37h336d617_1    conda-forge
     rapids                    0.18.0a210302   cuda10.1_py37_g58c5d18_220    rapidsai-nightly
     rapids-blazing            0.18.0a210302   cuda10.1_py37_g58c5d18_220    rapidsai-nightly
     rapids-xgboost            0.18.0a210302   cuda10.1_py37_g58c5d18_220    rapidsai-nightly
     re2                       2020.10.01           he1b5a44_0    conda-forge
     readline                  8.0                  he28a2e2_2    conda-forge
     requests                  2.25.1             pyhd3deb0d_0    conda-forge
     rmm                       0.18.0          cuda_10.1_py37_ga4ee6b7_0    rapidsai
     rtree                     0.9.7            py37h0b55af0_1    conda-forge
     s2n                       1.0.0                h9b69904_0    conda-forge
     s3fs                      0.5.2              pyhd8ed1ab_0    conda-forge
     sasl                      0.2.1           py37h3340039_1002    conda-forge
     scikit-learn              0.23.1           py37h8a51577_0    conda-forge
     scipy                     1.5.3            py37h8911b10_0    conda-forge
     seaborn                   0.11.1               hd8ed1ab_1    conda-forge
     seaborn-base              0.11.1             pyhd8ed1ab_1    conda-forge
     send2trash                1.5.0                      py_0    conda-forge
     setuptools                49.6.0           py37h89c1867_3    conda-forge
     shapely                   1.7.1            py37hba0730f_1    conda-forge
     simpervisor               0.4                pyhd8ed1ab_0    conda-forge
     six                       1.15.0             pyh9f0ad1d_0    conda-forge
     snappy                    1.1.8                he1b5a44_3    conda-forge
     sortedcontainers          2.3.0              pyhd8ed1ab_0    conda-forge
     spdlog                    1.7.0                hc9558a2_2    conda-forge
     sqlalchemy                1.4.3            py37h5e8e339_0    conda-forge
     sqlite                    3.34.0               h74cdb3f_0    conda-forge
     statsmodels               0.12.2           py37h902c9e0_0    conda-forge
     streamz                   0.6.2              pyh44b312d_0    conda-forge
     tbb                       2020.2               h4bd325d_3    conda-forge
     tblib                     1.6.0                      py_0    conda-forge
     terminado                 0.9.2            py37h89c1867_0    conda-forge
     testpath                  0.4.4                      py_0    conda-forge
     threadpoolctl             2.1.0              pyh5ca1d4c_0    conda-forge
     thrift                    0.13.0           py37hcd2ae1e_2    conda-forge
     thrift_sasl               0.4.2            py37h8f50634_0    conda-forge
     tiledb                    2.1.6                h1022b9d_0    conda-forge
     tk                        8.6.10               h21135ba_1    conda-forge
     toml                      0.10.2             pyhd8ed1ab_0    conda-forge
     toolz                     0.11.1                     py_0    conda-forge
     tornado                   6.1              py37h5e8e339_1    conda-forge
     tqdm                      4.58.0             pyhd8ed1ab_0    conda-forge
     traitlets                 5.0.5                      py_0    conda-forge
     treelite                  1.0.0            py37hc731546_0    conda-forge
     treelite-runtime          1.0.0                    pypi_0    pypi
     typing-extensions         3.7.4.3                       0    conda-forge
     typing_extensions         3.7.4.3                    py_0    conda-forge
     tzcode                    2021a                h7f98852_1    conda-forge
     ucx                       1.9.0+gcd9efd3       cuda10.1_0    rapidsai-nightly
     ucx-proc                  1.0.0                       gpu    rapidsai-nightly
     ucx-py                    0.18.0a210323   py37_gcd9efd3_19    rapidsai-nightly
     umap-learn                0.5.1            py37h89c1867_0    conda-forge
     urllib3                   1.26.3             pyhd8ed1ab_0    conda-forge
     virtualenv                20.4.7                   pypi_0    pypi
     virtualenv-clone          0.5.4                    pypi_0    pypi
     wcwidth                   0.2.5              pyh9f0ad1d_2    conda-forge
     webencodings              0.5.1                      py_1    conda-forge
     websockets                8.1              py37h5e8e339_3    conda-forge
     werkzeug                  2.0.1                    pypi_0    pypi
     wheel                     0.36.2             pyhd3deb0d_0    conda-forge
     widgetsnbextension        3.5.1            py37h89c1867_4    conda-forge
     wrapt                     1.12.1           py37h5e8e339_3    conda-forge
     wtforms                   2.3.3                    pypi_0    pypi
     xarray                    0.17.0             pyhd8ed1ab_0    conda-forge
     xerces-c                  3.2.3                h9d8b166_2    conda-forge
     xgboost                   1.3.3dev.rapidsai0.18  cuda10.1py37_0    rapidsai-nightly
     xorg-kbproto              1.0.7             h7f98852_1002    conda-forge
     xorg-libice               1.0.10               h7f98852_0    conda-forge
     xorg-libsm                1.2.3             hd9c2040_1000    conda-forge
     xorg-libx11               1.7.0                h7f98852_0    conda-forge
     xorg-libxau               1.0.9                h7f98852_0    conda-forge
     xorg-libxdmcp             1.1.3                h7f98852_0    conda-forge
     xorg-libxext              1.3.4                h7f98852_1    conda-forge
     xorg-libxrender           0.9.10            h7f98852_1003    conda-forge
     xorg-renderproto          0.11.1            h7f98852_1002    conda-forge
     xorg-xextproto            7.3.0             h7f98852_1002    conda-forge
     xorg-xproto               7.0.31            h7f98852_1007    conda-forge
     xz                        5.2.5                h516909a_1    conda-forge
     yaml                      0.2.5                h516909a_0    conda-forge
     yarl                      1.6.3            py37h5e8e339_1    conda-forge
     zeromq                    4.3.4                h9c3ff4c_0    conda-forge
     zict                      2.0.0                      py_0    conda-forge
     zipp                      3.4.0                      py_0    conda-forge
     zlib                      1.2.11            h516909a_1010    conda-forge
     zstd                      1.4.8                hdf46e1d_0    conda-forge
     
</pre></details>",2021-06-22T15:07:25Z,0,0,Pierrick Bruneau,Luxembourg Institute of Science and Technology,False
155,[FEA] Replace `infer_dtype_from_object` with `infer_dtype`,"Currently we depend on `infer_dtype_from_object` in `utils/dtype.py`:
https://github.com/rapidsai/cudf/blob/9510ef6a97102782906423ff5fd132c0e4bb08fd/python/cudf/cudf/utils/dtypes.py#L11

Since `infer_dtype_from_object` is not publicly documented in pandas, it is arguably better to switch to the [public documented](https://pandas.pydata.org/docs/reference/api/pandas.api.types.infer_dtype.html) version of `infer_dtype`.

The switch is not trivial. As two functions does not always have the same behavior:
```python
>>> infer_dtype(""timedelta64"")
'string'
>>> infer_dtype_from_object(""timedelta64"")
<class 'numpy.timedelta64'>
```
```python
>>> infer_dtype(np.datetime64)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""pandas/_libs/lib.pyx"", line 1350, in pandas._libs.lib.infer_dtype
  File ""pandas/_libs/lib.pyx"", line 1229, in pandas._libs.lib._try_infer_map
AttributeError: 'getset_descriptor' object has no attribute 'name'
>>> infer_dtype_from_object(np.datetime64)
<class 'numpy.datetime64'>
```",2021-06-29T00:32:40Z,0,0,Michael Wang,Nvidia Rapids,True
156,[FEA] Scalar factory functions do not let you provide a validity bool.,"
Scalars can be null and contain a validity bool.  The constructors for the various scalars take an `is_valid` parameter for this purpose.  But the factory functions themselves don't let you specify one in the call. Seems like an oversight. This applies to all scalar types.
```
struct_scalar(table&& data,
                bool is_valid                       = true,
                rmm::cuda_stream_view stream        = rmm::cuda_stream_default,
                rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());
```

```
std::unique_ptr<scalar> make_struct_scalar(
  table_view const& data,
  rmm::cuda_stream_view stream        = rmm::cuda_stream_default,
  rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());
```",2021-06-30T18:32:32Z,0,0,,,False
157,[FEA] Update `cudf::sort` to use new CUB merge sort ,"**Is your feature request related to a problem? Please describe.**

CUB recently moved Thrust's merge sort implementation into CUB, see: https://github.com/NVIDIA/cub/pull/322

This new version has two advantages over using Thrust's merge sort:
- It doesn't synchronize
- It supports out-of-place sorting

The latter will eliminate a redundant step of materializing a sequence vector for the input and we can instead use a counting iterator. 

**Describe the solution you'd like**
We should replace the `thrust::sort` call with the new CUB merge sort.

",2021-06-30T18:59:56Z,0,0,Jake Hemstad,@NVIDIA,True
158,[BUG] Shared-memory from_arrow no longer zero-copy,"cc @trxcllnt @galipremsagar 

Looking into implementing zero-copy `from_arrow` for node-rapids for Arrow arrays stored in IPC format in shared memory, we noticed that Python cudf [`from_arrow`](https://github.com/rapidsai/cudf/blob/branch-21.08/python/cudf/cudf/core/column/column.py#L222) delegates to `libcudf.interop.from_arrow` which will always result in a copy. Evidently previously it was possible to avoid a copy when and arrow table had been created via ` GpuArrowReader` from GPU memory. 

",2021-07-01T20:24:51Z,0,0,Bryan Van de Ven,Nvidia,True
159,[BUG] Runtime error when comparing `DatetimeIndex` to `RangeIndex`,"**Describe the bug**
Attempting to compare a `DatetimeIndex` and `RangeIndex` using `Index.equals()` causes a `RuntimeError`, seemingly failing on `cudf._lib.cpp.binaryop.binary_operation()`.

**Steps/Code to reproduce bug**
```python
import pandas as pd
import cudf

df = cudf.from_pandas(pd._testing.makeTimeDataFrame())  # len(df.index) == 30
df.index.equals(cudf.RangeIndex(start=0, stop=30, step=1))
```

Results in:

```python
RuntimeError                              Traceback (most recent call last)
<ipython-input-12-9f75d59fb3b4> in <module>
----> 1 df.index.equals(cudf.RangeIndex(start=0, stop=30, step=1))

~/compose/etc/conda/cuda_11.2.72/envs/rapids/lib/python3.8/contextlib.py in inner(*args, **kwds)
     73         def inner(*args, **kwds):
     74             with self._recreate_cm():
---> 75                 return func(*args, **kwds)
     76         return inner
     77 

~/cudf/python/cudf/cudf/core/index.py in equals(self, other, **kwargs)
    145 
    146         try:
--> 147             return super().equals(other, check_types=check_types)
    148         except TypeError:
    149             return False

~/cudf/python/cudf/cudf/core/frame.py in equals(self, other, **kwargs)
    594             self._data.values(), other._data.values()
    595         ):
--> 596             if not self_col.equals(other_col, check_dtypes=check_types):
    597                 return False
    598 

~/cudf/python/cudf/cudf/core/column/column.py in equals(self, other, check_dtypes)
    165             if self.dtype != other.dtype:
    166                 return False
--> 167         null_equals = self._null_equals(other)
    168         return null_equals.all()
    169 

~/cudf/python/cudf/cudf/core/column/column.py in _null_equals(self, other)
    169 
    170     def _null_equals(self, other: ColumnBase) -> ColumnBase:
--> 171         return self.binary_operator(""NULL_EQUALS"", other)
    172 
    173     def all(self) -> bool:

~/cudf/python/cudf/cudf/core/column/datetime.py in binary_operator(self, op, rhs, reflect)
    343         if reflect:
    344             lhs, rhs = rhs, lhs
--> 345         return libcudf.binaryop.binaryop(lhs, rhs, op, out_dtype)
    346 
    347     def fillna(

~/cudf/python/cudf/cudf/_lib/binaryop.pyx in cudf._lib.binaryop.binaryop()
    192     else:
    193         is_string_col = is_string_dtype(lhs.dtype)
--> 194         result = binaryop_v_v(
    195             lhs,
    196             rhs,

~/cudf/python/cudf/cudf/_lib/binaryop.pyx in cudf._lib.binaryop.binaryop_v_v()
    108     with nogil:
    109         c_result = move(
--> 110             cpp_binaryop.binary_operation(
    111                 c_lhs,
    112                 c_rhs,

RuntimeError: Deserialization failed
```

Subsequent attempts result in the same traceback, but with a different `RuntimeError`:

```python
RuntimeError: Uninitialized
```

**Expected behavior**
I would expect a boolean `False`, stating that the two indices are not equal.

**Environment overview (please complete the following information)**
 - Environment location: Docker (rapids-compose)
 - Method of cuDF install: from source (latest)

**Environment details**
<details><summary>Click here to see environment details</summary><pre>
     
     **git***
     commit 26f932ddbbecd79d6eade3096c85025088bf75c5 (HEAD -> packed-columns-serialization, origin/packed-columns-serialization)
     Merge: 1ac1b400ab e855eb9ea0
     Author: Charles Blackmon-Luca <20627856+charlesbluca@users.noreply.github.com>
     Date:   Tue Jul 6 13:41:07 2021 -0400
     
     Merge remote-tracking branch 'upstream/branch-21.08' into packed-columns-serialization
     **git submodules***
     
     ***OS Information***
     DISTRIB_ID=Ubuntu
     DISTRIB_RELEASE=18.04
     DISTRIB_CODENAME=bionic
     DISTRIB_DESCRIPTION=""Ubuntu 18.04.5 LTS""
     NAME=""Ubuntu""
     VERSION=""18.04.5 LTS (Bionic Beaver)""
     ID=ubuntu
     ID_LIKE=debian
     PRETTY_NAME=""Ubuntu 18.04.5 LTS""
     VERSION_ID=""18.04""
     HOME_URL=""https://www.ubuntu.com/""
     SUPPORT_URL=""https://help.ubuntu.com/""
     BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
     PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
     VERSION_CODENAME=bionic
     UBUNTU_CODENAME=bionic
     Linux charlesbluca-HP-Z8-G4-Workstation 5.8.0-50-generic #56~20.04.1-Ubuntu SMP Mon Apr 12 21:46:35 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux
     
     ***GPU Information***
     Thu Jul  8 09:51:43 2021
     +-----------------------------------------------------------------------------+
     | NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |
     |-------------------------------+----------------------+----------------------+
     | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
     | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
     |                               |                      |               MIG M. |
     |===============================+======================+======================|
     |   0  NVIDIA Quadro R...  On   | 00000000:15:00.0 Off |                  Off |
     | 34%   37C    P8    29W / 260W |     10MiB / 48601MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   1  NVIDIA Quadro R...  On   | 00000000:2D:00.0  On |                  Off |
     | 34%   61C    P0    71W / 260W |    728MiB / 48578MiB |      1%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     
     +-----------------------------------------------------------------------------+
     | Processes:                                                                  |
     |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
     |        ID   ID                                                   Usage      |
     |=============================================================================|
     +-----------------------------------------------------------------------------+
     
     ***CPU***
     Architecture:        x86_64
     CPU op-mode(s):      32-bit, 64-bit
     Byte Order:          Little Endian
     CPU(s):              12
     On-line CPU(s) list: 0-11
     Thread(s) per core:  2
     Core(s) per socket:  6
     Socket(s):           1
     NUMA node(s):        1
     Vendor ID:           GenuineIntel
     CPU family:          6
     Model:               85
     Model name:          Intel(R) Xeon(R) Gold 6128 CPU @ 3.40GHz
     Stepping:            4
     CPU MHz:             1399.488
     CPU max MHz:         3700.0000
     CPU min MHz:         1200.0000
     BogoMIPS:            6800.00
     Virtualization:      VT-x
     L1d cache:           32K
     L1i cache:           32K
     L2 cache:            1024K
     L3 cache:            19712K
     NUMA node0 CPU(s):   0-11
     Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke md_clear flush_l1d
     
     ***CMake***
     /home/charlesbluca/dev/rapids/compose/etc/conda/cuda_11.2.72/envs/rapids/bin/cmake
     cmake version 3.20.5
     
     CMake suite maintained and supported by Kitware (kitware.com/cmake).
     
     ***g++***
     /usr/local/bin/g++
     g++ (Ubuntu 9.4.0-1ubuntu1~18.04) 9.4.0
     Copyright (C) 2019 Free Software Foundation, Inc.
     This is free software; see the source for copying conditions.  There is NO
     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
     
     
     ***nvcc***
     /usr/local/bin/nvcc
     nvcc: NVIDIA (R) Cuda compiler driver
     Copyright (c) 2005-2021 NVIDIA Corporation
     Built on Sun_Feb_14_21:12:58_PST_2021
     Cuda compilation tools, release 11.2, V11.2.152
     Build cuda_11.2.r11.2/compiler.29618528_0
     
     ***Python***
     /home/charlesbluca/dev/rapids/compose/etc/conda/cuda_11.2.72/envs/rapids/bin/python
     Python 3.8.10
     
     ***Environment Variables***
     PATH                            : /home/charlesbluca/dev/rapids/compose/etc/conda/cuda_11.2.72/envs/rapids/bin:/home/charlesbluca/dev/rapids/compose/etc/conda/cuda_11.2.72/condabin:/home/charlesbluca/dev/rapids/compose/etc/conda/cuda_11.2.72/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/cuda/bin
     LD_LIBRARY_PATH                 : /home/charlesbluca/dev/rapids/compose/etc/conda/cuda_11.2.72/envs/rapids/lib:/home/charlesbluca/dev/rapids/compose/etc/conda/cuda_11.2.72/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/i386-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/lib:/home/charlesbluca/dev/rapids/rmm/build/release:/home/charlesbluca/dev/rapids/cudf/cpp/build/release:/home/charlesbluca/dev/rapids/raft/cpp/build/release:/home/charlesbluca/dev/rapids/cuml/cpp/build/release:/home/charlesbluca/dev/rapids/cugraph/cpp/build/release:/home/charlesbluca/dev/rapids/cuspatial/cpp/build/release
     NUMBAPRO_NVVM                   :
     NUMBAPRO_LIBDEVICE              :
     CONDA_PREFIX                    : /home/charlesbluca/dev/rapids/compose/etc/conda/cuda_11.2.72/envs/rapids
     PYTHON_PATH                     :
     
     ***conda packages***
     /home/charlesbluca/dev/rapids/compose/etc/conda/cuda_11.2.72/condabin/conda
     # packages in environment at /home/charlesbluca/dev/rapids/compose/etc/conda/cuda_11.2.72/envs/rapids:
     #
     # Name                    Version                   Build  Channel
     _libgcc_mutex             0.1                 conda_forge    conda-forge
     _openmp_mutex             4.5                      1_llvm    conda-forge
     abseil-cpp                20210324.2           h9c3ff4c_0    conda-forge
     alabaster                 0.7.12                     py_0    conda-forge
     appdirs                   1.4.4              pyh9f0ad1d_0    conda-forge
     argon2-cffi               20.1.0           py38h497a2fe_2    conda-forge
     arrow-cpp                 4.0.1           py38hb823b37_2_cuda    conda-forge
     arrow-cpp-proc            3.0.0                      cuda    conda-forge
     async_generator           1.10                       py_0    conda-forge
     attrs                     21.2.0             pyhd8ed1ab_0    conda-forge
     aws-c-cal                 0.5.11               h95a6274_0    conda-forge
     aws-c-common              0.6.2                h7f98852_0    conda-forge
     aws-c-event-stream        0.2.7               h3541f99_13    conda-forge
     aws-c-io                  0.10.5               hfb6a706_0    conda-forge
     aws-checksums             0.1.11               ha31a3da_7    conda-forge
     aws-sdk-cpp               1.8.186              hb4091e7_3    conda-forge
     babel                     2.9.1              pyh44b312d_0    conda-forge
     backcall                  0.2.0              pyh9f0ad1d_0    conda-forge
     backports                 1.0                        py_2    conda-forge
     backports.functools_lru_cache 1.6.4              pyhd8ed1ab_0    conda-forge
     black                     19.10b0                  py38_0    conda-forge
     bleach                    3.3.0              pyh44b312d_0    conda-forge
     bokeh                     2.3.2            py38h578d9bd_0    conda-forge
     boost-cpp                 1.76.0               h312852a_1    conda-forge
     brotli                    1.0.9                h9c3ff4c_4    conda-forge
     brotlipy                  0.7.0           py38h497a2fe_1001    conda-forge
     bzip2                     1.0.8                h7f98852_4    conda-forge
     c-ares                    1.17.1               h7f98852_1    conda-forge
     ca-certificates           2021.5.30            ha878542_0    conda-forge
     cachetools                4.2.2              pyhd8ed1ab_0    conda-forge
     certifi                   2021.5.30        py38h578d9bd_0    conda-forge
     cffi                      1.14.5           py38ha65f79e_0    conda-forge
     cfgv                      3.3.0              pyhd8ed1ab_0    conda-forge
     chardet                   4.0.0            py38h578d9bd_1    conda-forge
     clang                     8.0.1                hc9558a2_2    conda-forge
     clang-tools               8.0.1                hc9558a2_2    conda-forge
     clangxx                   8.0.1                         2    conda-forge
     click                     8.0.1            py38h578d9bd_0    conda-forge
     cloudpickle               1.6.0                      py_0    conda-forge
     cmake                     3.20.5               h8897547_0    conda-forge
     cmake-format              0.6.11             pyh9f0ad1d_0    conda-forge
     cmake_setuptools          0.1.3                      py_0    rapidsai
     colorama                  0.4.4              pyh9f0ad1d_0    conda-forge
     commonmark                0.9.1                      py_0    conda-forge
     cryptography              3.4.7            py38ha5dfef3_0    conda-forge
     cudatoolkit               11.2.72              h2bc3f7f_0    nvidia
     cupy                      9.2.0            py38ha69542f_0    conda-forge
     cython                    0.29.23          py38h709712a_1    conda-forge
     cytoolz                   0.11.0           py38h497a2fe_3    conda-forge
     dask                      2021.6.2+11.gcae3f8ff          pypi_0    pypi
     dask-core                 2021.6.2           pyhd8ed1ab_0    conda-forge
     dask-cuda                 21.8.0a0+32.g79bc44e           dev_0    <develop>
     dataclasses               0.8                pyhc8e2a94_1    conda-forge
     debugpy                   1.3.0            py38h709712a_0    conda-forge
     decorator                 5.0.9              pyhd8ed1ab_0    conda-forge
     defusedxml                0.7.1              pyhd8ed1ab_0    conda-forge
     distlib                   0.3.2              pyhd8ed1ab_0    conda-forge
     distributed               2021.6.2+32.gcbcec9cd          pypi_0    pypi
     dlpack                    0.5                  h9c3ff4c_0    conda-forge
     docutils                  0.16             py38h578d9bd_3    conda-forge
     double-conversion         3.1.5                h9c3ff4c_2    conda-forge
     editdistance-s            1.0.0            py38h1fd1430_1    conda-forge
     entrypoints               0.3             py38h32f6830_1002    conda-forge
     execnet                   1.9.0              pyhd8ed1ab_0    conda-forge
     expat                     2.4.1                h9c3ff4c_0    conda-forge
     fastavro                  1.4.2            py38h497a2fe_0    conda-forge
     fastrlock                 0.6              py38h709712a_1    conda-forge
     filelock                  3.0.12             pyh9f0ad1d_0    conda-forge
     flake8                    3.8.3                      py_1    conda-forge
     flatbuffers               2.0.0                h9c3ff4c_0    conda-forge
     freetype                  2.10.4               h0708190_1    conda-forge
     fsspec                    2021.6.1           pyhd8ed1ab_0    conda-forge
     future                    0.18.2           py38h578d9bd_3    conda-forge
     gflags                    2.2.2             he1b5a44_1004    conda-forge
     glog                      0.5.0                h48cff8f_0    conda-forge
     gmp                       6.2.1                h58526e2_0    conda-forge
     grpc-cpp                  1.38.1               h36ce80c_0    conda-forge
     heapdict                  1.0.1                      py_0    conda-forge
     huggingface_hub           0.0.13             pyhd8ed1ab_0    conda-forge
     hypothesis                6.14.0             pyhd8ed1ab_0    conda-forge
     icu                       68.1                 h58526e2_0    conda-forge
     identify                  2.2.10             pyhd8ed1ab_0    conda-forge
     idna                      2.10               pyh9f0ad1d_0    conda-forge
     imagesize                 1.2.0                      py_0    conda-forge
     importlib-metadata        4.6.0            py38h578d9bd_0    conda-forge
     importlib_metadata        4.6.0                hd8ed1ab_0    conda-forge
     iniconfig                 1.1.1              pyh9f0ad1d_0    conda-forge
     ipykernel                 6.0.0            py38hd0cf306_0    conda-forge
     ipython                   7.25.0           py38hd0cf306_1    conda-forge
     ipython_genutils          0.2.0                      py_1    conda-forge
     isort                     5.0.7            py38h32f6830_0    conda-forge
     jbig                      2.1               h7f98852_2003    conda-forge
     jedi                      0.18.0           py38h578d9bd_2    conda-forge
     jinja2                    3.0.1              pyhd8ed1ab_0    conda-forge
     joblib                    1.0.1              pyhd8ed1ab_0    conda-forge
     jpeg                      9d                   h36c2ea0_0    conda-forge
     jsonschema                3.2.0            py38h32f6830_1    conda-forge
     jupyter_client            6.1.12             pyhd8ed1ab_0    conda-forge
     jupyter_core              4.7.1            py38h578d9bd_0    conda-forge
     jupyterlab_pygments       0.1.2              pyh9f0ad1d_0    conda-forge
     krb5                      1.19.1               hcc1bbae_0    conda-forge
     lcms2                     2.12                 hddcbb42_0    conda-forge
     ld_impl_linux-64          2.36.1               hea4e1c9_0    conda-forge
     lerc                      2.2.1                h9c3ff4c_0    conda-forge
     libblas                   3.9.0                     8_mkl    conda-forge
     libcblas                  3.9.0                     8_mkl    conda-forge
     libcurl                   7.77.0               h2574ce0_0    conda-forge
     libdeflate                1.7                  h7f98852_5    conda-forge
     libedit                   3.1.20210216         h27cfd23_1    defaults
     libev                     4.33                 h516909a_1    conda-forge
     libevent                  2.1.10               hcdb4288_3    conda-forge
     libffi                    3.3                  h58526e2_2    conda-forge
     libgcc-ng                 9.3.0               h2828fa1_19    conda-forge
     liblapack                 3.9.0                     8_mkl    conda-forge
     libllvm10                 10.0.1               he513fc3_3    conda-forge
     libllvm8                  8.0.1                hc9558a2_0    conda-forge
     libnghttp2                1.43.0               h812cca2_0    conda-forge
     libpng                    1.6.37               h21135ba_2    conda-forge
     libprotobuf               3.16.0               h780b84a_0    conda-forge
     libsodium                 1.0.18               h36c2ea0_1    conda-forge
     libssh2                   1.9.0                ha56f1ee_6    conda-forge
     libstdcxx-ng              9.3.0               h6de172a_19    conda-forge
     libthrift                 0.14.2               he6d91bd_1    conda-forge
     libtiff                   4.3.0                hf544144_1    conda-forge
     libutf8proc               2.6.1                h7f98852_0    conda-forge
     libuv                     1.41.0               h7f98852_0    conda-forge
     libwebp-base              1.2.0                h7f98852_2    conda-forge
     llvm-openmp               11.1.0               h4bd325d_1    conda-forge
     llvmlite                  0.36.0           py38h4630a5e_0    conda-forge
     locket                    0.2.1            py38h06a4308_1    defaults
     lz4-c                     1.9.3                h9c3ff4c_0    conda-forge
     markdown                  3.3.4              pyhd8ed1ab_0    conda-forge
     markupsafe                2.0.1            py38h497a2fe_0    conda-forge
     matplotlib-inline         0.1.2              pyhd8ed1ab_2    conda-forge
     mccabe                    0.6.1                      py_1    conda-forge
     mimesis                   4.0.0              pyh9f0ad1d_0    conda-forge
     mistune                   0.8.4           py38h497a2fe_1004    conda-forge
     mkl                       2020.4             h726a3e6_304    conda-forge
     more-itertools            8.8.0              pyhd8ed1ab_0    conda-forge
     msgpack-python            1.0.2            py38h1fd1430_1    conda-forge
     mypy                      0.782                      py_0    conda-forge
     mypy_extensions           0.4.3            py38h578d9bd_3    conda-forge
     nbclient                  0.5.3              pyhd8ed1ab_0    conda-forge
     nbconvert                 6.1.0            py38h578d9bd_0    conda-forge
     nbformat                  5.1.3              pyhd8ed1ab_0    conda-forge
     nbsphinx                  0.8.6              pyhd8ed1ab_1    conda-forge
     ncurses                   6.2                  h58526e2_4    conda-forge
     nest-asyncio              1.5.1              pyhd8ed1ab_0    conda-forge
     ninja                     1.10.2               h4bd325d_0    conda-forge
     nodeenv                   1.6.0              pyhd8ed1ab_0    conda-forge
     notebook                  6.4.0              pyha770c72_0    conda-forge
     numba                     0.53.1           py38h8b71fd7_1    conda-forge
     numpy                     1.21.0           py38h9894fe3_0    conda-forge
     numpydoc                  1.1.0                      py_1    conda-forge
     nvtx                      0.2.3            py38h497a2fe_0    conda-forge
     olefile                   0.46               pyh9f0ad1d_1    conda-forge
     openjpeg                  2.4.0                hb52868f_1    conda-forge
     openssl                   1.1.1k               h7f98852_0    conda-forge
     orc                       1.6.8                h58a87f1_0    conda-forge
     packaging                 20.9               pyh44b312d_0    conda-forge
     pandas                    1.2.5            py38h1abd341_0    conda-forge
     pandoc                    1.19.2.1             hea2e7c5_1    defaults
     pandocfilters             1.4.3            py38h06a4308_1    defaults
     parquet-cpp               1.5.1                         1    conda-forge
     parso                     0.8.2              pyhd8ed1ab_0    conda-forge
     partd                     1.2.0              pyhd8ed1ab_0    conda-forge
     pathspec                  0.8.1              pyhd3deb0d_0    conda-forge
     pexpect                   4.8.0            py38h32f6830_1    conda-forge
     pickleshare               0.7.5           py38h32f6830_1002    conda-forge
     pillow                    8.3.0            py38h8e6f84c_0    conda-forge
     pip                       21.1.3             pyhd8ed1ab_0    conda-forge
     pluggy                    0.13.1           py38h578d9bd_4    conda-forge
     pre-commit                2.13.0           py38h578d9bd_0    conda-forge
     pre_commit                2.13.0               hd8ed1ab_0    conda-forge
     prometheus_client         0.11.0             pyhd8ed1ab_0    conda-forge
     prompt-toolkit            3.0.19             pyha770c72_0    conda-forge
     protobuf                  3.16.0           py38h709712a_0    conda-forge
     psutil                    5.8.0            py38h497a2fe_1    conda-forge
     ptvsd                     4.3.2                    pypi_0    pypi
     ptyprocess                0.7.0              pyhd3deb0d_0    conda-forge
     py                        1.10.0             pyhd3deb0d_0    conda-forge
     py-cpuinfo                8.0.0              pyhd8ed1ab_0    conda-forge
     pyarrow                   4.0.1           py38hb53058b_2_cuda    conda-forge
     pycodestyle               2.6.0              pyh9f0ad1d_0    conda-forge
     pycparser                 2.20               pyh9f0ad1d_2    conda-forge
     pyflakes                  2.2.0              pyh9f0ad1d_0    conda-forge
     pygments                  2.9.0              pyhd8ed1ab_0    conda-forge
     pynvml                    11.0.0             pyhd8ed1ab_0    conda-forge
     pyopenssl                 20.0.1             pyhd8ed1ab_0    conda-forge
     pyorc                     0.4.0                    pypi_0    pypi
     pyparsing                 2.4.7              pyh9f0ad1d_0    conda-forge
     pyrsistent                0.17.3           py38h497a2fe_2    conda-forge
     pysocks                   1.7.1            py38h578d9bd_3    conda-forge
     pytest                    6.2.4            py38h578d9bd_0    conda-forge
     pytest-benchmark          3.4.1              pyhd8ed1ab_0    conda-forge
     pytest-forked             1.3.0              pyhd3deb0d_0    conda-forge
     pytest-xdist              2.3.0              pyhd8ed1ab_0    conda-forge
     python                    3.8.10          h49503c6_1_cpython    conda-forge
     python-dateutil           2.8.1                      py_0    conda-forge
     python_abi                3.8                      2_cp38    conda-forge
     pytorch                   1.7.1           cpu_py38h36eccb8_2    conda-forge
     pytz                      2021.1             pyhd8ed1ab_0    conda-forge
     pyyaml                    5.4.1            py38h497a2fe_0    conda-forge
     pyzmq                     22.1.0           py38h2035c66_0    conda-forge
     rapidjson                 1.1.0             he1b5a44_1002    conda-forge
     re2                       2021.06.01           h9c3ff4c_0    conda-forge
     readline                  8.1                  h46c0cb4_0    conda-forge
     recommonmark              0.7.1              pyhd8ed1ab_0    conda-forge
     regex                     2021.4.4         py38h497a2fe_0    conda-forge
     requests                  2.25.1             pyhd3deb0d_0    conda-forge
     rhash                     1.4.1                h7f98852_0    conda-forge
     s2n                       1.0.10               h9b69904_0    conda-forge
     sacremoses                0.0.43             pyh9f0ad1d_0    conda-forge
     send2trash                1.7.1              pyhd8ed1ab_0    conda-forge
     setuptools                52.0.0           py38h06a4308_0    defaults
     six                       1.16.0             pyh6c4a22f_0    conda-forge
     snappy                    1.1.8                he1b5a44_3    conda-forge
     snowballstemmer           2.1.0              pyhd8ed1ab_0    conda-forge
     sortedcontainers          2.4.0              pyhd8ed1ab_0    conda-forge
     spdlog                    1.8.5                h4bd325d_0    conda-forge
     sphinx                    4.0.2              pyh6c4a22f_1    conda-forge
     sphinx-copybutton         0.3.3              pyhd8ed1ab_0    conda-forge
     sphinx-markdown-tables    0.0.15             pyhd3deb0d_0    conda-forge
     sphinx_rtd_theme          0.5.2              pyhd8ed1ab_1    conda-forge
     sphinxcontrib-applehelp   1.0.2                      py_0    conda-forge
     sphinxcontrib-devhelp     1.0.2                      py_0    conda-forge
     sphinxcontrib-htmlhelp    2.0.0              pyhd8ed1ab_0    conda-forge
     sphinxcontrib-jsmath      1.0.1                      py_0    conda-forge
     sphinxcontrib-qthelp      1.0.3                      py_0    conda-forge
     sphinxcontrib-serializinghtml 1.1.5              pyhd8ed1ab_0    conda-forge
     sphinxcontrib-websupport  1.2.4              pyh9f0ad1d_0    conda-forge
     sqlite                    3.36.0               h9cd32fc_0    conda-forge
     streamz                   0.6.2              pyh44b312d_0    conda-forge
     tblib                     1.7.0              pyhd8ed1ab_0    conda-forge
     terminado                 0.10.1           py38h578d9bd_0    conda-forge
     testpath                  0.5.0              pyhd8ed1ab_0    conda-forge
     tk                        8.6.10               h21135ba_1    conda-forge
     tokenizers                0.10.1           py38hb63a372_0    conda-forge
     toml                      0.10.2             pyhd8ed1ab_0    conda-forge
     toolz                     0.11.1                     py_0    conda-forge
     tornado                   6.1              py38h497a2fe_1    conda-forge
     tqdm                      4.61.1             pyhd8ed1ab_0    conda-forge
     traitlets                 5.0.5                      py_0    conda-forge
     transformers              4.8.2              pyhd8ed1ab_0    conda-forge
     typed-ast                 1.4.3            py38h497a2fe_0    conda-forge
     typing-extensions         3.10.0.0             hd8ed1ab_0    conda-forge
     typing_extensions         3.10.0.0           pyha770c72_0    conda-forge
     urllib3                   1.26.6             pyhd8ed1ab_0    conda-forge
     virtualenv                20.4.7           py38h578d9bd_0    conda-forge
     wcwidth                   0.2.5              pyh9f0ad1d_2    conda-forge
     webencodings              0.5.1                      py_1    conda-forge
     wheel                     0.36.2             pyhd3deb0d_0    conda-forge
     xz                        5.2.5                h516909a_1    conda-forge
     yaml                      0.2.5                h516909a_0    conda-forge
     zeromq                    4.3.4                h9c3ff4c_0    conda-forge
     zict                      2.0.0                      py_0    conda-forge
     zipp                      3.4.1              pyhd8ed1ab_0    conda-forge
     zlib                      1.2.11            h516909a_1010    conda-forge
     zstd                      1.5.0                ha95c52a_0    conda-forge
     
</pre></details>

**Additional context**
I ran into this issue testing the pack/unpack API on [`test_serialize.py`](https://github.com/rapidsai/cudf/blob/53b3c1696ba7e545e0f67566415259362ace9276/python/cudf/cudf/tests/test_serialize.py#L15-L71) because of a specific index equality check in `cudf._lib.copying._CPackedColumns.from_py_table()`:

https://github.com/rapidsai/cudf/blob/53b3c1696ba7e545e0f67566415259362ace9276/python/cudf/cudf/_lib/copying.pyx#L798-L800
",2021-07-08T13:57:55Z,0,0,Charles Blackmon-Luca,@rapidsai,True
160,"[BUG] Missing ""inplace"" argument in ""dask_cudf.DataFrame.drop"" although it exists in documentation","Hi guys,

I tried `gdf.drop(columns=['Churn_Value'], inplace=True)` where `gdf` is a `dask_cudf.DataFrame` and I got this error `TypeError: drop() got an unexpected keyword argument 'inplace'` even though it is listed among the list of parameters in the doc for `dask_cudf.DataFrame.drop` :

```
dask_cudf.DataFrame.drop(
    self,
    labels=None,
    axis=0,
    columns=None,
    errors='raise',
)
Docstring:
Drop specified labels from rows or columns.

This docstring was copied from pandas.core.frame.DataFrame.drop.

Some inconsistencies with the Dask version may exist.

Remove rows or columns by specifying label names and corresponding
axis, or by specifying directly index or column names. When using a
multi-index, labels on different levels can be removed by specifying
the level.

Parameters
----------
labels : single label or list-like
    Index or column labels to drop.
axis : {0 or 'index', 1 or 'columns'}, default 0
    Whether to drop labels from the index (0 or 'index') or
    columns (1 or 'columns').
index : single label or list-like  (Not supported in Dask)
    Alternative to specifying axis (``labels, axis=0``
    is equivalent to ``index=labels``).
columns : single label or list-like
    Alternative to specifying axis (``labels, axis=1``
    is equivalent to ``columns=labels``).
level : int or level name, optional  (Not supported in Dask)
    For MultiIndex, level from which the labels will be removed.
inplace : bool, default False  (Not supported in Dask)
    If False, return a copy. Otherwise, do operation
    inplace and return None.
errors : {'ignore', 'raise'}, default 'raise'
    If 'ignore', suppress error and only existing labels are
    dropped.

Returns
-------
DataFrame or None
    DataFrame without the removed index or column labels or
    None if ``inplace=True``.
```
Can you guys check this out ?
Thanks :)",2021-07-09T07:32:08Z,0,0,Bassem Karoui,,False
161,[FEA] Ability to write and read index columns in `orc` format,"**Is your feature request related to a problem? Please describe.**
I'd like to have the ability to pass on the index columns aswell through `orc` files. We do this incase of `parquet` but not `orc`. When we roundtrip a dataframe from cudf `orc` writer & reader, the index data is lost:
```python
>>> df = cudf.DataFrame({'a': [1, 2, 3]}, index=['a', 'b', 'c'])
>>> df
   a
a  1
b  2
c  3
>>> df.to_orc('sample.orc')
>>> cudf.read_orc('sample.orc')
   a
0  1
1  2
2  3
```

**Describe the solution you'd like**
We might have to uniquely name the index columns and write and retrieve them in libcudf layer.

cc: @vuule @rgsl888prabhu 
",2021-07-09T16:41:50Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
162,[FEA] Support COUNT_VALID and COUNT_ALL for scan and group by scan,"**Is your feature request related to a problem? Please describe.**
As a part of #8440 it was decided that for running window aggregations that scan and group by scan should be used. COUNT_ALL is supported on group by scan, but not by scan itself.  COUNT_VALID is not supported by either.

**Describe the solution you'd like**
It would be nice to have these supported.

**Describe alternatives you've considered**
Right now I have put together a solution that uses SUM, is_null, and copy_if_else to replicate COUNT_VALID, and COUNT_ALL.  This is not ideal and I would like to move away from it.

**Additional context**
This is not super critical because I have a working solution, but it would improve performance and reduce memory usage.
",2021-07-09T19:02:37Z,0,0,Robert (Bobby) Evans,Nvidia,True
163,[BUG] DataFrame constructor incorrectly loads nested dictionary input,"**What is your question?**
Got different results when trying to convert dictionary to dataframe using pandas and cudf
![image](https://user-images.githubusercontent.com/87376384/125461623-98da6483-d5eb-4278-bdeb-944276949a5e.png)
",2021-07-13T13:38:28Z,0,0,,,False
164,[FEA] Adding support for categorical column indexes,"**Is your feature request related to a problem? Please describe.**
Categorical column indexes exists in a weird place of quasi-support in cuDF; while it is possible to set a dataframe's column index to be a `pd.CategoricalIndex` without any error or warning, it isn't actually possible for the index to be recreated with `df.columns`, which contrasts the behavior of Pandas:

```python
import cudf
import pandas as pd

pdf = pd.DataFrame({""a"": [1, 2, 3], ""b"": [4, 5, 6]})
pdf.columns = pdf.columns.astype(""category"")

gdf = cudf.from_pandas(pdf)

print(pdf.columns)  # CategoricalIndex(['a', 'b'], categories=['a', 'b'], ordered=False, dtype='category')
print(gdf.columns)  # Index(['a', 'b'], dtype='object')
```

This means that while there are user-facing issues which come as a result of using cuDF's ""categorical"" column indexes (such as #7365), the ability to test for them is limited in that we cannot do the standard comparison to Pandas dataframes here:

```python
from cudf.testing._utils import assert_eq

assert_eq(pdf, gdf)  # AssertionError: DataFrame.columns are different
```

**Describe the solution you'd like**
After chatting with @shwina, it seems like an ideal solution that *can't* be done here is to use the individual categorical scalars instead of their string names as data when constructing the `ColumnAccessor` in the [columns setter method](https://github.com/rapidsai/cudf/blob/0e2a448e15eec4b6d5d20f469a6f79be2f31b923/python/cudf/cudf/core/dataframe.py#L2676-L2704). However, this isn't possible, as neither Pandas nor cuDF offer categorical scalars.

An alternative to this would be to have a boolean attribute either of the dataframe or `ColumnAccessor` saying whether or not the column index is categorical; this could then be used by `ColumnAccessor.to_pandas_index()`to properly reconstruct the index with categories if needed. This would come with its own consequences, specifically either

- a relatively niche param/attribute of `ColumnAccessor` that is only used for dataframes
- an attribute of dataframes that now must be explicitly copied from one to another in the case of copies

**Describe alternatives you've considered**
A possible alternative that @shwina and I explored, but were unable to get working, is to pass specific kwargs to `assert_eq` such that it would only check the column index names, but not the index type. Passing different combos of `check_categorical=False`, `check_column_type=False`, etc. we were unable to get a passing test when comparing these indexes.

**Additional context**
This issue came up while working on #8560, where added test cases would require this feature and needed to be xfailed.
",2021-07-14T21:38:56Z,0,0,Charles Blackmon-Luca,@rapidsai,True
165,[BUG] Writing `NA`s in a view results in zeros in the parent dataframe,"I don't know exactly if it's a bug or a feature, but I have noticed that we don't have parity with pandas in the following example because `NA`s are interpreted as zeros when writing in the parent dataframe:

```python
import cudf
import pandas as pd

cudf0 = cudf.DataFrame([[1.], [2.]], columns=[""A""])
cudf1 = cudf0[:1]
cudf1[""A""] = None

print(""cuDF (parent):"")
print(cudf0, end='\n\n')
print(""cuDF (view):"")
print(cudf1, end='\n\n')

pd0 = pd.DataFrame([[1.], [2.]], columns=[""A""])
pd1 = pd0[:1]
pd1[""A""][0] = None

print(""pandas (parent):"")
print(pd0, end='\n\n')
print(""pandas (view):"")
print(pd1, end='\n\n')
```

Output:
```
cuDF (parent):
     A
0  0.0
1  2.0

cuDF (view):
      A
0  <NA>

pandas (parent):
     A
0  NaN
1  2.0

pandas (view):
    A
0 NaN
```

Note that if we use the value `NaN` instead of `None` (aka missing value / `<NA>`), it works like pandas.",2021-07-16T11:17:54Z,0,0,Louis Sugy,@NVIDIA,True
166,[FEA] Properly raise when attempting to cast `NA` to bool inside UDFs,"**Is your feature request related to a problem? Please describe.**
With the merge of https://github.com/rapidsai/cudf/pull/8213 we have initial support for user defined functions that involve nulls. However the behavior in the following edge case does not match pandas if the input column contains nulls:

```python
def f(x):
    if x > 2:
        return 3
    else:
        return 4
```
In cases where `x is cudf.NA`, the first condition will eventually resolve to `if(cudf.NA)`. In the cuDF view, `NA` is treated as falsy for lack of a better solution in the initial implementation, and thus the code will take the first branch and return 3. However in pandas, it will *raise* and go down neither branch, complaining that `The boolean value of NA is ambiguous`. This is probably the correct behavior. 

**Describe the solution you'd like**
Make it so that numba returns a non zero exit code from kernels if this situation occurs, and then propagate that error to the user as a python error.

**Describe alternatives you've considered**
Making nulls falsy leads to different results than pandas.

**Additional context**
cc @gmarkall ",2021-07-19T19:04:59Z,0,0,,NVIDIA,True
167,[FEA] Exception message when opening timezone file should contain timezone name,"**Is your feature request related to a problem? Please describe.**
While trying to load an ORC file with libcudf, an exception was thrown that looks like this:
```
21/07/16 09:37:30 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (172.25.0.3 executor 3): ai.rapids.cudf.CudfException: cuDF failure at: /home/jenkins/agent/workspace/jenkins-cudf-release-17-cuda11/cpp/src/io/orc/timezone.cpp:136: Failed to open the timezone file.
        at ai.rapids.cudf.Table.readORC(Native Method)
        at ai.rapids.cudf.Table.readORC(Table.java:829)
        at com.nvidia.spark.rapids.GpuOrcPartitionReader.$anonfun$readToTable$4(GpuOrcScan.scala:853)
[...]
```

Unfortunately it's not easy to determine _which_ timezone file was causing the error.

**Describe the solution you'd like**
The exception message thrown when accessing a timezone file should contain the path to the timezone file.

**Describe alternatives you've considered**
Instead of reporting the full pathname it could report just the timezone name, but having the full path may better help users determine which system package they should install in order to properly process the timezone that appears in the ORC file.
",2021-07-20T13:59:44Z,0,0,Jason Lowe,NVIDIA,True
168,[BUG] dataframe reindex NaN,"**Describe the bug**
cuDF DataFrame is not reindexed as intented. Using a multiindex, it ends up in NaNs.

**Steps/Code to reproduce bug**
```
data_df = cudf.read_parquet('../input/optiver-realized-volatility-prediction/book_train.parquet/stock_id=0/c439ef22282f412ba39e9137a3fdabac.parquet')
offsets = data_df.groupby(['time_id'], as_index=False).agg({'seconds_in_bucket':'min'}).reset_index(drop=True)
offsets.columns = ['time_id', 'offset']
data_df = cudf.merge(data_df, offsets, on = ['time_id'], how = 'left')
data_df.seconds_in_bucket = data_df.seconds_in_bucket - data_df.offset
# MultiIndex.from_product uses pandas in the background
# That's why we need to transform the data into pd dataframe
data_df = data_df.set_index(['time_id', 'seconds_in_bucket'])
columns = [col for col in data_df.columns.values]
data_df = data_df.reindex(cudf.MultiIndex.from_product([data_df.to_pandas().index.levels[0], np.arange(0,600)], names = ['time_id', 'seconds_in_bucket']), columns=columns).fillna(method='ffill')
data_df = cudf.DataFrame(data_df.reset_index())
```
https://www.kaggle.com/medali1992/optiver-train-dataset?scriptVersionId=68637709

This workaround works:
```
indices = cudf.MultiIndex.from_product([data_df.to_pandas().index.levels[0], np.arange(0,600)], names = ['time_id', 'seconds_in_bucket'])
data_df = cudf.DataFrame().set_index(indices).join(data_df, how=""left"").fillna(method='ffill').reset_index(drop=True)
```

**Expected behavior**
I expect reindex function to get the correct values instead of NaNs as in pandas.

**Environment overview (please complete the following information)**
Kaggle GPU Docker, RAPIDS 21.06

",2021-07-20T18:56:35Z,0,0,Ahmet Erdem,,False
169,[BUG] Warnings generated during Javadoc build,"**Describe the bug**
When building Javadoc for the cudf Java bindings the following warnings are generated:
```
7 warnings
[WARNING] Javadoc Warnings
[WARNING] /home/jlowe/src/cudf/java/src/main/java/ai/rapids/cudf/ColumnView.java:2652: warning - @note: is an unknown tag.
[WARNING] /home/jlowe/src/cudf/java/src/main/java/ai/rapids/cudf/HostColumnVector.java:248: warning - Tag @see: can't find build(int, Consumer) in ai.rapids.cudf.HostColumnVector
[WARNING] /home/jlowe/src/cudf/java/src/main/java/ai/rapids/cudf/HostColumnVector.java:260: warning - Tag @see: can't find build(int, int, Consumer) in ai.rapids.cudf.HostColumnVector
[WARNING] /home/jlowe/src/cudf/java/src/main/java/ai/rapids/cudf/HostColumnVector.java:248: warning - Tag @see cannot be used in inline documentation.  It can only be used in the following types of documentation: overview, package, class/interface, constructor, field, method.
[WARNING] /home/jlowe/src/cudf/java/src/main/java/ai/rapids/cudf/HostColumnVector.java:248: warning - Tag @see cannot be used in inline documentation.  It can only be used in the following types of documentation: overview, package, class/interface, constructor, field, method.
[WARNING] /home/jlowe/src/cudf/java/src/main/java/ai/rapids/cudf/HostColumnVector.java:260: warning - Tag @see cannot be used in inline documentation.  It can only be used in the following types of documentation: overview, package, class/interface, constructor, field, method.
[WARNING] /home/jlowe/src/cudf/java/src/main/java/ai/rapids/cudf/HostColumnVector.java:260: warning - Tag @see cannot be used in inline documentation.  It can only be used in the following types of documentation: overview, package, class/interface, constructor, field, method.
```

**Steps/Code to reproduce bug**
```shell
$ cd cudf/java
$ mvn javadoc:javadoc
```

**Expected behavior**
There should be no warnings during a Javadoc build
",2021-07-26T17:53:42Z,0,0,Jason Lowe,NVIDIA,True
170,[FEA] Add nested struct support for comparison operations,"**Is your feature request related to a problem? Please describe.**
For Spark we are pushing to get more support for structs in a number of operators.  We already have some support for sorting structs, so we should be able to come up with a way to do comparisons of nested structs too.  NOTE this does not include lists as children of the structs just structs that contains basic types including strings and other structs.

The operations we would like to support include the BINARY ops EQUAL, NOT_EQUAL, LESS, GREATER, LESS_EQUAL, GREATER_EQUAL, NULL_EQUALS, and if possible NULL_MAX and NULL_MIN.

This should follow the same pattern we have supported for sorting with the order of precedence for the children in a struct go from first to last.  In this case we would like nulls within the struct columns to be less than other values, but equal to each other. meaning `Struct(null)` is less than `Struct(5)` and `Struct(null)` == `Struct(null)`.  Nulls at the top level still depend on the operator being performed. For NULL_EQUALS nulls are equal to each other.

**Describe the solution you'd like**
It would be great if we could do this as regular binary ops, but if we need them to be separate APIs that works too. If null equality/etc needs to be configurable for the python APIs a separate API is fine.

**Describe alternatives you've considered**
We could flatten the struct columns ourselves and do a number of different operations to combine the results back together to get the right answer. But cudf already has a flatten method behind the scenes so why replicate that when others could benefit from it too.",2021-08-05T13:56:27Z,0,0,Robert (Bobby) Evans,Nvidia,True
171,[FEA] support max and min aggregations for nested structs,"**Is your feature request related to a problem? Please describe.**
In the Spark plugin we have a push to try and support as much of structs as we can.  Cudf supports sorting of nested structs (no lists just basic types and other structs).  It would really be great if we could support max and min aggregations on these as well. There are a lot of different types of max/min aggregations and if we cannot get them all at once we can take it a bit at a time too. We would like to be able to support this for.

- [X] Sort-based group by aggregations
- [ ] Hash-based group by aggregations
- [X] Group by scans
- [X] reductions
- [X] scans
- [X] window operations

like described in https://github.com/rapidsai/cudf/issues/8964 null child column values would come before non-null child column values.

**Describe the solution you'd like**
Just what I asked for min/max aggregations that can work on structs.

**Describe alternatives you've considered**
I'm not sure there is an alternative that we can support.

**Additional context**
Add any other context, code examples, or references to existing implementations about the feature request here.
",2021-08-05T18:04:00Z,0,0,Robert (Bobby) Evans,Nvidia,True
172,[BUG] Drop function for Multicolumn index doesn't work,"**Describe the bug**
Drop function for Multicolumn index doesn't work as expected
The error I get is: 'One or more values not found in axis'

**Steps/Code to reproduce bug**
```python
import cudf
df=cudf.DataFrame()
df['src_lat']=[-46.0,-46.0,-46.0, -34.0,-34.0,-34.0, 11.5, 11.5, 11.5]; df['src_long']=df['src_lat']+1
df['dst_lat']=[-46.0, -34.0,11.5,-46.0, -34.0,11.5,-46.0, -34.0,11.5];df['dst_long']=df['dst_lat']+1
df['val']=[0.1,0.2,0.1,0.3,0.2,0.1,0.3,0.3,0.1]
df = df.pivot(index=['src_lat', 'src_long'], columns=['dst_lat', 'dst_long'], values=['val'])
df.columns = df.index
df.drop([(11.5, 12.5), (-46.0, -45.0)])
```
",2021-08-06T15:48:46Z,0,0,,,False
173,[FEA] Support the `min_count` argument in groupby aggregations,"Updated 5/13/2024: `numeric_only` is now supported (as of #10629). `min_count` is not yet supported.

In Pandas,  groupby aggregations (e.g., [`max`](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.GroupBy.max.html)) accept the following arguments:

- `min_count`: the minimum number of non-null values required per group in order for the result to be non-null
- `numeric_only`: only aggregate numeric columns

It would be nice for cuDF to support these as well:

```python
In [6]: df = cudf.DataFrame({'a': [1, 1, 1, 2, 2], 'b': ['a', 'b', 'c', 'd', 'e'], 'c': [1, 2, 3, 4, 5]})

In [7]: df
Out[7]:
   a  b  c
0  1  a  1
1  1  b  2
2  1  c  3
3  2  d  4
4  2  e  5

In [8]: df.groupby('a').max(numeric_only=True)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-8-612714d07c42> in <module>
----> 1 df.groupby('a').max(numeric_only=True)

TypeError: max() got an unexpected keyword argument 'numeric_only'

In [9]: df.to_pandas().groupby('a').max(numeric_only=True)
Out[9]:
   c
a
1  3
2  5
```
",2021-08-10T18:17:43Z,0,0,Ashwin Srinath,Voltron Data,False
174,[BUG] Specifying out-of-range column index for `usecols` in `read_csv()` causes illegal memory access,"**Describe the bug**
When specifying an out of range column index for `usecols` when using `read_csv()` (e.g. `usecols=[2]` for a 2 column CSV file), an illegal memory access occurs - this can sometimes lead to segfault.

**Steps/Code to reproduce bug**
Follow this guide http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports to craft a minimal bug report. This helps us reproduce the issue you're having and resolve the issue more quickly.
```python
import cudf

filename = 'foo.csv'
lines = [
  ""num,text"",
  ""123,abc"",
  ""456,def"",
  ""789,ghi""
]

with open(filename, 'w') as fp:
    fp.write('\n'.join(lines)+'\n')

cudf.read_csv(filename, usecols=[2])
```
```
MemoryError                               Traceback (most recent call last)
<ipython-input-6-df792974f123> in <module>
----> 1 cudf.read_csv(filename, usecols=[100])

~/compose/etc/conda/cuda_11.2/envs/rapids/lib/python3.8/contextlib.py in inner(*args, **kwds)
     73         def inner(*args, **kwds):
     74             with self._recreate_cm():
---> 75                 return func(*args, **kwds)
     76         return inner
     77 

~/cudf/python/cudf/cudf/io/csv.py in read_csv(filepath_or_buffer, lineterminator, quotechar, quoting, doublequote, header, mangle_dupe_cols, usecols, sep, delimiter, delim_whitespace, skipinitialspace, names, dtype, skipfooter, skiprows, dayfirst, compression, thousands, decimal, true_values, false_values, nrows, byte_range, skip_blank_lines, parse_dates, comment, na_values, keep_default_na, na_filter, prefix, index_col, **kwargs)
     68         na_values = [na_values]
     69 
---> 70     return libcudf.csv.read_csv(
     71         filepath_or_buffer,
     72         lineterminator=lineterminator,

~/cudf/python/cudf/cudf/_lib/csv.pyx in cudf._lib.csv.read_csv()
    392     cdef table_with_metadata c_result
    393     with nogil:
--> 394         c_result = move(cpp_read_csv(read_csv_options_c))
    395 
    396     meta_names = [name.decode() for name in c_result.metadata.column_names]

MemoryError: std::bad_alloc: CUDA error at: ../include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorIllegalAddress an illegal memory access was encountered
```

**Expected behavior**
I would expect a `ValueError`, similar to what Pandas throws in the same scenario:

```
ValueError                                Traceback (most recent call last)
<ipython-input-3-5b5ea1ba4d89> in <module>
----> 1 pd.read_csv(filename, usecols=[2])

~/compose/etc/conda/cuda_11.2/envs/rapids/lib/python3.8/site-packages/pandas/io/parsers.py in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)
    608     kwds.update(kwds_defaults)
    609 
--> 610     return _read(filepath_or_buffer, kwds)
    611 
    612 

~/compose/etc/conda/cuda_11.2/envs/rapids/lib/python3.8/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds)
    460 
    461     # Create the parser.
--> 462     parser = TextFileReader(filepath_or_buffer, **kwds)
    463 
    464     if chunksize or iterator:

~/compose/etc/conda/cuda_11.2/envs/rapids/lib/python3.8/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds)
    817             self.options[""has_index_names""] = kwds[""has_index_names""]
    818 
--> 819         self._engine = self._make_engine(self.engine)
    820 
    821     def close(self):

~/compose/etc/conda/cuda_11.2/envs/rapids/lib/python3.8/site-packages/pandas/io/parsers.py in _make_engine(self, engine)
   1048             )
   1049         # error: Too many arguments for ""ParserBase""
-> 1050         return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]
   1051 
   1052     def _failover_to_python(self):

~/compose/etc/conda/cuda_11.2/envs/rapids/lib/python3.8/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds)
   1932 
   1933             if len(self.names) < len(usecols):
-> 1934                 _validate_usecols_names(usecols, self.names)
   1935 
   1936         self._validate_parse_dates_presence(self.names)

~/compose/etc/conda/cuda_11.2/envs/rapids/lib/python3.8/site-packages/pandas/io/parsers.py in _validate_usecols_names(usecols, names)
   1160     missing = [c for c in usecols if c not in names]
   1161     if len(missing) > 0:
-> 1162         raise ValueError(
   1163             f""Usecols do not match columns, columns expected but not found: {missing}""
   1164         )

ValueError: Usecols do not match columns, columns expected but not found: [2]

```

**Environment overview (please complete the following information)**
 - Environment location: dgx12
 - Method of cuDF install: from source

**Environment details**
<details><summary>Click here to see environment details</summary><pre>
     
     **git***
     commit 7d892d11736a6cfb0d4bd6109cbe72570379aa02 (HEAD -> branch-21.10, upstream/branch-21.10, origin/branch-21.10, origin/HEAD)
     Author: Ashwin Srinath <3190405+shwina@users.noreply.github.com>
     Date:   Tue Aug 10 14:19:23 2021 -0400
     
     Add groupby first and last aggregations (#9004)
     
     
     
     Authors:
     - Ashwin Srinath (https://github.com/shwina)
     
     Approvers:
     - Sheilah Kirui (https://github.com/skirui-source)
     - Christopher Harris (https://github.com/cwharris)
     - Richard (Rick) Zamora (https://github.com/rjzamora)
     
     URL: https://github.com/rapidsai/cudf/pull/9004
     **git submodules***
     
     ***OS Information***
     DISTRIB_ID=Ubuntu
     DISTRIB_RELEASE=18.04
     DISTRIB_CODENAME=bionic
     DISTRIB_DESCRIPTION=""Ubuntu 18.04.5 LTS""
     NAME=""Ubuntu""
     VERSION=""18.04.5 LTS (Bionic Beaver)""
     ID=ubuntu
     ID_LIKE=debian
     PRETTY_NAME=""Ubuntu 18.04.5 LTS""
     VERSION_ID=""18.04""
     HOME_URL=""https://www.ubuntu.com/""
     SUPPORT_URL=""https://help.ubuntu.com/""
     BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
     PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
     VERSION_CODENAME=bionic
     UBUNTU_CODENAME=bionic
     Linux dgx12 4.15.0-76-generic #86-Ubuntu SMP Fri Jan 17 17:24:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
     
     ***GPU Information***
     Wed Aug 11 10:01:00 2021
     +-----------------------------------------------------------------------------+
     | NVIDIA-SMI 460.39       Driver Version: 460.39       CUDA Version: 11.2     |
     |-------------------------------+----------------------+----------------------+
     | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
     | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
     |                               |                      |               MIG M. |
     |===============================+======================+======================|
     |   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |
     | N/A   33C    P0    55W / 300W |   2804MiB / 32510MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |
     | N/A   33C    P0    57W / 300W |    818MiB / 32510MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |
     | N/A   28C    P0    41W / 300W |      3MiB / 32510MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |
     | N/A   28C    P0    41W / 300W |      3MiB / 32510MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |
     | N/A   30C    P0    42W / 300W |      3MiB / 32510MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |
     | N/A   30C    P0    41W / 300W |      3MiB / 32510MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |
     | N/A   32C    P0    43W / 300W |      3MiB / 32510MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |
     | N/A   29C    P0    41W / 300W |      3MiB / 32510MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     
     +-----------------------------------------------------------------------------+
     | Processes:                                                                  |
     |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
     |        ID   ID                                                   Usage      |
     |=============================================================================|
     +-----------------------------------------------------------------------------+
     
     ***CPU***
     Architecture:        x86_64
     CPU op-mode(s):      32-bit, 64-bit
     Byte Order:          Little Endian
     CPU(s):              80
     On-line CPU(s) list: 0-79
     Thread(s) per core:  2
     Core(s) per socket:  20
     Socket(s):           2
     NUMA node(s):        2
     Vendor ID:           GenuineIntel
     CPU family:          6
     Model:               79
     Model name:          Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz
     Stepping:            1
     CPU MHz:             3391.229
     CPU max MHz:         3600.0000
     CPU min MHz:         1200.0000
     BogoMIPS:            4389.83
     Virtualization:      VT-x
     L1d cache:           32K
     L1i cache:           32K
     L2 cache:            256K
     L3 cache:            51200K
     NUMA node0 CPU(s):   0-19,40-59
     NUMA node1 CPU(s):   20-39,60-79
     Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap intel_pt xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts md_clear flush_l1d
     
     ***CMake***
     /raid/charlesb/dev/rapids/compose/etc/conda/cuda_11.2/envs/rapids/bin/cmake
     cmake version 3.21.1
     
     CMake suite maintained and supported by Kitware (kitware.com/cmake).
     
     ***g++***
     /usr/local/bin/g++
     g++ (Ubuntu 9.4.0-1ubuntu1~18.04) 9.4.0
     Copyright (C) 2019 Free Software Foundation, Inc.
     This is free software; see the source for copying conditions.  There is NO
     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
     
     
     ***nvcc***
     /usr/local/bin/nvcc
     nvcc: NVIDIA (R) Cuda compiler driver
     Copyright (c) 2005-2021 NVIDIA Corporation
     Built on Sun_Feb_14_21:12:58_PST_2021
     Cuda compilation tools, release 11.2, V11.2.152
     Build cuda_11.2.r11.2/compiler.29618528_0
     
     ***Python***
     /raid/charlesb/dev/rapids/compose/etc/conda/cuda_11.2/envs/rapids/bin/python
     Python 3.8.10
     
     ***Environment Variables***
     PATH                            : /raid/charlesb/dev/rapids/compose/etc/conda/cuda_11.2/envs/rapids/bin:/raid/charlesb/dev/rapids/compose/etc/conda/cuda_11.2/condabin:/raid/charlesb/dev/rapids/compose/etc/conda/cuda_11.2/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/cuda/bin
     LD_LIBRARY_PATH                 : /raid/charlesb/dev/rapids/compose/etc/conda/cuda_11.2/envs/rapids/lib:/raid/charlesb/dev/rapids/compose/etc/conda/cuda_11.2/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/i386-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/lib:/raid/charlesb/dev/rapids/rmm/build/release:/raid/charlesb/dev/rapids/cudf/cpp/build/release:/raid/charlesb/dev/rapids/raft/cpp/build/release:/raid/charlesb/dev/rapids/cuml/cpp/build/release:/raid/charlesb/dev/rapids/cugraph/cpp/build/release:/raid/charlesb/dev/rapids/cuspatial/cpp/build/release
     NUMBAPRO_NVVM                   :
     NUMBAPRO_LIBDEVICE              :
     CONDA_PREFIX                    : /raid/charlesb/dev/rapids/compose/etc/conda/cuda_11.2/envs/rapids
     PYTHON_PATH                     :
     
     ***conda packages***
     /raid/charlesb/dev/rapids/compose/etc/conda/cuda_11.2/condabin/conda
     # packages in environment at /raid/charlesb/dev/rapids/compose/etc/conda/cuda_11.2/envs/rapids:
     #
     # Name                    Version                   Build  Channel
     _libgcc_mutex             0.1                 conda_forge    conda-forge
     _openmp_mutex             4.5                      1_llvm    conda-forge
     abseil-cpp                20210324.2           h9c3ff4c_0    conda-forge
     alabaster                 0.7.12                     py_0    conda-forge
     appdirs                   1.4.4              pyh9f0ad1d_0    conda-forge
     argon2-cffi               20.1.0           py38h497a2fe_2    conda-forge
     arrow-cpp                 4.0.1           py38hf0991f3_4_cuda    conda-forge
     arrow-cpp-proc            3.0.0                      cuda    conda-forge
     async_generator           1.10                       py_0    conda-forge
     attrs                     21.2.0             pyhd8ed1ab_0    conda-forge
     aws-c-cal                 0.5.11               h95a6274_0    conda-forge
     aws-c-common              0.6.2                h7f98852_0    conda-forge
     aws-c-event-stream        0.2.7               h3541f99_13    conda-forge
     aws-c-io                  0.10.5               hfb6a706_0    conda-forge
     aws-checksums             0.1.11               ha31a3da_7    conda-forge
     aws-sdk-cpp               1.8.186              hb4091e7_3    conda-forge
     babel                     2.9.1              pyh44b312d_0    conda-forge
     backcall                  0.2.0              pyh9f0ad1d_0    conda-forge
     backports                 1.0                        py_2    conda-forge
     backports.functools_lru_cache 1.6.4              pyhd8ed1ab_0    conda-forge
     black                     19.10b0                  py38_0    conda-forge
     bleach                    3.3.1              pyhd8ed1ab_0    conda-forge
     bokeh                     2.3.3            py38h578d9bd_0    conda-forge
     brotlipy                  0.7.0           py38h497a2fe_1001    conda-forge
     bzip2                     1.0.8                h7f98852_4    conda-forge
     c-ares                    1.17.1               h7f98852_1    conda-forge
     ca-certificates           2021.5.30            ha878542_0    conda-forge
     cachetools                4.2.2              pyhd8ed1ab_0    conda-forge
     certifi                   2021.5.30        py38h578d9bd_0    conda-forge
     cffi                      1.14.6           py38ha65f79e_0    conda-forge
     cfgv                      3.3.0              pyhd8ed1ab_0    conda-forge
     chardet                   4.0.0            py38h578d9bd_1    conda-forge
     charset-normalizer        2.0.0              pyhd8ed1ab_0    conda-forge
     click                     8.0.1            py38h578d9bd_0    conda-forge
     cloudpickle               1.6.0                      py_0    conda-forge
     cmake                     3.21.1               h8897547_0    conda-forge
     cmake-format              0.6.11             pyh9f0ad1d_0    conda-forge
     cmake_setuptools          0.1.3                      py_0    rapidsai
     colorama                  0.4.4              pyh9f0ad1d_0    conda-forge
     commonmark                0.9.1                      py_0    conda-forge
     cryptography              3.4.7            py38ha5dfef3_0    conda-forge
     cudatoolkit               11.2.72              h2bc3f7f_0    nvidia
     cupy                      9.2.0            py38ha69542f_0    conda-forge
     cython                    0.29.24          py38h709712a_0    conda-forge
     cytoolz                   0.11.0           py38h497a2fe_3    conda-forge
     dask                      2021.7.1+6.g03747f2d          pypi_0    pypi
     dataclasses               0.8                pyhc8e2a94_1    conda-forge
     debugpy                   1.4.1            py38h709712a_0    conda-forge
     decorator                 5.0.9              pyhd8ed1ab_0    conda-forge
     defusedxml                0.7.1              pyhd8ed1ab_0    conda-forge
     distlib                   0.3.2              pyhd8ed1ab_0    conda-forge
     distributed               2021.7.1+7.g50fd3ff3          pypi_0    pypi
     dlpack                    0.5                  h9c3ff4c_0    conda-forge
     docutils                  0.16             py38h578d9bd_3    conda-forge
     double-conversion         3.1.5                h9c3ff4c_2    conda-forge
     editdistance-s            1.0.0            py38h1fd1430_1    conda-forge
     entrypoints               0.3             py38h32f6830_1002    conda-forge
     execnet                   1.9.0              pyhd8ed1ab_0    conda-forge
     expat                     2.4.1                h9c3ff4c_0    conda-forge
     fastavro                  1.4.4            py38h497a2fe_0    conda-forge
     fastrlock                 0.6              py38h709712a_1    conda-forge
     filelock                  3.0.12             pyh9f0ad1d_0    conda-forge
     flake8                    3.8.3                      py_1    conda-forge
     freetype                  2.10.4               h0708190_1    conda-forge
     fsspec                    2021.7.0           pyhd8ed1ab_0    conda-forge
     future                    0.18.2           py38h578d9bd_3    conda-forge
     gflags                    2.2.2             he1b5a44_1004    conda-forge
     glog                      0.5.0                h48cff8f_0    conda-forge
     gmp                       6.2.1                h58526e2_0    conda-forge
     grpc-cpp                  1.38.1               h36ce80c_0    conda-forge
     heapdict                  1.0.1                      py_0    conda-forge
     huggingface_hub           0.0.14             pyhd8ed1ab_0    conda-forge
     hypothesis                6.14.5             pyhd8ed1ab_0    conda-forge
     identify                  2.2.11             pyhd8ed1ab_0    conda-forge
     idna                      3.1                pyhd3deb0d_0    conda-forge
     imagesize                 1.2.0                      py_0    conda-forge
     importlib-metadata        4.6.1            py38h578d9bd_0    conda-forge
     importlib_metadata        4.6.1                hd8ed1ab_0    conda-forge
     iniconfig                 1.1.1              pyh9f0ad1d_0    conda-forge
     ipykernel                 6.0.3            py38hd0cf306_0    conda-forge
     ipython                   7.25.0           py38hd0cf306_1    conda-forge
     ipython_genutils          0.2.0                      py_1    conda-forge
     isort                     5.6.4                      py_0    conda-forge
     jbig                      2.1               h7f98852_2003    conda-forge
     jedi                      0.18.0           py38h578d9bd_2    conda-forge
     jinja2                    3.0.1              pyhd8ed1ab_0    conda-forge
     joblib                    1.0.1              pyhd8ed1ab_0    conda-forge
     jpeg                      9d                   h36c2ea0_0    conda-forge
     jsonschema                3.2.0            py38h32f6830_1    conda-forge
     jupyter_client            6.1.12             pyhd8ed1ab_0    conda-forge
     jupyter_core              4.7.1            py38h578d9bd_0    conda-forge
     jupyterlab_pygments       0.1.2              pyh9f0ad1d_0    conda-forge
     krb5                      1.19.1               hcc1bbae_0    conda-forge
     lcms2                     2.12                 hddcbb42_0    conda-forge
     ld_impl_linux-64          2.36.1               hea4e1c9_1    conda-forge
     lerc                      2.2.1                h9c3ff4c_0    conda-forge
     libblas                   3.9.0                     8_mkl    conda-forge
     libbrotlicommon           1.0.9                h7f98852_5    conda-forge
     libbrotlidec              1.0.9                h7f98852_5    conda-forge
     libbrotlienc              1.0.9                h7f98852_5    conda-forge
     libcblas                  3.9.0                     8_mkl    conda-forge
     libcurl                   7.78.0               h2574ce0_0    conda-forge
     libdeflate                1.7                  h7f98852_5    conda-forge
     libedit                   3.1.20191231         he28a2e2_2    conda-forge
     libev                     4.33                 h516909a_1    conda-forge
     libevent                  2.1.10               hcdb4288_3    conda-forge
     libffi                    3.3                  h58526e2_2    conda-forge
     libgcc-ng                 11.1.0               hc902ee8_2    conda-forge
     liblapack                 3.9.0                     8_mkl    conda-forge
     libllvm10                 10.0.1               he513fc3_3    conda-forge
     libnghttp2                1.43.0               h812cca2_0    conda-forge
     libpng                    1.6.37               h21135ba_2    conda-forge
     libprotobuf               3.16.0               h780b84a_0    conda-forge
     libsodium                 1.0.18               h36c2ea0_1    conda-forge
     libssh2                   1.9.0                ha56f1ee_6    conda-forge
     libstdcxx-ng              11.1.0               h56837e0_2    conda-forge
     libthrift                 0.14.2               he6d91bd_1    conda-forge
     libtiff                   4.3.0                hf544144_1    conda-forge
     libutf8proc               2.6.1                h7f98852_0    conda-forge
     libuv                     1.42.0               h7f98852_0    conda-forge
     libwebp-base              1.2.0                h7f98852_2    conda-forge
     llvm-openmp               12.0.1               h4bd325d_1    conda-forge
     llvmlite                  0.36.0           py38h4630a5e_0    conda-forge
     locket                    0.2.0                      py_2    conda-forge
     lz4-c                     1.9.3                h9c3ff4c_0    conda-forge
     markdown                  3.3.4              pyhd8ed1ab_0    conda-forge
     markupsafe                2.0.1            py38h497a2fe_0    conda-forge
     matplotlib-inline         0.1.2              pyhd8ed1ab_2    conda-forge
     mccabe                    0.6.1                      py_1    conda-forge
     mimesis                   4.0.0              pyh9f0ad1d_0    conda-forge
     mistune                   0.8.4           py38h497a2fe_1004    conda-forge
     mkl                       2020.4             h726a3e6_304    conda-forge
     more-itertools            8.8.0              pyhd8ed1ab_0    conda-forge
     msgpack-python            1.0.2            py38h1fd1430_1    conda-forge
     mypy                      0.782                      py_0    conda-forge
     mypy_extensions           0.4.3            py38h578d9bd_3    conda-forge
     nbclient                  0.5.3              pyhd8ed1ab_0    conda-forge
     nbconvert                 6.1.0            py38h578d9bd_0    conda-forge
     nbformat                  5.1.3              pyhd8ed1ab_0    conda-forge
     nbsphinx                  0.8.6              pyhd8ed1ab_1    conda-forge
     ncurses                   6.2                  h58526e2_4    conda-forge
     nest-asyncio              1.5.1              pyhd8ed1ab_0    conda-forge
     ninja                     1.10.2               h4bd325d_0    conda-forge
     nodeenv                   1.6.0              pyhd8ed1ab_0    conda-forge
     notebook                  6.4.0              pyha770c72_0    conda-forge
     numba                     0.53.1           py38h8b71fd7_1    conda-forge
     numpy                     1.21.1           py38h9894fe3_0    conda-forge
     numpydoc                  1.1.0                      py_1    conda-forge
     nvtx                      0.2.3            py38h497a2fe_0    conda-forge
     olefile                   0.46               pyh9f0ad1d_1    conda-forge
     openjpeg                  2.4.0                hb52868f_1    conda-forge
     openssl                   1.1.1k               h7f98852_0    conda-forge
     orc                       1.6.9                h58a87f1_0    conda-forge
     packaging                 21.0               pyhd8ed1ab_0    conda-forge
     pandas                    1.2.5            py38h1abd341_0    conda-forge
     pandoc                    1.19.2                        0    conda-forge
     pandocfilters             1.4.2                      py_1    conda-forge
     parquet-cpp               1.5.1                         1    conda-forge
     parso                     0.8.2              pyhd8ed1ab_0    conda-forge
     partd                     1.2.0              pyhd8ed1ab_0    conda-forge
     pathspec                  0.9.0              pyhd8ed1ab_0    conda-forge
     pexpect                   4.8.0            py38h32f6830_1    conda-forge
     pickleshare               0.7.5           py38h32f6830_1002    conda-forge
     pillow                    8.3.1            py38h8e6f84c_0    conda-forge
     pip                       21.2.1             pyhd8ed1ab_0    conda-forge
     pluggy                    0.13.1           py38h578d9bd_4    conda-forge
     pre-commit                2.13.0           py38h578d9bd_0    conda-forge
     pre_commit                2.13.0               hd8ed1ab_0    conda-forge
     prometheus_client         0.11.0             pyhd8ed1ab_0    conda-forge
     prompt-toolkit            3.0.19             pyha770c72_0    conda-forge
     protobuf                  3.16.0           py38h709712a_0    conda-forge
     psutil                    5.8.0            py38h497a2fe_1    conda-forge
     ptvsd                     4.3.2                    pypi_0    pypi
     ptyprocess                0.7.0              pyhd3deb0d_0    conda-forge
     py                        1.10.0             pyhd3deb0d_0    conda-forge
     py-cpuinfo                8.0.0              pyhd8ed1ab_0    conda-forge
     pyarrow                   4.0.1           py38hb53058b_4_cuda    conda-forge
     pycodestyle               2.6.0              pyh9f0ad1d_0    conda-forge
     pycparser                 2.20               pyh9f0ad1d_2    conda-forge
     pyflakes                  2.2.0              pyh9f0ad1d_0    conda-forge
     pygments                  2.9.0              pyhd8ed1ab_0    conda-forge
     pyopenssl                 20.0.1             pyhd8ed1ab_0    conda-forge
     pyorc                     0.4.0                    pypi_0    pypi
     pyparsing                 2.4.7              pyh9f0ad1d_0    conda-forge
     pyrsistent                0.17.3           py38h497a2fe_2    conda-forge
     pysocks                   1.7.1            py38h578d9bd_3    conda-forge
     pytest                    6.2.4            py38h578d9bd_0    conda-forge
     pytest-benchmark          3.4.1              pyhd8ed1ab_0    conda-forge
     pytest-forked             1.3.0              pyhd3deb0d_0    conda-forge
     pytest-xdist              2.3.0              pyhd8ed1ab_0    conda-forge
     python                    3.8.10          h49503c6_1_cpython    conda-forge
     python-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge
     python_abi                3.8                      2_cp38    conda-forge
     pytorch                   1.9.0           cpu_py38h91ab35c_0    conda-forge
     pytz                      2021.1             pyhd8ed1ab_0    conda-forge
     pyyaml                    5.4.1            py38h497a2fe_0    conda-forge
     pyzmq                     22.1.0           py38h2035c66_0    conda-forge
     rapidjson                 1.1.0             he1b5a44_1002    conda-forge
     re2                       2021.06.01           h9c3ff4c_0    conda-forge
     readline                  8.1                  h46c0cb4_0    conda-forge
     recommonmark              0.7.1              pyhd8ed1ab_0    conda-forge
     regex                     2021.7.6         py38h497a2fe_0    conda-forge
     requests                  2.26.0             pyhd8ed1ab_0    conda-forge
     rhash                     1.4.1                h7f98852_0    conda-forge
     s2n                       1.0.10               h9b69904_0    conda-forge
     sacremoses                0.0.43             pyh9f0ad1d_0    conda-forge
     send2trash                1.7.1              pyhd8ed1ab_0    conda-forge
     setuptools                49.6.0           py38h578d9bd_3    conda-forge
     six                       1.16.0             pyh6c4a22f_0    conda-forge
     sleef                     3.5.1                h7f98852_1    conda-forge
     snappy                    1.1.8                he1b5a44_3    conda-forge
     snowballstemmer           2.1.0              pyhd8ed1ab_0    conda-forge
     sortedcontainers          2.4.0              pyhd8ed1ab_0    conda-forge
     spdlog                    1.8.5                h4bd325d_0    conda-forge
     sphinx                    4.1.2              pyh6c4a22f_1    conda-forge
     sphinx-copybutton         0.4.0              pyhd8ed1ab_0    conda-forge
     sphinx-markdown-tables    0.0.15             pyhd3deb0d_0    conda-forge
     sphinx_rtd_theme          0.5.2              pyhd8ed1ab_1    conda-forge
     sphinxcontrib-applehelp   1.0.2                      py_0    conda-forge
     sphinxcontrib-devhelp     1.0.2                      py_0    conda-forge
     sphinxcontrib-htmlhelp    2.0.0              pyhd8ed1ab_0    conda-forge
     sphinxcontrib-jsmath      1.0.1                      py_0    conda-forge
     sphinxcontrib-qthelp      1.0.3                      py_0    conda-forge
     sphinxcontrib-serializinghtml 1.1.5              pyhd8ed1ab_0    conda-forge
     sphinxcontrib-websupport  1.2.4              pyh9f0ad1d_0    conda-forge
     sqlite                    3.36.0               h9cd32fc_0    conda-forge
     streamz                   0.6.2              pyh44b312d_0    conda-forge
     tblib                     1.7.0              pyhd8ed1ab_0    conda-forge
     terminado                 0.10.1           py38h578d9bd_0    conda-forge
     testpath                  0.5.0              pyhd8ed1ab_0    conda-forge
     tk                        8.6.10               h21135ba_1    conda-forge
     tokenizers                0.10.1           py38hb63a372_0    conda-forge
     toml                      0.10.2             pyhd8ed1ab_0    conda-forge
     toolz                     0.11.1                     py_0    conda-forge
     tornado                   6.1              py38h497a2fe_1    conda-forge
     tqdm                      4.61.2             pyhd8ed1ab_1    conda-forge
     traitlets                 5.0.5                      py_0    conda-forge
     transformers              4.9.0              pyhd8ed1ab_0    conda-forge
     typed-ast                 1.4.3            py38h497a2fe_0    conda-forge
     typing-extensions         3.10.0.0             hd8ed1ab_0    conda-forge
     typing_extensions         3.10.0.0           pyha770c72_0    conda-forge
     urllib3                   1.26.6             pyhd8ed1ab_0    conda-forge
     virtualenv                20.4.7           py38h578d9bd_0    conda-forge
     wcwidth                   0.2.5              pyh9f0ad1d_2    conda-forge
     webencodings              0.5.1                      py_1    conda-forge
     wheel                     0.36.2             pyhd3deb0d_0    conda-forge
     xz                        5.2.5                h516909a_1    conda-forge
     yaml                      0.2.5                h516909a_0    conda-forge
     zeromq                    4.3.4                h9c3ff4c_0    conda-forge
     zict                      2.0.0                      py_0    conda-forge
     zipp                      3.5.0              pyhd8ed1ab_0    conda-forge
     zlib                      1.2.11            h516909a_1010    conda-forge
     zstd                      1.5.0                ha95c52a_0    conda-forge
     
</pre></details>

**Additional context**
I encountered this bug while looking into #8973
",2021-08-11T17:01:48Z,0,0,Charles Blackmon-Luca,@rapidsai,True
175,Template function docstrings,"Many cuDF methods have very similar docstrings, including when the same method is present between different classes, but we currently reproduce most of those docstrings. This increases maintenance burden and the chances of docstrings going out of date. As we work to consolidate more logic a la #9038 we should also consider using more programmatic approaches to share docstrings between related functions, something that `pandas` does extensively (this has come up in a few of the PRs that already started on the work in #9038).

The main downside to such an approach is that it incurs import time costs that will potentially make `import cudf` slower. Before diving too deep into trying any such approach, we should verify that the additional import time is not prohibitively slow. I'm reasonably confident that the additional cost will be negligible at the moment, though, because currently importing `cupy` and `numba` and setting up the `rmm` allocator for these account for >90% of the import time. I suspect that unless and until those import times drop, any docstring-related logic that we inject at import time will have a relatively insignificant effect in comparison.",2021-08-16T23:46:47Z,0,0,Vyas Ramasubramani,@rapidsai,True
176,[FEA] Expanding window functions in libcudf/cuDF,"**Is your feature request related to a problem? Please describe.**
Related to https://github.com/rapidsai/cudf/issues/1263, we could use support for rolling functions in the unweighted expanding window case - e.g. functions for which each point in the result sequence is dependent on all of the data 'before' it in the source sequence. 

https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.expanding.html

- count
- sum
- mean
- median
- var
- std
- min
- max
- corr
- cov
- skew
- kurt
- apply

Since many of these aggregations can be expressed as a recurrence relation they might be computable using the same machinery already being developed for `ewm`. 


**Describe the solution you'd like**
Implement expanding windows in cudf/libcudf that produce expanding window scans of the data. 

**Describe alternatives you've considered**
Wrap rolling machinery with `window_size=len(data)`, which would likely be inefficient. 

**Additional context**
Add any other context, code examples, or references to existing implementations about the feature request here.
",2021-08-24T14:36:17Z,1,0,,NVIDIA,True
177,[FEA] Add a `cudf.api.indexers.BaseIndexer` class equivalent to `pd.api.indexer.BaseIndexer`,"**Is your feature request related to a problem? Please describe.**
For context: https://github.com/rapidsai/cudf/pull/9085#discussion_r695093462
We will have to provide a class equivalent to `BaseIndexer` in `cudf`, we currently use the pandas `BaseIndexer` in our code.

Note :
As part of this FEA, we will have to add a dispatch mechanism in `dask` & probably figure out how to dispatch correctly in `dask-sql`",2021-08-24T18:32:31Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
178,[FEA] Add support for lists and lists of nested types in serial hash functions,"**Is your feature request related to a problem? Please describe.**
Support lists of lists and multiple nested lists + structs in serial mumur3 and md5 hash functions.

**Describe the solution you'd like**
Currently lists are supported, but not lists of lists or lists containing structs. Expand and improve the current implementation to handle these complex nested types.

**Additional context**
Would rely on #9119 in some way, both are part of a larger effort of supporting nested lists/structs better.",2021-08-25T22:54:19Z,0,0,Ryan Lee,NVIDIA,True
179,[FEA] Filter join probe table rows that contain nulls when nulls are not equal,"**Is your feature request related to a problem? Please describe.**

In the hash join implementation, we allow controlling the behavior of whether two null elements are considered equal. If nulls are _not_ equal, then two rows that contain nulls can never be considered equal. We exploit this in the build phase of the hash join by constructing a bitmask from ANDing the bitmasks from all the input columns and using that ""row bitmask"" to filter out any rows that contain a null element from being inserted into the hash table. 

When it comes time to probe the hash table, we do not currently take advantage of this some optimization.

**Describe the solution you'd like**

When probing the hash map and nulls are considered not equal, we should build bitmask from ANDing all of the bitmasks of the probe table and only probe the map when a row does not contain any nulls. 

**Additional context**

The hash join implementation is currently going a complete refactor to use the cuCollections `static_multimap`. To support the filtered insert, we added an `insert_if` function. I think we can also add a `retrieve_if` function to support the filtered probing.
",2021-08-31T15:23:59Z,0,0,Jake Hemstad,@NVIDIA,True
180,[FEA] Support multiple sources/destinations for gather and scatter,"Currently, we have the `gather` and `scatter` APIs that perform on a single source table. Sometimes, we may want to operate on multiple sources/destinations so it would be great if we can support that. In such cases, the input iterators into these APIs would be a zip iterator or a pair of iterators pointing to `{column_index, row_index}` of the source/destination tables (https://github.com/rapidsai/cudf/pull/9130#discussion_r701405591). ",2021-09-02T21:03:07Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
181,[BUG] Unable to create a struct column from an arrow dictionary array,"**Describe the bug**
We currently support converting a `category` column in `cudf` to a `DictionaryArray` in `pyarrow`:

```python
>>> import cudf
>>> s = cudf.Series([1, 2, 3], dtype='category')
>>> s.to_arrow()
<pyarrow.lib.DictionaryArray object at 0x7f7980299ba0>

-- dictionary:
  [
    1,
    2,
    3
  ]
-- indices:
  [
    0,
    1,
    2
  ]
>>> s.to_arrow().type
DictionaryType(dictionary<values=int64, indices=int8, ordered=0>)
```

But we don't seem to be supporting the construction of `StructDtype` & `StructColumn` from our constructors:
```python
>>> import pyarrow as pa
>>> f = pa.array([1, 2, 3])
>>> pa.StructArray.from_arrays([s.to_arrow(), f], names=['a', 'd']).type
StructType(struct<a: dictionary<values=int64, indices=int8, ordered=0>, d: int64>)
>>> pa.StructArray.from_arrays([s.to_arrow(), f], names=['a', 'd'])
<pyarrow.lib.StructArray object at 0x7f7980055760>
-- is_valid: all not null
-- child 0 type: dictionary<values=int64, indices=int8, ordered=0>

  -- dictionary:
    [
      1,
      2,
      3
    ]
  -- indices:
    [
      0,
      1,
      2
    ]
-- child 1 type: int64
  [
    1,
    2,
    3
  ]
>>> cudf.Series.from_arrow(pa.StructArray.from_arrays([s.to_arrow(), f], names=['a', 'd']))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/nvme/0/pgali/envs/cudfdev/lib/python3.8/site-packages/cudf/core/series.py"", line 1111, in __repr__
    preprocess = self.copy()
  File ""/nvme/0/pgali/envs/cudfdev/lib/python3.8/site-packages/cudf/core/frame.py"", line 317, in copy
    new_frame._data = self._data.copy(deep=deep)
  File ""/nvme/0/pgali/envs/cudfdev/lib/python3.8/site-packages/cudf/core/column_accessor.py"", line 315, in copy
    {k: v.copy(deep=True) for k, v in self._data.items()},
  File ""/nvme/0/pgali/envs/cudfdev/lib/python3.8/site-packages/cudf/core/column_accessor.py"", line 315, in <dictcomp>
    {k: v.copy(deep=True) for k, v in self._data.items()},
  File ""/nvme/0/pgali/envs/cudfdev/lib/python3.8/site-packages/cudf/core/column/struct.py"", line 102, in copy
    result = super().copy(deep=deep)
  File ""/nvme/0/pgali/envs/cudfdev/lib/python3.8/site-packages/cudf/core/column/column.py"", line 419, in copy
    return cast(T, result._with_type_metadata(self.dtype))
  File ""/nvme/0/pgali/envs/cudfdev/lib/python3.8/site-packages/cudf/core/column/struct.py"", line 134, in _with_type_metadata
    names=dtype.fields.keys(),
  File ""/nvme/0/pgali/envs/cudfdev/lib/python3.8/site-packages/cudf/core/dtypes.py"", line 289, in fields
    return {
  File ""/nvme/0/pgali/envs/cudfdev/lib/python3.8/site-packages/cudf/core/dtypes.py"", line 290, in <dictcomp>
    field.name: cudf.utils.dtypes.cudf_dtype_from_pa_type(field.type)
  File ""/nvme/0/pgali/envs/cudfdev/lib/python3.8/site-packages/cudf/utils/dtypes.py"", line 233, in cudf_dtype_from_pa_type
    return pandas_dtype(typ.to_pandas_dtype())
  File ""pyarrow/types.pxi"", line 200, in pyarrow.lib.DataType.to_pandas_dtype
NotImplementedError: dictionary<values=int64, indices=int8, ordered=0>
>>> s
0    1
1    2
2    3
dtype: category
Categories (3, int64): [1, 2, 3]
>>> s.to_frame()
   0
0  1
1  2
2  3
>>> s.to_frame().to_struct()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/nvme/0/pgali/envs/cudfdev/lib/python3.8/site-packages/cudf/core/dataframe.py"", line 7001, in to_struct
    col = cudf.core.column.build_struct_column(
  File ""/nvme/0/pgali/envs/cudfdev/lib/python3.8/site-packages/cudf/core/column/column.py"", line 1655, in build_struct_column
    dtype = StructDtype(
  File ""/nvme/0/pgali/envs/cudfdev/lib/python3.8/site-packages/cudf/core/dtypes.py"", line 281, in __init__
    pa_fields = {
  File ""/nvme/0/pgali/envs/cudfdev/lib/python3.8/site-packages/cudf/core/dtypes.py"", line 282, in <dictcomp>
    k: cudf.utils.dtypes.cudf_dtype_to_pa_type(v)
  File ""/nvme/0/pgali/envs/cudfdev/lib/python3.8/site-packages/cudf/utils/dtypes.py"", line 211, in cudf_dtype_to_pa_type
    raise NotImplementedError()
NotImplementedError
```

**Steps/Code to reproduce bug**
Follow this guide http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports to craft a minimal bug report. This helps us reproduce the issue you're having and resolve the issue more quickly.



**Environment overview (please complete the following information)**
 - Environment location: [Bare-metal]
 - Method of cuDF install: [from source]

**Environment details**
Please run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details
<details><summary>Click here to see environment details</summary><pre>
     
     **git***
     commit 8d602b7e99c187db7063529def92a5d343a53a54 (HEAD -> 7618, upstream/branch-21.10, branch-21.10)
     Author: shaneding <shane200195@gmail.com>
     Date:   Fri Sep 3 18:09:10 2021 -0700
     
     Implemented bindings for `ceil` timestamp operation (#9141)
     
     Closes #8682. Added python bindings for `ceil` operation implemented in #8942.
     
     Authors:
     - https://github.com/shaneding
     
     Approvers:
     - Ashwin Srinath (https://github.com/shwina)
     - https://github.com/brandon-b-miller
     
     URL: https://github.com/rapidsai/cudf/pull/9141
     **git submodules***
     
     ***OS Information***
     DISTRIB_ID=Ubuntu
     DISTRIB_RELEASE=18.04
     DISTRIB_CODENAME=bionic
     DISTRIB_DESCRIPTION=""Ubuntu 18.04.4 LTS""
     NAME=""Ubuntu""
     VERSION=""18.04.4 LTS (Bionic Beaver)""
     ID=ubuntu
     ID_LIKE=debian
     PRETTY_NAME=""Ubuntu 18.04.4 LTS""
     VERSION_ID=""18.04""
     HOME_URL=""https://www.ubuntu.com/""
     SUPPORT_URL=""https://help.ubuntu.com/""
     BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
     PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
     VERSION_CODENAME=bionic
     UBUNTU_CODENAME=bionic
     Linux dt07 4.15.0-76-generic #86-Ubuntu SMP Fri Jan 17 17:24:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
     
     ***GPU Information***
     Sat Sep  4 14:32:58 2021
     +-----------------------------------------------------------------------------+
     | NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |
     |-------------------------------+----------------------+----------------------+
     | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
     | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
     |                               |                      |               MIG M. |
     |===============================+======================+======================|
     |   0  Tesla T4            On   | 00000000:3B:00.0 Off |                    0 |
     | N/A   50C    P0    28W /  70W |    384MiB / 15109MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   1  Tesla T4            On   | 00000000:5E:00.0 Off |                    0 |
     | N/A   37C    P8     9W /  70W |      3MiB / 15109MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   2  Tesla T4            On   | 00000000:AF:00.0 Off |                    0 |
     | N/A   32C    P8    11W /  70W |      3MiB / 15109MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   3  Tesla T4            On   | 00000000:D8:00.0 Off |                    0 |
     | N/A   32C    P8     9W /  70W |      3MiB / 15109MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     
     +-----------------------------------------------------------------------------+
     | Processes:                                                                  |
     |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
     |        ID   ID                                                   Usage      |
     |=============================================================================|
     |    0   N/A  N/A     58965      C   ...i/envs/cudfdev/bin/python      381MiB |
     +-----------------------------------------------------------------------------+
     
     ***CPU***
     Architecture:        x86_64
     CPU op-mode(s):      32-bit, 64-bit
     Byte Order:          Little Endian
     CPU(s):              64
     On-line CPU(s) list: 0-63
     Thread(s) per core:  2
     Core(s) per socket:  16
     Socket(s):           2
     NUMA node(s):        2
     Vendor ID:           GenuineIntel
     CPU family:          6
     Model:               85
     Model name:          Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz
     Stepping:            4
     CPU MHz:             2076.027
     BogoMIPS:            4200.00
     Virtualization:      VT-x
     L1d cache:           32K
     L1i cache:           32K
     L2 cache:            1024K
     L3 cache:            22528K
     NUMA node0 CPU(s):   0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62
     NUMA node1 CPU(s):   1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63
     Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts pku ospke md_clear flush_l1d
     
     ***CMake***
     /nvme/0/pgali/envs/cudfdev/bin/cmake
     cmake version 3.21.2
     
     CMake suite maintained and supported by Kitware (kitware.com/cmake).
     
     ***g++***
     /usr/bin/g++
     g++ (Ubuntu 9.4.0-1ubuntu1~18.04) 9.4.0
     Copyright (C) 2019 Free Software Foundation, Inc.
     This is free software; see the source for copying conditions.  There is NO
     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
     
     
     ***nvcc***
     /usr/local/cuda/bin/nvcc
     nvcc: NVIDIA (R) Cuda compiler driver
     Copyright (c) 2005-2021 NVIDIA Corporation
     Built on Sun_Feb_14_21:12:58_PST_2021
     Cuda compilation tools, release 11.2, V11.2.152
     Build cuda_11.2.r11.2/compiler.29618528_0
     
     ***Python***
     /nvme/0/pgali/envs/cudfdev/bin/python
     Python 3.8.10
     
     ***Environment Variables***
     PATH                            : /nvme/0/pgali/envs/cudfdev/bin:/nvme/0/pgali/envs/cudfdev/bin:/nvme/0/pgali/envs/cudfdev/bin:/usr/share/swift/usr/bin:/home/nfs/pgali/bin:/home/nfs/pgali/.local/bin:/home/nfs/pgali/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/lib/jvm/default-java/bin:/usr/share/sbt-launcher-packaging/bin/sbt-launch.jar/bin:/usr/lib/spark/bin:/usr/lib/spark/sbin:/usr/local/cuda/bin:/usr/local/cuda/bin
     LD_LIBRARY_PATH                 : /usr/local/cuda/lib64:/usr/local/cuda/lib64::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64
     NUMBAPRO_NVVM                   :
     NUMBAPRO_LIBDEVICE              :
     CONDA_PREFIX                    : /nvme/0/pgali/envs/cudfdev
     PYTHON_PATH                     :
     
     ***conda packages***
     /home/nfs/pgali/anaconda3/condabin/conda
     # packages in environment at /nvme/0/pgali/envs/cudfdev:
     #
     # Name                    Version                   Build  Channel
     _libgcc_mutex             0.1                 conda_forge    conda-forge
     _openmp_mutex             4.5                      1_llvm    conda-forge
     abseil-cpp                20210324.2           h9c3ff4c_0    conda-forge
     alabaster                 0.7.12                     py_0    conda-forge
     appdirs                   1.4.4              pyh9f0ad1d_0    conda-forge
     argon2-cffi               20.1.0           py38h497a2fe_2    conda-forge
     arrow-cpp                 5.0.0           py38h327e1ba_4_cuda    conda-forge
     arrow-cpp-proc            3.0.0                      cuda    conda-forge
     async_generator           1.10                       py_0    conda-forge
     attrs                     21.2.0             pyhd8ed1ab_0    conda-forge
     aws-c-cal                 0.5.11               h95a6274_0    conda-forge
     aws-c-common              0.6.2                h7f98852_0    conda-forge
     aws-c-event-stream        0.2.7               h3541f99_13    conda-forge
     aws-c-io                  0.10.5               hfb6a706_0    conda-forge
     aws-checksums             0.1.11               ha31a3da_7    conda-forge
     aws-sdk-cpp               1.8.186              hb4091e7_3    conda-forge
     babel                     2.9.1              pyh44b312d_0    conda-forge
     backcall                  0.2.0              pyh9f0ad1d_0    conda-forge
     backports                 1.0                        py_2    conda-forge
     backports.functools_lru_cache 1.6.4              pyhd8ed1ab_0    conda-forge
     beautifulsoup4            4.9.3              pyhb0f4dca_0    conda-forge
     binutils_impl_linux-64    2.36.1               h193b22a_2    conda-forge
     black                     19.10b0                    py_4    conda-forge
     bleach                    4.1.0              pyhd8ed1ab_0    conda-forge
     bokeh                     2.3.3            py38h578d9bd_0    conda-forge
     brotlipy                  0.7.0           py38h497a2fe_1001    conda-forge
     bzip2                     1.0.8                h7f98852_4    conda-forge
     c-ares                    1.17.2               h7f98852_0    conda-forge
     ca-certificates           2021.5.30            ha878542_0    conda-forge
     cachetools                4.2.2              pyhd8ed1ab_0    conda-forge
     certifi                   2021.5.30        py38h578d9bd_0    conda-forge
     cffi                      1.14.6           py38ha65f79e_0    conda-forge
     cfgv                      3.3.1              pyhd8ed1ab_0    conda-forge
     chardet                   4.0.0            py38h578d9bd_1    conda-forge
     clang                     11.0.0               ha770c72_2    conda-forge
     clang-11                  11.0.0          default_ha5c780c_2    conda-forge
     clang-tools               11.0.0          default_ha5c780c_2    conda-forge
     clangxx                   11.0.0          default_ha5c780c_2    conda-forge
     click                     8.0.1            py38h578d9bd_0    conda-forge
     cloudpickle               1.6.0                      py_0    conda-forge
     cmake                     3.21.2               h8897547_0    conda-forge
     cmake_setuptools          0.1.3                      py_0    rapidsai
     colorama                  0.4.4              pyh9f0ad1d_0    conda-forge
     commonmark                0.9.1                      py_0    conda-forge
     cryptography              3.4.7            py38ha5dfef3_0    conda-forge
     cudatoolkit               11.2.72              h2bc3f7f_0    nvidia
     cudf                      21.10.0a0+267.g8d602b7e99.dirty          pypi_0    pypi
     cupy                      9.4.0            py38h7818112_0    conda-forge
     cython                    0.29.24          py38h709712a_0    conda-forge
     cytoolz                   0.11.0           py38h497a2fe_3    conda-forge
     dask                      2021.9.0           pyhd8ed1ab_0    conda-forge
     dask-core                 2021.9.0           pyhd8ed1ab_0    conda-forge
     dask-cudf                 21.10.0a0+267.g8d602b7e99.dirty          pypi_0    pypi
     dataclasses               0.8                pyhc8e2a94_3    conda-forge
     debugpy                   1.4.1            py38h709712a_0    conda-forge
     decorator                 5.0.9              pyhd8ed1ab_0    conda-forge
     defusedxml                0.7.1              pyhd8ed1ab_0    conda-forge
     distlib                   0.3.2              pyhd8ed1ab_0    conda-forge
     distributed               2021.9.0         py38h578d9bd_0    conda-forge
     dlpack                    0.5                  h9c3ff4c_0    conda-forge
     docutils                  0.16             py38h578d9bd_3    conda-forge
     double-conversion         3.1.5                h9c3ff4c_2    conda-forge
     editdistance-s            1.0.0            py38h1fd1430_1    conda-forge
     entrypoints               0.3             pyhd8ed1ab_1003    conda-forge
     execnet                   1.9.0              pyhd8ed1ab_0    conda-forge
     expat                     2.4.1                h9c3ff4c_0    conda-forge
     fastavro                  1.4.4            py38h497a2fe_0    conda-forge
     fastrlock                 0.6              py38h709712a_1    conda-forge
     filelock                  3.0.12             pyh9f0ad1d_0    conda-forge
     flake8                    3.8.3                      py_1    conda-forge
     freetype                  2.10.4               h0708190_1    conda-forge
     fsspec                    2021.8.1           pyhd8ed1ab_0    conda-forge
     future                    0.18.2           py38h578d9bd_3    conda-forge
     gcc_impl_linux-64         11.1.0               h6b5115b_8    conda-forge
     gflags                    2.2.2             he1b5a44_1004    conda-forge
     glog                      0.5.0                h48cff8f_0    conda-forge
     gmp                       6.2.1                h58526e2_0    conda-forge
     grpc-cpp                  1.39.1               h850795e_1    conda-forge
     heapdict                  1.0.1                      py_0    conda-forge
     huggingface_hub           0.0.16             pyhd8ed1ab_0    conda-forge
     hypothesis                6.17.4             pyhd8ed1ab_0    conda-forge
     identify                  2.2.13             pyhd8ed1ab_0    conda-forge
     idna                      2.10               pyh9f0ad1d_0    conda-forge
     imagesize                 1.2.0                      py_0    conda-forge
     importlib-metadata        4.8.1            py38h578d9bd_0    conda-forge
     importlib_metadata        4.8.1                hd8ed1ab_0    conda-forge
     iniconfig                 1.1.1              pyh9f0ad1d_0    conda-forge
     ipykernel                 6.3.1            py38he5a9106_0    conda-forge
     ipython                   7.27.0           py38he5a9106_0    conda-forge
     ipython_genutils          0.2.0                      py_1    conda-forge
     isort                     5.6.4                      py_0    conda-forge
     jbig                      2.1               h7f98852_2003    conda-forge
     jedi                      0.18.0           py38h578d9bd_2    conda-forge
     jinja2                    3.0.1              pyhd8ed1ab_0    conda-forge
     joblib                    1.0.1              pyhd8ed1ab_0    conda-forge
     jpeg                      9d                   h36c2ea0_0    conda-forge
     jsonschema                3.2.0              pyhd8ed1ab_3    conda-forge
     jupyter_client            7.0.2              pyhd8ed1ab_0    conda-forge
     jupyter_core              4.7.1            py38h578d9bd_0    conda-forge
     jupyterlab_pygments       0.1.2              pyh9f0ad1d_0    conda-forge
     kernel-headers_linux-64   2.6.32              he073ed8_14    conda-forge
     krb5                      1.19.2               hcc1bbae_0    conda-forge
     lcms2                     2.12                 hddcbb42_0    conda-forge
     ld_impl_linux-64          2.36.1               hea4e1c9_2    conda-forge
     lerc                      2.2.1                h9c3ff4c_0    conda-forge
     libblas                   3.9.0                     8_mkl    conda-forge
     libbrotlicommon           1.0.9                h7f98852_5    conda-forge
     libbrotlidec              1.0.9                h7f98852_5    conda-forge
     libbrotlienc              1.0.9                h7f98852_5    conda-forge
     libcblas                  3.9.0                     8_mkl    conda-forge
     libclang-cpp11            11.0.0          default_ha5c780c_2    conda-forge
     libcurl                   7.78.0               h2574ce0_0    conda-forge
     libdeflate                1.7                  h7f98852_5    conda-forge
     libedit                   3.1.20191231         he28a2e2_2    conda-forge
     libev                     4.33                 h516909a_1    conda-forge
     libevent                  2.1.10               hcdb4288_3    conda-forge
     libffi                    3.3                  h58526e2_2    conda-forge
     libgcc-devel_linux-64     11.1.0               h80e7780_8    conda-forge
     libgcc-ng                 11.1.0               hc902ee8_8    conda-forge
     libgomp                   11.1.0               hc902ee8_8    conda-forge
     liblapack                 3.9.0                     8_mkl    conda-forge
     libllvm10                 10.0.1               he513fc3_3    conda-forge
     libllvm11                 11.0.1               hf817b99_0    conda-forge
     libnghttp2                1.43.0               h812cca2_0    conda-forge
     libpng                    1.6.37               h21135ba_2    conda-forge
     libprotobuf               3.16.0               h780b84a_0    conda-forge
     librmm                    21.10.00a210904 cuda11.2_g8527317_28    rapidsai-nightly
     libsanitizer              11.1.0               h56837e0_8    conda-forge
     libsodium                 1.0.18               h36c2ea0_1    conda-forge
     libssh2                   1.10.0               ha56f1ee_0    conda-forge
     libstdcxx-ng              11.1.0               h56837e0_8    conda-forge
     libthrift                 0.14.2               he6d91bd_1    conda-forge
     libtiff                   4.3.0                hf544144_1    conda-forge
     libutf8proc               2.6.1                h7f98852_0    conda-forge
     libuv                     1.42.0               h7f98852_0    conda-forge
     libwebp-base              1.2.1                h7f98852_0    conda-forge
     llvm-openmp               12.0.1               h4bd325d_1    conda-forge
     llvmlite                  0.36.0           py38h4630a5e_0    conda-forge
     locket                    0.2.0                      py_2    conda-forge
     lz4-c                     1.9.3                h9c3ff4c_1    conda-forge
     markdown                  3.3.4              pyhd8ed1ab_0    conda-forge
     markupsafe                2.0.1            py38h497a2fe_0    conda-forge
     matplotlib-inline         0.1.2              pyhd8ed1ab_2    conda-forge
     mccabe                    0.6.1                      py_1    conda-forge
     mimesis                   4.0.0              pyh9f0ad1d_0    conda-forge
     mistune                   0.8.4           py38h497a2fe_1004    conda-forge
     mkl                       2020.4             h726a3e6_304    conda-forge
     more-itertools            8.9.0              pyhd8ed1ab_0    conda-forge
     msgpack-python            1.0.2            py38h1fd1430_1    conda-forge
     mypy                      0.782                      py_0    conda-forge
     mypy_extensions           0.4.3            py38h578d9bd_3    conda-forge
     nbclient                  0.5.4              pyhd8ed1ab_0    conda-forge
     nbconvert                 6.1.0            py38h578d9bd_0    conda-forge
     nbformat                  5.1.3              pyhd8ed1ab_0    conda-forge
     nbsphinx                  0.8.7              pyhd8ed1ab_0    conda-forge
     ncurses                   6.2                  h58526e2_4    conda-forge
     nest-asyncio              1.5.1              pyhd8ed1ab_0    conda-forge
     ninja                     1.10.2               h4bd325d_0    conda-forge
     nodeenv                   1.6.0              pyhd8ed1ab_0    conda-forge
     notebook                  6.4.3              pyha770c72_0    conda-forge
     numba                     0.53.1           py38h8b71fd7_1    conda-forge
     numpy                     1.21.2           py38he2449b9_0    conda-forge
     numpydoc                  1.1.0                      py_1    conda-forge
     nvtx                      0.2.3            py38h497a2fe_0    conda-forge
     olefile                   0.46               pyh9f0ad1d_1    conda-forge
     openjpeg                  2.4.0                hb52868f_1    conda-forge
     openssl                   1.1.1l               h7f98852_0    conda-forge
     orc                       1.6.10               h58a87f1_0    conda-forge
     packaging                 21.0               pyhd8ed1ab_0    conda-forge
     pandas                    1.2.5            py38h1abd341_0    conda-forge
     pandoc                    1.19.2                        0    conda-forge
     pandocfilters             1.4.2                      py_1    conda-forge
     parquet-cpp               1.5.1                         2    conda-forge
     parso                     0.8.2              pyhd8ed1ab_0    conda-forge
     partd                     1.2.0              pyhd8ed1ab_0    conda-forge
     pathspec                  0.9.0              pyhd8ed1ab_0    conda-forge
     pexpect                   4.8.0              pyh9f0ad1d_2    conda-forge
     pickleshare               0.7.5                   py_1003    conda-forge
     pillow                    8.3.2            py38h8e6f84c_0    conda-forge
     pip                       21.2.4             pyhd8ed1ab_0    conda-forge
     pluggy                    0.13.1           py38h578d9bd_4    conda-forge
     pre-commit                2.15.0           py38h578d9bd_0    conda-forge
     pre_commit                2.15.0               hd8ed1ab_0    conda-forge
     prometheus_client         0.11.0             pyhd8ed1ab_0    conda-forge
     prompt-toolkit            3.0.20             pyha770c72_0    conda-forge
     protobuf                  3.16.0           py38h709712a_0    conda-forge
     psutil                    5.8.0            py38h497a2fe_1    conda-forge
     ptyprocess                0.7.0              pyhd3deb0d_0    conda-forge
     py                        1.10.0             pyhd3deb0d_0    conda-forge
     py-cpuinfo                8.0.0              pyhd8ed1ab_0    conda-forge
     pyarrow                   5.0.0           py38hed47224_4_cuda    conda-forge
     pycodestyle               2.6.0              pyh9f0ad1d_0    conda-forge
     pycparser                 2.20               pyh9f0ad1d_2    conda-forge
     pydata-sphinx-theme       0.6.3              pyhd8ed1ab_0    conda-forge
     pyflakes                  2.2.0              pyh9f0ad1d_0    conda-forge
     pygments                  2.10.0             pyhd8ed1ab_0    conda-forge
     pyopenssl                 20.0.1             pyhd8ed1ab_0    conda-forge
     pyorc                     0.4.0                    pypi_0    pypi
     pyparsing                 2.4.7              pyh9f0ad1d_0    conda-forge
     pyrsistent                0.17.3           py38h497a2fe_2    conda-forge
     pysocks                   1.7.1            py38h578d9bd_3    conda-forge
     pytest                    6.2.5            py38h578d9bd_0    conda-forge
     pytest-benchmark          3.4.1              pyhd8ed1ab_0    conda-forge
     pytest-forked             1.3.0              pyhd3deb0d_0    conda-forge
     pytest-xdist              2.3.0              pyhd8ed1ab_0    conda-forge
     python                    3.8.10          h49503c6_1_cpython    conda-forge
     python-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge
     python_abi                3.8                      2_cp38    conda-forge
     pytorch                   1.9.0           cpu_py38h91ab35c_0    conda-forge
     pytz                      2021.1             pyhd8ed1ab_0    conda-forge
     pyyaml                    5.4.1            py38h497a2fe_1    conda-forge
     pyzmq                     22.2.1           py38h2035c66_0    conda-forge
     rapidjson                 1.1.0             he1b5a44_1002    conda-forge
     re2                       2021.09.01           h9c3ff4c_0    conda-forge
     readline                  8.1                  h46c0cb4_0    conda-forge
     recommonmark              0.7.1              pyhd8ed1ab_0    conda-forge
     regex                     2021.8.28        py38h497a2fe_0    conda-forge
     requests                  2.25.1             pyhd3deb0d_0    conda-forge
     rhash                     1.4.1                h7f98852_0    conda-forge
     rmm                       21.10.00a210904 cuda_11.2_py38_g8527317_28    rapidsai-nightly
     s2n                       1.0.10               h9b69904_0    conda-forge
     sacremoses                0.0.43             pyh9f0ad1d_0    conda-forge
     send2trash                1.8.0              pyhd8ed1ab_0    conda-forge
     setuptools                57.4.0           py38h578d9bd_0    conda-forge
     six                       1.16.0             pyh6c4a22f_0    conda-forge
     sleef                     3.5.1                h7f98852_1    conda-forge
     snappy                    1.1.8                he1b5a44_3    conda-forge
     snowballstemmer           2.1.0              pyhd8ed1ab_0    conda-forge
     sortedcontainers          2.4.0              pyhd8ed1ab_0    conda-forge
     soupsieve                 2.0.1                      py_1    conda-forge
     spdlog                    1.8.5                h4bd325d_0    conda-forge
     sphinx                    4.1.2              pyh6c4a22f_1    conda-forge
     sphinx-copybutton         0.4.0              pyhd8ed1ab_0    conda-forge
     sphinx-markdown-tables    0.0.15             pyhd3deb0d_0    conda-forge
     sphinxcontrib-applehelp   1.0.2                      py_0    conda-forge
     sphinxcontrib-devhelp     1.0.2                      py_0    conda-forge
     sphinxcontrib-htmlhelp    2.0.0              pyhd8ed1ab_0    conda-forge
     sphinxcontrib-jsmath      1.0.1                      py_0    conda-forge
     sphinxcontrib-qthelp      1.0.3                      py_0    conda-forge
     sphinxcontrib-serializinghtml 1.1.5              pyhd8ed1ab_0    conda-forge
     sphinxcontrib-websupport  1.2.4              pyh9f0ad1d_0    conda-forge
     sqlite                    3.36.0               h9cd32fc_0    conda-forge
     streamz                   0.6.2              pyh44b312d_0    conda-forge
     sysroot_linux-64          2.12                he073ed8_14    conda-forge
     tblib                     1.7.0              pyhd8ed1ab_0    conda-forge
     terminado                 0.11.1           py38h578d9bd_0    conda-forge
     testpath                  0.5.0              pyhd8ed1ab_0    conda-forge
     tk                        8.6.11               h27826a3_1    conda-forge
     tokenizers                0.10.1           py38hb63a372_0    conda-forge
     toml                      0.10.2             pyhd8ed1ab_0    conda-forge
     toolz                     0.11.1                     py_0    conda-forge
     tornado                   6.1              py38h497a2fe_1    conda-forge
     tqdm                      4.62.2             pyhd8ed1ab_0    conda-forge
     traitlets                 5.1.0              pyhd8ed1ab_0    conda-forge
     transformers              4.9.2              pyhd8ed1ab_0    conda-forge
     typed-ast                 1.4.3            py38h497a2fe_0    conda-forge
     typing-extensions         3.10.0.0             hd8ed1ab_0    conda-forge
     typing_extensions         3.10.0.0           pyha770c72_0    conda-forge
     urllib3                   1.26.6             pyhd8ed1ab_0    conda-forge
     virtualenv                20.4.7           py38h578d9bd_0    conda-forge
     wcwidth                   0.2.5              pyh9f0ad1d_2    conda-forge
     webencodings              0.5.1                      py_1    conda-forge
     wheel                     0.37.0             pyhd8ed1ab_1    conda-forge
     xz                        5.2.5                h516909a_1    conda-forge
     yaml                      0.2.5                h516909a_0    conda-forge
     zeromq                    4.3.4                h9c3ff4c_1    conda-forge
     zict                      2.0.0                      py_0    conda-forge
     zipp                      3.5.0              pyhd8ed1ab_0    conda-forge
     zlib                      1.2.11            h516909a_1010    conda-forge
     zstd                      1.5.0                ha95c52a_0    conda-forge
     
</pre></details>


**Additional context**
Add any other context about the problem here.
",2021-09-04T21:33:33Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
182,[FEA][JNI] expose an RMM allocator API in cuDF JNI,"We currently support in the Java side some combinations of allocators and wrappers that make it easy to setup pools (default, arena, and now async), backing allocators (cuda, managed memory). But it is getting to the point that the options in these allocators don't all fit the min/max pool size pattern.

- The non-pooled allocators don't have a minimum or maximum setting.
- The async allocator doesn't support managed memory, and it also has a third setting ""threshold"" that isn't exposed in RMM but it probably could/should, and we don't really have a way to express that.
- The allocators are composed with the tracking and likely the limiting adapters as well, by default. 

For general users of the cuDF JNI side of things the above combinations may not be desired. We propose exposing an RMM that would allow better composability of the various allocators we use and test with (note this is a good chance to clean up some tech debt as well for allocators we no longer use or intend to support). ",2021-09-09T22:15:09Z,0,0,Alessandro Bellina,NVIDIA,True
183,[FEA] Apply `sliced_child` when calling to `slice`,"I observe that there are a lot of bugs related to the situations when an API directly accesses the child columns of a sliced column instead of calling to `get_sliced_child`. As such, the `slice` API is a kind of shallow slice, not a deep slice. Maybe shallow slice is more efficient as it can avoid unnecessary slicing of the children columns when we don't care, it has caused a lot of (potential) bugs that cost a lot of developer time.

An instance of such bugs is here: https://github.com/rapidsai/cudf/pull/9218. In the past, I have also dealt with many similar situations but I could catch them immediately through unit tests. If a developer forgets to write unit tests for sliced input, the bug may be there.

I would like to rewrite `slice` into deep slicing, i.e., recursively calling to `slice` on all children columns of the column being sliced. This way, when we access its children column through the APIs `child_begin()`, `child_end`, or `child(idx)` we will have the expected results all the time. Although we have talked about this before and didn't do anything as deep slicing is expensive, I still decided to raise the issue again as it still causes bugs.

An alternative solution to this issue is to rename the existing `slice` API into `shallow_slice` then add another `slice` version that does recursively calling `shallow_slice` on the columns. So, a developer will only call `shallow_slice` if he/she knows exactly that just the shallow version is needed in the context. Otherwise, a more expensive `slice` version will produce the correct results in most situations.",2021-09-11T13:07:19Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
184,[FEA] Add an internal utility API to return an offsets column of a sliced column starting with zero,"For nested sliced columns that have an offsets column child, their offsets columns may contain values that do not start from zero. For example:
```
offsets = [5, 7, 20, ...]
```

Many operations on these sliced columns need to generate an output offsets column that starts with zero. For example, with the input column having offsets given above, the output offsets column should be:
```
offsets = [0, 2, 13, ...]
```
Such output offsets column is generated simply by subtracting all the values with the first value. Yes, very simple. 

I would like to have an internal API implementing this feature. Currently, there are several other APIs using it by implementing private code in their `.cu` files. For example:
 * `lists/segmented_sort.cu`
 * `lists/drop_list_duplicates` (FYI: https://github.com/rapidsai/cudf/pull/9202#discussion_r712266408)",2021-09-20T19:59:02Z,2,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
185,[FEA] Enable Page-level filtering based on the ColumnIndex feature from parquet 1.11,"**Is your feature request related to a problem? Please describe.**
The high level goal is to be able to reduce the amount of data that we read from disk on parquet files with a ColumnIndex/PageIndex.

https://github.com/apache/parquet-format/blob/master/PageIndex.md

**Describe the solution you'd like**

In Spark we currently do some hacked up things when reading parquet files. We use [parquet-mr](https://github.com/apache/parquet-mr) to read the metadata about the file(s). We then let it do a predicate push down to find the row groups that fit the requested predicate. Finally we read the pages for the desired columns in those row groups and put it all back together as a new in-memory parquet file that we send down to CUDF. It is ugly, but it let us do add a lot of features in Spark before CUDF could support them. It still lets us read the data using the existing Hadoop File System interface, which because it is a ""standard"" that our customers can and do replace. We might be able to move to the Arrow FileSystem API, but I'll talk about that in the alternatives.

Ideally I would like an API where we can send compressed pages and metadata to CUDF for decoding.  The metadata would include things like the file and row group that the pages came from and what range of column indicies within those pages we would like to be read.

The hard part with filtering using a ColumnIndex is that the pages within a row group are not split on the same row boundaries, like a row group is.  An example might help here. Lets say we have two columns A and B in a row group. The predicate to push down is `A > 100`, which corresponds to page 5 in the row group. That page is for rows 500-599.  Column B requires us to load 2 pages to cover that same range. In this case lets say pages 10 and 11 which cover 450 - 549 and 550 to 700 respectively.  So we would have to hand CUDF the pages 5, 10, and 11 along with the metadata about the row group and file so CUDF can know how to decode the data, and information to say only decode the rows 500 to 599 and throw away anything else that is outside of that. In a real situation it is probably going to be a lot more complicated.

Ideally this would let us pass down row groups from multiple different files too. I am not 100% sure how the row group filtering works on a multi-file source_info.

**Describe alternatives you've considered**
The other alternative is for us to start using the Arrow FileSystem API and also have cudf implement row number filtering (need to check, but I think the row numbers are relative to the start of the row group and not total within the file) similar to the `set_row_groups` API that currently exists.

This is kind of hard for us to do.

1. One of the main performance features that we have is overlapping I/O with computation. Spark likes to use lots of threads, more than we want to allow on the GPU at any point in time for memory reasons. So we let some onto the GPU, but we let the others read data to CPU memory. This lets us overlap the slow reading from a remote file system with computation on the GPU. We would need some kind of callback, or multiple APIs so that we could have CUDF read all of the needed data into host memory, and then we can wait until it is time to run at which point we can finish processing the data.
2. JNI is very slow for moving data. We use a number of tricks/alternative APIs to get around this. This is especially true when calling back from native code into Java. So having an API that goes from java to C back to java so it can read data over a socket (through C again, but a slightly more optimized interface than JNI) is far from ideal especially if it is going to involve small reads.

**Additional context**
This is related to #9268, but the read side compared to the write side of it.",2021-09-22T11:58:44Z,0,0,Robert (Bobby) Evans,Nvidia,True
186,[BUG] Comparing decimal values with positive scale to MAX LONG or MIN LONG produces incorrect results,"**Describe the bug**
We have code to check if casting a Decimal to a long will overflow. The default code creates an INT64 scalar value with the min long value in it, and another with the max long value in it.  Then it uses binary ops like LESS, GREATER, LESS_EQUAL, and GREATER_EQUAL to see if the values are within the given range. This works generally great for bytes, shorts and ints, but for longs (INT64) it always comes back as true.  The values are in the desired range. I have to explicitly create a decimal scalar with a corresponding scale to make this check work. My guess is that both values are being cast to an INT64 before doing the comparison and the result by definition cannot be outside the range of those values.",2021-09-23T13:44:29Z,0,0,Robert (Bobby) Evans,Nvidia,True
187,[BUG] pyorc does not read string column statistics of cuDF generated files,"When reading the statistics for an ORC file written by cuDF, the result for sum is wrong when read using cuDF and absent when using pyorc.

```python
In [1]: import cudf

In [2]: import pyorc

In [3]: gdf = cudf.DataFrame({'b':[1,7], 'a':['Badam khao', 'roz']})

In [4]: gdf.to_orc(""temp.orc"")

In [5]: cudf.io.orc.read_orc_statistics([""temp.orc""])
Out[5]: 
([{'col0': {'number_of_values': 2},
   'b': {'number_of_values': 2, 'minimum': 1, 'maximum': 7, 'sum': 8},
   'a': {'number_of_values': 2,
    'minimum': 'Badam khao',
    'maximum': 'roz',
    'sum': -7}}],
 [{'col0': {'number_of_values': 2},
   'b': {'number_of_values': 2, 'minimum': 1, 'maximum': 7, 'sum': 8},
   'a': {'number_of_values': 2,
    'minimum': 'Badam khao',
    'maximum': 'roz',
    'sum': -7}}])

In [6]: f = open(""temp.orc"", 'rb')

In [7]: r = pyorc.Reader(f)

In [8]: r[1].statistics
Out[8]: 
{'has_null': False,
 'number_of_values': 2,
 'minimum': 1,
 'maximum': 7,
 'sum': 8,
 'kind': <TypeKind.LONG: 4>}

In [9]: r[2].statistics
Out[9]: {'has_null': False, 'number_of_values': 2, 'kind': <TypeKind.STRING: 7>}
```

#### Expected result
Sum statistics contains the sum of lengths of all the strings in the column. We do correctly compute this in libcudf, so it should be present when reading with pyorc and correct when reading with cudf.

There's two issues here:

- [x] String sum statistics are encoded incorrectly (will be fixed by https://github.com/rapidsai/cudf/pull/11740)
- [ ] pyroc does not read cuDF-written ORC string statistics",2021-09-27T11:01:54Z,0,0,Devavret Makkar,@VoltronData,False
188,[BUG][FEA] Convert Dask Array to Dask cuDF DataFrame causes ArrowInvalid Error,"While creating a synthetic dataset with [cuML Dask make_classification](https://docs.rapids.ai/api/cuml/stable/api.html#cuml.dask.datasets.classification.make_classification) to create into a cuGraph object I get 
```
ArrowInvalid: Could not convert 250000 with type cupy._core.core.ndarray: did not recognize Python value type when inferring an Arrow data type
```

when converting a Dask Array (output from cuML dask make_classification ) -> Dask DataFrame -> Dask cuDF DataFrame to instantiate a cuGraph DiGraph object. I think 250000 refers to the second index division (0 being the first).

See attached sample code to reproduce, which also has other things I tried as well.

Feature: For a possible future feature to create dask_cudf DataFrames from Dask Arrays directly? Something like `dask_cudf.DataFrame.from_dask_array(dask_array, columns=[] ...)` or similar would be nice.

**Expected behavior**
Create a dask cuDF DataFrame and run louvain.

**Environment details**
docker container withRapids 21.06 on a DGX

Here's sample code
[snippet.txt](https://github.com/rapidsai/cudf/files/7304748/snippet.txt)

",2021-10-07T15:01:30Z,0,0,,@NVIDIA,True
189,[FEA] Add a sentinel `constexpr` value to denote `INVALID_INDEX`,"Currently, various cudf APIs need to use a special constant to denote an **invalid** row index for certain operations. However, there is not any guideline on what constant should be used to mark an invalid index row. Therefore, different APIs implement their own `INVALID_INDEX` constant.

For example:
 * `lists/explode.cu`: `constexpr size_type InvalidIndex = -1;`
 * `detail/scatter.cuh`:  `std::numeric_limits<size_type>::lowest()` is used
 * `lists/extract.cu`: `std::numeric_limits<size_type>::max()` is used

And maybe more. As such, the constants used for `INVALID_INDEX` is not agreed across cudf development. We should explicitly add a sentinel constant in some of cudf main header and replace the existing ones with it. By doing so, we can enforce consistency and reduce confusion (why this file uses `-1` while other file uses `numeric_limit::lowest()` or `numeric_limit::max()`).",2021-10-13T03:26:58Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
190,[BUG] JNI testORCWriteToBufferChunked fails if `device_write_async` just returns a future,"The JNI cuDF bindings have a custom writer sink. Recent changes to the `cudf::io::data_sink` api added a `device_write_async` method and we updated our custom sink to pass CI and retain the old behavior. 

If I try to return a future in this API, and do the work we used to do inside of it, our java tests fail for Orc but not for Parquet. 

```
diff --git a/java/src/main/native/src/TableJni.cpp b/java/src/main/native/src/TableJni.cpp
index 9e07f44..da036e9 100644
--- a/java/src/main/native/src/TableJni.cpp
+++ b/java/src/main/native/src/TableJni.cpp
@@ -145,9 +145,9 @@ public:
 
   std::future<void> device_write_async(void const *gpu_data, size_t size,
                                        rmm::cuda_stream_view stream) override {
-    // Call the sync version until figuring out how to write asynchronously.
-    device_write(gpu_data, size, stream);
-    return std::async(std::launch::deferred, [] {});
+    return std::async(std::launch::deferred, [=] {
+      device_write(gpu_data, size, stream);
+    });
   }
```

Test failure:

```
ai.rapids.cudf.CudfException: cuDF failure at: /cudf/cpp/src/io/orc/reader_impl.cu:1321: Invalid index rowgroup stream data
	at ai.rapids.cudf.Table.readORC(Native Method)
	at ai.rapids.cudf.Table.readORC(Table.java:925)
	at ai.rapids.cudf.TableTest.testORCWriteToBufferChunked(TableTest.java:7058)
```

Discussing with @devavret it seems that the issue is the value of `bytes_written`. The synchronous way, `bytes_written` in the sink was guaranteed to get updated in the same stack, but with the future approach, we have no such guarantee.

I am adding this issue to try and document why we worked around this, and if there's a fix to our custom sink or if it's an API that needs to get changed in cuDF.",2021-10-13T14:05:52Z,0,0,Alessandro Bellina,NVIDIA,True
191,[BUG] AST join slows down significantly with a small left table,"We found an issue late in 21.10 (https://github.com/NVIDIA/spark-rapids/issues/3736) with the AST driven inner join where given a smaller left side table our performance would drop significantly with AST, as opposed to using an unconditional join + a separate filter. This issue is to discuss whether cuDF could detect this and adjust how it handles such a join.

If the larger table is on the left, AST is at least ~1.3x faster than join + filter approach. If the smaller table is on the left, AST can be 10x slower than the join + filter approach. We can work around this for inner joins specifically, since we can swap in the plugin the sides fairly easily (https://github.com/NVIDIA/spark-rapids/issues/3832), but this is hacky and seems like it is something cuDF should be able to handle, especially for all the other types of joins where we can't just swap sides like this.

The reason for the swap of tables in Spark, is that there is logic to build left or build right, depending on the byte sizes of the inputs. The spark-rapids plugin will split into smaller chunks the left side (if building right), or the right side (if building left), streaming these chunks against the side of the join that wasn't split (aka the build side). In this case we went from a build-left regime which was good for the AST since the left side was not split, to a build-right regime when the left side started to get split.

The reason why these kernels are so sensitive to the left side, is because that's what is getting used to compute the grid size:
```
  detail::grid_1d config(left_table->num_rows(), DEFAULT_JOIN_BLOCK_SIZE);
```

And in our example (https://github.com/NVIDIA/spark-rapids/issues/3736) the left table could be hundreds of rows, where the right table could be 1M rows.",2021-10-18T17:27:37Z,0,0,Alessandro Bellina,NVIDIA,True
192,Add unit tests for fixed point in quantiles ,"**Describe the bug**
Quantiles support fixed point columns. but it is not tested in unit tests (both gtest and pytest).

**Expected behavior**
add unit tests for fixed_point in quantiles gtest

**Additional context**
https://github.com/rapidsai/cudf/issues/9468
While adding `FixedPointTypes` to `cudf::test::AllTypes`, QUANTILES_TEST (Failed).

[  FAILED  ] QuantileUnsupportedTypesTest/10.TestZeroElements, where TypeParam = numeric::fixed_point<int, (numeric::Radix)10>
[  FAILED  ] QuantileUnsupportedTypesTest/10.TestOneElements, where TypeParam = numeric::fixed_point<int, (numeric::Radix)10>
[  FAILED  ] QuantileUnsupportedTypesTest/10.TestMultipleElements, where TypeParam = numeric::fixed_point<int, (numeric::Radix)10>
[  FAILED  ] QuantileUnsupportedTypesTest/11.TestZeroElements, where TypeParam = numeric::fixed_point<long, (numeric::Radix)10>
[  FAILED  ] QuantileUnsupportedTypesTest/11.TestOneElements, where TypeParam = numeric::fixed_point<long, (numeric::Radix)10>
[  FAILED  ] QuantileUnsupportedTypesTest/11.TestMultipleElements, where TypeParam = numeric::fixed_point<long, (numeric::Radix)10>

 6 FAILED TESTS

",2021-10-19T14:12:16Z,0,0,Karthikeyan,NVIDIA,True
193,[FEA] enhancement: not allow getData from ColumnVector of nested type,"**Is your feature request related to a problem? Please describe.**
When getData from a ColumnVector of nested type such as LIST, the address will be null, and that will trigger a NPE.


**Describe the solution you'd like**
When calling getData API from a ColumnVector, check the data type, if it's nested type, throw exception to inform that no data could be get from a nested type ColumnVector.

**Additional context**

```scala
// assume this columnVector is of nested type : LIST
val data = columnVector.getData  // null
val addr = childData.getAddress
```
",2021-10-22T08:40:09Z,0,0,Allen Xu,@Nvidia,True
194,"[FEA] Support ""on"" parameter in cudf.DataFrame.join","Version used: cuml==21.8.2

**Steps/Code to reproduce bug**
```
left = cudf.DataFrame([100, 101], columns=[""item_id""])
right = cudf.DataFrame([""a"",""b""], index=[100,101], columns=[""item_name""])

# This will result in having only <NA> in item_name
left.join(right, on=""item_id"")

# This works as expected
left.to_pandas().join(right.to_pandas(), on=""item_id"")

# Workarround
left.merge(right, left_on=""item_id"", right_index=True)
```
",2021-10-25T10:19:46Z,0,0,Nico Kreiling,scieneers,False
195,[FEA] Java tests for AST casts,"**Is your feature request related to a problem? Please describe.**
#9379 added Java bindings for AST casting but without any Java tests of those bindings.

**Describe the solution you'd like**
Java tests should be added for AST casting to test the correctness of the Java bindings for this feature.
",2021-10-25T20:27:23Z,0,0,Jason Lowe,NVIDIA,True
196,[FEA] create LIST type ColumnVector of fixed row size from a device memory buffer,"**Is your feature request related to a problem? Please describe.**
In [spark-rapids-ml](https://github.com/NVIDIA/spark-rapids-ml), it is common(to build a matrix-like data) to build new `fixed row length` ColumnVector of LIST type, with a device memory buffer. 

**Describe the solution you'd like**
imagine there's a result memory buffer `void* ret` to be returned. cuDF should be able to create a fixed length ColumnVector of LIST type with number of rows and number of columns provided.
e.g. the result memory is [1,2,3,4,5,6], with number of rows=3, number of cols=2, then cuDF provides a method to give out a CV with data layout like: 

[1,2]
[3,4]
[5,6]

**Describe alternatives you've considered**


**Additional context**
it can be done either in native or java.",2021-10-27T10:47:05Z,0,0,Allen Xu,@Nvidia,True
197,[FEA] Restructure AST internals to reduce stack depth and register pressure,"**Is your feature request related to a problem? Please describe.**
The AST evaluation process currently used for conditional joins and the `compute_column` APIs is heavily dependent on multiple levels of device-side dispatch, both implicit and explicit. Explicit dispatches are performed based on argument types -- which happens once for unary ops and twice for binary ops -- and based on the `ast_operator`. Implicit dispatch is performed based on the types of data references (column, literal, or intermediate) in the deepest part of the code. Additionally, the entire evaluator is templated on the output type to support writing the results either to a column or to a stack variable (typically allocated within a kernel for the purpose of storing a thread-local output). The high complexity of these different features results in two major bottlenecks for performance:
- As documented in #5902, we are asking a lot of the compiler to inline all the function calls. However, if the compiler fails to do so, the resulting stack frames introduced in the kernel lead to variables spilling into local memory. The latency associated with increased local memory traffic has a substantial impact on performance. Lots of the recent work done to mitigate that has improved the situation, but #9530 shows that we still stand to make performance gains by increasing inlining (in that case, we observed nearly 2x improvements for benchmarks involving nullable columns).
- The highly nested function calls means that even if the compiler successfully inlines functions, it may not be consistently minimizing register usage if it doesn't recognize which variables can be safely reused at different levels of the call stack. #9210 exhibited substantial performance improvements through suitable passing of const references, but revealed that pass-by-value semantics of const objects did not always result in the same performance improvements, suggesting that the compiler was not necessarily making the best use of the available information to minimize copying of data.

Various changes in #8145 helped reduce this complexity in a number of ways, but the current benchmarks still indicate that the various kernels are limited by register pressure or local memory traffic (depending on the complexity of the non-AST components of the kernel and the impact of null data). 

**Describe the solution you'd like**
A major factor in the introduction of stack frames and the added complexity for the compiler in determining which variables it can safely leave in registers is the multi-level dispatch. We should consider replacing the compile-time dispatch-based solutions for the operator functors with an approach based on dynamic polymorphism of virtual operator functors. We could use something like the visitor pattern to handle type dispatch for binary operations, which would have the added benefit of naturally handling type casting using the language's own casting rules. While this would entail runtime vtable lookups for virtual functions, the tradeoff would be a dramatic simplification of the code the compiler generates since it would no longer need to instantiate large switch statements for every single templated code path and could instead expend its inline budget on more effectively inlining existing code.

**Describe alternatives you've considered**
Most of the alternatives to simplifying the code have already been completed (or attempted and discarded) in #8145. While those refactorings decreased complexity, going forward complexity is only likely to increase as we add more operators or support for additional cases, and I don't see any other way to simplify this code further.

**Additional context**
This change would be a substantial undertaking that would involve rewriting a significant chunk of the parsing and evaluation internals of the AST code. As such, we'll probably want to spend the better part of a release prototyping and testing.
",2021-10-28T23:54:47Z,1,0,Vyas Ramasubramani,@rapidsai,True
198,[FEA] Supporting `timedelta64` dtypes for ceil/floor operations,"**Is your feature request related to a problem? Please describe.**
This issue is created as a follow-up to #9571  and #9554 where we add support for ceil/floor operations for `datetime64[ns]` data type. In this case, we would like additional support for `timedelta64[ns]` types as well.

**Describe the solution you'd like**
```python
import cudf
tdIndex = cudf.TimedeltaIndex(data =['4 day 8h 20min 35us 45ns', '+17:42:19.999999',
'9 day 3h 08:16:02.000055', '+22:35:25.000075'])
tdIndex.ceil(freq='T'))
```
Output:
```bash
TimedeltaIndex(['4 days 08:21:00', '0 days 17:43:00', '9 days 11:17:00',
'0 days 22:36:00'],
dtype='timedelta64[ns]', freq=None)
```

**Additional context**
Similar [to how pandas supports it](https://pandas.pydata.org/docs/reference/api/pandas.TimedeltaIndex.ceil.html?pandas.TimedeltaIndex.ceil#pandas-timedeltaindex-ceil)",2021-10-29T16:12:50Z,0,0,Mayank Anand,,False
199,[BUG] Aggregation with NaN not matching Pandas behavior,"Aggregation(mean/sum) on cols NaN result in NaN value. even with dropna as `True`

```
>>> df  = cudf.DataFrame()
>>> df['a'] = [1,2,3,0,4]
>>> df['b'] = [1,1,1,0,1]
>>> df['c'] = df.a/df.b
>>> df['s'] = 0
>>> df
   a  b    c  s
0  1  1  1.0  0
1  2  1  2.0  0
2  3  1  3.0  0
3  0  0  NaN  0
4  4  1  4.0  0
>>> df[['c', 's']].groupby(['s']).mean()
    c
s    
0 NaN
```

**Expected/Pandas behavior**
```
>>> import pandas as pd
>>> df = pd.DataFrame()
>>> df['a'] = [1,2,3,0,4]
>>> df['b'] = [1,1,1,0,1]
>>> df['c'] = df.a/df.b
>>> df['s'] = 0
>>> df
   a  b    c  s
0  1  1  1.0  0
1  2  1  2.0  0
2  3  1  3.0  0
3  0  0  NaN  0
4  4  1  4.0  0
>>> df[['c', 's']].groupby(['s']).mean()
     c
s     
0  2.5
```",2021-11-01T17:48:00Z,0,0,Lahir Marni,,False
200,[FEA] Define a standard mechanism for querying GPU memory usage,"**Is your feature request related to a problem? Please describe.**
Python's `sys` module provides the `sys.getsizeof` function to determine the size of a Python object. The behavior of `getsizeof` when applied to a user-defined class may be customized by overriding the `__sizeof__` attribute. For the purpose of computing the size of a Python object backed by GPU memory, however, `getsizeof` has a couple of major drawbacks:
1. `getsizeof` is traditionally defined as a shallow calculation, so the `sizeof` a container will not recursively traverse nested elements. The `sizeof` a list of lists is essentially equivalent to (in pseudocode) `sizeof(PyObject *) * len(list)`, with few extra bytes allocated for the overhead of the list's metadata. The internet is rife with recipes for performing a corresponding deep calculation, but they typically have sharp edges and in practice users of GPU libraries usually want the deep calculation if they are making this request. Even if a suitable override of this attribute could be defined that always returned the deep calculation, it would not be desirable to do so since it would overload the standard meaning of the operator in Python.
2. GPU memory allocations are more complex than host calculations in the sense that there are multiple ""pools"" of memory from which a buffer might be allocated and the user may be interested in having those separated out. In addition to standard device allocations via cudaMalloc, a user may also have requested pinned host memory or managed memory. Any API for querying GPU memory usage must be sufficiently general to support all of these types of information.

Various higher-level Python libraries that leverage GPU libraries under the hood would benefit from a standardized approach to requesting total GPU memory allocations. For instance, Dask could leverage this calculation to determine when to spill memory to disk.

**Describe the solution you'd like**
It would be nice to define a standard protocol for all Python libraries backed by GPU memory to expose the allocations underlying a Python object. The most obvious possibility for this would be a new protocol to correspond with `__cuda_array_interface__`, something like `__cuda_sizeof__` that would return a dictionary of allocated memory by type, but other implementations are also possible. It would be important to consider whether this would always be a deep calculation or if there would be any cases when a shallow calculation might be appropriate, for instance with containers (like those that might live in [cuCollections](https://github.com/NVIDIA/cuCollections/)). It would also be important to consider how it should behave for slices: for instance, would `s = cudf.Series(1000); s[::2].__cuda_sizeof__` indicate the size of the a column of size 1000 or 500?

It may also be necessary for users to have some way to account for host memory allocations, but I think it makes the most sense to have that calculation be entirely independent of this protocol. That does raise some questions about the suitable way to treat pinned memory (host memory allocated via `cudaHostAlloc`).

**Describe alternatives you've considered**
None at this stage.

**Additional context**
This proposal comes out of a discussion precipitated in https://github.com/rapidsai/cudf/pull/9544#issuecomment-955011270. That PR removed the `__sizeof__` overrides in cuDF, which were likely to be more confusing than helpful, and standardized the `memory_usage` method of cuDF objects. `memory_usage` is a pandas function that we seek to mimic, but our goal of making cuDF objects pandas-compatible makes this method unsuitable for adaptation into a new ""gpusizeof"" protocol.",2021-11-03T17:04:08Z,0,0,Vyas Ramasubramani,@rapidsai,True
201,[FEA] Reduce implicit conversions to minimize unnecessary column materialization,"**Is your feature request related to a problem? Please describe.**
A number of deceptively harmless functions in cuDF Python actually involve insidious memory allocations that in aggregate have substantial deleterious impacts on performance. The two most prominent examples of this are `ColumnBase.astype` calls and all `RangeIndex` logic that involves materializing an `Int64Index`, although others may exist as well. While these conversions are sometimes unavoidable, in many cases they are not. `RangeIndex` conversions are particularly harmful because the class is designed precisely to avoid such overheads, so many internal code paths are likely creating unnecessary and unexpected memory pressure.

**Describe the solution you'd like**
Type conversions that are currently performed deep in the call stack should be moved to the highest possible level. This change would prevent APIs from performing the cast except where absolutely necessary, and at minimum would reduce the number of calls to validation functions like `is_categorical_dtype`. Internal functions should accept a relatively narrow set of parameter types, and only user-facing functions should perform casts to handle the near-arbitrary user data that pandas APIs require us to accept.

On the Index front, there are a number of tasks that should be tackled separately. Roughly, the following two tasks should be possible to address in parallel:
- Currently `RangeIndex` implicitly defers all methods that aren't implemented to `Int64Index` via the `__getattr__` override. We should remove this override and actually implement the relevant methods in `RangeIndex`. In cases where materializing an `Int64Index` is necessary, we should exploit the sort of monkey-patching that we do for defining unary and binary operations for `RangeIndex` to add those methods explicitly. In other words, we should use an allowlist rather than a blocklist to choose methods where the conversion happens. 
  - Done in #10538
- `BaseIndex` should be reduced as closely as possible to an abstract class. While there are a subset of APIs that truly make sense for all types of index objects, in almost all cases the optimal implementation for `RangeIndex` (and `MultiIndex`, for that matter) is very different from the implementation for `GenericIndex`. In addition, this change reduces cognitive load for developers by simplifying the inheritance hierarchy.
  - Done in #10389

Once those two tasks are accomplished to whatever degree makes sense, we should revisit removing two attributes that contribute to this problem:
- `BaseIndex._values`: This property doesn't really make sense since it's internal-only and there isn't a useful implementation for MultiIndex. It's a trivial alias for `_column` in `GenericIndex`, but it leads to automatic Int64Index creation when called on a `RangeIndex`. Removing the property would encourage developers to prevent that. Addressing the first two issues above should reduce the need for this property substantially and make it easier to assess where we're really using it.
- `BaseIndex._data`: We promise that this property will exist so that `BaseIndex` looks like a `Frame` in places that it needs to, but this again causes unnecessary memory allocations when used for `RangeIndex`. Any code path that is relying on this attribute existing for `RangeIndex` should be rewritten, again only explicitly creating a column when necessary. Only `Frame` methods should really expect `_data` to work.

Finally, we should also be cognizant of where we copy indexes. Since indexes are immutable, in many cases we can avoid making copies altogether. It is fine for multiple frames etc to refer to the same index since any operation that ""mutates"" an index should just be creating a new index object and assigning it under the hood.

**Additional context**
This issue was originally raised in the cudf refactoring roadmap, but after being brought up in https://github.com/rapidsai/cudf/pull/9558#discussion_r740584876 I've documented it as a Github issue for greater visibility.
",2021-11-03T22:41:02Z,0,0,Vyas Ramasubramani,@rapidsai,True
202,[FEA] argmin and argmax for Series and Indexes,"For pandas API compatibility, it would be nice to implement argmax and argmin on various Series and Indexes. Argmax (argmin) is a reduction that accepts a column or index and returns the *integer position* of the largest (smallest) value in the data. If the maximum (minimum) is achieved in multiple locations, the first row position is returned.

We already have per group argmin and argmax aggregations in libcudf, as they were implemented for `Groupby.{idxmax, idxmin}` in cuDF Python. They are not currently part of the reduction machinery, [as far as I can tell from a quick glance](https://github.com/rapidsai/cudf/blob/c6bc111a18803b4a56e8090f237af00e5b5296ae/cpp/src/reductions/reductions.cpp) so this may require a separate implementation.

Note that argmin and argmax are different from idxmin and idxmax. The latter methods return the *index label* associated with the minimum or maximum value. The index label may not be an integer.

Implementing Series and Index argmin and argmax would likely enable implementing https://github.com/rapidsai/cudf/issues/1480 .",2021-11-04T14:57:07Z,0,0,Nick Becker,@NVIDIA,True
203,[FEA] Series and DataFrame idxmax and idxmin,"For API compatibility, cuDF should support [idxmax](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.idxmax.html) and [idxmin](https://pandas.pydata.org/docs/reference/api/pandas.Series.idxmax.html) on Series and DataFrames.

Note that argmin and argmax (https://github.com/rapidsai/cudf/issues/9601) are slightly different from idxmin and idxmax. The latter methods return the index label associated with the first occurrence of the minimum or maximum value. The index label may not be an integer (see example below).

On DataFrmes, these methods return a Series in which each row corresponds to one column in the original dataFrame.

```python
import pandas as pd
​
df = pd.DataFrame({'consumption': [10.51, 103.11, 55.48],
                   'co2_emissions': [37.2, 19.66, 1712]},
                   index=['Pork', 'Wheat Products', 'Beef'])
print(df, ""\n"")
print(df.idxmax(), ""\n"")
print(df.idxmin())
                consumption  co2_emissions
Pork                  10.51          37.20
Wheat Products       103.11          19.66
Beef                  55.48        1712.00 

consumption      Wheat Products
co2_emissions              Beef
dtype: object 

consumption                Pork
co2_emissions    Wheat Products
dtype: object
```
```python

import pandas as pd
​
df = pd.DataFrame({'consumption': [10.51, 103.11, 55.48],
                   'co2_emissions': [37.2, 19.66, 1712]})
print(df, ""\n"")
print(df.idxmax(), ""\n"")
print(df.idxmin())
   consumption  co2_emissions
0        10.51          37.20
1       103.11          19.66
2        55.48        1712.00 

consumption      1
co2_emissions    2
dtype: int64 

consumption      0
co2_emissions    1
dtype: int64
```",2021-11-04T15:06:25Z,0,0,Nick Becker,@NVIDIA,True
204,[FEA] Series and DataFrame between_time,"For pandas API compatibility, we can implement [Series](https://pandas.pydata.org/docs/reference/api/pandas.Series.between_time.html) and [DataFrame.between_time](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.between_time.html). `between_time` ""select[s] values between particular times of the day (e.g., 9:00-9:30 AM). By setting start_time to be later than end_time, you can get the times that are not between the two times.""

If the index is not a DatetimeIndex, this method throws a TypeError. DateTimes without time components are considered as if the time component were ""0:00:00"". Valid `{start, end}_time` must be in the interval [0:00 and 24:00).

The API documentation does not indicate what resolutions are valid or invalid for `{start, end}_time`, but it is documented as only including granularity down to seconds in the [utility function](https://github.com/pandas-dev/pandas/blob/d9488016443ccd81a02321797c491eab7b404863/pandas/core/indexes/datetimes.py#L819-L833):
```
        Return index locations of values between particular times of day
        (e.g., 9:00-9:30AM).
        Parameters
        ----------
        start_time, end_time : datetime.time, str
            Time passed either as object (datetime.time) or as string in
            appropriate format (""%H:%M"", ""%H%M"", ""%I:%M%p"", ""%I%M%p"",
            ""%H:%M:%S"", ""%H%M%S"", ""%I:%M:%S%p"",""%I%M%S%p"").
        include_start : bool, default True
        include_end : bool, default True
```

```python
import pandas as pd
​
i = pd.date_range('2018-04-09', periods=4, freq='1D20min')
ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)
​
print(ts)
print(ts.between_time('0:15', '0:45'))
print(ts.between_time('0:45', '0:15'))
                     A
2018-04-09 00:00:00  1
2018-04-10 00:20:00  2
2018-04-11 00:40:00  3
2018-04-12 01:00:00  4
                     A
2018-04-10 00:20:00  2
2018-04-11 00:40:00  3
                     A
2018-04-09 00:00:00  1
2018-04-12 01:00:00  4
```

```python
i = pd.Series([""2021-01-01"", ""2021-02-10""], dtype=""datetime64[ns]"")
s = pd.Series([0,1], index=i)
print(s, ""\n"")
print(s.between_time(""0:00:01"", ""23:59:59""), ""\n"")
print(s.between_time(""0:00"", ""0:15""))
2021-01-01    0
2021-02-10    1
dtype: int64 

Series([], dtype: int64) 

2021-01-01    0
2021-02-10    1
dtype: int64
```
",2021-11-08T21:40:22Z,0,0,Nick Becker,@NVIDIA,True
205,[FEA] Initial support for string UDFs via Numba ,"**Is your feature request related to a problem? Please describe.**
Currently we can't use string columns inside UDFs. This is for a number of reasons. Firstly, there is limited support for strings in general in Numba, which forms the basis of our UDF framework. Secondly even if strings were supported in numba, we would still need to extend numba for it to be able to properly generate kernels that work as we expect on the buffers containing our string data. Lastly, there are special memory considerations on the GPU that complicate the situation further.

**Describe the solution you'd like**
Recently @davidwendt has experimented with a c++ class which solves many of the nuances around handling single strings that live on the device inside UDFs. @gmarkall subsequently wrote a proof of concept showing how simple string functions such as `len` can be overloaded using numba to map to the methods contained in that c++ class and baked into a kernel. We would like to plumb this machinery through cuDF. This roughly consists of the following steps:

1. Make it so that when cuDF is built, the c++ string class and its methods are precompiled and made available as a blob of PTX or similar that we can link to when building a kernel in python.
2. Create the pipeline in python that writes, links, compiles and executes the correct kernels that can leverage the aformentioned PTX blobs at runtime.
3. Create numba typing and lowering that overloads calls to common string functions in python and maps them to the corresponding methods of the c++ class. Ideally we'd do all of them although some may be more complex than others due to memory considerations. Thats 43 functions:
- `capitalize`
- `casefold`
- `center`
- `count`
- `encode`
- `endswith`
- `expandtabs`
- `find`
- `format`
- `format_map`
- `index`
- `isalnum`
- `isalpha`
- `isascii`
- `isdecimal`
- `isdigit`
- `islower`
- `isprintable`
- `isspace`
- `istitle`
- `isupper`
- `join`
- `ljust`
- `lower`
- `lstrip`
- `maketrans`
- `removeprefix`
- `removesuffix`
- `replace`
- `rfind`
- `rindex`
- `rjust`
- `rpartition`
- `rsplit`
- `rstrip`
- `split`
- `splitlines`
- `startswith`
- `swapcase`
- `title`
- `translate`
- `upper`
- `zfill`


Concretely, when we encounter a UDF that is written like this for example:

```python
def f(row):
    return len(row['str_field'])
```

Our code should
- Detect a declaration of `len` that we will write which expects a `MaskedType(string)` and returns a `MaskedType(int64)`
- Detect an implementation of the above (lowering) which calls a compiled version of the c++ class's `len` method when provided a pointer to the start of the string
- Write a kernel that distributes the individual column strings amongst parallel threads and runs the function capturing its output elementwise
- Run it
- If necessary assembles the results into a column

**Describe alternatives you've considered**


**Additional context**
If we can get this to work it lays the groundwork for being able to use other more complex types inside UDFs in the future, following the same pattern of using numba to map python code to external function calls that we write to operate on a single data element.

Similar issue for `applymap` https://github.com/rapidsai/cudf/issues/3802",2021-11-09T20:22:03Z,0,0,,NVIDIA,True
206,[BUG] groupby.size ignores as_index=False ,"Unlike other aggregations, groupby.size appears to ignore the `as_index` argument.

```python
import cudf
​
df = cudf.datasets.randomdata()
print(df.groupby(""id"", as_index=False).size().head())
print(df.to_pandas().groupby(""id"", as_index=False).size().head())
id
1003    1
942     1
1018    1
971     1
982     2
dtype: int32
    id  size
0  942     1
1  971     1
2  982     2
3  989     1
4  997     1
```

Environment:
<details>
conda list | grep ""rapids""
# packages in environment at /home/nicholasb/conda/envs/rapids-21.12:
cucim                     21.12.00a211108 cuda_11.2_py37_ge521dd4_16    rapidsai-nightly
cudf                      21.12.00a211109 cuda_11.2_py37_g3280be232d_233    rapidsai-nightly
cudf_kafka                21.12.00a211109 py37_g3280be232d_233    rapidsai-nightly
cugraph                   21.12.00a211109 cuda11.2_py37_g7b0df6e5_75    rapidsai-nightly
cuml                      21.12.00a211109 cuda11.2_py37_gb6c8bc84a_74    rapidsai-nightly
cusignal                  21.12.00a211109 py37_g8d0f511_6    rapidsai-nightly
cuspatial                 21.12.00a211101 py37_g9550f76_5    rapidsai-nightly
custreamz                 21.12.00a211109 py37_g3280be232d_233    rapidsai-nightly
cuxfilter                 21.12.00a211109 py37_g8b30a46_7    rapidsai-nightly
dask-cuda                 21.12.00a211109         py37_30    rapidsai-nightly
dask-cudf                 21.12.00a211109 cuda_11.2_py37_g3280be232d_233    rapidsai-nightly
libcucim                  21.12.00a211108 cuda11.2_ge521dd4_16    rapidsai-nightly
libcudf                   21.12.00a211109 cuda11.2_g3280be232d_233    rapidsai-nightly
libcudf_kafka             21.12.00a211109 g3280be232d_233    rapidsai-nightly
libcugraph                21.12.00a211109 cuda11.2_g7b0df6e5_75    rapidsai-nightly
libcuml                   21.12.00a211109 cuda11.2_gb6c8bc84a_74    rapidsai-nightly
libcumlprims              21.12.00a211102 cuda11.2_gd22e2e0_1    rapidsai-nightly
libcuspatial              21.12.00a211109 cuda11.2_g9550f76_5    rapidsai-nightly
librmm                    21.12.00a211109 cuda11_g59d6f34_22_has_cma    rapidsai-nightly
libxgboost                1.5.0dev.rapidsai21.12      cuda11.2_0    rapidsai-nightly
py-xgboost                1.5.0dev.rapidsai21.12  cuda11.2py37_0    rapidsai-nightly
rapids                    21.12.00a211109 cuda11.2_py37_gcd51acb_59    rapidsai-nightly
rapids-xgboost            21.12.00a211109 cuda11.2_py37_gcd51acb_59    rapidsai-nightly
rmm                       21.12.00a211109 cuda_11_py37_g59d6f34_22    rapidsai-nightly
ucx                       1.11.1+gc58db6b      cuda11.2_0    rapidsai-nightly
ucx-proc                  1.0.0                       gpu    rapidsai-nightly
ucx-py                    0.23.0a211109   py37_gc58db6b_24    rapidsai-nightly
xgboost                   1.5.0dev.rapidsai21.12  cuda11.2py37_0    rapidsai-nightly
</details>",2021-11-09T21:09:45Z,0,0,Nick Becker,@NVIDIA,True
207,[FEA] Add `sliced_child_begin()` and `sliced_child_end()` iterators for `column_view`,"Currently, the `column_view` class offers the accessor `child_begin()` and `child_end()` iterators that allow iterating over the children of a column. However, when we want to access the sliced children column (in many places), we have to do that manually. It would be great if we can cache the sliced children and provide iterators `sliced_child_begin()` and `sliced_child_end()` that allow quickly iterating over them.

Since different classes (`structs_column_view` and `lists_column_view`) have different interfaces for generating `column_view` of sliced child, we may have to implement the proposed iterators separately in these classes instead of implementing in the `column_view` base class.",2021-11-10T19:01:59Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
208,[FEA] Extend cudf's replace API `normalize_nans_and_zeros` to accept nested types,"Currently, the replace API `normalize_nans_and_zeros` only accepts a non-nested input column that is either FLOAT32 or FLOAT64 type. As a result, when we want to replace `-NaN` or `-0.0` in a child column of a nested column, we have to manually recursively iterate over all the children columns of the given column, check to see whether the current child column is FLOAT32 or FLOAT64 type before calling to `normalize_nans_and_zeros`.

We should extend `normalize_nans_and_zeros` to do this automatically. In other words, it should accept an input column of any type.",2021-11-10T19:10:36Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
209,[FEA] Byte Pair Encoding Tokenizer,"**Is your feature request related to a problem? Please describe.**

We should add byte pair encoding tokenizer to cuDF.  Like  our [subword-tokenizer](https://docs.rapids.ai/api/cudf/21.12/api_docs/api/cudf.core.subword_tokenizer.SubwordTokenizer.__call__.html) adds  a bridge to Bert link models.  Byte Pair EncodingTokenizer is used by `roberta`, `gpt-2` , `gpt-3` and will give us a bridge to a lot of DL models. 

We should focus porting a pre-trained tokenizer first. 


**Describe the solution you'd like**

The implimentation should follow [GPT-2 tokenizer ](https://huggingface.co/transformers/_modules/transformers/models/gpt2/tokenization_gpt2.html#GPT2Tokenizer) but should be extendable to the robert-a , `gpt-3`, `megatron` etc.  We should follow the HuggingFace API for this. 

**Algorithim:**


1. Add an identifier `(</w>)` at the end of each word to identify the end of a word and then calculate the word frequency in the text.
2. Split the word into characters and then calculate the character frequency.
3. From the character tokens, for a predefined number of iterations, count the frequency of the consecutive byte pairs and merge the most frequently occurring byte pairing.
4. Keep iterating until you have reached the iteration limit (set by you) or until you have reached the token limit.

Ref: [Link](https://www.freecodecamp.org/news/evolution-of-tokenization/)


**Additional context**

Best Explanation of Algorithm: https://leimao.github.io/blog/Byte-Pair-Encoding/


CC: @randerzander , @beckernick 

",2021-11-11T00:33:11Z,1,0,Vibhu Jawa,Nvidia,True
210,[FEA] Improve cudf.read_csv empty file error message,"It's embarrassingly common to accidentally produce empty ""CSV"" files, then for a downstream system to fail on attempting to read them.

If I'm trying to read an empty file with Pandas, I get a helpful error message indicating the problem.

```
import pandas as pd

with open('test.csv', 'w') as fp:
    fp.write('')

pd.read_csv('test.csv')
```
```
---------------------------------------------------------------------------
EmptyDataError                            Traceback (most recent call last)
/tmp/ipykernel_3734411/2382990058.py in <module>
      4     fp.write('')
      5 
----> 6 pd.read_csv('test.csv')

~/conda/envs/dsql-dask-main/lib/python3.8/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs)
    309                     stacklevel=stacklevel,
    310                 )
--> 311             return func(*args, **kwargs)
    312 
    313         return wrapper

~/conda/envs/dsql-dask-main/lib/python3.8/site-packages/pandas/io/parsers/readers.py in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)
    584     kwds.update(kwds_defaults)
    585 
--> 586     return _read(filepath_or_buffer, kwds)
    587 
    588 

~/conda/envs/dsql-dask-main/lib/python3.8/site-packages/pandas/io/parsers/readers.py in _read(filepath_or_buffer, kwds)
    480 
    481     # Create the parser.
--> 482     parser = TextFileReader(filepath_or_buffer, **kwds)
    483 
    484     if chunksize or iterator:

~/conda/envs/dsql-dask-main/lib/python3.8/site-packages/pandas/io/parsers/readers.py in __init__(self, f, engine, **kwds)
    809             self.options[""has_index_names""] = kwds[""has_index_names""]
    810 
--> 811         self._engine = self._make_engine(self.engine)
    812 
    813     def close(self):

~/conda/envs/dsql-dask-main/lib/python3.8/site-packages/pandas/io/parsers/readers.py in _make_engine(self, engine)
   1038             )
   1039         # error: Too many arguments for ""ParserBase""
-> 1040         return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]
   1041 
   1042     def _failover_to_python(self):

~/conda/envs/dsql-dask-main/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py in __init__(self, src, **kwds)
     67         kwds[""dtype""] = ensure_dtype_objs(kwds.get(""dtype"", None))
     68         try:
---> 69             self._reader = parsers.TextReader(self.handles.handle, **kwds)
     70         except Exception:
     71             self.handles.close()

~/conda/envs/dsql-dask-main/lib/python3.8/site-packages/pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__()

EmptyDataError: No columns to parse from file
```

When I try to read an empty CSV file from Dask-cudf (or cudf directly), it's not clear if I perhaps OOMed or some other non-input-file-related problem:
```
~/conda/envs/dsql-dask-main/lib/python3.8/site-packages/cudf-21.12.0a0+254.g84e5a03032-py3.8-linux-x86_64.egg/cudf/io/csv.py in read_csv(filepath_or_buffer, lineterminator, quotechar, quoting, doublequote, header, mangle_dupe_cols, usecols, sep, delimiter, delim_whitespace, skipinitialspace, names, dtype, skipfooter, skiprows, dayfirst, compression, thousands, decimal, true_values, false_values, nrows, byte_range, skip_blank_lines, parse_dates, comment, na_values, keep_default_na, na_filter, prefix, index_col, use_python_file_object, **kwargs)
     78         na_values = [na_values]
     79 
---> 80     return libcudf.csv.read_csv(
     81         filepath_or_buffer,
     82         lineterminator=lineterminator,

cudf/_lib/csv.pyx in cudf._lib.csv.read_csv()

cudf/_lib/csv.pyx in cudf._lib.csv.make_csv_reader_options()

cudf/_lib/io/utils.pyx in cudf._lib.io.utils.make_source_info()

IndexError: Out of bounds on buffer access (axis 0)
```",2021-11-12T22:16:00Z,0,0,Randy Gelhausen,,False
211,[QST] Proper way to do mean centering for fixed-row-length LIST type column,"In machine leaning practice, it's very common to calculate mean value and do mean subtractions for input column data. For example:

The input is a LIST type column data with fixed row-length of 3:
```
[1,2,3]
[3,1,2]
[2,3,1]
```
User wants to do mean centering for it: calculate the mean value for each sub column, in this case, it's always 2. Then do mean subtractions:
```
[-1, 0, 1]
[1, -1, 0]
[0, 1, -1] 
```

Is their any nice way to do it? Currently I can only think of:
1. use `cudf::lists::extract_list_element` to extract all sub column
2. calculate mean value for each of them
3. do subtraction
4. construct them back to a LIST column.",2021-11-15T08:02:44Z,0,0,Allen Xu,@Nvidia,True
212,[FEA] `pack`/`unpack` functions to merge/split (multiple) `device_buffer`(s),"**Is your feature request related to a problem? Please describe.**

It would be useful to have a `pack` function to merge multiple `device_buffer`s into a single `device_buffer`. This is helpful in situations where having one large `device_buffer` to read from is more performant. However it ultimately consists of many smaller data segments that would need to be merged together. Example use cases include sending data with UCX and spilling data from device to host.

Similarly it would be useful to have an `unpack` function to split a `device_buffer` into multiple `device_buffer`s. This is helpful in situations where having one large `device_buffer` to write into is more performant. However it ultimately consists of many smaller data segments that may need to be freed at different times. Example use cases include receiving data with UCX and unspilling data from host to device.

**Describe the solution you'd like**

For `pack` it would be nice if it simply takes several `device_buffer`s in `vector` form and return a single one. Additionally it would be nice if `pack` could recognize when `device_buffer`s are contiguous in memory and avoid a copy. Though admittedly this last part is tricky (maybe less so if `unpack` is used regularly?). If we allow `pack` to change the order (to benefit from contiguous memory for example), we may want additional information about where the data segments live in the larger `device_buffer`.

For `unpack` it would be nice if it takes a single `device_buffer` and `size_t`s in `vector` form to split and return a `vector` of multiple `device_buffer`s. Additionally it would be nice if `unpack` did not perform any copies. Hopefully that is straightforward, but there may be things I'm not understanding.

**Describe alternatives you've considered**

One might consider using variadics in C++ for the arguments. While nice at the C++ level, this seems tricky to use from the Cython and Python levels. Hence the suggestion to just use `vector`.

`pack` itself could be implemented by a user simply allocating a larger buffer and copying over. Would be nice to avoid the extra allocation when possible though (which may require knowledge that RMM has about the allocations).

**Additional context**

Having `unpack` in particular would be helpful for aggregated receives. A natural extension of this would be to have `pack` for aggregated sends. All-in-all this should allow transmitting a larger amount of data at once with UCX and thus benefiting from this use case it is more honed for. PR  ( https://github.com/dask/distributed/pull/3453 ) provides a WIP implementation of aggregated receives for context.

Also having `pack` would be useful when spilling several `device_buffer`s from device to host as it would allow us to pack them into one `device_buffer` before transferring ( https://github.com/rapidsai/dask-cuda/issues/250 ). Having `unpack` would help us break up the allocation whenever the object is unspilled.

This need has also come up in downstream contexts ( https://github.com/rapidsai/cudf/issues/3793 ). Maybe they would benefit from an upstream solution as well?",2020-03-01T22:37:33Z,0,0,,,False
213,[FEA] Deprecate Series.as_mask,"**Is your feature request related to a problem? Please describe.**
Doctests (#9815) show the following doctest failing:
https://github.com/rapidsai/cudf/blob/582cc6e466c7d941e1b34893fd56fbd42fe90d68/python/cudf/cudf/core/series.py#L1783-L1793

**Describe the solution you'd like**
From a preliminary discussion with @vyasr, we probably want to deprecate and remove `as_mask` from the public API.

In the short term, I am removing the doctests for `Series.as_mask().to_host_array()` in #9815 (f881890a2da5e139358e3d2f531d663853b53748) because it's somewhat untestable and not helpful to demonstrate to users if we deprecate and remove the feature. It appears that the buffer has a size of 64 bytes. I assume that's the smallest allocation RMM can give, but that's not obvious to users who look at the output data of the doctest. Furthermore, the doctest shows uninitialized (garbage) data in the output of `to_host_array()` which is misleading.",2021-12-02T21:33:01Z,0,0,Bradley Dice,@NVIDIA @rapidsai,True
214,[FEA] Improve the performance of sample,"**Is your feature request related to a problem? Please describe.**
The sample without replacement is slower than the CPU.

**Additional context**
With replacement is really fast. 
This indicates that thrust::shuffle_copy is the reason for the slowness.
```
// replacement is true
spark.time(spark.range(Int.MaxValue * 12L).sample(true, 0.01, 0).selectExpr(""SUM(id)"", ""COUNT(id)"").show())
+-------------------+---------+
|            sum(id)|count(id)|
+-------------------+---------+
|3320410258800588221|257697960|
+-------------------+---------+
Time taken: 670 ms

// without replacement
scala> spark.time(spark.range(Int.MaxValue).sample(0.01, 0).agg(functions.sum(""id"")).show())
+-----------------+                                                             
|          sum(id)|
+-----------------+
|23058247476802342|
+-----------------+

Time taken: 1608 ms
```

[Sample code link](https://github.com/rapidsai/cudf/blob/branch-22.02/cpp/include/cudf/copying.hpp#L935)",2021-12-03T09:29:34Z,0,0,Chong Gao,,False
215,[BUG] cuDF cannot concat single level index frame with multiIndex frame,"**Describe the bug**
Exception is raised when concatenating a single level index frame with a multiIndex frame

**Steps/Code to reproduce bug**
```python
>>> midx = cudf.MultiIndex.from_tuples([(1, 2), (3, 4)])
>>> df1 = cudf.DataFrame([1, 1], index=midx)
>>> df2 = cudf.DataFrame([2, 2])
>>> cudf.concat([df1, df2])
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/yhw/dev/rapids/cudf/python/cudf/cudf/core/reshape.py"", line 384, in concat
    result = cudf.DataFrame._concat(
  File ""/home/yhw/dev/rapids/compose/etc/conda/cuda_11.5/envs/rapids/lib/python3.8/contextlib.py"", line 75, in inner
    return func(*args, **kwds)
  File ""/home/yhw/dev/rapids/cudf/python/cudf/cudf/core/dataframe.py"", line 1482, in _concat
    # order. This strips the given index/column names and replaces the
  File ""/home/yhw/dev/rapids/cudf/python/cudf/cudf/core/dataframe.py"", line 6800, in _cast_cols_to_common_dtypes
IndexError: list assignment index out of range
```

**Expected behavior**
Match pandas.
```python
>>> pd.concat([df1.to_pandas(), df2.to_pandas()])
        0
(1, 2)  1
(3, 4)  1
0       2
1       2
```

**Environment overview (please complete the following information)**
 - Environment location: [compose]
 - Method of cuDF install: [compose]

**Environment details**
<details><summary>Click here to see environment details</summary><pre>
     
     **git***
     commit 86d8c6ea68b2886366251f21f421727657cf1a83 (HEAD -> catogorical_refactor, origin/catogorical_refactor)
     Author: Michael Wang <michaelwang0905@icloud.com>
     Date:   Mon Nov 29 10:54:48 2021 -0800
     
     remove stale test code and some mypy fixes
     **git submodules***
     
     ***OS Information***
     DISTRIB_ID=Ubuntu
     DISTRIB_RELEASE=20.04
     DISTRIB_CODENAME=focal
     DISTRIB_DESCRIPTION=""Ubuntu 20.04.3 LTS""
     NAME=""Ubuntu""
     VERSION=""20.04.3 LTS (Focal Fossa)""
     ID=ubuntu
     ID_LIKE=debian
     PRETTY_NAME=""Ubuntu 20.04.3 LTS""
     VERSION_ID=""20.04""
     HOME_URL=""https://www.ubuntu.com/""
     SUPPORT_URL=""https://help.ubuntu.com/""
     BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
     PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
     VERSION_CODENAME=focal
     UBUNTU_CODENAME=focal
     Linux yhw-HP-Z8-G4-Workstation 5.11.0-40-generic #44~20.04.2-Ubuntu SMP Tue Oct 26 18:07:44 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux
     
     ***GPU Information***
     Mon Dec  6 16:21:45 2021
     +-----------------------------------------------------------------------------+
     | NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |
     |-------------------------------+----------------------+----------------------+
     | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
     | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
     |                               |                      |               MIG M. |
     |===============================+======================+======================|
     |   0  Quadro RTX 8000     On   | 00000000:15:00.0 Off |                  Off |
     | 34%   42C    P8    28W / 260W |    958MiB / 48601MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     
     +-----------------------------------------------------------------------------+
     | Processes:                                                                  |
     |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
     |        ID   ID                                                   Usage      |
     |=============================================================================|
     +-----------------------------------------------------------------------------+
     
     ***CPU***
     Architecture:                    x86_64
     CPU op-mode(s):                  32-bit, 64-bit
     Byte Order:                      Little Endian
     Address sizes:                   46 bits physical, 48 bits virtual
     CPU(s):                          12
     On-line CPU(s) list:             0-11
     Thread(s) per core:              2
     Core(s) per socket:              6
     Socket(s):                       1
     NUMA node(s):                    1
     Vendor ID:                       GenuineIntel
     CPU family:                      6
     Model:                           85
     Model name:                      Intel(R) Xeon(R) Gold 6128 CPU @ 3.40GHz
     Stepping:                        4
     CPU MHz:                         1934.972
     CPU max MHz:                     3700.0000
     CPU min MHz:                     1200.0000
     BogoMIPS:                        6800.00
     Virtualization:                  VT-x
     L1d cache:                       192 KiB
     L1i cache:                       192 KiB
     L2 cache:                        6 MiB
     L3 cache:                        19.3 MiB
     NUMA node0 CPU(s):               0-11
     Vulnerability Itlb multihit:     KVM: Mitigation: VMX disabled
     Vulnerability L1tf:              Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable
     Vulnerability Mds:               Mitigation; Clear CPU buffers; SMT vulnerable
     Vulnerability Meltdown:          Mitigation; PTI
     Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp
     Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
     Vulnerability Spectre v2:        Mitigation; Full generic retpoline, IBPB conditional, IBRS_FW, STIBP conditional, RSB filling
     Vulnerability Srbds:             Not affected
     Vulnerability Tsx async abort:   Mitigation; Clear CPU buffers; SMT vulnerable
     Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke md_clear flush_l1d
     
     ***CMake***
     /home/yhw/dev/rapids/compose/etc/conda/cuda_11.5/envs/rapids/bin/cmake
     cmake version 3.21.3
     
     CMake suite maintained and supported by Kitware (kitware.com/cmake).
     
     ***g++***
     /usr/local/bin/g++
     g++ (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
     Copyright (C) 2019 Free Software Foundation, Inc.
     This is free software; see the source for copying conditions.  There is NO
     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
     
     
     ***nvcc***
     /usr/local/bin/nvcc
     nvcc: NVIDIA (R) Cuda compiler driver
     Copyright (c) 2005-2021 NVIDIA Corporation
     Built on Mon_Sep_13_19:13:29_PDT_2021
     Cuda compilation tools, release 11.5, V11.5.50
     Build cuda_11.5.r11.5/compiler.30411180_0
     
     ***Python***
     /home/yhw/dev/rapids/compose/etc/conda/cuda_11.5/envs/rapids/bin/python
     Python 3.8.12
     
     ***Environment Variables***
     PATH                            : /home/yhw/dev/rapids/compose/etc/conda/cuda_11.5/envs/rapids/bin:/home/yhw/dev/rapids/compose/etc/conda/cuda_11.5/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/cuda/bin
     LD_LIBRARY_PATH                 : /home/yhw/dev/rapids/compose/etc/conda/cuda_11.5/envs/rapids/lib:/home/yhw/dev/rapids/compose/etc/conda/cuda_11.5/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/lib:/home/yhw/dev/rapids/rmm/build/release:/home/yhw/dev/rapids/cudf/cpp/build/release:/home/yhw/dev/rapids/raft/cpp/build/release:/home/yhw/dev/rapids/cuml/cpp/build/release:/home/yhw/dev/rapids/cugraph/cpp/build/release:/home/yhw/dev/rapids/cuspatial/cpp/build/release
     NUMBAPRO_NVVM                   :
     NUMBAPRO_LIBDEVICE              :
     CONDA_PREFIX                    : /home/yhw/dev/rapids/compose/etc/conda/cuda_11.5/envs/rapids
     PYTHON_PATH                     :
     
     ***conda packages***
     /home/yhw/dev/rapids/compose/etc/conda/cuda_11.5/bin/conda
     # packages in environment at /home/yhw/dev/rapids/compose/etc/conda/cuda_11.5/envs/rapids:
     #
     # Name                    Version                   Build  Channel
     _libgcc_mutex             0.1                 conda_forge    conda-forge
     _openmp_mutex             4.5                      1_llvm    conda-forge
     abseil-cpp                20210324.2           h9c3ff4c_0    conda-forge
     alabaster                 0.7.12                     py_0    conda-forge
     appdirs                   1.4.4              pyh9f0ad1d_0    conda-forge
     argon2-cffi               21.1.0           py38h497a2fe_2    conda-forge
     arrow-cpp                 5.0.0           py38h5b78be3_17_cuda    conda-forge
     arrow-cpp-proc            3.0.0                      cuda    conda-forge
     async_generator           1.10                       py_0    conda-forge
     attrs                     21.2.0             pyhd8ed1ab_0    conda-forge
     aws-c-auth                0.6.7                hfef2836_0    conda-forge
     aws-c-cal                 0.5.12               h70efedd_7    conda-forge
     aws-c-common              0.6.17               h7f98852_0    conda-forge
     aws-c-compression         0.2.14               h7c7754b_7    conda-forge
     aws-c-event-stream        0.2.7               hb80ed28_31    conda-forge
     aws-c-http                0.6.10               h58a30cf_2    conda-forge
     aws-c-io                  0.10.13              he836878_5    conda-forge
     aws-c-mqtt                0.7.9                h042a236_0    conda-forge
     aws-c-s3                  0.1.27              hae5f17b_11    conda-forge
     aws-c-sdkutils            0.1.1                h7c7754b_4    conda-forge
     aws-checksums             0.1.12               h7c7754b_6    conda-forge
     aws-crt-cpp               0.17.8               h82bac0c_1    conda-forge
     aws-sdk-cpp               1.9.148              hfe59705_0    conda-forge
     babel                     2.9.1              pyh44b312d_0    conda-forge
     backcall                  0.2.0              pyh9f0ad1d_0    conda-forge
     backports                 1.0                        py_2    conda-forge
     backports-zoneinfo        0.2.1                    pypi_0    pypi
     backports.functools_lru_cache 1.6.4              pyhd8ed1ab_0    conda-forge
     beautifulsoup4            4.10.0             pyha770c72_0    conda-forge
     binutils_impl_linux-64    2.36.1               h193b22a_2    conda-forge
     black                     19.10b0                    py_4    conda-forge
     bleach                    4.1.0              pyhd8ed1ab_0    conda-forge
     bokeh                     2.4.2            py38h578d9bd_0    conda-forge
     brotlipy                  0.7.0           py38h497a2fe_1003    conda-forge
     bzip2                     1.0.8                h7f98852_4    conda-forge
     c-ares                    1.18.1               h7f98852_0    conda-forge
     ca-certificates           2021.10.8            ha878542_0    conda-forge
     cachetools                4.2.4              pyhd8ed1ab_0    conda-forge
     certifi                   2021.10.8        py38h578d9bd_1    conda-forge
     cffi                      1.15.0           py38h3931269_0    conda-forge
     cfgv                      3.3.1              pyhd8ed1ab_0    conda-forge
     charset-normalizer        2.0.8              pyhd8ed1ab_0    conda-forge
     clang                     11.1.0               ha770c72_1    conda-forge
     clang-11                  11.1.0          default_ha53f305_1    conda-forge
     clang-tools               11.1.0          default_ha53f305_1    conda-forge
     clangxx                   11.1.0          default_ha53f305_1    conda-forge
     click                     8.0.3            py38h578d9bd_1    conda-forge
     cloudpickle               2.0.0              pyhd8ed1ab_0    conda-forge
     cmake                     3.21.3               h8897547_0    conda-forge
     cmake-format              0.6.11             pyh9f0ad1d_0    conda-forge
     cmake_setuptools          0.1.3                      py_0    rapidsai
     colorama                  0.4.4              pyh9f0ad1d_0    conda-forge
     commonmark                0.9.1                      py_0    conda-forge
     cryptography              36.0.0           py38h3e25421_0    conda-forge
     cudatoolkit               11.5.0               h36ae40a_9    nvidia
     cupy                      9.6.0            py38h177b0fd_0    conda-forge
     cyrus-sasl                2.1.27               h230043b_5    conda-forge
     cython                    0.29.24          py38h709712a_1    conda-forge
     cytoolz                   0.11.2           py38h497a2fe_1    conda-forge
     dask                      2021.11.2+18.g43247806          pypi_0    pypi
     dataclasses               0.8                pyhc8e2a94_3    conda-forge
     debugpy                   1.5.1            py38h709712a_0    conda-forge
     decorator                 5.1.0              pyhd8ed1ab_0    conda-forge
     defusedxml                0.7.1              pyhd8ed1ab_0    conda-forge
     distlib                   0.3.3              pyhd8ed1ab_0    conda-forge
     distributed               2021.11.2+19.gababd210          pypi_0    pypi
     dlpack                    0.5                  h9c3ff4c_0    conda-forge
     docutils                  0.17.1           py38h578d9bd_1    conda-forge
     double-conversion         3.1.6                h9c3ff4c_0    conda-forge
     editdistance-s            1.0.0            py38h1fd1430_2    conda-forge
     entrypoints               0.3             py38h32f6830_1002    conda-forge
     execnet                   1.9.0              pyhd8ed1ab_0    conda-forge
     expat                     2.4.1                h9c3ff4c_0    conda-forge
     fastavro                  1.4.7            py38h497a2fe_1    conda-forge
     fastrlock                 0.8              py38h709712a_1    conda-forge
     filelock                  3.4.0              pyhd8ed1ab_0    conda-forge
     flake8                    3.8.3                      py_1    conda-forge
     freetype                  2.10.4               h0708190_1    conda-forge
     fsspec                    2021.11.1          pyhd8ed1ab_0    conda-forge
     future                    0.18.2           py38h578d9bd_4    conda-forge
     gcc_impl_linux-64         11.2.0              h82a94d6_11    conda-forge
     gcovr                     5.0                pyhd8ed1ab_0    conda-forge
     gettext                   0.19.8.1          h73d1719_1008    conda-forge
     gflags                    2.2.2             he1b5a44_1004    conda-forge
     glog                      0.5.0                h48cff8f_0    conda-forge
     gmp                       6.2.1                h58526e2_0    conda-forge
     grpc-cpp                  1.42.0               h7e358d9_0    conda-forge
     heapdict                  1.0.1                      py_0    conda-forge
     huggingface_hub           0.1.2              pyhd8ed1ab_0    conda-forge
     hypothesis                6.30.1             pyhd8ed1ab_0    conda-forge
     icu                       69.1                 h9c3ff4c_0    conda-forge
     identify                  2.3.7              pyhd8ed1ab_0    conda-forge
     idna                      3.1                pyhd3deb0d_0    conda-forge
     imagesize                 1.3.0              pyhd8ed1ab_0    conda-forge
     importlib-metadata        4.8.2            py38h578d9bd_0    conda-forge
     importlib_metadata        4.8.2                hd8ed1ab_0    conda-forge
     importlib_resources       5.4.0              pyhd8ed1ab_0    conda-forge
     iniconfig                 1.1.1              pyh9f0ad1d_0    conda-forge
     ipykernel                 6.5.0            py38he5a9106_1    conda-forge
     ipython                   7.30.1           py38h578d9bd_0    conda-forge
     ipython_genutils          0.2.0                      py_1    conda-forge
     isort                     5.6.4                      py_0    conda-forge
     jbig                      2.1               h7f98852_2003    conda-forge
     jedi                      0.18.1           py38h578d9bd_0    conda-forge
     jinja2                    3.0.3              pyhd8ed1ab_0    conda-forge
     joblib                    1.1.0              pyhd8ed1ab_0    conda-forge
     jpeg                      9d                   h36c2ea0_0    conda-forge
     jsonschema                4.2.1              pyhd8ed1ab_0    conda-forge
     jupyter_client            7.1.0              pyhd8ed1ab_0    conda-forge
     jupyter_core              4.9.1            py38h578d9bd_1    conda-forge
     jupyterlab_pygments       0.1.2              pyh9f0ad1d_0    conda-forge
     kernel-headers_linux-64   2.6.32              he073ed8_15    conda-forge
     krb5                      1.19.2               hcc1bbae_3    conda-forge
     lcms2                     2.12                 hddcbb42_0    conda-forge
     ld_impl_linux-64          2.36.1               hea4e1c9_2    conda-forge
     lerc                      3.0                  h9c3ff4c_0    conda-forge
     libblas                   3.9.0            12_linux64_mkl    conda-forge
     libbrotlicommon           1.0.9                h7f98852_6    conda-forge
     libbrotlidec              1.0.9                h7f98852_6    conda-forge
     libbrotlienc              1.0.9                h7f98852_6    conda-forge
     libcblas                  3.9.0            12_linux64_mkl    conda-forge
     libclang-cpp11.1          11.1.0          default_ha53f305_1    conda-forge
     libcurl                   7.80.0               h2574ce0_0    conda-forge
     libdeflate                1.8                  h7f98852_0    conda-forge
     libedit                   3.1.20191231         he28a2e2_2    conda-forge
     libev                     4.33                 h516909a_1    conda-forge
     libevent                  2.1.10               h9b69904_4    conda-forge
     libffi                    3.4.2                h7f98852_5    conda-forge
     libgcc-devel_linux-64     11.2.0              h0952999_11    conda-forge
     libgcc-ng                 11.2.0              h1d223b6_11    conda-forge
     libgcrypt                 1.9.4                h7f98852_0    conda-forge
     libgomp                   11.2.0              h1d223b6_11    conda-forge
     libgpg-error              1.42                 h9c3ff4c_0    conda-forge
     libgsasl                  1.10.0               h5b4c23d_0    conda-forge
     libiconv                  1.16                 h516909a_0    conda-forge
     liblapack                 3.9.0            12_linux64_mkl    conda-forge
     libllvm11                 11.1.0               hf817b99_2    conda-forge
     libnghttp2                1.43.0               h812cca2_1    conda-forge
     libnsl                    2.0.0                h7f98852_0    conda-forge
     libntlm                   1.4               h7f98852_1002    conda-forge
     libpng                    1.6.37               h21135ba_2    conda-forge
     libprotobuf               3.18.1               h780b84a_0    conda-forge
     librdkafka                1.7.0                hc49e61c_0    conda-forge
     libsanitizer              11.2.0              he4da1e4_11    conda-forge
     libsodium                 1.0.18               h36c2ea0_1    conda-forge
     libssh2                   1.10.0               ha56f1ee_2    conda-forge
     libstdcxx-ng              11.2.0              he4da1e4_11    conda-forge
     libthrift                 0.15.0               he6d91bd_1    conda-forge
     libtiff                   4.3.0                h6f004c6_2    conda-forge
     libutf8proc               2.6.1                h7f98852_0    conda-forge
     libuv                     1.42.0               h7f98852_0    conda-forge
     libwebp-base              1.2.1                h7f98852_0    conda-forge
     libxml2                   2.9.12               h885dcf4_1    conda-forge
     libxslt                   1.1.33               h0ef7038_3    conda-forge
     libzlib                   1.2.11            h36c2ea0_1013    conda-forge
     llvm-openmp               12.0.1               h4bd325d_1    conda-forge
     llvmlite                  0.37.0           py38h4630a5e_1    conda-forge
     locket                    0.2.0                      py_2    conda-forge
     lxml                      4.6.4            py38hf1fe3a4_0    conda-forge
     lz4-c                     1.9.3                h9c3ff4c_1    conda-forge
     markdown                  3.3.6              pyhd8ed1ab_0    conda-forge
     markupsafe                2.0.1            py38h497a2fe_1    conda-forge
     matplotlib-inline         0.1.3              pyhd8ed1ab_0    conda-forge
     mccabe                    0.6.1                      py_1    conda-forge
     mimesis                   4.0.0              pyh9f0ad1d_0    conda-forge
     mistune                   0.8.4           py38h497a2fe_1005    conda-forge
     mkl                       2021.4.0           h8d4b97c_729    conda-forge
     more-itertools            8.12.0             pyhd8ed1ab_0    conda-forge
     msgpack-python            1.0.3            py38h1fd1430_0    conda-forge
     mypy                      0.782                      py_0    conda-forge
     mypy_extensions           0.4.3            py38h578d9bd_4    conda-forge
     nbclient                  0.5.9              pyhd8ed1ab_0    conda-forge
     nbconvert                 6.3.0            py38h578d9bd_1    conda-forge
     nbformat                  5.1.3              pyhd8ed1ab_0    conda-forge
     nbsphinx                  0.8.7              pyhd8ed1ab_0    conda-forge
     ncurses                   6.2                  h58526e2_4    conda-forge
     nest-asyncio              1.5.1              pyhd8ed1ab_0    conda-forge
     ninja                     1.10.2               h4bd325d_1    conda-forge
     nodeenv                   1.6.0              pyhd8ed1ab_0    conda-forge
     notebook                  6.4.6              pyha770c72_0    conda-forge
     numba                     0.54.1           py38h4bf6c61_0    conda-forge
     numpy                     1.20.3           py38h9894fe3_1    conda-forge
     numpydoc                  1.1.0                      py_1    conda-forge
     nvtx                      0.2.3            py38h497a2fe_1    conda-forge
     olefile                   0.46               pyh9f0ad1d_1    conda-forge
     openjpeg                  2.4.0                hb52868f_1    conda-forge
     openssl                   1.1.1l               h7f98852_0    conda-forge
     orc                       1.7.1                h68e2c4e_0    conda-forge
     packaging                 21.3               pyhd8ed1ab_0    conda-forge
     pandas                    1.3.4            py38h43a58ef_1    conda-forge
     pandoc                    1.19.2                        0    conda-forge
     pandocfilters             1.5.0              pyhd8ed1ab_0    conda-forge
     parquet-cpp               1.5.1                         1    conda-forge
     parso                     0.8.2              pyhd8ed1ab_0    conda-forge
     partd                     1.2.0              pyhd8ed1ab_0    conda-forge
     pathspec                  0.9.0              pyhd8ed1ab_0    conda-forge
     pexpect                   4.8.0            py38h32f6830_1    conda-forge
     pickleshare               0.7.5           py38h32f6830_1002    conda-forge
     pillow                    8.4.0            py38h8e6f84c_0    conda-forge
     pip                       21.3.1             pyhd8ed1ab_0    conda-forge
     pluggy                    1.0.0            py38h578d9bd_2    conda-forge
     pre-commit                2.16.0           py38h578d9bd_0    conda-forge
     prometheus_client         0.12.0             pyhd8ed1ab_0    conda-forge
     prompt-toolkit            3.0.22             pyha770c72_0    conda-forge
     protobuf                  3.18.1           py38h709712a_0    conda-forge
     psutil                    5.8.0            py38h497a2fe_2    conda-forge
     ptvsd                     4.3.2                    pypi_0    pypi
     ptxcompiler               0.1.0            py38hb739d79_0    rapidsai
     ptyprocess                0.7.0              pyhd3deb0d_0    conda-forge
     pudb                      2021.2.2                 pypi_0    pypi
     py                        1.11.0             pyh6c4a22f_0    conda-forge
     py-cpuinfo                8.0.0              pyhd8ed1ab_0    conda-forge
     pyarrow                   5.0.0           py38hb90f683_17_cuda    conda-forge
     pycodestyle               2.6.0              pyh9f0ad1d_0    conda-forge
     pycparser                 2.21               pyhd8ed1ab_0    conda-forge
     pydata-sphinx-theme       0.7.2              pyhd8ed1ab_0    conda-forge
     pydocstyle                6.1.1              pyhd8ed1ab_0    conda-forge
     pyflakes                  2.2.0              pyh9f0ad1d_0    conda-forge
     pygments                  2.10.0             pyhd8ed1ab_0    conda-forge
     pyopenssl                 21.0.0             pyhd8ed1ab_0    conda-forge
     pyorc                     0.5.0                    pypi_0    pypi
     pyparsing                 3.0.6              pyhd8ed1ab_0    conda-forge
     pyrsistent                0.18.0           py38h497a2fe_0    conda-forge
     pysocks                   1.7.1            py38h578d9bd_4    conda-forge
     pytest                    6.2.5            py38h578d9bd_1    conda-forge
     pytest-benchmark          3.4.1              pyhd8ed1ab_0    conda-forge
     pytest-forked             1.3.0              pyhd3deb0d_0    conda-forge
     pytest-pudb               0.7.0                    pypi_0    pypi
     pytest-xdist              2.4.0              pyhd8ed1ab_0    conda-forge
     python                    3.8.12          hb7a2778_2_cpython    conda-forge
     python-confluent-kafka    1.7.0            py38h497a2fe_2    conda-forge
     python-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge
     python-snappy             0.6.0            py38h49bdff1_1    conda-forge
     python_abi                3.8                      2_cp38    conda-forge
     pytorch                   1.10.0          cpu_py38h1ee18c8_0    conda-forge
     pytz                      2021.3             pyhd8ed1ab_0    conda-forge
     pyyaml                    6.0              py38h497a2fe_3    conda-forge
     pyzmq                     22.3.0           py38h2035c66_1    conda-forge
     rapidjson                 1.1.0             he1b5a44_1002    conda-forge
     re2                       2021.11.01           h9c3ff4c_0    conda-forge
     readline                  8.1                  h46c0cb4_0    conda-forge
     recommonmark              0.7.1              pyhd8ed1ab_0    conda-forge
     regex                     2021.11.10       py38h497a2fe_0    conda-forge
     requests                  2.26.0             pyhd8ed1ab_1    conda-forge
     rhash                     1.4.1                h7f98852_0    conda-forge
     s2n                       1.3.0                h9b69904_0    conda-forge
     sacremoses                0.0.46             pyhd8ed1ab_0    conda-forge
     send2trash                1.8.0              pyhd8ed1ab_0    conda-forge
     setuptools                59.4.0           py38h578d9bd_0    conda-forge
     six                       1.16.0             pyh6c4a22f_0    conda-forge
     sleef                     3.5.1                h9b69904_2    conda-forge
     snappy                    1.1.8                he1b5a44_3    conda-forge
     snowballstemmer           2.2.0              pyhd8ed1ab_0    conda-forge
     sortedcontainers          2.4.0              pyhd8ed1ab_0    conda-forge
     soupsieve                 2.3                pyhd8ed1ab_0    conda-forge
     spdlog                    1.8.5                h4bd325d_0    conda-forge
     sphinx                    4.3.1              pyh6c4a22f_0    conda-forge
     sphinx-copybutton         0.4.0              pyhd8ed1ab_0    conda-forge
     sphinx-markdown-tables    0.0.15             pyhd3deb0d_0    conda-forge
     sphinxcontrib-applehelp   1.0.2                      py_0    conda-forge
     sphinxcontrib-devhelp     1.0.2                      py_0    conda-forge
     sphinxcontrib-htmlhelp    2.0.0              pyhd8ed1ab_0    conda-forge
     sphinxcontrib-jsmath      1.0.1                      py_0    conda-forge
     sphinxcontrib-qthelp      1.0.3                      py_0    conda-forge
     sphinxcontrib-serializinghtml 1.1.5              pyhd8ed1ab_1    conda-forge
     sphinxcontrib-websupport  1.2.4              pyhd8ed1ab_1    conda-forge
     sqlite                    3.37.0               h9cd32fc_0    conda-forge
     streamz                   0.6.3              pyh6c4a22f_0    conda-forge
     sysroot_linux-64          2.12                he073ed8_15    conda-forge
     tbb                       2021.4.0             h4bd325d_1    conda-forge
     tblib                     1.7.0              pyhd8ed1ab_0    conda-forge
     terminado                 0.12.1           py38h578d9bd_1    conda-forge
     testpath                  0.5.0              pyhd8ed1ab_0    conda-forge
     tk                        8.6.11               h27826a3_1    conda-forge
     tokenizers                0.10.3           py38hb63a372_1    conda-forge
     toml                      0.10.2             pyhd8ed1ab_0    conda-forge
     toolz                     0.11.2             pyhd8ed1ab_0    conda-forge
     tornado                   6.1              py38h497a2fe_2    conda-forge
     tqdm                      4.62.3             pyhd8ed1ab_0    conda-forge
     traitlets                 5.1.1              pyhd8ed1ab_0    conda-forge
     transformers              4.10.3             pyhd8ed1ab_0    conda-forge
     typed-ast                 1.4.3            py38h497a2fe_1    conda-forge
     typing-extensions         4.0.1                hd8ed1ab_0    conda-forge
     typing_extensions         4.0.1              pyha770c72_0    conda-forge
     urllib3                   1.26.7             pyhd8ed1ab_0    conda-forge
     urwid                     2.1.2                    pypi_0    pypi
     urwid-readline            0.13                     pypi_0    pypi
     virtualenv                20.4.7           py38h578d9bd_1    conda-forge
     wcwidth                   0.2.5              pyh9f0ad1d_2    conda-forge
     webencodings              0.5.1                      py_1    conda-forge
     wheel                     0.37.0             pyhd8ed1ab_1    conda-forge
     xz                        5.2.5                h516909a_1    conda-forge
     yaml                      0.2.5                h516909a_0    conda-forge
     zeromq                    4.3.4                h9c3ff4c_1    conda-forge
     zict                      2.0.0                      py_0    conda-forge
     zipp                      3.6.0              pyhd8ed1ab_0    conda-forge
     zlib                      1.2.11            h36c2ea0_1013    conda-forge
     zstd                      1.5.0                ha95c52a_0    conda-forge
     
</pre></details>
",2021-12-07T00:22:06Z,0,0,Michael Wang,Nvidia Rapids,True
216,[FEA] Add version of extract_re that takes an index,"**Is your feature request related to a problem? Please describe.**
From Spark, when we call `extract_re` we often are only interested in extracting a single group rather than all the groups in the pattern. We currently call `extract_re` which returns a `Table` and we then get the column we are interested in and discard the others. It would be more efficient if we could pass the column index to cuDF so that only one column needs instantiating.

**Describe the solution you'd like**
I would like a signature something like `extract_re(pattern, index)`.

**Describe alternatives you've considered**
None

**Additional context**
None
",2021-12-07T17:56:51Z,0,0,Andy Grove,@Apple,False
217,[FEA] Reduce benchmark search space where applicable,"**Significantly lower the variations of parameters for the long-running benchmarks.**

Quote from https://github.com/rapidsai/cudf/issues/5773#issue-666438739

> Type Dispatcher, for example, has too many parameter variations. One parameter range goes from 1024 --> 67 million going up by a power of two each interval. This can easily be cut down by doing a power of 4. This approach can be applied to many of our biggest offenders in runtime.

**Reduce the number of benchmarks that have an iteration count of <= 2**

We have benchmarks that are run a single time, which provides insufficient iterations to trust the results.

If valid for smaller data sizes, they should be be moved to do so.
If they need to run on larger datasize we need to switch them to NVBench, and state they need longer runtimes 
",2021-12-07T18:53:20Z,0,0,Robert Maynard,NVIDIA,True
218,[BUG] `IntervalIndex.dropna` does not behave correctly,"**Describe the bug**
An `IntervalIndex` with null doesn't produce well-formed result after calling dropna.

**Steps/Code to reproduce bug**
```python
>>> i = cudf.from_pandas(pd.IntervalIndex([pd.Interval(0, 1), None, pd.Interval(2, 3)], dtype=pd.IntervalDtype()))
>>> i.dropna()
IntervalIndex([{'left': 0.0, 'right': 1.0}, {'left': 2.0, 'right': 3.0}], dtype='struct')
```

**Expected behavior**
```python
>>> pd.IntervalIndex([pd.Interval(0, 1), None, pd.Interval(2, 3)], dtype=pd.IntervalDtype()).dropna()
IntervalIndex([(0.0, 1.0], (2.0, 3.0]], dtype='interval[float64, right]')
```
Ours maybe:
```python
IntervalIndex([(0.0, 1.0], (2.0, 3.0]], dtype='interval')
```",2021-12-08T01:30:43Z,0,0,Michael Wang,Nvidia Rapids,True
219,`lists::contains()` needs to switch to use `cudf::nullate`,"This is a follow-on issue from #9510.

The backend code for `lists::contains()` and `lists::index_of()` currently use `bool`s in `lookup_functor` for branching on whether the `search_keys` contain null elements. This would be a whole lot neater if `cudf::nullate` were used instead.

This would require `cudf::make_pair_rep_iterator` to support `nullate` first, which obfuscates the already drawn out change in #9510. The proposed switch to `cudf::nullate` will ideally be a focused change, handled separately.",2021-12-08T20:16:46Z,0,0,MithunR,NVIDIA,True
220,`describe` casts all values to `str` for several types,"The [`describe` implementations](https://github.com/rapidsai/cudf/blob/b3b299ae22f31adb8f380d7add1ce2bdb726ab26/python/cudf/cudf/core/series.py#L3317-L3423), especially for datetime and timedelta types, appear to be casting all the values to `str` (aside from the numeric implementation). This does not align with Pandas behavior:
```python
>>> s = pd.Series([
...   np.datetime64(""2000-01-01""),
...   np.datetime64(""2010-01-01""),
...   np.datetime64(""2010-01-01""),
... ])
>>> print(type(s.describe()[""top""]))
<class 'pandas._libs.tslibs.timestamps.Timestamp'>
```

I recognize there is an issue here with different types, namely that `count` and `freq` are not of the same type as `mean`, `min`, percentiles, or `max`. This also affects numerical columns which will upcast integer values like `count` to floating types.

Some options to resolve this (and their downsides):
1. Current implementation: return all values as `str` (results are on GPU ...but data is not usable as `str` type).
2. Return a `pd.DataFrame` or `dict` that can have multiple types (not a GPU DataFrame).

I propose changing behavior to adopt option (2), and return a `pd.DataFrame`. The summary doesn't really need to be a GPU DataFrame since it contains so few values. (Do we have precedent for this kind of behavior returning a CPU (Pandas) DataFrame?)

_Originally posted by @bdice in https://github.com/rapidsai/cudf/pull/9867#discussion_r768149355_",2021-12-14T18:19:52Z,0,0,Bradley Dice,@NVIDIA @rapidsai,True
221,[FEA] Address performance regression in semi/anti joins from switching to cuco,"**Is your feature request related to a problem? Please describe.**
`cuco::static_map` is currently slower than cudf's internal unordered map. As part of a general move towards relying on cuCollections for data structures, however, we have made the replacement in #9666.

**Describe the solution you'd like**
`cuco::static_multimap` was optimized as part of its incorporation into cudf in #8934, but because of the current structure of cuCollections these optimizations have not been ported to `cuco::static_map`. While addressing https://github.com/NVIDIA/cuCollections/issues/110 we should profile and make sure that the resulting optimizations are sufficient to close the performance gap.

**Additional context**
The performance loss was deemed acceptable in order to progress faster towards #9695, which will rely on features that `cuco::static_map` offers that `cudf::concurrent_unordered_map` does not.
",2022-01-05T17:31:58Z,0,0,Vyas Ramasubramani,@rapidsai,True
222,[BUG] dask_cudf from_delayed throws exception when meta is included in the from_delayed,"Exception:

```
Columns: [_col10, _col5, _col0, _col12, visit_date]
Index: [], 'from_delayed')
kwargs:    {}
Exception: ""ValueError('Metadata mismatch found in `from_delayed`.\\n\\nExpected partition of type `pandas.core.frame.DataFrame` but got `cudf.core.dataframe.DataFrame`')""
```

Reproducer:
```
import cudf
import numpy as np
from dask.dataframe import from_delayed
from dask.delayed import delayed
import numpy as np
import cudf
from dask.distributed import Client
from dask_cuda import LocalCUDACluster
import dask_cudf

df = cudf.DataFrame()
for i in range(100):
    df[f'_col{i}'] = np.random.randint(20, size=100)
    
df.to_orc('test1.orc')
df.to_orc('test0.orc')

files = [(('a',), 'test0.orc'), (('a',), 'test1.orc')]
print(files)


def rd(f, cols, meta=None):
    if partitions:
        cols = list(set(cols) - set([x[0] for x in partitions]))
    if type(f).__name__ == 'str':
        f = ((), f)
    df = cudf.read_orc(f[1], columns=cols,  use_index=False)
    if partitions:
        for i, col in enumerate(partitions):
            df[col[0]] = f[0][i]
            df[col[0]] = df[col[0]].astype(col[1])
    if meta:
        return df[list(meta.keys())]
    else:
        return df


partitions =[('visit_date', 'str')]
cols = ['_col0', '_col12', '_col10', '_col5']
meta = dict(rd(files[0], cols, meta=None).dtypes)
for i in partitions:
    meta[i[0]] = i[1]
    
print(meta)

def main():
    c=Client(LocalCUDACluster())
    dfs=[delayed(rd)(f, cols=cols, meta=meta) for f in files[:2]]
    xx = dask_cudf.from_delayed(dfs, meta=meta)
    print(xx.compute())


if __name__ == ""__main__"":
    main()
```

this issue is very similar to issue here: https://github.com/dask/dask/issues/8528

the difference is from_delayed with meta, throws another exception ""The columns in the computed data do not match the columns in the provided metadata"" even though both dfs have same columns and dtypes.
since we provide the meta here, we run into different issue.",2022-01-05T19:52:25Z,0,0,Lahir Marni,,False
223,[BUG] `DataFrame.merge` does not assign `dtype` as expected in result as pandas does,"The `dtype` in the result of a merge is inconsistent with the equivalent pandas merge for the same inputs:
cuDF:
```
>>> df1=cudf.DataFrame({""a"":[1,2,3,4]})
>>> df2=cudf.DataFrame(columns=[""a""])
>>> r=df2.merge(df1,how=""outer"")
>>> df1[""a""].dtype
dtype('int64')
>>> r[""a""].dtype
dtype('float64')
```
pandas:
```
>>> pdf1=pd.DataFrame({""a"":[1,2,3,4]})
>>> pdf2=pd.DataFrame(columns=[""a""])
>>> pr=pdf2.merge(pdf1,how=""outer"")
>>> pdf1[""a""].dtype
dtype('int64')
>>> pr[""a""].dtype
dtype('int64')
```",2022-01-06T16:01:41Z,0,0,Rick Ratzel,,False
224,[FEA] Supporting separators with more than 1 character in cudf.read_csv method,"**Is your feature request related to a problem? Please describe.**
When calling the method _cudf.read_csv_, the _sep_ argument only accepts a 1 character string - which was an old issue in pandas. However, they have already included support for multi-character separators and it would be a useful thing to have on cudf.

**Describe the solution you'd like**
Calling the _cudf.read_csv_ method as 

```python
cudf.read_csv(filepath, sep='::')
```

**Describe alternatives you've considered**
Perhaps allowing users to use a regex as a separator definer would provide a more robust solution.

**Additional context**
Python code example

```python
import pandas as pd
import cudf

filepath = './example.csv'
with open(filepath, 'w') as f:
    f.write('column0::column1\n')
    f.write('value00::value01\n')
    f.write('value10::value11\n')

df_pandas = pd.read_csv(filepath, sep='::')
print ('Loaded csv file from pandas')
df_cudf = cudf.read_csv(filepath, sep='::')
print ('Loaded csv file from cudf')
```

Current output: `ValueError: only single character unicode strings can be converted to Py_UCS4, got length 2`",2022-01-06T20:26:21Z,0,0,Joao Felipe Guedes,Globo.com,False
225,[FEA] Reorganize and improve Python tests,"**Note for developers**
This is a meta-issue aiming to categorize a wide range of issues. Developers who want to tackle a specific item from the checklist below should create a new issue for just that item, self-assign that issue, and then link it to the checklist above.

**Is your feature request related to a problem? Please describe.**
There are currently a number of different problems that make it difficult to find, add, or run tests.
- Tests are partially and inconsistently organized by functionality, data types, and parametrizations, so it's not clear which file tests of a specific function might be in. The partial organization by dtype is particularly confusing because it means that in some files we test a single function across many dtypes, whereas in other files we test many functions for a single dtype.
- Test files are too large and contain too many tests.
- There are currently many tests that raise warnings as well as many xfailed tests that actually xpass.
- Tests are currently slow to run because we rely on excessive parametrization and we test many private APIs. Additionally, cuIO tests are especially slow because the corresponding libcudf APIs are difficult to test, so a greater burden is placed on the Python APIs to capture a wider range of issues.

**Describe the solution you'd like**
These are tasks that we propose to undertake to address the various issues discussed above:

- [x] See #7386. Change pytest to run fail on xpasses by specifying the `strict` parameter. [Here are the relevant pytest docs](https://docs.pytest.org/en/latest/how-to/skipping.html#strict-parameter). 
- [x] Make pytest fail when it encounters warnings. We can accomplish this by setting in the appropriate config file (probably setup.cfg). (Done in #12468)
  - [x] #10363
  - [x] Remove other warnings from Python tests (this was done across a large number of PRs)
- [ ] Reorganize test files to match the groupings in our API documentation. See also #4730
- [ ] Reorganize tests into classes that reflect the actual class hierarchies. For example, tests that should be performed for Series and DataFrame objects can go into a `test_indexed_frame.py` file.
- [ ] Reduce excessive parametrization. Many individual tests are actually run hundreds or even thousands of times because we construct a matrix of parameters. In many cases those parametrizations aren't actually testing anything useful, for instance when a test is parametrized by input size. In cases where we're worried about how the underlying libcudf algorithm behaves when block sizes are exceeded, for instance, we should push those tests down to C++. Python tests should favor fewer parameters and in some cases more specialized tests to handle specific edge cases.
- [ ] Where appropriate, we should replace parametrization with fixtures. Fixtures will be typically evaluated lazily unlike parameters, which are materialized when the decorated functions are defined, so changing this should improve collection time and reduce the frequency with which we see failures during collection. Additionally, it makes it easier to debug failing tests when breakpoints are placed in the code; we don't want to hit breakpoints during collection. #2530 is related to this and may be resolved by addressing this task, but it's not quite as broad.
- [ ] Remove tests of private functions/classes. If those functions/classes have corresponding public APIs, we need to make sure that those APIs are tested. If they are, then the tests of private code may be removed, otherwise those tests should be rewritten in terms of the corresponding public APIs.
- [x] Stop using pd._testing (#6435)
- [ ] #10001
- [ ] Implement standard fixtures or use a similar approach to ensure consistent dtype coverage in tests. As part of this, decide what constitutes sufficient dtype coverage
",2022-01-07T19:52:39Z,0,0,Vyas Ramasubramani,@rapidsai,True
226,Reduce runtime of I/O tests,cuIO tests are some of the slowest in our test suite because of the sizes of the data and the number of parameters that are tested. Finding ways to make those tests more compact would substantially accelerate running our test suite.,2022-01-07T19:53:34Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
227,[FEA] Java AST should allow passing Scalar instance as literal,"**Is your feature request related to a problem? Please describe.**
When building a cudf AST expression in Java, sometimes there are cases where a `Scalar` instance already exists for a literal.  The current Java bindings do not allow a `Scalar` to be used directly, so one must synchronize and fetch the validity and value to manually construct a corresponding AST `Literal` instance.

**Describe the solution you'd like**
The Java bindings should allow for an AST `Literal` instance to take a `Scalar` argument as the literal value to use, which will include both the validity and value.  No synchronization is necessary for this constructor.  When compiled, the resulting `CompiledExpression` instance should increment the reference count of the `Scalar` instance, as it is being actively referenced by the compiled AST, and closing the `CompiledExpression` should close the corresponding `Scalar` instance.",2022-01-07T20:44:17Z,0,0,Jason Lowe,NVIDIA,True
228,[FEA] Refactor various conditional join implementations for simplicity and API consistency,"**Is your feature request related to a problem? Please describe.**
#9917 and #10037 added new join functions that use a mixture of hash lookups and AST expression evaluation. In the interest of time, a number of important performance improvements, API design questions, and general internal refactorings were overlooked. This issue aims to track those potential improvements for future work.

**Describe the solution you'd like**
- [x] Implement benchmarks for mixed joins. This is a critical first step to evaluate the importance of other changes.
- [ ] Implement object-oriented APIs for mixed joins.
- [ ] Remove size APIs for semi/anti joins (both mixed and pure conditional) in favor of a single-kernel approach. The size APIs aren't really necessary since the primary use case for those is knowing when the results will be large enough to spill in multi-GPU cases, and for semi/anti joins the size is bounded by just the number of rows a table N (rather than N^2). It would be simpler and more efficient to use an approach like the hash semi/anti joins, which essentially just generate a gather mask.
  - [x] #15250 for mixed semi/anti-joins
  - [x] https://github.com/rapidsai/cudf/pull/14646#discussion_r1568113565 for conditional semi/anti-joins
- [x] Rework conditional join internals for semi/anti joins to use kernels that don't allocate the second output vector, which is wasteful.
  - [x] https://github.com/rapidsai/cudf/pull/14646
- [ ] Find cleaner solutions for the expression evaluator shared memory handling, see https://github.com/rapidsai/cudf/pull/9917#discussion_r779021242.
- [ ] Reduce compile time if possible (https://github.com/rapidsai/cudf/pull/9917#discussion_r771559204).
- [ ] Explore more code sharing (perhaps via templating) between mixed and pure conditional joins, as well as more sharing with the existing hash join infrastructure (should be facilitated by implementing the object-oriented API).
- [ ] Revisit naming of different join APIs. In the long term the joins probably shouldn't include names based on whether they are equality, conditional, or mixed joins, they should just be named by the type of join (inner, left, etc) and rely on the signature to differentiate the rest.",2022-01-13T19:13:12Z,0,0,Vyas Ramasubramani,@rapidsai,True
229,[FEA] Add Localization support to cudf's Datetime capabilities,"**Is your feature request related to a problem? Please describe.**
cuDF does not support UTC offsets, `utc=True` in `cudf.to_datetime()`, `.tz_localize()`, nor external packages localization parameters (like `dateutils.parse` and `pytz`) in its datetime accessor.  We had an issue where a user tried to do that, and got errors

https://stackoverflow.com/questions/70511547/compatibility-of-datetime-with-cudf-and-pandas-for-filter-datetime-in-python/70704931#70704931 (issue code and proposed work around solution is below)


**Describe the solution you'd like**
where `testdata.csv` is 
```
Datetime,Open,High,Low,Close,Adj Close,Volume 
2021-10-22 13:30:00+00:00,149.69,149.75,149.01,149.04,149.04,4032096.0 
2021-10-22 13:40:00+00:00,149.69,150.175,148.845,149.92,149.92,19671400.0
2021-11-22 13:50:00+00:00,149.975,150.18,149.5601,149.75,149.75,11911828.0 
```
pandas works, but cudf doesn't, declaring that the output doesn't looke like it's in datetime format
```
import pandas as pd
#import cudf as pd # uncomment to see cudf error out
import time 
import datetime 
import dateutil

if __name__ == ""__main__"":
    Zeit_start = datetime.datetime.now()
    AGdata_search = pd.read_csv(""testdata.csv"",parse_dates=['Datetime'],infer_datetime_format=True,cache_dates=False)
    AGdata_TEST = AGdata_search.loc[(AGdata_search['Datetime'] >= dateutil.parser.parse(""2021-11-02 13:44:00+00:00""))] 
    AGdata_TEST.to_csv(""output.csv"", encoding='utf-8',index=False)
```
These are the Pandas dtypes
```
Datetime     datetime64[ns, UTC]
Open                     float64
High                     float64
Low                      float64
Close                    float64
Adj Close                float64
Volume                   float64
dtype: object
```

cuDF dtype would only recognize:
```
Datetime     datetime64[ns]
```

Also tried these with obvious, but various errors that would work in Pandas
```
AGdata_search['Datetime'].dt.tz_localize(None)
# AttributeError: Can only use .dt accessor with datetimelike values

AGdata_search['Datetime'].astype('datetime64[ns]')
# not a recognized datetime format

AGdata_search['Datetime'].astype('datetime64[ns, UTC]')
# unrecognized dtype error

AGdata_search['Datetime'] = cudf.to_datetime(AGdata_search['Datetime'], utc=True)
# no affect

import pytz
AGdata_search['Datetime'].tz_localize(pytz.utc)
# AttributeError: 'Series' object has no attribute 'tz_localize'
```

**Describe alternatives you've considered**
The workaround proposed was to explicitly declare the format code 
```
AGdata_search['Datetime'] = cudf.to_datetime(AGdata_search['Datetime'], format='%Y-%m-D %H:%M:%S+%z')
```
and the full solution presented was:
```
import cudf as cudf 
import time 
import datetime 
import dateutil

if __name__ == ""__main__"":
    Zeit_start = datetime.datetime.now()
    AGdata_search = cudf.read_csv(""testdata.csv"")
    AGdata_search['Datetime'] = cudf.to_datetime(AGdata_search['Datetime'], format='%Y-%m-%d %H:%M:%S+%z') # this makes it work)
    AGdata_TEST = AGdata_search.loc[(AGdata_search['Datetime'] >= dateutil.parser.parse(""2021-11-02 13:44:00+00:00""))]
    AGdata_TEST.to_csv(""output.csv"", encoding='utf-8',index=False)
```

**Additional context**
This format seems to be used often in the financial market analysis.

here is some other pandas code that works only in pandas i stumbled on while trying to find a workaround
```
import pandas as pd
import pytz

index = pd.date_range('20140101 21:55', freq='15S', periods=5)
df = pd.DataFrame(1, index=index, columns=['X'])
print(df)
#                      X
# 2014-01-01 21:55:00  1
# 2014-01-01 21:55:15  1
# 2014-01-01 21:55:30  1
# 2014-01-01 21:55:45  1
# 2014-01-01 21:56:00  1

# [5 rows x 1 columns]
print(df.index)
# <class 'pandas.tseries.index.DatetimeIndex'>
# [2014-01-01 21:55:00, ..., 2014-01-01 21:56:00]
# Length: 5, Freq: 15S, Timezone: None

eastern = pytz.timezone('US/Eastern')
df.index = df.index.tz_localize(pytz.utc).tz_convert(eastern)
print(df)
#                            X
# 2014-01-01 16:55:00-05:00  1
# 2014-01-01 16:55:15-05:00  1
# 2014-01-01 16:55:30-05:00  1
# 2014-01-01 16:55:45-05:00  1
# 2014-01-01 16:56:00-05:00  1

# [5 rows x 1 columns]

print(df.index)
# <class 'pandas.tseries.index.DatetimeIndex'>
# [2014-01-01 16:55:00-05:00, ..., 2014-01-01 16:56:00-05:00]
# Length: 5, Freq: 15S, Timezone: US/Eastern
```",2022-01-14T02:13:28Z,1,0,Taurean Dyer,,False
230,[FEA] Add support for equivalence of Pandas 'unstack' on Series,"**Is your feature request related to a problem? Please describe.**
Hi,

While porting some code from Pandas to cuDF, I have noticed that cuDF series do not support `unstack` method.
As an additional request, It would be great if `fill_values` could be supported in both `cudf.DataFrame.unstack` and `cudf.Series.unstack` methods. Thanks!

**Describe the solution you'd like**
To have that method supported in cuDF series.

**Describe alternatives you've considered**
Creating a cuDF DataFrame from the cuDF series, use `unstack` on the DataFrame.
",2022-01-15T23:04:26Z,0,0,Miguel Martínez,NVIDIA,True
231,`BaseIndex` and `IndexedFrame` has overlapping logics,"From review comments: https://github.com/rapidsai/cudf/pull/9832#pullrequestreview-845209255, recent python refactors #9832 #9807 has created some duplicate logics in `BaseIndex` and `IndexedFrame`. The main diverging part of the implementation mostly lies in column selection (handling index columns and user inputs). For some methods (such as `_apply_boolean_mask`), which unconditionally pass in all columns for `index`/`indexed_frame`, we should be able to merge common code path while dispatching just the column selection part to each object to reduce code duplication.",2022-01-18T18:44:59Z,1,0,Michael Wang,Nvidia Rapids,True
232,[FEA] Exploit Symmetry for `groupby.corr` and `groupby.cov`,"In https://github.com/rapidsai/cudf/pull/9889#discussion_r771493338, @bdice made an observation that due to symmetry, there's a chance of utilizing that to reduce computation by only applying the upper half of the matrix. In an attempt to reduce the scope of the PR, this is the issue that tracks the suggestion separately. If applicable, `groupby.corr` should also share the logic.

cc @skirui-source ",2022-01-20T22:39:24Z,1,0,Michael Wang,Nvidia Rapids,True
233,[FEA] Expand ORC and Parquet benchmarks to cover different stripe/rowgroup sizes,"Add a set of benchmarks with varying stripe/rowgroup sizes to each affected component:

- [ ] ORC reader
- [ ] ORC writer
- [ ] Parquet reader
- [ ] Parquet writer

Use the new benchmarks to evaluate the effects of these options and potentially determine the optimal settings.",2022-01-21T00:06:41Z,0,0,Vukasin Milovanovic,NVIDIA,True
234,[BUG] Dask cuDF cummax and cummin fail due to missing axis argument in cuDF.Series.where,"Dask cuDF `cummax` and `cummin` fail due to  missing axis argument in `cuDF.Series.where` if npartitions > 1.

```python
import cudf
import dask_cudf
​
df = cudf.DataFrame({""x"": range(10)})
ddf = dask_cudf.from_cudf(df, 2)
​
ddf.x.cummin().compute()
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Input In [1], in <module>
      4 df = cudf.DataFrame({""x"": range(10)})
      5 ddf = dask_cudf.from_cudf(df, 2)
----> 7 ddf.x.cummin().compute()

File ~/conda/envs/rapids-22.02/lib/python3.8/site-packages/dask/base.py:288, in DaskMethodsMixin.compute(self, **kwargs)
    264 def compute(self, **kwargs):
    265     """"""Compute this dask collection
    266 
    267     This turns a lazy Dask collection into its in-memory equivalent.
   (...)
    286     dask.base.compute
    287     """"""
--> 288     (result,) = compute(self, traverse=False, **kwargs)
    289     return result

File ~/conda/envs/rapids-22.02/lib/python3.8/site-packages/dask/base.py:571, in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)
    568     keys.append(x.__dask_keys__())
    569     postcomputes.append(x.__dask_postcompute__())
--> 571 results = schedule(dsk, keys, **kwargs)
    572 return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])

File ~/conda/envs/rapids-22.02/lib/python3.8/site-packages/dask/local.py:553, in get_sync(dsk, keys, **kwargs)
    548 """"""A naive synchronous version of get_async
    549 
    550 Can be useful for debugging.
    551 """"""
    552 kwargs.pop(""num_workers"", None)  # if num_workers present, remove it
--> 553 return get_async(
    554     synchronous_executor.submit,
    555     synchronous_executor._max_workers,
    556     dsk,
    557     keys,
    558     **kwargs,
    559 )

File ~/conda/envs/rapids-22.02/lib/python3.8/site-packages/dask/local.py:496, in get_async(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)
    494 while state[""waiting""] or state[""ready""] or state[""running""]:
    495     fire_tasks(chunksize)
--> 496     for key, res_info, failed in queue_get(queue).result():
    497         if failed:
    498             exc, tb = loads(res_info)

File ~/conda/envs/rapids-22.02/lib/python3.8/concurrent/futures/_base.py:437, in Future.result(self, timeout)
    435     raise CancelledError()
    436 elif self._state == FINISHED:
--> 437     return self.__get_result()
    439 self._condition.wait(timeout)
    441 if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:

File ~/conda/envs/rapids-22.02/lib/python3.8/concurrent/futures/_base.py:389, in Future.__get_result(self)
    387 if self._exception:
    388     try:
--> 389         raise self._exception
    390     finally:
    391         # Break a reference cycle with the exception in self._exception
    392         self = None

File ~/conda/envs/rapids-22.02/lib/python3.8/site-packages/dask/local.py:538, in SynchronousExecutor.submit(self, fn, *args, **kwargs)
    536 fut = Future()
    537 try:
--> 538     fut.set_result(fn(*args, **kwargs))
    539 except BaseException as e:
    540     fut.set_exception(e)

File ~/conda/envs/rapids-22.02/lib/python3.8/site-packages/dask/local.py:234, in batch_execute_tasks(it)
    230 def batch_execute_tasks(it):
    231     """"""
    232     Batch computing of multiple tasks with `execute_task`
    233     """"""
--> 234     return [execute_task(*a) for a in it]

File ~/conda/envs/rapids-22.02/lib/python3.8/site-packages/dask/local.py:234, in <listcomp>(.0)
    230 def batch_execute_tasks(it):
    231     """"""
    232     Batch computing of multiple tasks with `execute_task`
    233     """"""
--> 234     return [execute_task(*a) for a in it]

File ~/conda/envs/rapids-22.02/lib/python3.8/site-packages/dask/local.py:225, in execute_task(key, task_info, dumps, loads, get_id, pack_exception)
    223     failed = False
    224 except BaseException as e:
--> 225     result = pack_exception(e, dumps)
    226     failed = True
    227 return key, result, failed

File ~/conda/envs/rapids-22.02/lib/python3.8/site-packages/dask/local.py:220, in execute_task(key, task_info, dumps, loads, get_id, pack_exception)
    218 try:
    219     task, data = loads(task_info)
--> 220     result = _execute_task(task, data)
    221     id = get_id()
    222     result = dumps((result, id))

File ~/conda/envs/rapids-22.02/lib/python3.8/site-packages/dask/core.py:119, in _execute_task(arg, cache, dsk)
    115     func, args = arg[0], arg[1:]
    116     # Note: Don't assign the subtask results to a variable. numpy detects
    117     # temporaries by their reference count and can execute certain
    118     # operations in-place.
--> 119     return func(*(_execute_task(a, cache) for a in args))
    120 elif not ishashable(arg):
    121     return arg

File ~/conda/envs/rapids-22.02/lib/python3.8/site-packages/dask/dataframe/methods.py:314, in cummin_aggregate(x, y)
    312 def cummin_aggregate(x, y):
    313     if is_series_like(x) or is_dataframe_like(x):
--> 314         return x.where((x < y) | x.isnull(), y, axis=x.ndim - 1)
    315     else:  # scalar
    316         return x if x < y else y

TypeError: where() got an unexpected keyword argument 'axis'
```

```python
import cudf
import dask_cudf
​
df = cudf.DataFrame({""x"": range(10)})
ddf = dask_cudf.from_cudf(df, 2)
​
ddf.x.cummin().compute()
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Input In [9], in <module>
      4 df = cudf.DataFrame({""x"": range(10)})
      5 ddf = dask_cudf.from_cudf(df, 2)
----> 7 ddf.x.cummin().compute()

File ~/conda/envs/rapids-22.02/lib/python3.8/site-packages/dask/base.py:288, in DaskMethodsMixin.compute(self, **kwargs)
    264 def compute(self, **kwargs):
    265     """"""Compute this dask collection
    266 
    267     This turns a lazy Dask collection into its in-memory equivalent.
   (...)
    286     dask.base.compute
    287     """"""
--> 288     (result,) = compute(self, traverse=False, **kwargs)
    289     return result

File ~/conda/envs/rapids-22.02/lib/python3.8/site-packages/dask/base.py:571, in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)
    568     keys.append(x.__dask_keys__())
    569     postcomputes.append(x.__dask_postcompute__())
--> 571 results = schedule(dsk, keys, **kwargs)
    572 return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])

File ~/conda/envs/rapids-22.02/lib/python3.8/site-packages/dask/local.py:553, in get_sync(dsk, keys, **kwargs)
    548 """"""A naive synchronous version of get_async
    549 
    550 Can be useful for debugging.
    551 """"""
    552 kwargs.pop(""num_workers"", None)  # if num_workers present, remove it
--> 553 return get_async(
    554     synchronous_executor.submit,
    555     synchronous_executor._max_workers,
    556     dsk,
    557     keys,
    558     **kwargs,
    559 )

File ~/conda/envs/rapids-22.02/lib/python3.8/site-packages/dask/local.py:496, in get_async(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)
    494 while state[""waiting""] or state[""ready""] or state[""running""]:
    495     fire_tasks(chunksize)
--> 496     for key, res_info, failed in queue_get(queue).result():
    497         if failed:
    498             exc, tb = loads(res_info)

File ~/conda/envs/rapids-22.02/lib/python3.8/concurrent/futures/_base.py:437, in Future.result(self, timeout)
    435     raise CancelledError()
    436 elif self._state == FINISHED:
--> 437     return self.__get_result()
    439 self._condition.wait(timeout)
    441 if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:

File ~/conda/envs/rapids-22.02/lib/python3.8/concurrent/futures/_base.py:389, in Future.__get_result(self)
    387 if self._exception:
    388     try:
--> 389         raise self._exception
    390     finally:
    391         # Break a reference cycle with the exception in self._exception
    392         self = None

File ~/conda/envs/rapids-22.02/lib/python3.8/site-packages/dask/local.py:538, in SynchronousExecutor.submit(self, fn, *args, **kwargs)
    536 fut = Future()
    537 try:
--> 538     fut.set_result(fn(*args, **kwargs))
    539 except BaseException as e:
    540     fut.set_exception(e)

File ~/conda/envs/rapids-22.02/lib/python3.8/site-packages/dask/local.py:234, in batch_execute_tasks(it)
    230 def batch_execute_tasks(it):
    231     """"""
    232     Batch computing of multiple tasks with `execute_task`
    233     """"""
--> 234     return [execute_task(*a) for a in it]

File ~/conda/envs/rapids-22.02/lib/python3.8/site-packages/dask/local.py:234, in <listcomp>(.0)
    230 def batch_execute_tasks(it):
    231     """"""
    232     Batch computing of multiple tasks with `execute_task`
    233     """"""
--> 234     return [execute_task(*a) for a in it]

File ~/conda/envs/rapids-22.02/lib/python3.8/site-packages/dask/local.py:225, in execute_task(key, task_info, dumps, loads, get_id, pack_exception)
    223     failed = False
    224 except BaseException as e:
--> 225     result = pack_exception(e, dumps)
    226     failed = True
    227 return key, result, failed

File ~/conda/envs/rapids-22.02/lib/python3.8/site-packages/dask/local.py:220, in execute_task(key, task_info, dumps, loads, get_id, pack_exception)
    218 try:
    219     task, data = loads(task_info)
--> 220     result = _execute_task(task, data)
    221     id = get_id()
    222     result = dumps((result, id))

File ~/conda/envs/rapids-22.02/lib/python3.8/site-packages/dask/core.py:119, in _execute_task(arg, cache, dsk)
    115     func, args = arg[0], arg[1:]
    116     # Note: Don't assign the subtask results to a variable. numpy detects
    117     # temporaries by their reference count and can execute certain
    118     # operations in-place.
--> 119     return func(*(_execute_task(a, cache) for a in args))
    120 elif not ishashable(arg):
    121     return arg

File ~/conda/envs/rapids-22.02/lib/python3.8/site-packages/dask/dataframe/methods.py:314, in cummin_aggregate(x, y)
    312 def cummin_aggregate(x, y):
    313     if is_series_like(x) or is_dataframe_like(x):
--> 314         return x.where((x < y) | x.isnull(), y, axis=x.ndim - 1)
    315     else:  # scalar
    316         return x if x < y else y

TypeError: where() got an unexpected keyword argument 'axis'
```


Env:
<details>
!conda list | grep ""rapids\|dask""
# packages in environment at /home/nicholasb/conda/envs/rapids-22.02:
cucim                     22.02.00a220121 cuda_11_py38_g0e199fc_37    rapidsai-nightly
cudf                      22.02.00a220121 cuda_11_py38_g53a31d1b01_303    rapidsai-nightly
cudf_kafka                22.02.00a220121 py38_g53a31d1b01_303    rapidsai-nightly
cugraph                   22.02.00a220119 cuda11_py38_gefbff09a_70    rapidsai-nightly
cuml                      22.02.00a220118 cuda11_py38_g592834c13_88    rapidsai-nightly
cusignal                  22.02.00a220121 py39_g80eadba_11    rapidsai-nightly
cuspatial                 22.02.00a220121 py38_g5d619e7_19    rapidsai-nightly
custreamz                 22.02.00a220121 py38_g53a31d1b01_303    rapidsai-nightly
cuxfilter                 22.02.00a220121 py38_g4287bac_13    rapidsai-nightly
dask                      2021.11.2          pyhd8ed1ab_0    conda-forge
dask-core                 2021.11.2          pyhd8ed1ab_0    conda-forge
dask-cuda                 22.02.00a220121         py38_49    rapidsai-nightly
dask-cudf                 22.02.00a220121 cuda_11_py38_g53a31d1b01_303    rapidsai-nightly
libcucim                  22.02.00a220121 cuda11_g0e199fc_37    rapidsai-nightly
libcudf                   22.02.00a220121 cuda11_g53a31d1b01_303    rapidsai-nightly
libcudf_kafka             22.02.00a220121 g53a31d1b01_303    rapidsai-nightly
libcugraph                22.02.00a220119 cuda11_gefbff09a_70    rapidsai-nightly
libcugraph_etl            22.02.00a220121 cuda11_gc0096791_73    rapidsai-nightly
libcuml                   22.02.00a220118 cuda11_g592834c13_88    rapidsai-nightly
libcumlprims              22.02.00a220119 cuda11_g0342bdb_15    rapidsai-nightly
libcuspatial              22.02.00a220121 cuda11_g5d619e7_19    rapidsai-nightly
librmm                    22.02.00a220121 cuda11_g30eb83b_31    rapidsai-nightly
libxgboost                1.5.0dev.rapidsai22.02      cuda11.2_0    rapidsai-nightly
ptxcompiler               0.2.0            py38hb739d79_0    rapidsai-nightly
py-xgboost                1.5.0dev.rapidsai22.02  cuda11.2py38_0    rapidsai-nightly
pylibcugraph              22.02.00a220119 cuda11_py38_gefbff09a_70    rapidsai-nightly
rapids                    22.02.00a220118 cuda11_py38_g3986715_134    rapidsai-nightly
rapids-xgboost            22.02.00a220118 cuda11_py38_g3986715_134    rapidsai-nightly
rmm                       22.02.00a220121 cuda11_py38_g30eb83b_31_has_cma    rapidsai-nightly
ucx                       1.12.0+gd367332      cuda11.2_0    rapidsai-nightly
ucx-proc                  1.0.0                       gpu    rapidsai-nightly
ucx-py                    0.24.0a220121   py38_gd367332_26    rapidsai-nightly
xgboost                   1.5.0dev.rapidsai22.02  cuda11.2py38_0    rapidsai-nightly
</details>",2022-01-21T16:51:11Z,0,0,Nick Becker,@NVIDIA,True
235,[BUG] ValueError: 'category' column dtypes are currently not supported by the gpu accelerated parquet writer,"The following exception is thrown when a column of type 'int' is written using to_parquet, and read with read_parquet and re-written with to-parquet again.

So, the first to_parquet is changing the `int` col type to catg, but when reading again, and the same file is re-written it fails.

Exception: ValueError: 'category' column dtypes are currently not supported by the gpu accelerated parquet writer

Reproducer Code:
```
import cudf
df = cudf.DataFrame()

df['a'] = [0 , 0 , 0, 1]
df['b'] = [0 , 0 , 0, 1]

df['a']  = df.a.astype('int')
df.to_parquet('test', partition_cols=['a'], partition_file_name=f'0.parquet')
df = cudf.read_parquet('test/a=0/0.parquet')
df.to_parquet('test.pq')
```",2022-01-24T22:45:20Z,0,0,Lahir Marni,,False
236,"[FEA] Support ""mode"" argument to to_csv","I'm trying to iteratively write (append) to an existing CSV file:
```
import cudf

df = cudf.DataFrame({'id': [0, 1, 2]})
df.to_csv('test.csv', header=False, index=False, mode='a')
```
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Input In [3], in <module>
      1 import cudf
      3 df = cudf.DataFrame({'id': [0, 1, 2]})
----> 4 df.to_csv('test.csv', header=False, index=False, mode=""a"")

File ~/conda/envs/dsql-1-25/lib/python3.8/site-packages/cudf/core/dataframe.py:5749, in DataFrame.to_csv(self, path_or_buf, sep, na_rep, columns, header, index, line_terminator, chunksize, encoding, compression, **kwargs)
   5746 """"""{docstring}""""""
   5747 from cudf.io import csv as csv
-> 5749 return csv.to_csv(
   5750     self,
   5751     path_or_buf=path_or_buf,
   5752     sep=sep,
   5753     na_rep=na_rep,
   5754     columns=columns,
   5755     header=header,
   5756     index=index,
   5757     line_terminator=line_terminator,
   5758     chunksize=chunksize,
   5759     encoding=encoding,
   5760     compression=compression,
   5761     **kwargs,
   5762 )

File ~/conda/envs/dsql-1-25/lib/python3.8/contextlib.py:75, in ContextDecorator.__call__.<locals>.inner(*args, **kwds)
     72 @wraps(func)
     73 def inner(*args, **kwds):
     74     with self._recreate_cm():
---> 75         return func(*args, **kwds)

File ~/conda/envs/dsql-1-25/lib/python3.8/site-packages/cudf/io/csv.py:148, in to_csv(df, path_or_buf, sep, na_rep, columns, header, index, line_terminator, chunksize, encoding, compression, **kwargs)
    145     path_or_buf = StringIO()
    146     return_as_string = True
--> 148 path_or_buf = ioutils.get_writer_filepath_or_buffer(
    149     path_or_data=path_or_buf, mode=""w"", **kwargs
    150 )
    152 if columns is not None:
    153     try:

TypeError: get_writer_filepath_or_buffer() got multiple values for keyword argument 'mode'
```

As a workaround, I could always write to a new, separate CSV file and concat them together afterwards.",2022-01-25T18:43:02Z,0,0,Randy Gelhausen,,False
237,[BUG] Sum and multiply aggregations promote unsigned input types to a signed output,"**Describe the bug**
When performing aggregations, the output types are often upscaled to help combat overflow situations.  For example, performing a sum aggregation on an INT32 column will produce an INT64 result.  However performing a sum aggregation on a UINT32 column produces an INT64 result rather than a UINT64 result.

**Steps/Code to reproduce bug**
Perform a sum aggregation with an input column of UINT32 and note that the result is INT64.  Here's a snippet of a session doing this with the cudf Java API in the Spark REPL shell:
```
scala> import ai.rapids.cudf._
import ai.rapids.cudf._

scala> val t = new Table(ColumnVector.fromInts(0), ColumnVector.fromUnsignedInts(0))
t: ai.rapids.cudf.Table = Table{columns=[ColumnVector{rows=1, type=INT32, nullCount=Optional[0], offHeap=(ID: 5 7feec1b5ac90)}, ColumnVector{rows=1, type=UINT32, nullCount=Optional[0], offHeap=(ID: 9 7feec1b5a280)}], cudfTable=140663428850592, rows=1}

scala> t.groupBy(0).aggregate(GroupByAggregation.sum().onColumn(1))
res0: ai.rapids.cudf.Table = Table{columns=[ColumnVector{rows=1, type=INT32, nullCount=Optional.empty, offHeap=(ID: 10 7ff0b7039860)}, ColumnVector{rows=1, type=INT64, nullCount=Optional.empty, offHeap=(ID: 11 7ff0b7039760)}], cudfTable=140671839344304, rows=1}
```

**Expected behavior**
Unsigned input types should be promoted to unsigned output types for any aggregations where the sign of the result cannot change for unsigned inputs (e.g.: sum and multiply)

**Additional context**
See @jrhemstad's comment at https://github.com/rapidsai/cudf/issues/10102#issuecomment-1023502832",2022-01-27T19:24:04Z,0,0,Jason Lowe,NVIDIA,True
238,[FEA] Support for groupby rolling aggregations on time windows in dask-cudf,"Today, Dask supports several groupby rolling operations on time frequency windows (e.g., ""1 hour""), including canonical reductions like sum, max, min, mean, std, and var.

These operations are not supported with dask-cuDF. We should support these operations.

They currently fail with the following error:

```python
import dask
import dask_cudf
​
ddf = dask.datasets.timeseries(freq=""1H"")
gddf = dask_cudf.from_dask_dataframe(ddf)
​
print(ddf.groupby(""name"").x.rolling(""1D"").var().head())
​
gddf.groupby(""name"").x.rolling('1D').mean().head()
name     timestamp          
Zelda    2000-01-01 00:00:00   NaN
Sarah    2000-01-01 01:00:00   NaN
George   2000-01-01 02:00:00   NaN
Norbert  2000-01-01 03:00:00   NaN
Wendy    2000-01-01 04:00:00   NaN
Name: x, dtype: float64
/home/nicholasb/conda/envs/cudf-22.04/lib/python3.8/site-packages/dask_cudf/groupby.py:54: FutureWarning: index is deprecated and will be removed in a future release.
  by=self.index,
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Input In [16], in <module>
      5 gddf = dask_cudf.from_dask_dataframe(ddf)
      7 print(ddf.groupby(""name"").x.rolling(""1D"").var().head())
----> 9 gddf.groupby(""name"").x.rolling('1D').mean().head()

File ~/conda/envs/cudf-22.04/lib/python3.8/site-packages/dask/dataframe/rolling.py:352, in Rolling.mean(self)
    350 @derived_from(pd_Rolling)
    351 def mean(self):
--> 352     return self._call_method(""mean"")

File ~/conda/envs/cudf-22.04/lib/python3.8/site-packages/dask/dataframe/rolling.py:492, in RollingGroupby._call_method(self, method_name, *args, **kwargs)
    491 def _call_method(self, method_name, *args, **kwargs):
--> 492     return super()._call_method(
    493         method_name,
    494         *args,
    495         groupby_kwargs=self._groupby_kwargs,
    496         groupby_slice=self._groupby_slice,
    497         **kwargs,
    498     )

File ~/conda/envs/cudf-22.04/lib/python3.8/site-packages/dask/dataframe/rolling.py:300, in Rolling._call_method(self, method_name, *args, **kwargs)
    298 def _call_method(self, method_name, *args, **kwargs):
    299     rolling_kwargs = self._rolling_kwargs()
--> 300     meta = self.pandas_rolling_method(
    301         self.obj._meta_nonempty, rolling_kwargs, method_name, *args, **kwargs
    302     )
    304     if self._has_single_partition:
    305         # There's no overlap just use map_partitions
    306         return self.obj.map_partitions(
    307             self.pandas_rolling_method,
    308             rolling_kwargs,
   (...)
    313             **kwargs,
    314         )

File ~/conda/envs/cudf-22.04/lib/python3.8/site-packages/dask/dataframe/rolling.py:488, in RollingGroupby.pandas_rolling_method(df, rolling_kwargs, name, groupby_kwargs, groupby_slice, *args, **kwargs)
    486 if groupby_slice:
    487     groupby = groupby[groupby_slice]
--> 488 rolling = groupby.rolling(**rolling_kwargs)
    489 return getattr(rolling, name)(*args, **kwargs).sort_index(level=-1)

File ~/conda/envs/cudf-22.04/lib/python3.8/site-packages/cudf/core/groupby/groupby.py:729, in GroupBy.rolling(self, *args, **kwargs)
    720 def rolling(self, *args, **kwargs):
    721     """"""
    722     Returns a `RollingGroupby` object that enables rolling window
    723     calculations on the groups.
   (...)
    727     cudf.core.window.Rolling
    728     """"""
--> 729     return cudf.core.window.rolling.RollingGroupby(self, *args, **kwargs)

TypeError: __init__() got an unexpected keyword argument 'win_type'
```


Env:
<details>
conda list | grep ""rapids\|dask""
cudf                      22.04.00a220131 cuda_11_py38_gc25d35b361_93    rapidsai-nightly
dask                      2022.1.0           pyhd8ed1ab_0    conda-forge
dask-core                 2022.1.0           pyhd8ed1ab_0    conda-forge
dask-cudf                 22.04.00a220131 cuda_11_py38_gc25d35b361_93    rapidsai-nightly
libcudf                   22.04.00a220131 cuda11_gc25d35b361_93    rapidsai-nightly
librmm                    22.04.00a220131 cuda11_g81d523a_15    rapidsai-nightly
ptxcompiler               0.2.0            py38h98f4b32_0    rapidsai-nightly
rmm                       22.04.00a220131 cuda11_py38_g81d523a_15_has_cma    rapidsai-nightly
</details>",2022-01-31T19:30:57Z,0,0,Nick Becker,@NVIDIA,True
239,[BUG] Groupby rolling window aggregations on time windows fail for SeriesGroupby,"For time windows, grouped rolling window aggregations currently fail for SeriesGroupby objects, but succeed on DataFrameGroupby objects.

```python
import dask
import cudf
​
df = cudf.datasets.timeseries()
​
print(df.groupby(""name"").rolling('1D').mean().head())
​
print(df.groupby(""name"").x.rolling('1D').mean().head())
                                    id         x         y
name  timestamp                                           
Alice 2000-01-01 00:00:36   992.000000 -0.875947  0.624249
      2000-01-01 00:01:51  1008.000000 -0.380349  0.473271
      2000-01-01 00:02:30   998.333333 -0.472583  0.249997
      2000-01-01 00:02:59   994.750000 -0.396048  0.408755
      2000-01-01 00:03:02   998.400000 -0.483499  0.170219
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Input In [28], in <module>
      4 df = cudf.datasets.timeseries()
      6 print(df.groupby(""name"").rolling('1D').mean().head())
----> 7 print(df.groupby(""name"").x.rolling('1D').mean().head())

File ~/conda/envs/cudf-22.04/lib/python3.8/site-packages/cudf/core/groupby/groupby.py:729, in GroupBy.rolling(self, *args, **kwargs)
    720 def rolling(self, *args, **kwargs):
    721     """"""
    722     Returns a `RollingGroupby` object that enables rolling window
    723     calculations on the groups.
   (...)
    727     cudf.core.window.Rolling
    728     """"""
--> 729     return cudf.core.window.rolling.RollingGroupby(self, *args, **kwargs)

File ~/conda/envs/cudf-22.04/lib/python3.8/site-packages/cudf/core/window/rolling.py:440, in RollingGroupby.__init__(self, groupby, window, min_periods, center)
    435 gb_size = groupby.size().sort_index()
    436 self._group_starts = (
    437     gb_size.cumsum().shift(1).fillna(0).repeat(gb_size)
    438 )
--> 440 super().__init__(obj, window, min_periods=min_periods, center=center)

File ~/conda/envs/cudf-22.04/lib/python3.8/site-packages/cudf/core/window/rolling.py:179, in Rolling.__init__(self, obj, window, min_periods, center, axis, win_type)
    177 self.min_periods = min_periods
    178 self.center = center
--> 179 self._normalize()
    180 self.agg_params = {}
    181 if axis != 0:

File ~/conda/envs/cudf-22.04/lib/python3.8/site-packages/cudf/core/window/rolling.py:375, in Rolling._normalize(self)
    372     return
    374 if not isinstance(self.obj.index, cudf.core.index.DatetimeIndex):
--> 375     raise ValueError(
    376         ""window must be an integer for non datetime index""
    377     )
    379 self._time_window = True
    381 try:

ValueError: window must be an integer for non datetime index
```

We do not see this behavior for grouped rolling window aggregations not using time windows.

```python
import dask
import cudf
​
df = cudf.datasets.timeseries().reset_index(drop=True)
df.groupby(""name"").rolling(window=3).x.sum().head()
48            <NA>
71            <NA>
86     1.067738088
99     1.142080475
103    0.702573636
Name: x, dtype: float64
```

Env:
<details>
conda list | grep ""rapids\|dask""
cudf                      22.04.00a220131 cuda_11_py38_gc25d35b361_93    rapidsai-nightly
dask                      2022.1.0           pyhd8ed1ab_0    conda-forge
dask-core                 2022.1.0           pyhd8ed1ab_0    conda-forge
dask-cudf                 22.04.00a220131 cuda_11_py38_gc25d35b361_93    rapidsai-nightly
libcudf                   22.04.00a220131 cuda11_gc25d35b361_93    rapidsai-nightly
librmm                    22.04.00a220131 cuda11_g81d523a_15    rapidsai-nightly
ptxcompiler               0.2.0            py38h98f4b32_0    rapidsai-nightly
rmm                       22.04.00a220131 cuda11_py38_g81d523a_15_has_cma    rapidsai-nightly
</details>

Perhaps relevant to https://github.com/rapidsai/cudf/issues/10173",2022-01-31T19:38:43Z,0,0,Nick Becker,@NVIDIA,True
240,"[FEA] Support missing operators in cudf.Series (`__divmod__`, `__rdivmod__`, `__round__`).","**Is your feature request related to a problem? Please describe.**
`cudf.Series` does not have reverse binary operators for bitwise operators like `__rxor__`. This was caught after changing the output of `Series.hash_values` from a `cupy.array` to a `cudf.Series` in #9390, which broke NVTabular: https://github.com/NVIDIA-Merlin/NVTabular/pull/1376#pullrequestreview-868327738. The `cupy.array` class supports both directions, like `array ^ scalar` and `scalar ^ array`. Similarly, pandas supports both directions. Currently `cudf.Series` only supports `series ^ scalar`.

**Describe the solution you'd like**
Add reverse binary operators for all cases that are missing, which include at least bitwise operators like `__rxor__` but possibly others as well. Missing operators have been addressed several times in the past (#208, #213, #1292, #8598, ...). Resolving this issue should involve checking the full list in the [Python object model documentation](https://docs.python.org/3/reference/datamodel.html#object.__radd__) to make sure we catch everything that pandas supports.

**Additional context**
A workaround is to use the binary operator in the reverse order (for commutative operators).",2022-01-31T20:11:54Z,0,0,Bradley Dice,@NVIDIA @rapidsai,True
241,[FEA] Unify distinct_count column/table APIs.,"**Is your feature request related to a problem? Please describe.**
While reviewing #10030, I found that the column and table algorithms for `distinct_count` have completely different flags for null and NaN handling. The column API has `null_policy` (include/exclude) and `nan_policy` (NaN is/isn't null), while the table API has `null_equality` (nulls are equal/unequal).

This also applies to `unordered_distinct_count`, introduced in #10030.

**Describe the solution you'd like**
The distinct count APIs for column/table should use the same flags (meaning that all three flags should probably be available to both APIs). This would also allow the column API to be a pass-through implementation of the table API, with a table composed of only that column, rather than having two implementations ([table](https://github.com/rapidsai/cudf/blob/2c6b0dac61a6671642bb5b076e910e20e2bdd1b6/cpp/src/stream_compaction/distinct_count.cu#L38-L62), [column](https://github.com/rapidsai/cudf/blob/2c6b0dac61a6671642bb5b076e910e20e2bdd1b6/cpp/src/stream_compaction/distinct_count.cu#L140-L168)).",2022-02-01T17:00:15Z,0,0,Bradley Dice,@NVIDIA @rapidsai,True
242,[BUG] test_groupby_diff_row_mixed_numerics failed randomly with overflow in datetime subtraction,"**Describe the bug**
The test `test_groupby_diff_row_mixed_numerics` failed randomly in CI. The randomly generated values resulted in overflow during subtraction in a datetime column, which caused pandas to fail (cudf may also fail, but that line had not yet run).

<details>
<summary>Traceback here</summary>

```
Error Message

OverflowError: Overflow in int64 addition

Stacktrace

nelem = 10, shift_perc = 0.5, direction = 1

    @pytest.mark.parametrize(""nelem"", [10, 50, 100, 1000])
    @pytest.mark.parametrize(""shift_perc"", [0.5, 1.0, 1.5])
    @pytest.mark.parametrize(""direction"", [1, -1])
    def test_groupby_diff_row_mixed_numerics(nelem, shift_perc, direction):
        t = rand_dataframe(
            dtypes_meta=[
                {""dtype"": ""int64"", ""null_frequency"": 0, ""cardinality"": 10},
                {""dtype"": ""int64"", ""null_frequency"": 0.4, ""cardinality"": 10},
                {""dtype"": ""float32"", ""null_frequency"": 0.4, ""cardinality"": 10},
                {""dtype"": ""decimal64"", ""null_frequency"": 0.4, ""cardinality"": 10},
                {
                    ""dtype"": ""datetime64[ns]"",
                    ""null_frequency"": 0.4,
                    ""cardinality"": 10,
                },
                {
                    ""dtype"": ""timedelta64[ns]"",
                    ""null_frequency"": 0.4,
                    ""cardinality"": 10,
                },
            ],
            rows=nelem,
            use_threads=False,
        )
        pdf = t.to_pandas()
        gdf = cudf.from_pandas(pdf)
        n_shift = int(nelem * shift_perc) * direction
    
>       expected = pdf.groupby([""0""]).diff(periods=n_shift)

cudf/tests/test_groupby.py:1960: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/envs/rapids/lib/python3.9/site-packages/pandas/core/groupby/groupby.py:948: in wrapper
    return self._python_apply_general(curried, self._obj_with_exclusions)
/opt/conda/envs/rapids/lib/python3.9/site-packages/pandas/core/groupby/groupby.py:1309: in _python_apply_general
    keys, values, mutated = self.grouper.apply(f, data, self.axis)
/opt/conda/envs/rapids/lib/python3.9/site-packages/pandas/core/groupby/ops.py:852: in apply
    res = f(group)
/opt/conda/envs/rapids/lib/python3.9/site-packages/pandas/core/groupby/groupby.py:937: in curried
    return f(x, *args, **kwargs)
/opt/conda/envs/rapids/lib/python3.9/site-packages/pandas/core/frame.py:8435: in diff
    new_data = self._mgr.diff(n=periods, axis=axis)
/opt/conda/envs/rapids/lib/python3.9/site-packages/pandas/core/internals/managers.py:374: in diff
    return self.apply(""diff"", n=n, axis=axis)
/opt/conda/envs/rapids/lib/python3.9/site-packages/pandas/core/internals/managers.py:327: in apply
    applied = getattr(b, f)(**kwargs)
/opt/conda/envs/rapids/lib/python3.9/site-packages/pandas/core/internals/blocks.py:1754: in diff
    new_values = values - values.shift(n, axis=axis)
/opt/conda/envs/rapids/lib/python3.9/site-packages/pandas/core/ops/common.py:69: in new_method
    return method(self, other)
/opt/conda/envs/rapids/lib/python3.9/site-packages/pandas/core/arrays/datetimelike.py:1336: in __sub__
    result = self._sub_datetime_arraylike(other)
/opt/conda/envs/rapids/lib/python3.9/site-packages/pandas/core/arrays/datetimes.py:721: in _sub_datetime_arraylike
    new_values = checked_add_with_arr(self_i8, -other_i8, arr_mask=arr_mask)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

arr = array([[ 8466425224569324595, -9223372036854775808,  1593875653891806295,
        -9223372036854775808, -9223372036854775808, -8738661699754510466]])
b = array([[-9223372036854775808, -9223372036854775808, -9223372036854775808,
        -9223372036854775808, -9223372036854775808, -8466425224569324595]])
arr_mask = array([[ True,  True,  True,  True,  True, False]]), b_mask = None

    def checked_add_with_arr(
        arr: np.ndarray,
        b,
        arr_mask: np.ndarray | None = None,
        b_mask: np.ndarray | None = None,
    ) -> np.ndarray:
        """"""
        Perform array addition that checks for underflow and overflow.
    
        Performs the addition of an int64 array and an int64 integer (or array)
        but checks that they do not result in overflow first. For elements that
        are indicated to be NaN, whether or not there is overflow for that element
        is automatically ignored.
    
        Parameters
        ----------
        arr : array addend.
        b : array or scalar addend.
        arr_mask : np.ndarray[bool] or None, default None
            array indicating which elements to exclude from checking
        b_mask : np.ndarray[bool] or None, default None
            array or scalar indicating which element(s) to exclude from checking
    
        Returns
        -------
        sum : An array for elements x + b for each element x in arr if b is
              a scalar or an array for elements x + y for each element pair
              (x, y) in (arr, b).
    
        Raises
        ------
        OverflowError if any x + y exceeds the maximum or minimum int64 value.
        """"""
        # For performance reasons, we broadcast 'b' to the new array 'b2'
        # so that it has the same size as 'arr'.
        b2 = np.broadcast_to(b, arr.shape)
        if b_mask is not None:
            # We do the same broadcasting for b_mask as well.
            b2_mask = np.broadcast_to(b_mask, arr.shape)
        else:
            b2_mask = None
    
        # For elements that are NaN, regardless of their value, we should
        # ignore whether they overflow or not when doing the checked add.
        if arr_mask is not None and b2_mask is not None:
            not_nan = np.logical_not(arr_mask | b2_mask)
        elif arr_mask is not None:
            not_nan = np.logical_not(arr_mask)
        elif b_mask is not None:
            not_nan = np.logical_not(b2_mask)
        else:
            not_nan = np.empty(arr.shape, dtype=bool)
            not_nan.fill(True)
    
        # gh-14324: For each element in 'arr' and its corresponding element
        # in 'b2', we check the sign of the element in 'b2'. If it is positive,
        # we then check whether its sum with the element in 'arr' exceeds
        # np.iinfo(np.int64).max. If so, we have an overflow error. If it
        # it is negative, we then check whether its sum with the element in
        # 'arr' exceeds np.iinfo(np.int64).min. If so, we have an overflow
        # error as well.
        i8max = lib.i8max
        i8min = iNaT
    
        mask1 = b2 > 0
        mask2 = b2 < 0
    
        if not mask1.any():
            to_raise = ((i8min - b2 > arr) & not_nan).any()
        elif not mask2.any():
            to_raise = ((i8max - b2 < arr) & not_nan).any()
        else:
            to_raise = ((i8max - b2[mask1] < arr[mask1]) & not_nan[mask1]).any() or (
                (i8min - b2[mask2] > arr[mask2]) & not_nan[mask2]
            ).any()
    
        if to_raise:
>           raise OverflowError(""Overflow in int64 addition"")
E           OverflowError: Overflow in int64 addition

/opt/conda/envs/rapids/lib/python3.9/site-packages/pandas/core/algorithms.py:1112: OverflowError
```

</details>

CI log: https://gpuci.gpuopenanalytics.com/job/rapidsai/job/gpuci/job/cudf/job/prb/job/cudf-gpu-test/CUDA=11.5,GPU_LABEL=driver-495,LINUX_VER=ubuntu20.04,PYTHON=3.9/6540/testReport/junit/cudf.tests/test_groupby/test_groupby_diff_row_mixed_numerics_1_0_5_10_/

Failing line:
https://github.com/rapidsai/cudf/blob/0581975e0132bd4189b579595e480a94d94a7917/python/cudf/cudf/tests/test_groupby.py#L1960

**Steps/Code to reproduce bug**
Failures are random, based on the seed.

Here is a minimal reproduction in pure pandas:
```python
import numpy as np
import pandas as pd
min_timedelta = np.timedelta64(-(2**63) + 1)
max_timedelta = np.timedelta64(2**63 - 1)

df = pd.DataFrame({""id"": [0, 0], ""data"": [min_timedelta, max_timedelta]})

# This fails:
df.diff()
# [traceback...] OverflowError: Overflow in int64 addition

# This also fails:
df.groupby(""id"").diff()
# [traceback...] OverflowError: Overflow in int64 addition

# Weirdly, the diff succeeds on just the Series:
df[""data""].diff()
# 0                           NaT
# 1   -1 days +23:59:59.999999998
# Name: data, dtype: timedelta64[ns]
```

Here's how cudf currently handles those cases:

```python
>>> gdf = cudf.from_pandas(df)
>>> gdf.diff()  # Implemented in #9817, but not yet available in branch-22.04
...
AttributeError: DataFrame object has no attribute diff
>>> gdf.groupby(""id"").diff()  # Silently overflows, but represents time differently than pandas
                         data
0                        <NA>
1  -0 days 00:00:00.000000002
>>> gdf[""data""].diff()  # Calls through to numba?
...
ValueError: Cannot determine Numba type of <class 'cudf.core.column.timedelta.TimeDeltaColumn'>
...
NotImplementedError: dtype timedelta64[ns] is not yet supported via `__cuda_array_interface__`
>>> 
```

**Expected behavior**
This test should not randomly fail. We should fix a seed and/or choose bounds on the data that prevent overflow.

**Additional context**
Found while reviewing PR #10143.",2022-02-03T22:45:25Z,0,0,Bradley Dice,@NVIDIA @rapidsai,True
243,[BUG] Investigate group-by scan `COUNT` aggregation returning 0-based results,"It appears that the group-by scan aggregation result for `COUNT()` returns a 0-based value. i.e. when the group has 1 row, the `COUNT` seems to return 0.

Consider the JNI [`TableTest::testGroupByScan()`](https://github.com/rapidsai/cudf/blob/branch-22.04/java/src/test/java/ai/rapids/cudf/TableTest.java#L3953):
```java
try (Table t1 = new Table.TestBuilder()
    .column( ""1"",  ""1"",  ""1"",  ""1"",  ""1"",  ""1"",  ""1"",  ""2"",  ""2"",  ""2"",  ""2"")
    .column(   0,    1,    3,    3,    5,    5,    5,    5,    5,    5,    5)
// ...
// with `COUNT(*)` aggregation.
    GroupByScanAggregation.count(NullPolicy.INCLUDE).onColumn(2),
// ...
// yields the following results:
   .column(   0,    0,    0,    1,    0,    1,    2,    0,    1,    2,    3) // odd why is this not 1 based?
```
One would have expected the following results instead:
```java
   .column(   1,    1,    1,    2,    1,    2,    3,    1,    2,    3,    4)
```
This might well be by design, but it warrants investigation.",2022-02-07T20:27:52Z,0,0,MithunR,NVIDIA,True
244,[FEA] Dask-cuDF Resample,"Today, Dask's resample codepath [converts](https://github.com/dask/dask/blob/5e8a4813cf948250608b16747773a7dc52088eb6/dask/dataframe/tseries/resample.py#L132) the resampling frequencies into DateOffsets before downstream processing. This is convenient, but currently prevents using resample with Dask-cuDF, as cuDF does not provide support for DateOffsets in the Grouper API.

To enable CPU/GPU compatibility in Dask, it might be nice to explore whether it's necessary in Dask to convert resampling frequencies into DateOffsets. If it's not necessary, we might be able to identify an alternative that is compatible with cuDF's existing resample functionality.

```python
import cudf
​
index = cudf.date_range(start=""2001-01-01"", periods=10, freq=""1T"")
sr = cudf.Series(range(10), index=index)
​
sr.resample(cudf.DateOffset(minutes=10)).sum()
---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
Input In [83], in <module>
      3 index = cudf.date_range(start=""2001-01-01"", periods=10, freq=""1T"")
      4 sr = cudf.Series(range(10), index=index)
----> 6 sr.resample(cudf.DateOffset(minutes=10)).sum()

File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/cudf/core/indexed_frame.py:1238, in IndexedFrame.resample(self, rule, axis, closed, label, convention, kind, loffset, base, on, level, origin, offset)
   1223     raise NotImplementedError(
   1224         ""The following arguments are not ""
   1225         ""currently supported by resample:\n\n""
   (...)
   1232         ""- offset""
   1233     )
   1234 by = cudf.Grouper(
   1235     key=on, freq=rule, closed=closed, label=label, level=level
   1236 )
   1237 return (
-> 1238     cudf.core.resample.SeriesResampler(self, by=by)
   1239     if isinstance(self, cudf.Series)
   1240     else cudf.core.resample.DataFrameResampler(self, by=by)
   1241 )

File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/cudf/core/resample.py:37, in _Resampler.__init__(self, obj, by, axis, kind)
     36 def __init__(self, obj, by, axis=None, kind=None):
---> 37     by = _ResampleGrouping(obj, by)
     38     super().__init__(obj, by=by)

File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/cudf/core/groupby/groupby.py:1488, in _Grouping.__init__(self, obj, by, level)
   1485 # Need to keep track of named key columns
   1486 # to support `as_index=False` correctly
   1487 self._named_columns = []
-> 1488 self._handle_by_or_level(by, level)
   1490 if len(obj) and not len(self._key_columns):
   1491     raise ValueError(""No group keys passed"")

File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/cudf/core/groupby/groupby.py:1513, in _Grouping._handle_by_or_level(self, by, level)
   1511     self._handle_mapping(by)
   1512 elif isinstance(by, Grouper):
-> 1513     self._handle_grouper(by)
   1514 else:
   1515     try:

File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/cudf/core/groupby/groupby.py:1577, in _Grouping._handle_grouper(self, by)
   1575 def _handle_grouper(self, by):
   1576     if by.freq:
-> 1577         self._handle_frequency_grouper(by)
   1578     elif by.key:
   1579         self._handle_label(by.key)

File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/cudf/core/resample.py:107, in _ResampleGrouping._handle_frequency_grouper(self, by)
    104 closed = by.closed
    106 if isinstance(freq, (cudf.DateOffset, pd.DateOffset)):
--> 107     raise NotImplementedError(
    108         ""Resampling by DateOffset objects is not yet supported.""
    109     )
    110 if not isinstance(freq, str):
    111     raise TypeError(
    112         f""Unsupported type for freq: {type(freq).__name__}""
    113     )

NotImplementedError: Resampling by DateOffset objects is not yet supported.
```

```python
sr.groupby(cudf.Grouper(cudf.DateOffset(minutes=10))).mean()
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
Input In [94], in <module>
----> 1 sr.groupby(cudf.Grouper(cudf.DateOffset(minutes=10))).mean()

File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/cudf/core/series.py:3265, in Series.groupby(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)
   3257 if by is None and level is None:
   3258     raise TypeError(
   3259         ""groupby() requires either by or level to be specified.""
   3260     )
   3262 return (
   3263     cudf.core.resample.SeriesResampler(self, by=by)
   3264     if isinstance(by, cudf.Grouper) and by.freq
-> 3265     else SeriesGroupBy(
   3266         self, by=by, level=level, dropna=dropna, sort=sort
   3267     )
   3268 )

File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/cudf/core/groupby/groupby.py:82, in GroupBy.__init__(self, obj, by, level, sort, as_index, dropna)
     80     self.grouping = by
     81 else:
---> 82     self.grouping = _Grouping(obj, by, level)

File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/cudf/core/groupby/groupby.py:1488, in _Grouping.__init__(self, obj, by, level)
   1485 # Need to keep track of named key columns
   1486 # to support `as_index=False` correctly
   1487 self._named_columns = []
-> 1488 self._handle_by_or_level(by, level)
   1490 if len(obj) and not len(self._key_columns):
   1491     raise ValueError(""No group keys passed"")

File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/cudf/core/groupby/groupby.py:1513, in _Grouping._handle_by_or_level(self, by, level)
   1511     self._handle_mapping(by)
   1512 elif isinstance(by, Grouper):
-> 1513     self._handle_grouper(by)
   1514 else:
   1515     try:

File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/cudf/core/groupby/groupby.py:1579, in _Grouping._handle_grouper(self, by)
   1577     self._handle_frequency_grouper(by)
   1578 elif by.key:
-> 1579     self._handle_label(by.key)
   1580 else:
   1581     self._handle_level(by.level)

File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/cudf/core/groupby/groupby.py:1571, in _Grouping._handle_label(self, by)
   1570 def _handle_label(self, by):
-> 1571     self._key_columns.append(self._obj._data[by])
   1572     self.names.append(by)
   1573     self._named_columns.append(by)

File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/cudf/core/column_accessor.py:156, in ColumnAccessor.__getitem__(self, key)
    155 def __getitem__(self, key: Any) -> ColumnBase:
--> 156     return self._data[key]

KeyError: <DateOffset: minutes=10>
```

Env:
<details>
conda list | grep ""rapids\|dask""
# packages in environment at /home/nicholasb/conda/envs/rapids-22.04:
cucim                     22.04.00a220201 cuda_11_py38_g0861858_17    rapidsai-nightly
cudf                      22.04.00a220201 cuda_11_py38_g2c6b0dac61_95    rapidsai-nightly
cudf_kafka                22.04.00a220201 py38_g2c6b0dac61_95    rapidsai-nightly
cugraph                   22.04.00a220201 cuda11_py38_g2b950598_32    rapidsai-nightly
cuml                      22.04.00a220201 cuda11_py38_ga70044cf2_39    rapidsai-nightly
cusignal                  22.04.00a220201 py39_gc620d82_7    rapidsai-nightly
cuspatial                 22.04.00a220201 py38_ge00d63f_9    rapidsai-nightly
custreamz                 22.04.00a220201 py38_g2c6b0dac61_95    rapidsai-nightly
cuxfilter                 22.04.00a220201 py38_g17de7c0_7    rapidsai-nightly
dask                      2022.1.0           pyhd8ed1ab_0    conda-forge
dask-core                 2022.1.0           pyhd8ed1ab_0    conda-forge
dask-cuda                 22.04.00a220201          py38_8    rapidsai-nightly
dask-cudf                 22.04.00a220201 cuda_11_py38_g2c6b0dac61_95    rapidsai-nightly
libcucim                  22.04.00a220201 cuda11_g0861858_17    rapidsai-nightly
libcudf                   22.04.00a220201 cuda11_g2c6b0dac61_95    rapidsai-nightly
libcudf_kafka             22.04.00a220201  g2c6b0dac61_95    rapidsai-nightly
libcugraph                22.04.00a220201 cuda11_g2b950598_32    rapidsai-nightly
libcugraph_etl            22.04.00a220201 cuda11_g2b950598_32    rapidsai-nightly
libcuml                   22.04.00a220201 cuda11_ga70044cf2_39    rapidsai-nightly
libcumlprims              22.04.00a220121 cuda11_g130a9d4_8    rapidsai-nightly
libcuspatial              22.04.00a220201 cuda11_ge00d63f_9    rapidsai-nightly
librmm                    22.04.00a220201 cuda11_g81d523a_15    rapidsai-nightly
libxgboost                1.5.0dev.rapidsai22.04      cuda11.2_0    rapidsai-nightly
ptxcompiler               0.2.0            py38h98f4b32_0    rapidsai-nightly
py-xgboost                1.5.0dev.rapidsai22.04  cuda11.2py38_0    rapidsai-nightly
pylibcugraph              22.04.00a220201 cuda11_py38_g2b950598_32    rapidsai-nightly
rapids                    22.04.00a220201 cuda11_py38_g1db237c_48    rapidsai-nightly
rapids-xgboost            22.04.00a220201 cuda11_py38_g1db237c_48    rapidsai-nightly
rmm                       22.04.00a220201 cuda11_py38_g81d523a_15_has_cma    rapidsai-nightly
ucx                       1.12.0+gd367332      cuda11.2_0    rapidsai-nightly
ucx-proc                  1.0.0                       gpu    rapidsai-nightly
ucx-py                    0.25.00a220201  py38_gd367332_4    rapidsai-nightly
xgboost                   1.5.0dev.rapidsai22.04  cuda11.2py38_0    rapidsai-nightly
</details>",2022-02-08T00:10:15Z,0,0,Nick Becker,@NVIDIA,True
245,[FEA] Support for callable functions in groupby transform,"**What is your question?**
We would to run this code on a dataframe loaded using cudf, but encounter the following error.
if anyone have an answer for this problem, thank you

`df['rmean'] = df.groupby(['outlet', 'product'])['sales'].transform(lambda x: x.rolling(window=win).mean()).astype(np.float32)`

`AttributeError: type object 'cudf._lib.aggregation.GroupbyAggregation' has no attribute 'rolling'`
",2022-02-08T04:02:19Z,0,0,Muhammad Rizky Ferlanda,,False
246,[BUG] Python groupby rolling aggregations return index inconsistent with pandas,"Python groupby rolling aggregations return a single Index that corresponds to the original row position of the element, but in pandas return a MultiIndex that includes both the groupby key(s) and original row position.

This is not currently blocking any behavior with Dask + cuDF, as grouped rolling operations are blocked by #10173 


```python
import pandas as pd
import cudf
import numpy as np
​
df = cudf.datasets.randomdata(nrows=100000)
pdf = df.to_pandas()
​
print(pdf.groupby(['id']).rolling(window=3).x.mean().head())
print(df.groupby(['id']).rolling(window=3).x.mean().head())
id        
879  43605   NaN
881  3941    NaN
882  29855   NaN
884  14616   NaN
     70864   NaN
Name: x, dtype: float64
43605    <NA>
3941     <NA>
29855    <NA>
14616    <NA>
70864    <NA>
Name: x, dtype: float64
```",2022-02-08T17:18:48Z,0,0,Nick Becker,@NVIDIA,True
247,[FEA] JSON reader: ignores Java/C++ style comment,"This is part of FEA of https://github.com/NVIDIA/spark-rapids/issues/9
We have a JSON file

``` json
{""name"": // name
 ""Reynold Xin""}
```

Spark can parse it when enabling `allowComments` and `multiLine`

or 

``` json
{'name': /* hello */ 'Reynold Xin'}
```

Spark can parse it when enabling `allowComments` 

We expect there is a configure `allowComments` to control this behavior. ",2022-02-10T08:58:50Z,0,0,Bobby Wang,,False
248,[FEA] JSON reader: support unquoted JSON field names.,"This is part of FEA of https://github.com/NVIDIA/spark-rapids/issues/9
We have a JSON file

``` json
{name: ""Reynold Xin""}
```

Spark can parse it when enabling `allowUnquotedFieldNames`

CUDF parsing will throw exception

We expect there is a configure `allowUnquotedFieldNames` to control this behavior. ",2022-02-10T09:10:57Z,0,0,Bobby Wang,,False
249,[FEA] JSON reader: support multi-lines,"This is part of FEA of https://github.com/NVIDIA/spark-rapids/issues/9
We have a JSON file

``` json
{""name"":
   ""Reynold Xin""}
```

Spark can parse it when enabling `multiLine`

CUDF parsing will throw an exception

We expect there is a configure `multiLine` to control this behavior. ",2022-02-10T09:18:20Z,0,0,Bobby Wang,,False
250,[FEA] df.explode with multiple columns,"**Is your feature request related to a problem? Please describe.**
Pandas supports exploding multiple columns with `df.explode`, but currently cudf fails for that case.

**Describe the solution you'd like**
Support explode with multiple column names. The doc contains an example of the expected output from a multi column explode:
https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.explode.html#pandas-dataframe-explode


**Describe alternatives you've considered**
I haven't been able to come up with a workaround to this, but it might be 2 separate explodes + a merge of some kind to get to the same result. 

**Additional context**
Add any other context, code examples, or references to existing implementations about the feature request here.
",2022-02-11T00:28:05Z,0,0,Ayush Dattagupta,Nvidia,True
251,[FEA] Support dask correlation and covariance when columns include nulls,"Currently, Dask's `{DataFrame, Series}.corr` [codepath](https://github.com/dask/dask/blob/20e924618999febeac706b20212104fe4f3ea61d/dask/dataframe/core.py#L6462-L6467) relies on calling `df.values` in `_cov_corr_chunk`. When cudf columns have null values, we throw an error trying to convert to cupy.

I'd like to be able to do the following with Dask and have data containing nulls handled appropriately on GPUs and CPUs.


```python
import dask.dataframe as dd
import pandas as pd
import dask_cudf
import cudf
​
df = pd.DataFrame({
    ""a"":[0,0,1,1,1,0,1],
    ""b"":[10,None,3,None,5,4,2],
    ""c"":[10,-3,3,-6,5,4,-2]
})
gdf = cudf.from_pandas(df)
ddf = dd.from_pandas(df, 2)
gddf = dask_cudf.from_dask_dataframe(ddf)
​
​
print(ddf.corr().compute())
print(gddf.corr().compute())
          a         b         c
a  1.000000 -0.644831 -0.356138
b -0.644831  1.000000  0.933122
c -0.356138  0.933122  1.000000
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Input In [51], in <module>
     13 gddf = dask_cudf.from_dask_dataframe(ddf)
     16 print(ddf.corr().compute())
---> 17 print(gddf.corr().compute())

File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/dask/base.py:288, in DaskMethodsMixin.compute(self, **kwargs)
    264 def compute(self, **kwargs):
    265     """"""Compute this dask collection
    266 
    267     This turns a lazy Dask collection into its in-memory equivalent.
   (...)
    286     dask.base.compute
    287     """"""
--> 288     (result,) = compute(self, traverse=False, **kwargs)
    289     return result

File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/dask/base.py:571, in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)
    568     keys.append(x.__dask_keys__())
    569     postcomputes.append(x.__dask_postcompute__())
--> 571 results = schedule(dsk, keys, **kwargs)
    572 return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])

File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/dask/threaded.py:79, in get(dsk, result, cache, num_workers, pool, **kwargs)
     76     elif isinstance(pool, multiprocessing.pool.Pool):
     77         pool = MultiprocessingPoolExecutor(pool)
---> 79 results = get_async(
     80     pool.submit,
     81     pool._max_workers,
     82     dsk,
     83     result,
     84     cache=cache,
     85     get_id=_thread_get_id,
     86     pack_exception=pack_exception,
     87     **kwargs,
     88 )
     90 # Cleanup pools associated to dead threads
     91 with pools_lock:

File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/dask/local.py:507, in get_async(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)
    505         _execute_task(task, data)  # Re-execute locally
    506     else:
--> 507         raise_exception(exc, tb)
    508 res, worker_id = loads(res_info)
    509 state[""cache""][key] = res

File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/dask/local.py:315, in reraise(exc, tb)
    313 if exc.__traceback__ is not tb:
    314     raise exc.with_traceback(tb)
--> 315 raise exc

File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/dask/local.py:220, in execute_task(key, task_info, dumps, loads, get_id, pack_exception)
    218 try:
    219     task, data = loads(task_info)
--> 220     result = _execute_task(task, data)
    221     id = get_id()
    222     result = dumps((result, id))

File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/dask/core.py:119, in _execute_task(arg, cache, dsk)
    115     func, args = arg[0], arg[1:]
    116     # Note: Don't assign the subtask results to a variable. numpy detects
    117     # temporaries by their reference count and can execute certain
    118     # operations in-place.
--> 119     return func(*(_execute_task(a, cache) for a in args))
    120 elif not ishashable(arg):
    121     return arg

File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/dask/dataframe/core.py:6440, in cov_corr_chunk(df, corr)
   6438 shape = (df.shape[1], df.shape[1])
   6439 df = df.astype(""float64"", copy=False)
-> 6440 sums = np.zeros_like(df.values, shape=shape)
   6441 counts = np.zeros_like(df.values, shape=shape)
   6442 for idx, col in enumerate(df):

File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/cudf/core/frame.py:619, in Frame.values(self)
    606 @property
    607 def values(self):
    608     """"""
    609     Return a CuPy representation of the DataFrame.
    610 
   (...)
    617         The values of the DataFrame.
    618     """"""
--> 619     return self.to_cupy()

File ~/conda/envs/rapids-22.04/lib/python3.8/contextlib.py:75, in ContextDecorator.__call__.<locals>.inner(*args, **kwds)
     72 @wraps(func)
     73 def inner(*args, **kwds):
     74     with self._recreate_cm():
---> 75         return func(*args, **kwds)

File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/cudf/core/frame.py:719, in Frame.to_cupy(self, dtype, copy, na_value)
    693 @annotate(""FRAME_TO_CUPY"", color=""green"", domain=""cudf_python"")
    694 def to_cupy(
    695     self,
   (...)
    698     na_value=None,
    699 ) -> cupy.ndarray:
    700     """"""Convert the Frame to a CuPy array.
    701 
    702     Parameters
   (...)
    717     cupy.ndarray
    718     """"""
--> 719     return self._to_array(
    720         (lambda col: col.values.copy())
    721         if copy
    722         else (lambda col: col.values),
    723         cupy.empty,
    724         dtype,
    725         na_value,
    726     )

File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/cudf/core/frame.py:684, in Frame._to_array(self, get_column_values, make_empty_matrix, dtype, na_value)
    677 matrix = make_empty_matrix(
    678     shape=(len(self), ncol), dtype=dtype, order=""F""
    679 )
    680 for i, col in enumerate(self._data.values()):
    681     # TODO: col.values may fail if there is nullable data or an
    682     # unsupported dtype. We may want to catch and provide a more
    683     # suitable error.
--> 684     matrix[:, i] = get_column_values_na(col)
    685 return matrix

File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/cudf/core/frame.py:663, in Frame._to_array.<locals>.get_column_values_na(col)
    661 if na_value is not None:
    662     col = col.fillna(na_value)
--> 663 return get_column_values(col)

File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/cudf/core/frame.py:722, in Frame.to_cupy.<locals>.<lambda>(col)
    693 @annotate(""FRAME_TO_CUPY"", color=""green"", domain=""cudf_python"")
    694 def to_cupy(
    695     self,
   (...)
    698     na_value=None,
    699 ) -> cupy.ndarray:
    700     """"""Convert the Frame to a CuPy array.
    701 
    702     Parameters
   (...)
    717     cupy.ndarray
    718     """"""
    719     return self._to_array(
    720         (lambda col: col.values.copy())
    721         if copy
--> 722         else (lambda col: col.values),
    723         cupy.empty,
    724         dtype,
    725         na_value,
    726     )

File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/cudf/core/column/column.py:155, in ColumnBase.values(self)
    152     return cupy.array([], dtype=self.dtype)
    154 if self.has_nulls():
--> 155     raise ValueError(""Column must have no nulls."")
    157 return cupy.asarray(self.data_array_view)

ValueError: Column must have no nulls.
```

Environment: <details>
conda list | grep ""rapids\|dask\|cupy""
# packages in environment at /home/nicholasb/conda/envs/rapids-22.04:
cucim                     22.04.00a220215 cuda_11_py38_g12cc926_22    rapidsai-nightly
cudf                      22.04.00a220215 cuda_11_py38_g8b0737d7a4_164    rapidsai-nightly
cudf_kafka                22.04.00a220215 py38_g8b0737d7a4_164    rapidsai-nightly
cugraph                   22.04.00a220215 cuda11_py38_g5f971fef_53    rapidsai-nightly
cuml                      22.04.00a220215 cuda11_py38_g88e41e858_62    rapidsai-nightly
cupy                      9.6.0            py38h177b0fd_0    conda-forge
cusignal                  22.04.00a220215 py39_gc620d82_7    rapidsai-nightly
cuspatial                 22.04.00a220215 py38_gc63083c_11    rapidsai-nightly
custreamz                 22.04.00a220215 py38_g8b0737d7a4_164    rapidsai-nightly
cuxfilter                 22.04.00a220215 py38_g97fa691_8    rapidsai-nightly
dask                      2022.1.0           pyhd8ed1ab_0    conda-forge
dask-core                 2022.1.0           pyhd8ed1ab_0    conda-forge
dask-cuda                 22.04.00a220215         py38_15    rapidsai-nightly
dask-cudf                 22.04.00a220215 cuda_11_py38_g8b0737d7a4_164    rapidsai-nightly
dask-sql                  2022.1.0         py38h578d9bd_0    conda-forge
libcucim                  22.04.00a220215 cuda11_g12cc926_22    rapidsai-nightly
libcudf                   22.04.00a220215 cuda11_g8b0737d7a4_164    rapidsai-nightly
libcudf_kafka             22.04.00a220215 g8b0737d7a4_164    rapidsai-nightly
libcugraph                22.04.00a220215 cuda11_g5f971fef_53    rapidsai-nightly
libcugraph_etl            22.04.00a220215 cuda11_g5f971fef_53    rapidsai-nightly
libcuml                   22.04.00a220215 cuda11_g88e41e858_62    rapidsai-nightly
libcumlprims              22.04.00a220207 cuda11_gde69bdf_11    rapidsai-nightly
libcuspatial              22.04.00a220215 cuda11_gc63083c_11    rapidsai-nightly
librmm                    22.04.00a220215 cuda11_g653f331_23    rapidsai-nightly
libxgboost                1.5.2dev.rapidsai22.04      cuda11.2_0    rapidsai-nightly
ptxcompiler               0.2.0            py38h98f4b32_0    rapidsai-nightly
py-xgboost                1.5.2dev.rapidsai22.04  cuda11.2py38_0    rapidsai-nightly
pylibcugraph              22.04.00a220215 cuda11_py38_g5f971fef_53    rapidsai-nightly
rapids                    22.04.00a220215 cuda11_py38_g76071bf_87    rapidsai-nightly
rapids-xgboost            22.04.00a220215 cuda11_py38_g76071bf_87    rapidsai-nightly
rmm                       22.04.00a220203 cuda11_py38_g0515ca4_16_has_cma    rapidsai-nightly
ucx                       1.12.0+gd367332      cuda11.2_0    rapidsai-nightly
ucx-proc                  1.0.0                       gpu    rapidsai-nightly
ucx-py                    0.25.00a220205  py38_gd367332_4    rapidsai-nightly
xgboost                   1.5.2dev.rapidsai22.04  cuda11.2py38_0    rapidsai-nightly
</details>",2022-02-15T15:20:11Z,0,0,Nick Becker,@NVIDIA,True
252,[FEA] Dask support for groupby correlation and covariance,"Today, cuDF Python supports `groupby.corr` and has a PR in review for `groupby.cov`. I'd like to be able to use these with Dask on GPUs, like I can with Dask on CPUs (note that the distributed algorithm doesn't actually rely on these primitives).

The Dask [implementation](https://github.com/dask/dask/blob/20e924618999febeac706b20212104fe4f3ea61d/dask/dataframe/groupby.py#L355-L507) looks to have some challenges to running with a cuDF backend, including:
- Hardcoded pandas return types in some functions
- Passing arguments to groupby.apply (which cuDF doesn't currently support)
- Using groupby.apply(arbitrary_func), which can be slow in cuDF

```python
import dask.dataframe as dd
import pandas as pd
import dask_cudf
import cudf
​
df = pd.DataFrame({
    ""a"": [0,0,1,0,0,1,1,1,1,1],
    ""b"": [0,0.3,0,-4,0,-3,1,1,11,1],
    ""c"": [19,0,30,0,0,1,41,1,1,1],
})
gdf = cudf.from_pandas(df)
ddf = dd.from_pandas(df, 2)
gddf = dask_cudf.from_dask_dataframe(ddf)
​
print(ddf.groupby(""a"").corr().compute())
print(gddf.groupby(""a"").corr().compute())
            b         c
a                      
0 b  1.000000  0.300100
  c  0.300100  1.000000
1 b  1.000000 -0.200625
  c -0.200625  1.000000
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/dask/dataframe/utils.py:176, in raise_on_meta_error(funcname, udf)
    175 try:
--> 176     yield
    177 except Exception as e:

File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/dask/dataframe/core.py:5984, in _emulate(func, udf, *args, **kwargs)
   5983 with raise_on_meta_error(funcname(func), udf=udf):
-> 5984     return func(*_extract_meta(args, True), **_extract_meta(kwargs, True))

File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/dask/dataframe/groupby.py:447, in _cov_chunk(df, *by)
    445 x = g.sum()
--> 447 mul = g.apply(_mul_cols, cols=cols).reset_index(level=-1, drop=True)
    449 n = g[x.columns].count().rename(columns=lambda c: f""{c}-count"")

TypeError: apply() got an unexpected keyword argument 'cols'

The above exception was the direct cause of the following exception:

ValueError                                Traceback (most recent call last)
Input In [85], in <module>
     13 gddf = dask_cudf.from_dask_dataframe(ddf)
     15 print(ddf.groupby(""a"").corr().compute())
---> 16 print(gddf.groupby(""a"").corr().compute())

File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/dask/dataframe/groupby.py:1475, in _GroupBy.corr(self, ddof, split_every, split_out)
   1470 @derived_from(pd.DataFrame)
   1471 def corr(self, ddof=1, split_every=None, split_out=1):
   1472     """"""Groupby correlation:
   1473     corr(X, Y) = cov(X, Y) / (std_x * std_y)
   1474     """"""
-> 1475     return self.cov(split_every=split_every, split_out=split_out, std=True)

File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/dask/dataframe/groupby.py:1500, in _GroupBy.cov(self, ddof, split_every, split_out, std)
   1497         sliced_plus = list(self._slice) + list(self.by)
   1498         self.obj = self.obj[sliced_plus]
-> 1500 result = aca(
   1501     [self.obj, self.by]
   1502     if not isinstance(self.by, list)
   1503     else [self.obj] + self.by,
   1504     chunk=_cov_chunk,
   1505     aggregate=_cov_agg,
   1506     combine=_cov_combine,
   1507     token=self._token_prefix + ""cov"",
   1508     aggregate_kwargs={""ddof"": ddof, ""levels"": levels, ""std"": std},
   1509     combine_kwargs={""levels"": levels},
   1510     split_every=split_every,
   1511     split_out=split_out,
   1512     split_out_setup=split_out_on_index,
   1513     sort=self.sort,
   1514 )
   1516 if isinstance(self.obj, Series):
   1517     result = result[result.columns[0]]

File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/dask/dataframe/core.py:5935, in apply_concat_apply(args, chunk, aggregate, combine, meta, token, chunk_kwargs, aggregate_kwargs, combine_kwargs, split_every, split_out, split_out_setup, split_out_setup_kwargs, sort, ignore_index, **kwargs)
   5932         dsk[(b, j)] = (aggregate, conc)
   5934 if meta is no_default:
-> 5935     meta_chunk = _emulate(chunk, *args, udf=True, **chunk_kwargs)
   5936     meta = _emulate(
   5937         aggregate, _concat([meta_chunk], ignore_index), udf=True, **aggregate_kwargs
   5938     )
   5939 meta = make_meta(
   5940     meta,
   5941     index=(getattr(make_meta(dfs[0]), ""index"", None) if dfs else None),
   5942     parent_meta=dfs[0]._meta,
   5943 )

File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/dask/dataframe/core.py:5984, in _emulate(func, udf, *args, **kwargs)
   5979 """"""
   5980 Apply a function using args / kwargs. If arguments contain dd.DataFrame /
   5981 dd.Series, using internal cache (``_meta``) for calculation
   5982 """"""
   5983 with raise_on_meta_error(funcname(func), udf=udf):
-> 5984     return func(*_extract_meta(args, True), **_extract_meta(kwargs, True))

File ~/conda/envs/rapids-22.04/lib/python3.8/contextlib.py:131, in _GeneratorContextManager.__exit__(self, type, value, traceback)
    129     value = type()
    130 try:
--> 131     self.gen.throw(type, value, traceback)
    132 except StopIteration as exc:
    133     # Suppress StopIteration *unless* it's the same exception that
    134     # was passed to throw().  This prevents a StopIteration
    135     # raised inside the ""with"" statement from being suppressed.
    136     return exc is not value

File ~/conda/envs/rapids-22.04/lib/python3.8/site-packages/dask/dataframe/utils.py:197, in raise_on_meta_error(funcname, udf)
    188 msg += (
    189     ""Original error is below:\n""
    190     ""------------------------\n""
   (...)
    194     ""{2}""
    195 )
    196 msg = msg.format(f"" in `{funcname}`"" if funcname else """", repr(e), tb)
--> 197 raise ValueError(msg) from e

ValueError: Metadata inference failed in `_cov_chunk`.

You have supplied a custom function and Dask is unable to 
determine the type of output that that function returns. 

To resolve this please provide a meta= keyword.
The docstring of the Dask function you ran should have more information.

Original error is below:
------------------------
TypeError(""apply() got an unexpected keyword argument 'cols'"")

Traceback:
---------
  File ""/home/nicholasb/conda/envs/rapids-22.04/lib/python3.8/site-packages/dask/dataframe/utils.py"", line 176, in raise_on_meta_error
    yield
  File ""/home/nicholasb/conda/envs/rapids-22.04/lib/python3.8/site-packages/dask/dataframe/core.py"", line 5984, in _emulate
    return func(*_extract_meta(args, True), **_extract_meta(kwargs, True))
  File ""/home/nicholasb/conda/envs/rapids-22.04/lib/python3.8/site-packages/dask/dataframe/groupby.py"", line 447, in _cov_chunk
    mul = g.apply(_mul_cols, cols=cols).reset_index(level=-1, drop=True)
```",2022-02-15T15:42:17Z,0,0,Nick Becker,@NVIDIA,True
253,[BUG] cuDF covariance does not agree with pandas when ddof=N,"In PR #9889, @isVoid observed that cudf and pandas handle covariance calculations differently when `N == ddof`. Here is a minimal example showing a few differences in behavior with `ddof`. This is a bug because cudf should give the same results as pandas. The bug affects both `df.cov()` and `df.groupby(...).cov()`. Note that this is _not_ related to the pandas bug 45814 which was also found during #9889 (not directly linked here because it is not related) regarding `ddof` with missing data, because no data is missing here.

```python
import pandas as pd
import cudf

pdf = pd.DataFrame({""i"": [0, 0], ""a"": [0, 1]})
gdf = cudf.from_pandas(pdf)
print(""pandas cov"")
print(pdf.cov(ddof=2))
print(""cudf cov"")
print(gdf.cov(ddof=2))
print(""pandas groupby cov"")
print(pdf.groupby(""i"").cov(ddof=2))
print(""cudf groupby cov"")
print(gdf.groupby(""i"").cov(ddof=2))
```

Results (warnings not shown):
```python
pandas cov                                                                                                                                                                                    
    i    a
i NaN  NaN
a NaN  inf

cudf cov
     i    a
i  0.0  0.0
a  0.0  0.5

pandas groupby cov
       a
i       
0 a  inf

cudf groupby cov
        a
i        
0 a  <NA>
```

Notice that cudf isn't self-consistent between `df.cov()` and `df.groupby(...).cov()` in its results for `a`.

_Originally posted by @bdice in https://github.com/rapidsai/cudf/pull/9889#discussion_r807267844_",2022-02-16T02:15:44Z,0,0,Bradley Dice,@NVIDIA @rapidsai,True
254,[DOC] write a developer guide about native Host/Device memory resource ownership and reference counting in cuDF Java API,"## Report needed documentation

New developers using Java API in spark-rapids and elsewhere will greatly benefit from a developer guide regarding native host and device memory tracking. The documentation should exclude typical scenarios explaining how to transfer ownership, when to explicitly close and when to rely on higher-level containers to auto-close resources.

**Steps taken to search for needed documentation**
browsing code and following review comments.",2022-02-18T17:13:37Z,0,0,Gera Shegalov,@NVIDIA,True
255,"[BUG] groupby.fillna(method=""ffill"") doesn't work for NaN","**Describe the bug**
groupby.fillna(method=""ffill"") doesn't work for NaN. bfill also has same issue.

**Steps/Code to reproduce bug**
```
In [3]: df = cudf.DataFrame(
   ...:     [
   ...:         [""A"", 1],
   ...:         [""A"", np.nan],
   ...:         [""B"", 0],
   ...:         [""B"", None],
   ...:     ],
   ...:     columns=[""x"", ""y""],
   ...: )
   ...: 

In [4]: df
Out[4]: 
   x     y
0  A     1
1  A  <NA>
2  B     0
3  B  <NA>
```
fillna works for < NA >
```
In [5]: df.groupby(""x"")[""y""].fillna(method=""ffill"")
   ...: 
Out[5]: 
0    1
1    1
2    0
3    0
Name: y, dtype: int64
```
But it doesn't work for NaN
```
In [6]: df.loc[df[""y""].isnull(), ""y""] = np.nan
   ...: df
Out[6]: 
   x    y
0  A  1.0
1  A  NaN
2  B  0.0
3  B  NaN

In [7]: df.groupby(""x"")[""y""].fillna(method=""ffill"")
   ...: 
Out[7]: 
0    1.0
1    NaN
2    0.0
3    NaN
Name: y, dtype: float64
```
We need to replace NaN with < NA > using None
```
In [8]: df.loc[df[""y""].isnull(), ""y""] = None
   ...: df
Out[8]: 
   x     y
0  A   1.0
1  A  <NA>
2  B   0.0
3  B  <NA>

In [9]: df.groupby(""x"")[""y""].fillna(method=""ffill"")
   ...: 
Out[9]: 
0    1.0
1    1.0
2    0.0
3    0.0
Name: y, dtype: float64
```

**Environment overview (please complete the following information)**
cudf version is ""22.02.00""
",2022-02-26T12:49:43Z,0,0,Kazuki Onodera,NVIDIA,True
256,"Use ""ranger"" to prevent grid stride loop overflow","(updated Aug 2023)

### Background

We found a kernel indexing overflow issue, first discovered in the `fused_concatenate` kernels (https://github.com/rapidsai/cudf/issues/10333) and this issue is present in a number of our CUDA kernels that take the following form:

```
size_type output_index = threadIdx.x + blockIdx.x * blockDim.x;  
while (output_index < output_size) {
  output_index += blockDim.x * gridDim.x;
}
```

If we have an output_size of say 1.2 billion and a grid size that's the same, the following happens:  Some late thread id, say 1.19 billion attempts to add 1.2 billion (blockDim.x * gridDim.x) and overflows the size_type (signed 32 bits). 

We made a round of fixes in #10448, and then later found another instance of this error in #13838. Our first pass of investigation was not adequate to contain the issue, so we need to take another close look.


### Part 1 - First pass fix kernels with this issue

| Source file | Kernels | Status | 
|---|---|---|
| `copying/concatenate.cu` | `fused_concatenate_kernel` |  #10448 |
| `valid_if.cuh` | `valid_if_kernel` |  #10448 |
| `scatter.cu` | `marking_bitmask_kernel` |  #10448 |
| `replace/nulls.cu` | `replace_nulls_strings` | #10448 |
| `replace/nulls.cu` | `replace_nulls` |  #10448 |
| `rolling/rolling_detail.cuh` | `gpu_rolling` |  #10448 |
| `rolling/jit/kernel.cu` | `gpu_rolling_new` | #10448 |
| `transform/compute_column.cu` | `compute_column_kernel`  | #10448 |
|`copying/concatenate.cu` | `fused_concatenate_string_offset_kernel` |  #13838 |
| `replace/replace.cu` |   `replace_strings_first_pass` <br>   `replace_strings_second_pass` <br>  `replace_kernel` | #13905 |
| `copying/concatenate.cu` |   `concatenate_masks_kernel` <br>   `fused_concatenate_string_offset_kernel` <br>   `fused_concatenate_string_chars_kernel` <br>  `fused_concatenate_kernel` (int64) | #13906 | | 
| `hash/helper_functions.cuh` |   `init_hashtbl` | #13895  |
| `null_mask.cu` |  `set_null_mask_kernel` <br>   `copy_offset_bitmask` <br>   `count_set_bits_kernel` | #13895  | 
| `transform/row_bit_count.cu` |   `compute_row_sizes` | #13895  | 
| `multibyte_split.cu` |   `multibyte_split_init_kernel` <br>   `multibyte_split_seed_kernel` (auto??) <br>   `multibyte_split_kernel`  | #13910 | 
| IO modules: parquet, orc, json | | #13910 | 
| `io/utilities/parsing_utils.cu` |   `count_and_set_positions` (uint64_t)  | #13910 |  
| `conditional_join_kernels.cuh` |   `compute_conditional_join_output_size` <br>   `conditional_join` | #13971 | 
| `merge.cu` |   `materialize_merged_bitmask_kernel`  | #13972 | 
| `partitioning.cu` |   `compute_row_partition_numbers`  <br>  `compute_row_output_locations` <br>   `copy_block_partitions`  | #13973  | 
| `json_path.cu` |  `get_json_object_kernel`  | #13962 | 
 | `tdigest` |   `compute_percentiles_kernel` (int)  | #13962 | 
| `strings/attributes.cu` |   `count_characters_parallel_fn`  | #13968 |  
| `strings/convert/convert_urls.cu` |   `url_decode_char_counter` (int) <br>   `url_decode_char_replacer` (int)  | #13968 |  
| `text/subword/data_normalizer.cu` |   `kernel_data_normalizer` (uint32_t)  |  #13915  | 
| `text/subword/subword_tokenize.cu`  |  `kernel_compute_tensor_metadata` (uint32_t)  |  #13915 | 
| `text/subword/wordpiece_tokenizer.cu` |  `init_data_and_mark_word_start_and_ends` (uint32_t) <br>   `mark_string_start_and_ends` (uint32_t) <br>   `kernel_wordpiece_tokenizer` (uint32_t) | #13915  | 

### Part 2 - Take another pass over more challenging kernels


| Source file | Kernels | Status | 
|---|---|---|
| null_mash.cuh | [subtract_set_bits_range_boundaries_kernel](https://github.com/rapidsai/cudf/blob/b4da39cfbe569e290ae42ca9cf8ff868d5788757/cpp/include/cudf/detail/null_mask.cuh#L215) | |
| valid_if.cuh | [valid_if_n_kernel](https://github.com/rapidsai/cudf/blob/b4da39cfbe569e290ae42ca9cf8ff868d5788757/cpp/include/cudf/detail/valid_if.cuh#L154) | |
|copy_if_else.cuh | [copy_if_else_kernel](https://github.com/rapidsai/cudf/blob/b4da39cfbe569e290ae42ca9cf8ff868d5788757/cpp/include/cudf/detail/copy_if_else.cuh#L41) | |
| gather.cuh | [gather_chars_fn_string_parallel](https://github.com/rapidsai/cudf/blob/b4da39cfbe569e290ae42ca9cf8ff868d5788757/cpp/include/cudf/strings/detail/gather.cuh#L78) | |
| more? | search `gridDim.x` or `blockDim.x` to find more examples | | 




### Part 3 - Use [ranger](https://github.com/harrism/ranger) to prevent grid stride loop overflow
* incorporate the ranger header as a libcudf utility
* use ranger instead of manual indexing in libcudf kernels


### Additional information

There are also a number of kernels that have this pattern but probably don't ever overflow because they are indexing by bitmask words.  ([Example](https://github.com/rapidsai/cudf/blob/4c9ef5161268e2486938546deef00f7fc84c9a95/cpp/include/cudf/detail/copy_range.cuh#L41))
Additional, In this kernel, `source_idx` probably overflows, but harmlessly.

A snippet of code to see this in action:
```
size_type const size = 1200000000;
auto big = cudf::make_fixed_width_column(data_type{type_id::INT32}, size, mask_state::UNALLOCATED);  
auto x = cudf::rolling_window(*big, 1, 1, 1, cudf::detail::sum_aggregation{}); 
```

Note:  rmm may mask out of bounds accesses in some cases, so it's helpful to run with the plain cuda allocator.",2022-02-28T19:30:02Z,0,0,,,False
257,[FEA] Cleanup of unneeded functions in Python aggregation types.,"
With this PR  https://github.com/rapidsai/cudf/pull/10357 all aggregations in cudf are now represented as algorithm-specific aggregation classes.   There is some followup cleanup work to be done in the python bindings. See:

https://github.com/rapidsai/cudf/pull/10357#issuecomment-1054204554
",2022-03-07T16:28:51Z,0,0,,,False
258,[FEA] Support Correlation with Nullable DataFrames,"**Is your feature request related to a problem? Please describe.**
Today computing correlation for a dataframe with missing values is not possible, but pandas can do it:
```python
In [10]: gdf = cudf.DataFrame({""a"": [1, None, 1, 1, None, 2, 2], ""b"": range(7)})
In [11]: gdf.corr()
...
ValueError: Column must have no nulls.

In [14]: pdf = pd.DataFrame({""a"": [1, None, 1, 1, None, 2, 2], ""b"": range(7)})
In [15]: pdf.corr()
Out[15]: 
          a         b
a  1.000000  0.879427
b  0.879427  1.000000
```

**Describe the solution you'd like**
We should examine the techniques that pandas used here. In groupby cov/corr's case, we found that pandas uses pairwise deletion on missing values that may result in a non-PSD matrix. There's a test case to make sure cuDF matches with pandas in these situations. Here we should do the same.

**Describe alternatives you've considered**
There has been [discussions](https://github.com/rapidsai/cudf/pull/9889#discussion_r808439168) about merging groupby correlation/covariance with regular corr/cov so that they share the same algorithm backbone. But I don't think we need to achieve this in one big step. Extending existing function to support more inputs is a good start.
",2022-03-12T01:04:17Z,0,0,Michael Wang,Nvidia Rapids,True
259,[BUG] .to_parquet() and .to_csv() fails and get OOM with large DataFrames.,"**Describe the bug**
Writting large DataFrames to disk fails and gets memory error:
>>>df.to_parquet('myfile.parquet')
>>>RuntimeError: CUDA error at: /home/giba/anaconda3/envs/rapids-22.02/include/rmm/cuda_stream_view.hpp:81: cudaErrorIllegalAddress an illegal memory access was encountered

**Steps/Code to reproduce bug**
My DataFrame have shape: (414395052, 4)
dtypes: var0 int32, var1 int32, var2 int8, var3 int8
df.memory_usage().sum() returns: 4143950520  (4GB)

**Expected behavior**
File write to disk in .parquet or .csv format without issues.

**Environment overview (please complete the following information)**
Used conda with default RAPIDS 22.02 install.

**Environment details**
Using a 32GB V100 GPU.

**Additional context**
Sending cudf Dataframe to Pandas then calling .to_parquet() works:
>>> df.to_pandas().to_parquet('myfile.parquet')

",2022-03-13T14:00:51Z,0,0,Gilberto Titericz Junior,NVIDIA,True
260,[BUG] Count groupby-aggregations produce smaller dtype than expected,"**Describe the bug**
In pandas, calling `GroupBy.count ` appears to always produce an int64 column (regardless of whether it is actually necessary). Conversely cuDF outputs int32 columns.

**Steps/Code to reproduce bug**
```
>>> import cudf
>>> sr = cudf.Series([1, 1, 1, 2, 3])
>>> sr.value_counts()
1    3
2    1
3    1
dtype: int32
>>> sr.to_pandas().value_counts()
1    3
2    1
3    1
dtype: int64
```

**Expected behavior**
libcudf returning int32 columns from count aggregations makes sense since its column sizes are bound by `cudf::size_type = int32_t` anyway. However, cuDF Python should be converting the output to match the pandas convention, though.",2022-03-14T18:36:02Z,0,0,Vyas Ramasubramani,@rapidsai,True
261,[FEA] Refactors and next steps for segmented reductions,"This is issue contains a few proposals for improving the segmented reduction code introduced in #9621.

## Investigate sort-groupby aggregations
(Idea from @ttnghia)

With the ability to perform segmented reductions, sort-based groupby may be able to use [`group_offsets`](https://github.com/rapidsai/cudf/blob/57ff6f55b9fd44e8a8e10282d3f95d5f38e299ef/cpp/src/groupby/sort/sort_helper.cu#L188) to define its segments, rather than materializing a full column of sorted/monotonic [`group_labels`](https://github.com/rapidsai/cudf/blob/57ff6f55b9fd44e8a8e10282d3f95d5f38e299ef/cpp/src/groupby/sort/sort_helper.cu#L213). In effect, this allows us to replace a call to [`thrust::reduce_by_key`](https://github.com/rapidsai/cudf/blob/57ff6f55b9fd44e8a8e10282d3f95d5f38e299ef/cpp/src/groupby/sort/group_single_pass_reduction_util.cuh#L186) algorithm with a call to [`cub::DeviceSegmentedReduce::Reduce`](https://github.com/rapidsai/cudf/blob/7ff195677fc52ffc21e8c1060b0f270587d6995b/cpp/include/cudf/detail/reduction.cuh#L267), while eliminating the need to compute the `group_labels` column. I think this should be a more efficient algorithm, and also will require less intermediate memory allocation. Benchmarks should be performed when making this change.

## Refactor internal use of indices to 2N style (match CUB)

The indexing scheme used for segmented reduction is currently ""N+1"", like how list offsets are indexed. We want to refactor this to use ""2N"" indexing. This would align with `cub::DeviceSegmentedReduce::Reduce` and permit greater flexibility in the API internals. [See discussion here](https://github.com/rapidsai/cudf/pull/9621#discussion_r796250853) for details.

- @bdice: After discussion with @davidwendt, we decided this is not necessary in the short term. We can resolve this issue without changing the current implementation. The current implementation of N+1 aligns with segmented sort behavior. If there is a compelling need to change this in the future for expanded functionality, we can revisit.

## Compound reductions like mean

The segmented reduction code currently supports ""simple"" reductions. Support for ""compound"" reductions is needed. This includes multi-step calculations like mean, standard deviation, or sum of squares. Non-segmented compound reductions are defined here: https://github.com/rapidsai/cudf/blob/c1638869116aae2c6dde6024394279a2fb79e685/cpp/src/reductions/compound.cuh

## Fixes for output_type precision

@isVoid and I filed #9988 while working on #9621 because the documentation doesn't align with the implementation for when data is cast to the output dtype relative to when the reduction is performed. This affects segmented reduction as well.

## Explore rewriting `get_null_replacing_element_transformer` with nullate

It may be possible to clean up the [implementation of null element handling here](https://github.com/rapidsai/cudf/blob/7ff195677fc52ffc21e8c1060b0f270587d6995b/cpp/src/reductions/simple_segmented.cuh#L75-L85) by using [nullate](https://github.com/rapidsai/cudf/blob/c1638869116aae2c6dde6024394279a2fb79e685/cpp/include/cudf/column/column_device_view.cuh#L53).

- @bdice: After discussion with @davidwendt, we decided this is not worth changing. It might eliminate one or two lines of duplicate code but doesn't offer any benefits to compile time.

## Extend to more data types

We need to review the types supported by non-segmented reductions and ensure that segmented reductions support the same types. Decimal support has been requested here: https://github.com/rapidsai/cudf/issues/10417",2022-03-14T22:25:36Z,1,0,Bradley Dice,@NVIDIA @rapidsai,True
262,[BUG] Incorrect statistics for int96 timestamps in parquet,"**Describe the bug**
The statistics calculation is not working properly when dealing with int96 timestamp types. Adding statistics checks in `test_parquet_writer_int96_timestamps` will cause pytest failures.

**Steps/Code to reproduce bug**
At the end of `test_parquet_writer_int96_timestamps`, adding the following code:
```python
    # Read back from pyarrow
    pq_file = pq.ParquetFile(gdf_fname)
    # verify each row group's statistics
    for rg in range(0, pq_file.num_row_groups):
        pd_slice = pq_file.read_row_group(rg).to_pandas()

        # statistics are per-column. So need to verify independently
        for i, col in enumerate(pd_slice):
            stats = pq_file.metadata.row_group(rg).column(i).statistics

            actual_min = cudf.Series(pd_slice[col].explode().explode()).min()
            stats_min = stats.min
            assert normalized_equals(actual_min, stats_min)

            actual_max = cudf.Series(pd_slice[col].explode().explode()).max()
            stats_max = stats.max
            assert normalized_equals(actual_max, stats_max)
```
The statistics test will fail.

**Expected behavior**
The test should pass.

**Additional context**
Not sure if the issue is in libcudf or in the python layer. Adding cpp unit tests for parquet statistics can help track down the issue.",2022-03-15T17:44:55Z,0,0,Yunsong Wang,@NVIDIA @rapidsai,True
263,[FEA] Iterator versions of make_device_uvector_sync() and make_device_uvector_async(),"
Ran into some code in a PR that was staging results into a std::vector just so that it could be passed to a factory function.  Seems like it would be useful to have versions that take (device) iterators directly. ",2022-03-18T15:49:01Z,0,0,,,False
264,Remove partial support for duplicate MultIindex names unless they are all None,"Currently our MultiIndex class supports duplicate names, while DataFrames do not. The MultiIndex support is buggy, however, and we are frequently finding new edge cases that break it. Since pandas DataFrames do support duplicate names and [we explicitly choose not to](https://github.com/rapidsai/cudf/blob/branch-22.04/python/cudf/cudf/core/dataframe.py#L4377), I think it makes sense to do the same for MultiIndex. It improves our internal consistency and helps us write much more robust code. Making this change would probably fix a number of currently unknown/hidden bugs.

The major caveat here is that we do need to support MultiIndexes where all the names are None. However, handling this case would potentially be much simpler since we could use a sentinel or another class attribute to track whether names are meaningful or not. Default names could be integers, and any setting of names would require setting all column names to unique values.

An aside: If we ever did want to support duplicate names properly, it would involve a refactoring at the level of `ColumnAccessor`, which currently uses a dictionary as the underlying data structure to map names to columns. We would then need to update all of our functions that rely on `_from_data` to populate a new object that could support duplicate names rather than a dictionary. This is a substantial undertaking and out of scope for this issue.",2022-03-23T19:26:15Z,1,0,Vyas Ramasubramani,@rapidsai,True
265,Add `peak_memory_usage` to all nvbench benchmarks,"**Update (2023-12-05):**
Adding `peak_memory_usage` to nvbenchmarks can be accomplished with this pattern:
```
auto const mem_stats_logger = cudf::memory_stats_logger(); 

state.exec( ... );

state.add_buffer_size(
    mem_stats_logger.peak_memory_usage(), 
    ""peak_memory_usage"", 
    ""peak_memory_usage""
);
```
Please consult the cuIO benchmarks for how to add peak memory tracking. If we are tracking peak memory usage as well as bytes per second, then we can estimate memory footprint across the libcudf API.


**Original issue:**
#7770 added support for peak memory usage to cuIO benchmarks using rmm's `statistics_resource_adapter`. It would be nice to be able to expand that to all of our benchmarks so that we could more easily detect regressions in memory usage. This would be particularly useful for the Dask cuDF team, which is always looking to identify bottlenecks from memory usage. [There was already discussion](https://github.com/rapidsai/cudf/pull/7770#issuecomment-874473349) of doing this in #7770, so we should investigate following up now.",2022-03-28T23:45:36Z,0,0,Vyas Ramasubramani,@rapidsai,True
266,[BUG] Unexpected OOMs encountered with `read_csv` on WSL2,"**Describe the bug**
While testing the cuGraph's [UVM notebook](https://github.com/rapidsai/cugraph/blob/branch-22.04/notebooks/demo/uvm.ipynb), I encountered an OOM error when trying to read in a large (~26 GB) CSV dataset on an RTX 8000 48GB.

**Steps/Code to reproduce bug**
Sorry for the the lengthy reproducer - happy to switch over a more readily available large dataset if possible:

```python
import os
import urllib.request

import cudf
import rmm

rmm.mr.set_current_device_resource(rmm.mr.ManagedMemoryResource())

data_dir = './data/'
if not os.path.exists(data_dir):
    print('creating data directory')
    os.system('mkdir ./data')

# download the Twitter dataset
base_url = 'https://s3.us-east-2.amazonaws.com/rapidsai-data/cugraph/benchmark/'
fn = 'twitter-2010.csv'
comp = '.gz'
if not os.path.isfile(data_dir+fn):
    if not os.path.isfile(data_dir+fn+comp):
        print(f'Downloading {base_url+fn+comp} to {data_dir+fn+comp}')
        urllib.request.urlretrieve(base_url+fn+comp, data_dir+fn+comp)
    print(f'Decompressing {data_dir+fn+comp}...')
    os.system('gunzip '+data_dir+fn+comp)
    print(f'{data_dir+fn+comp} decompressed!')
else:
    print(f'Your data file, {data_dir+fn}, already exists')

# File path, assuming Notebook directory
input_data_path = data_dir+fn

e_list = cudf.read_csv(input_data_path, delimiter=' ', names=['src', 'dst'], dtype=['int32', 'int32'])
```

The above fails unless `nrows` is set to something under ~100,000,000:

```
MemoryError                               Traceback (most recent call last)
/tmp/ipykernel_880/4119338851.py in <module>
      3 
      4 # CSV reader
----> 5 e_list = cudf.read_csv(input_data_path, delimiter=' ', names=['src', 'dst'], dtype=['int32', 'int32'])
      6 
      7 # Print time

/opt/conda/envs/rapids/lib/python3.9/contextlib.py in inner(*args, **kwds)
     77         def inner(*args, **kwds):
     78             with self._recreate_cm():
---> 79                 return func(*args, **kwds)
     80         return inner
     81 

/opt/conda/envs/rapids/lib/python3.9/site-packages/cudf/io/csv.py in read_csv(filepath_or_buffer, lineterminator, quotechar, quoting, doublequote, header, mangle_dupe_cols, usecols, sep, delimiter, delim_whitespace, skipinitialspace, names, dtype, skipfooter, skiprows, dayfirst, compression, thousands, decimal, true_values, false_values, nrows, byte_range, skip_blank_lines, parse_dates, comment, na_values, keep_default_na, na_filter, prefix, index_col, use_python_file_object, **kwargs)
     71         na_values = [na_values]
     72 
---> 73     return libcudf.csv.read_csv(
     74         filepath_or_buffer,
     75         lineterminator=lineterminator,

cudf/_lib/csv.pyx in cudf._lib.csv.read_csv()

MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /rapids/rmm/include/rmm/mr/device/managed_memory_resource.hpp:74: cudaErrorMemoryAllocation out of memory
```

**Expected behavior**
I would expect the CSV dataset to be read in its entirety - the notebook and this code succeed on a standard Linux machine with a V100 32GB (DGX1).

**Environment overview (please complete the following information)**
 - Environment location: Docker
 - Method of cuDF install: Docker

```
docker run --gpus all -p 8888:8888 -p 8787:8787 -p 8786:8786 rapidsai/rapidsai-core-dev-nightly:22.04-cuda11.5-devel-ubuntu20.04-py3.9
```

**Environment details**
<details><summary>Click here to see environment details</summary><pre>
     
     **git***
print_env.sh: 10: [: true: unexpected operator
     Not inside a git repository
     
     ***OS Information***
     DISTRIB_ID=Ubuntu
     DISTRIB_RELEASE=20.04
     DISTRIB_CODENAME=focal
     DISTRIB_DESCRIPTION=""Ubuntu 20.04.4 LTS""
     NAME=""Ubuntu""
     VERSION=""20.04.4 LTS (Focal Fossa)""
     ID=ubuntu
     ID_LIKE=debian
     PRETTY_NAME=""Ubuntu 20.04.4 LTS""
     VERSION_ID=""20.04""
     HOME_URL=""https://www.ubuntu.com/""
     SUPPORT_URL=""https://help.ubuntu.com/""
     BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
     PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
     VERSION_CODENAME=focal
     UBUNTU_CODENAME=focal
     Linux 4ab1a0161978 5.10.102.1-microsoft-standard-WSL2 #1 SMP Wed Mar 2 00:30:59 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
     
     ***GPU Information***
     Thu Mar 31 14:45:02 2022
     +-----------------------------------------------------------------------------+
     | NVIDIA-SMI 510.47.03    Driver Version: 511.65       CUDA Version: 11.6     |
     |-------------------------------+----------------------+----------------------+
     | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
     | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
     |                               |                      |               MIG M. |
     |===============================+======================+======================|
     |   0  Quadro RTX 8000     On   | 00000000:15:00.0 Off |                  Off |
     | 34%   32C    P8    17W / 260W |    823MiB / 49152MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   1  Quadro RTX 8000     On   | 00000000:2D:00.0  On |                  Off |
     | 35%   61C    P0    72W / 260W |   1906MiB / 49152MiB |     15%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     
     +-----------------------------------------------------------------------------+
     | Processes:                                                                  |
     |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
     |        ID   ID                                                   Usage      |
     |=============================================================================|
     |    0   N/A  N/A       804      C   /python3.9                      N/A      |
     |    1   N/A  N/A       804      C   /python3.9                      N/A      |
     +-----------------------------------------------------------------------------+
     
     ***CPU***
     Architecture:                    x86_64
     CPU op-mode(s):                  32-bit, 64-bit
     Byte Order:                      Little Endian
     Address sizes:                   46 bits physical, 48 bits virtual
     CPU(s):                          12
     On-line CPU(s) list:             0-11
     Thread(s) per core:              2
     Core(s) per socket:              6
     Socket(s):                       1
     Vendor ID:                       GenuineIntel
     CPU family:                      6
     Model:                           85
     Model name:                      Intel(R) Xeon(R) Gold 6128 CPU @ 3.40GHz
     Stepping:                        4
     CPU MHz:                         3391.499
     BogoMIPS:                        6782.99
     Virtualization:                  VT-x
     Hypervisor vendor:               Microsoft
     Virtualization type:             full
     L1d cache:                       192 KiB
     L1i cache:                       192 KiB
     L2 cache:                        6 MiB
     L3 cache:                        19.3 MiB
     Vulnerability Itlb multihit:     KVM: Mitigation: VMX disabled
     Vulnerability L1tf:              Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable
     Vulnerability Mds:               Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown
     Vulnerability Meltdown:          Mitigation; PTI
     Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp
     Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
     Vulnerability Spectre v2:        Mitigation; Full generic retpoline, IBPB conditional, IBRS_FW, STIBP conditional, RSB filling
     Vulnerability Srbds:             Not affected
     Vulnerability Tsx async abort:   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown
     Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq vmx ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves flush_l1d arch_capabilities
     
     ***CMake***
     /opt/conda/envs/rapids/bin/cmake
     cmake version 3.20.5
     
     CMake suite maintained and supported by Kitware (kitware.com/cmake).
     
     ***g++***
     /usr/bin/g++
     g++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
     Copyright (C) 2019 Free Software Foundation, Inc.
     This is free software; see the source for copying conditions.  There is NO
     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
     
     
     ***nvcc***
     /usr/local/cuda/bin/nvcc
     nvcc: NVIDIA (R) Cuda compiler driver
     Copyright (c) 2005-2021 NVIDIA Corporation
     Built on Thu_Nov_18_09:45:30_PST_2021
     Cuda compilation tools, release 11.5, V11.5.119
     Build cuda_11.5.r11.5/compiler.30672275_0
     
     ***Python***
     /opt/conda/envs/rapids/bin/python
     Python 3.9.12
     
     ***Environment Variables***
     PATH                            : /opt/conda/envs/rapids/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
     LD_LIBRARY_PATH                 : /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/lib:/opt/conda/envs/rapids/lib
     NUMBAPRO_NVVM                   :
     NUMBAPRO_LIBDEVICE              :
     CONDA_PREFIX                    : /opt/conda/envs/rapids
     PYTHON_PATH                     :
     
     ***conda packages***
     conda is /opt/conda/envs/rapids/bin/conda
     /opt/conda/envs/rapids/bin/conda
     # packages in environment at /opt/conda/envs/rapids:
     #
     # Name                    Version                   Build  Channel
     _libgcc_mutex             0.1                 conda_forge    conda-forge
     _openmp_mutex             4.5                       1_gnu    conda-forge
     abseil-cpp                20211102.0           h27087fc_1    conda-forge
     aiobotocore               2.1.0              pyhd8ed1ab_0    conda-forge
     aiohttp                   3.8.1            py39h3811e60_0    conda-forge
     aioitertools              0.10.0             pyhd8ed1ab_0    conda-forge
     aiosignal                 1.2.0              pyhd8ed1ab_0    conda-forge
     alabaster                 0.7.12                     py_0    conda-forge
     anyio                     3.5.0            py39hf3d152e_0    conda-forge
     appdirs                   1.4.4              pyh9f0ad1d_0    conda-forge
     argon2-cffi               21.3.0             pyhd8ed1ab_0    conda-forge
     argon2-cffi-bindings      21.2.0           py39h3811e60_1    conda-forge
     arrow-cpp                 6.0.1           py39hbfed05e_11_cuda    conda-forge
     arrow-cpp-proc            3.0.0                      cuda    conda-forge
     asn1crypto                1.5.1              pyhd8ed1ab_0    conda-forge
     asvdb                     0.4.2               g90e8f2c_40    rapidsai-nightly
     async-timeout             4.0.2              pyhd8ed1ab_0    conda-forge
     atk-1.0                   2.36.0               h3371d22_4    conda-forge
     attrs                     21.4.0             pyhd8ed1ab_0    conda-forge
     autoconf                  2.69            pl5321hd708f79_11    conda-forge
     automake                  1.16.5          pl5321ha770c72_0    conda-forge
     aws-c-cal                 0.5.11               h95a6274_0    conda-forge
     aws-c-common              0.6.2                h7f98852_0    conda-forge
     aws-c-event-stream        0.2.7               h3541f99_13    conda-forge
     aws-c-io                  0.10.5               hfb6a706_0    conda-forge
     aws-checksums             0.1.11               ha31a3da_7    conda-forge
     aws-sam-translator        1.43.0             pyhd8ed1ab_0    conda-forge
     aws-sdk-cpp               1.8.186              hb4091e7_3    conda-forge
     aws-xray-sdk              2.9.0              pyhd8ed1ab_0    conda-forge
     babel                     2.9.1              pyh44b312d_0    conda-forge
     backcall                  0.2.0              pyh9f0ad1d_0    conda-forge
     backports                 1.0                        py_2    conda-forge
     backports.functools_lru_cache 1.6.4              pyhd8ed1ab_0    conda-forge
     beautifulsoup4            4.10.0             pyha770c72_0    conda-forge
     benchmark                 1.5.1                he1b5a44_2    conda-forge
     binutils_impl_linux-64    2.36.1               h193b22a_2    conda-forge
     black                     19.10b0                    py_4    conda-forge
     blas                      2.105                    netlib    conda-forge
     blas-devel                3.9.0                  5_netlib    conda-forge
     bleach                    4.1.0              pyhd8ed1ab_0    conda-forge
     blinker                   1.4                        py_1    conda-forge
     blosc                     1.21.0               h9c3ff4c_0    conda-forge
     bokeh                     2.4.2            py39hf3d152e_0    conda-forge
     boost                     1.72.0           py39ha90915f_1    conda-forge
     boost-cpp                 1.72.0               h312852a_5    conda-forge
     boto3                     1.20.24            pyhd8ed1ab_0    conda-forge
     botocore                  1.23.24            pyhd8ed1ab_0    conda-forge
     breathe                   4.33.0             pyhd8ed1ab_0    conda-forge
     brotli                    1.0.9                h7f98852_6    conda-forge
     brotli-bin                1.0.9                h7f98852_6    conda-forge
     brotlipy                  0.7.0           py39h3811e60_1003    conda-forge
     brunsli                   0.1                  h9c3ff4c_0    conda-forge
     bzip2                     1.0.8                h7f98852_4    conda-forge
     c-ares                    1.18.1               h7f98852_0    conda-forge
     c-blosc2                  2.0.4                h5f21a17_1    conda-forge
     ca-certificates           2021.10.8            ha878542_0    conda-forge
     cached-property           1.5.2                hd8ed1ab_1    conda-forge
     cached_property           1.5.2              pyha770c72_1    conda-forge
     cachetools                5.0.0              pyhd8ed1ab_0    conda-forge
     cairo                     1.16.0            h6cf1ce9_1008    conda-forge
     certifi                   2021.10.8        py39hf3d152e_1    conda-forge
     cffi                      1.15.0           py39h4bc2ebd_0    conda-forge
     cfgv                      3.3.1              pyhd8ed1ab_0    conda-forge
     cfitsio                   3.470                hb418390_7    conda-forge
     cfn-lint                  0.54.2           py39hf3d152e_0    conda-forge
     chardet                   4.0.0            py39hf3d152e_2    conda-forge
     charls                    2.2.0                h9c3ff4c_0    conda-forge
     charset-normalizer        2.0.12             pyhd8ed1ab_0    conda-forge
     clang                     11.1.0               ha770c72_1    conda-forge
     clang-11                  11.1.0          default_ha53f305_1    conda-forge
     clang-tools               11.1.0          default_ha53f305_1    conda-forge
     clangxx                   11.1.0          default_ha53f305_1    conda-forge
     click                     7.1.2              pyh9f0ad1d_0    conda-forge
     click-plugins             1.1.1                      py_0    conda-forge
     cligj                     0.7.2              pyhd8ed1ab_1    conda-forge
     cloudpickle               2.0.0              pyhd8ed1ab_0    conda-forge
     cmake                     3.20.5               h8897547_0    conda-forge
     cmake-format              0.6.11             pyh9f0ad1d_0    conda-forge
     cmake_setuptools          0.1.3                      py_0    rapidsai-nightly
     cmarkgfm                  0.8.0            py39hb9d737c_0    conda-forge
     colorama                  0.4.4              pyh9f0ad1d_0    conda-forge
     colorcet                  3.0.0              pyhd8ed1ab_0    conda-forge
     commonmark                0.9.1                      py_0    conda-forge
     conda                     4.10.3           py39hf3d152e_4    conda-forge
     conda-build               3.21.7           py39hf3d152e_0    conda-forge
     conda-package-handling    1.8.0            py39hb9d737c_0    conda-forge
     conda-verify              3.1.1           py39hf3d152e_1004    conda-forge
     coverage                  6.3.2            py39hb9d737c_1    conda-forge
     cryptography              36.0.2           py39hd97740a_0    conda-forge
     cuda-python               11.6.1           py39h3fd9d12_0    nvidia
     cudatoolkit               11.5.1               hcf5317a_9    nvidia
     cudf                      0+untagged.1.gc71fe1b          pypi_0    pypi
     cudf-kafka                0+untagged.1.gc71fe1b          pypi_0    pypi
     cugraph                   0+untagged.1.g87be0b3          pypi_0    pypi
     cuml                      0+untagged.1.g3798925          pypi_0    pypi
     cupy                      10.2.0           py39hc3c280e_0    conda-forge
     curl                      7.82.0               h7bff187_0    conda-forge
     cusignal                  0+untagged.1.g8878bf7          pypi_0    pypi
     cuspatial                 0+untagged.1.g3637da5          pypi_0    pypi
     cuxfilter                 0+untagged.1.g53c9564          pypi_0    pypi
     cycler                    0.11.0             pyhd8ed1ab_0    conda-forge
     cyrus-sasl                2.1.27               h230043b_5    conda-forge
     cython                    0.29.28          py39he80948d_0    conda-forge
     cytoolz                   0.11.2           py39h3811e60_1    conda-forge
     dask                      2022.3.0           pyhd8ed1ab_0    conda-forge
     dask-core                 2022.3.0           pyhd8ed1ab_0    conda-forge
     dask-cudf                 0+untagged.1.gc71fe1b          pypi_0    pypi
     dask-glm                  0.2.0                      py_1    conda-forge
     dask-labextension         5.2.0              pyhd8ed1ab_0    conda-forge
     dask-ml                   1.9.0              pyhd8ed1ab_0    conda-forge
     dataclasses               0.8                pyhc8e2a94_3    conda-forge
     datashader                0.13.0             pyh6c4a22f_0    conda-forge
     datashape                 0.5.4                      py_1    conda-forge
     dbus                      1.13.6               h5008d03_3    conda-forge
     debugpy                   1.5.1            py39he80948d_0    conda-forge
     decorator                 5.1.1              pyhd8ed1ab_0    conda-forge
     defusedxml                0.7.1              pyhd8ed1ab_0    conda-forge
     distlib                   0.3.4              pyhd8ed1ab_0    conda-forge
     distributed               2022.3.0           pyhd8ed1ab_0    conda-forge
     distro                    1.6.0              pyhd8ed1ab_0    conda-forge
     dlpack                    0.5                  h9c3ff4c_0    conda-forge
     docker-py                 5.0.3            py39hf3d152e_2    conda-forge
     docker-pycreds            0.4.0                      py_0    conda-forge
     docutils                  0.17.1           py39hf3d152e_1    conda-forge
     double-conversion         3.1.5                h9c3ff4c_2    conda-forge
     doxygen                   1.8.20               had0d8f1_0    conda-forge
     ecdsa                     0.17.0             pyhd8ed1ab_0    conda-forge
     entrypoints               0.4                pyhd8ed1ab_0    conda-forge
     execnet                   1.9.0              pyhd8ed1ab_0    conda-forge
     expat                     2.4.7                h27087fc_0    conda-forge
     faiss-proc                1.0.0                      cuda    rapidsai
     fastavro                  1.4.10           py39hb9d737c_0    conda-forge
     fastrlock                 0.8              py39he80948d_1    conda-forge
     feather-format            0.4.1              pyh9f0ad1d_0    conda-forge
     filelock                  3.6.0              pyhd8ed1ab_0    conda-forge
     filterpy                  1.4.5                      py_1    conda-forge
     fiona                     1.8.20           py39h427c1bf_1    conda-forge
     flake8                    3.8.4                      py_0    conda-forge
     flask                     2.0.3              pyhd8ed1ab_0    conda-forge
     flask_cors                3.0.10             pyhd3deb0d_0    conda-forge
     flit-core                 3.7.1              pyhd8ed1ab_0    conda-forge
     font-ttf-dejavu-sans-mono 2.37                 hab24e00_0    conda-forge
     font-ttf-inconsolata      3.000                h77eed37_0    conda-forge
     font-ttf-source-code-pro  2.038                h77eed37_0    conda-forge
     font-ttf-ubuntu           0.83                 hab24e00_0    conda-forge
     fontconfig                2.13.96              h8e229c2_2    conda-forge
     fonts-conda-ecosystem     1                             0    conda-forge
     fonts-conda-forge         1                             0    conda-forge
     fonttools                 4.31.2           py39hb9d737c_0    conda-forge
     freetype                  2.10.4               h0708190_1    conda-forge
     freexl                    1.0.6                h7f98852_0    conda-forge
     fribidi                   1.0.10               h36c2ea0_0    conda-forge
     frozenlist                1.3.0            py39h3811e60_0    conda-forge
     fsspec                    2022.2.0           pyhd8ed1ab_0    conda-forge
     future                    0.18.2           py39hf3d152e_4    conda-forge
     gcc_impl_linux-64         11.2.0              h82a94d6_14    conda-forge
     gcsfs                     2022.2.0           pyhd8ed1ab_0    conda-forge
     gdal                      3.3.1            py39h218ed2d_3    conda-forge
     gdk-pixbuf                2.42.8               hff1cb4f_0    conda-forge
     geopandas                 0.9.0              pyhd8ed1ab_1    conda-forge
     geopandas-base            0.9.0              pyhd8ed1ab_1    conda-forge
     geos                      3.9.1                h9c3ff4c_2    conda-forge
     geotiff                   1.6.0                h4f31c25_6    conda-forge
     gettext                   0.19.8.1          h73d1719_1008    conda-forge
     gflags                    2.2.2             he1b5a44_1004    conda-forge
     giflib                    5.2.1                h36c2ea0_2    conda-forge
     git                       2.35.1          pl5321h36853c3_0    conda-forge
     git-lfs                   3.1.2                ha770c72_0    conda-forge
     glib                      2.70.2               h780b84a_4    conda-forge
     glib-tools                2.70.2               h780b84a_4    conda-forge
     glob2                     0.7                        py_0    conda-forge
     glog                      0.5.0                h48cff8f_0    conda-forge
     gmock                     1.10.0               h4bd325d_7    conda-forge
     gmp                       6.2.1                h58526e2_0    conda-forge
     google-api-core           2.5.0              pyhd8ed1ab_0    conda-forge
     google-auth               2.6.2              pyh6c4a22f_0    conda-forge
     google-auth-oauthlib      0.5.1              pyhd8ed1ab_0    conda-forge
     google-cloud-core         2.2.2              pyh6c4a22f_0    conda-forge
     google-cloud-storage      2.1.0              pyh6c4a22f_0    conda-forge
     google-crc32c             1.1.2            py39hb81f231_2    conda-forge
     google-resumable-media    2.1.0              pyh6c4a22f_0    conda-forge
     googleapis-common-protos  1.56.0           py39hf3d152e_0    conda-forge
     gpuci-tools               0.3.1                        12    gpuci
     graphite2                 1.3.13            h58526e2_1001    conda-forge
     graphql-core              3.2.0              pyhd8ed1ab_0    conda-forge
     graphviz                  2.50.0               h85b4f2f_1    conda-forge
     grpc-cpp                  1.44.0               h3d78c48_1    conda-forge
     grpcio                    1.45.0           py39h0f497a6_0    conda-forge
     gtest                     1.10.0               h4bd325d_7    conda-forge
     gtk2                      2.24.33              h539f30e_1    conda-forge
     gts                       0.7.6                h64030ff_2    conda-forge
     h5py                      3.6.0           nompi_py39h7e08c79_100    conda-forge
     harfbuzz                  3.1.1                h83ec7ef_0    conda-forge
     hdbscan                   0.8.28           py39hce5d2b2_1    conda-forge
     hdf4                      4.2.15               h10796ff_3    conda-forge
     hdf5                      1.12.1          nompi_h2386368_104    conda-forge
     heapdict                  1.0.1                      py_0    conda-forge
     holoviews                 1.14.6             pyhd8ed1ab_0    conda-forge
     html5lib                  1.1                pyh9f0ad1d_0    conda-forge
     httpretty                 1.1.4              pyhd8ed1ab_0    conda-forge
     huggingface_hub           0.4.0              pyhd8ed1ab_0    conda-forge
     hypothesis                6.39.5             pyhd8ed1ab_0    conda-forge
     icu                       68.2                 h9c3ff4c_0    conda-forge
     identify                  2.4.12             pyhd8ed1ab_0    conda-forge
     idna                      3.3                pyhd8ed1ab_0    conda-forge
     imagecodecs               2021.8.26        py39h44211f0_1    conda-forge
     imageio                   2.16.1             pyhcf75d05_0    conda-forge
     imagesize                 1.3.0              pyhd8ed1ab_0    conda-forge
     importlib-metadata        4.11.3           py39hf3d152e_0    conda-forge
     importlib_metadata        4.11.3               hd8ed1ab_0    conda-forge
     iniconfig                 1.1.1              pyh9f0ad1d_0    conda-forge
     ipykernel                 6.9.2            py39hef51801_0    conda-forge
     ipython                   7.31.1           py39hf3d152e_0    conda-forge
     ipython_genutils          0.2.0                      py_1    conda-forge
     ipywidgets                7.7.0              pyhd8ed1ab_0    conda-forge
     isort                     5.6.4                      py_0    conda-forge
     itsdangerous              2.1.2              pyhd8ed1ab_0    conda-forge
     jbig                      2.1               h7f98852_2003    conda-forge
     jedi                      0.18.1           py39hf3d152e_0    conda-forge
     jeepney                   0.7.1              pyhd8ed1ab_0    conda-forge
     jinja2                    3.1.1              pyhd8ed1ab_0    conda-forge
     jmespath                  0.10.0             pyh9f0ad1d_0    conda-forge
     joblib                    1.1.0              pyhd8ed1ab_0    conda-forge
     jpeg                      9e                   h7f98852_0    conda-forge
     json-c                    0.15                 h98cffda_0    conda-forge
     json5                     0.9.5              pyh9f0ad1d_0    conda-forge
     jsondiff                  1.3.1              pyhd8ed1ab_0    conda-forge
     jsonpatch                 1.32               pyhd8ed1ab_0    conda-forge
     jsonpointer               2.0                        py_0    conda-forge
     jsonschema                3.2.0              pyhd8ed1ab_3    conda-forge
     junit-xml                 1.9                pyh9f0ad1d_0    conda-forge
     jupyter-packaging         0.7.12             pyhd8ed1ab_0    conda-forge
     jupyter-server-proxy      3.2.1              pyhd8ed1ab_0    conda-forge
     jupyter_client            7.1.2              pyhd8ed1ab_0    conda-forge
     jupyter_core              4.9.2            py39hf3d152e_0    conda-forge
     jupyter_server            1.15.6             pyhd8ed1ab_1    conda-forge
     jupyter_sphinx            0.3.2            py39hf3d152e_0    conda-forge
     jupyterlab                3.3.2              pyhd8ed1ab_0    conda-forge
     jupyterlab-favorites      3.0.0              pyhd8ed1ab_0    conda-forge
     jupyterlab-nvdashboard    0.7.0a220307              py_21    rapidsai-nightly
     jupyterlab_pygments       0.1.2              pyh9f0ad1d_0    conda-forge
     jupyterlab_server         2.11.0             pyhd8ed1ab_0    conda-forge
     jupyterlab_widgets        1.1.0              pyhd8ed1ab_0    conda-forge
     jxrlib                    1.1                  h7f98852_2    conda-forge
     kealib                    1.4.14               h87e4c3c_3    conda-forge
     kernel-headers_linux-64   2.6.32              he073ed8_15    conda-forge
     keyring                   23.4.0           py39hf3d152e_2    conda-forge
     keyutils                  1.6.1                h166bdaf_0    conda-forge
     kiwisolver                1.4.0            py39hf939315_0    conda-forge
     krb5                      1.19.3               h3790be6_0    conda-forge
     lapack                    3.9.0                    netlib    conda-forge
     lcms2                     2.12                 hddcbb42_0    conda-forge
     ld_impl_linux-64          2.36.1               hea4e1c9_2    conda-forge
     lerc                      3.0                  h9c3ff4c_0    conda-forge
     libaec                    1.0.6                h9c3ff4c_0    conda-forge
     libarchive                3.5.2                hccf745f_1    conda-forge
     libblas                   3.9.0           5_h92ddd45_netlib    conda-forge
     libbrotlicommon           1.0.9                h7f98852_6    conda-forge
     libbrotlidec              1.0.9                h7f98852_6    conda-forge
     libbrotlienc              1.0.9                h7f98852_6    conda-forge
     libcblas                  3.9.0           5_h92ddd45_netlib    conda-forge
     libclang-cpp11.1          11.1.0          default_ha53f305_1    conda-forge
     libcrc32c                 1.1.2                h9c3ff4c_0    conda-forge
     libcugraphops             22.04.00a220329 cuda11_g91ab46e_30    rapidsai-nightly
     libcumlprims              22.04.00a220324 cuda11_g99e8d8f_15    rapidsai-nightly
     libcurl                   7.82.0               h7bff187_0    conda-forge
     libcusolver               11.3.4.124           h33c3c4e_0    nvidia
     libcypher-parser          0.6.2                         1    rapidsai-nightly
     libdap4                   3.20.6               hd7c4107_2    conda-forge
     libdeflate                1.8                  h7f98852_0    conda-forge
     libedit                   3.1.20191231         he28a2e2_2    conda-forge
     libev                     4.33                 h516909a_1    conda-forge
     libevent                  2.1.10               h9b69904_4    conda-forge
     libfaiss                  1.7.0           cuda112h5bea7ad_8_cuda    conda-forge
     libffi                    3.4.2                h7f98852_5    conda-forge
     libgcc-devel_linux-64     11.2.0              h0952999_14    conda-forge
     libgcc-ng                 11.2.0              h1d223b6_14    conda-forge
     libgcrypt                 1.10.0               h7f98852_0    conda-forge
     libgd                     2.3.3                h6ad9fb6_0    conda-forge
     libgdal                   3.3.1                h6214c1d_3    conda-forge
     libgfortran-ng            11.2.0              h69a702a_14    conda-forge
     libgfortran5              11.2.0              h5c6108e_14    conda-forge
     libglib                   2.70.2               h174f98d_4    conda-forge
     libgomp                   11.2.0              h1d223b6_14    conda-forge
     libgpg-error              1.44                 h9eb791d_0    conda-forge
     libgsasl                  1.10.0               h5b4c23d_0    conda-forge
     libhwloc                  2.3.0                h5e5b7d1_1    conda-forge
     libiconv                  1.16                 h516909a_0    conda-forge
     libkml                    1.3.0             hd79254b_1012    conda-forge
     liblapack                 3.9.0           5_h92ddd45_netlib    conda-forge
     liblapacke                3.9.0           5_h92ddd45_netlib    conda-forge
     liblief                   0.11.5               h9c3ff4c_1    conda-forge
     libllvm11                 11.1.0               hf817b99_3    conda-forge
     libnetcdf                 4.8.1           nompi_hb3fd0d9_101    conda-forge
     libnghttp2                1.47.0               h727a467_0    conda-forge
     libnsl                    2.0.0                h7f98852_0    conda-forge
     libntlm                   1.4               h7f98852_1002    conda-forge
     libpng                    1.6.37               h21135ba_2    conda-forge
     libpq                     13.5                 hd57d9b9_1    conda-forge
     libprotobuf               3.19.4               h780b84a_0    conda-forge
     librdkafka                1.7.0                hc49e61c_1    conda-forge
     librmm                    22.04.00a220329 cuda11_g220ba88_44    rapidsai-nightly
     librsvg                   2.52.5               hc3c00ef_1    conda-forge
     librttopo                 1.1.0                h1185371_6    conda-forge
     libsanitizer              11.2.0              he4da1e4_14    conda-forge
     libsodium                 1.0.18               h36c2ea0_1    conda-forge
     libspatialindex           1.9.3                h9c3ff4c_4    conda-forge
     libspatialite             5.0.1                h8694cbe_6    conda-forge
     libssh2                   1.10.0               ha56f1ee_2    conda-forge
     libstdcxx-ng              11.2.0              he4da1e4_14    conda-forge
     libthrift                 0.15.0               he6d91bd_1    conda-forge
     libtiff                   4.3.0                h6f004c6_2    conda-forge
     libtmglib                 3.9.0           5_h92ddd45_netlib    conda-forge
     libtool                   2.4.6             h9c3ff4c_1008    conda-forge
     libutf8proc               2.7.0                h7f98852_0    conda-forge
     libuuid                   2.32.1            h7f98852_1000    conda-forge
     libuv                     1.43.0               h7f98852_0    conda-forge
     libwebp                   1.2.2                h3452ae3_0    conda-forge
     libwebp-base              1.2.2                h7f98852_1    conda-forge
     libxcb                    1.13              h7f98852_1004    conda-forge
     libxml2                   2.9.12               h72842e0_0    conda-forge
     libzip                    1.8.0                h4de3113_1    conda-forge
     libzlib                   1.2.11            h166bdaf_1014    conda-forge
     libzopfli                 1.0.3                h9c3ff4c_0    conda-forge
     lightgbm                  3.3.2            py39he80948d_0    conda-forge
     llvmlite                  0.38.0           py39h1bbdace_0    conda-forge
     locket                    0.2.1                    pypi_0    pypi
     lz4-c                     1.9.3                h9c3ff4c_1    conda-forge
     lzo                       2.10              h516909a_1000    conda-forge
     m4                        1.4.18            h516909a_1001    conda-forge
     make                      4.3                  hd18ef5c_1    conda-forge
     mapclassify               2.4.3              pyhd8ed1ab_0    conda-forge
     markdown                  3.3.6              pyhd8ed1ab_0    conda-forge
     markupsafe                2.1.1            py39hb9d737c_0    conda-forge
     matplotlib-base           3.5.1            py39h2fa2bec_0    conda-forge
     matplotlib-inline         0.1.3              pyhd8ed1ab_0    conda-forge
     mccabe                    0.6.1                      py_1    conda-forge
     mimesis                   4.0.0              pyh9f0ad1d_0    conda-forge
     mistune                   0.8.4           py39h3811e60_1005    conda-forge
     mock                      4.0.3            py39hf3d152e_2    conda-forge
     moto                      3.1.2              pyhd8ed1ab_0    conda-forge
     msgpack-python            1.0.3            py39h1a9c180_0    conda-forge
     multidict                 6.0.2            py39h3811e60_0    conda-forge
     multipledispatch          0.6.0                      py_0    conda-forge
     munch                     2.5.0                      py_0    conda-forge
     munkres                   1.1.4              pyh9f0ad1d_0    conda-forge
     mypy                      0.782                      py_0    conda-forge
     mypy_extensions           0.4.3            py39hf3d152e_4    conda-forge
     nbclassic                 0.3.7              pyhd8ed1ab_0    conda-forge
     nbclient                  0.5.13             pyhd8ed1ab_0    conda-forge
     nbconvert                 6.4.4            py39hf3d152e_0    conda-forge
     nbformat                  5.2.0              pyhd8ed1ab_0    conda-forge
     nbsphinx                  0.8.8              pyhd8ed1ab_0    conda-forge
     nccl                      2.12.7.1             h0800d71_0    conda-forge
     ncurses                   6.3                  h9c3ff4c_0    conda-forge
     nest-asyncio              1.5.4              pyhd8ed1ab_0    conda-forge
     networkx                  2.6.3              pyhd8ed1ab_1    conda-forge
     ninja                     1.10.2               h4bd325d_1    conda-forge
     nltk                      3.6.7              pyhd8ed1ab_0    conda-forge
     nodeenv                   1.6.0              pyhd8ed1ab_0    conda-forge
     nodejs                    14.18.3              h92b4a50_1    conda-forge
     notebook                  6.4.10             pyha770c72_0    conda-forge
     notebook-shim             0.1.0              pyhd8ed1ab_0    conda-forge
     numba                     0.55.1           py39h56b8d98_0    conda-forge
     numpy                     1.21.5           py39haac66dc_0    conda-forge
     numpydoc                  1.2                pyhd8ed1ab_0    conda-forge
     nvtx                      0.2.3            py39h3811e60_1    conda-forge
     oauthlib                  3.2.0              pyhd8ed1ab_0    conda-forge
     openjpeg                  2.4.0                hb52868f_1    conda-forge
     openslide                 3.4.1                h978ee9a_4    conda-forge
     openssl                   1.1.1n               h166bdaf_0    conda-forge
     orc                       1.7.3                h1be678f_0    conda-forge
     packaging                 21.3               pyhd8ed1ab_0    conda-forge
     pandas                    1.3.5            py39hde0f152_0    conda-forge
     pandoc                    1.19.2                        0    conda-forge
     pandocfilters             1.5.0              pyhd8ed1ab_0    conda-forge
     panel                     0.12.6             pyhd8ed1ab_0    conda-forge
     pango                     1.48.10              h54213e6_2    conda-forge
     param                     1.12.0             pyh6c4a22f_0    conda-forge
     parquet-cpp               1.5.1                         2    conda-forge
     parso                     0.8.3              pyhd8ed1ab_0    conda-forge
     partd                     1.2.0              pyhd8ed1ab_0    conda-forge
     patchelf                  0.14.5               h58526e2_0    conda-forge
     pathspec                  0.9.0              pyhd8ed1ab_0    conda-forge
     patsy                     0.5.2              pyhd8ed1ab_0    conda-forge
     pcre                      8.45                 h9c3ff4c_0    conda-forge
     pcre2                     10.37                h032f7d1_0    conda-forge
     perl                      5.32.1          2_h7f98852_perl5    conda-forge
     pexpect                   4.8.0              pyh9f0ad1d_2    conda-forge
     pickleshare               0.7.5                   py_1003    conda-forge
     pillow                    9.0.1            py39hae2aec6_2    conda-forge
     pip                       22.0.4             pyhd8ed1ab_0    conda-forge
     pixman                    0.40.0               h36c2ea0_0    conda-forge
     pkg-config                0.29.2            h36c2ea0_1008    conda-forge
     pkginfo                   1.8.2              pyhd8ed1ab_0    conda-forge
     platformdirs              2.5.1              pyhd8ed1ab_0    conda-forge
     pluggy                    1.0.0            py39hf3d152e_2    conda-forge
     poppler                   21.03.0              h93df280_0    conda-forge
     poppler-data              0.4.11               hd8ed1ab_0    conda-forge
     postgresql                13.5                 h2510834_1    conda-forge
     pre-commit                2.17.0           py39hf3d152e_0    conda-forge
     proj                      8.0.1                h277dcde_0    conda-forge
     prometheus_client         0.13.1             pyhd8ed1ab_0    conda-forge
     prompt-toolkit            3.0.27             pyha770c72_0    conda-forge
     protobuf                  3.19.4           py39he80948d_0    conda-forge
     psutil                    5.9.0            py39h3811e60_0    conda-forge
     pthread-stubs             0.4               h36c2ea0_1001    conda-forge
     ptxcompiler               0.2.0            py39h107f55c_0    rapidsai-nightly
     ptyprocess                0.7.0              pyhd3deb0d_0    conda-forge
     py                        1.11.0             pyh6c4a22f_0    conda-forge
     py-cpuinfo                8.0.0              pyhd8ed1ab_0    conda-forge
     py-lief                   0.11.5           py39he80948d_1    conda-forge
     pyarrow                   6.0.1           py39h1ed2e5d_11_cuda    conda-forge
     pyasn1                    0.4.8                      py_0    conda-forge
     pyasn1-modules            0.2.7                      py_0    conda-forge
     pycodestyle               2.6.0              pyh9f0ad1d_0    conda-forge
     pycosat                   0.6.3           py39h3811e60_1009    conda-forge
     pycparser                 2.21               pyhd8ed1ab_0    conda-forge
     pyct                      0.4.6                      py_0    conda-forge
     pyct-core                 0.4.6                      py_0    conda-forge
     pydata-sphinx-theme       0.8.1              pyhd8ed1ab_0    conda-forge
     pydeck                    0.5.0              pyh9f0ad1d_0    conda-forge
     pydocstyle                6.1.1              pyhd8ed1ab_0    conda-forge
     pyee                      8.1.0              pyhd8ed1ab_0    conda-forge
     pyflakes                  2.2.0              pyh9f0ad1d_0    conda-forge
     pygal                     3.0.0                    pypi_0    pypi
     pygments                  2.11.2             pyhd8ed1ab_0    conda-forge
     pyjwt                     2.3.0              pyhd8ed1ab_1    conda-forge
     pylibcugraph              0+untagged.1.g87be0b3          pypi_0    pypi
     pylibraft                 0+untagged.1.g6aa9ac8          pypi_0    pypi
     pynndescent               0.5.6              pyh6c4a22f_0    conda-forge
     pynvml                    11.4.1             pyhd8ed1ab_0    conda-forge
     pyopenssl                 22.0.0             pyhd8ed1ab_0    conda-forge
     pyorc                     0.6.0            py39h40ac9ea_0    conda-forge
     pyparsing                 3.0.7              pyhd8ed1ab_0    conda-forge
     pyppeteer                 1.0.2              pyhd8ed1ab_0    conda-forge
     pyproj                    3.1.0            py39ha9a7ae0_4    conda-forge
     pyrsistent                0.18.1           py39h3811e60_0    conda-forge
     pysocks                   1.7.1            py39hf3d152e_4    conda-forge
     pytest                    7.1.1            py39hf3d152e_0    conda-forge
     pytest-asyncio            0.12.0           py39hde42818_2    conda-forge
     pytest-benchmark          3.4.1              pyhd8ed1ab_0    conda-forge
     pytest-cov                3.0.0              pyhd8ed1ab_0    conda-forge
     pytest-forked             1.4.0              pyhd8ed1ab_0    conda-forge
     pytest-timeout            2.1.0              pyhd8ed1ab_0    conda-forge
     pytest-xdist              2.5.0              pyhd8ed1ab_0    conda-forge
     python                    3.9.12          h9a8a25e_1_cpython    conda-forge
     python-confluent-kafka    1.7.0            py39h3811e60_2    conda-forge
     python-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge
     python-jose               3.3.0              pyh6c4a22f_1    conda-forge
     python-libarchive-c       4.0              py39hf3d152e_0    conda-forge
     python-louvain            0.15               pyhd8ed1ab_1    conda-forge
     python-snappy             0.6.0            py39h300dd49_1    conda-forge
     python_abi                3.9                      2_cp39    conda-forge
     pytz                      2022.1             pyhd8ed1ab_0    conda-forge
     pyu2f                     0.1.5              pyhd8ed1ab_0    conda-forge
     pyviz_comms               2.1.0              pyhd8ed1ab_0    conda-forge
     pywavelets                1.3.0            py39hd257fcd_0    conda-forge
     pyyaml                    6.0              py39h3811e60_3    conda-forge
     pyzmq                     22.3.0           py39h37b5a0c_1    conda-forge
     raft                      0+untagged.1.g6aa9ac8          pypi_0    pypi
     rapidjson                 1.1.0             he1b5a44_1002    conda-forge
     re2                       2022.02.01           h9c3ff4c_0    conda-forge
     readline                  8.1                  h46c0cb4_0    conda-forge
     readme_renderer           27.0               pyh9f0ad1d_0    conda-forge
     recommonmark              0.7.1              pyhd8ed1ab_0    conda-forge
     regex                     2022.3.15        py39hb9d737c_0    conda-forge
     requests                  2.27.1             pyhd8ed1ab_0    conda-forge
     requests-oauthlib         1.3.1              pyhd8ed1ab_0    conda-forge
     requests-toolbelt         0.9.1                      py_0    conda-forge
     responses                 0.20.0             pyhd8ed1ab_0    conda-forge
     rfc3986                   2.0.0              pyhd8ed1ab_0    conda-forge
     rhash                     1.4.1                h7f98852_0    conda-forge
     ripgrep                   13.0.0               h2f28480_2    conda-forge
     rmm                       0+untagged.1.g220ba88          pypi_0    pypi
     rsa                       4.8                pyhd8ed1ab_0    conda-forge
     rtree                     0.9.7            py39hb102c33_3    conda-forge
     ruamel_yaml               0.15.80         py39h3811e60_1006    conda-forge
     s2n                       1.0.10               h9b69904_0    conda-forge
     s3fs                      2022.2.0           pyhd8ed1ab_0    conda-forge
     s3transfer                0.5.2              pyhd8ed1ab_0    conda-forge
     sacremoses                0.0.49             pyhd8ed1ab_0    conda-forge
     sccache                   0.2.15               h9b69904_1    conda-forge
     scikit-build              0.13.1             pyhca92ed8_0    conda-forge
     scikit-image              0.19.2           py39hde0f152_0    conda-forge
     scikit-learn              0.24.2           py39h7c5d8c9_1    conda-forge
     scipy                     1.6.0            py39hee8e79c_0    conda-forge
     seaborn                   0.11.2               hd8ed1ab_0    conda-forge
     seaborn-base              0.11.2             pyhd8ed1ab_0    conda-forge
     secretstorage             3.3.1            py39hf3d152e_1    conda-forge
     send2trash                1.8.0              pyhd8ed1ab_0    conda-forge
     setuptools                59.8.0           py39hf3d152e_0    conda-forge
     shapely                   1.8.0            py39ha61afbd_0    conda-forge
     shellcheck                0.8.0                ha770c72_0    conda-forge
     simpervisor               0.4                pyhd8ed1ab_0    conda-forge
     six                       1.16.0             pyh6c4a22f_0    conda-forge
     snappy                    1.1.8                he1b5a44_3    conda-forge
     sniffio                   1.2.0            py39hf3d152e_2    conda-forge
     snowballstemmer           2.2.0              pyhd8ed1ab_0    conda-forge
     sortedcontainers          2.4.0              pyhd8ed1ab_0    conda-forge
     soupsieve                 2.3.1              pyhd8ed1ab_0    conda-forge
     spdlog                    1.8.5                h4bd325d_1    conda-forge
     sphinx                    4.4.0              pyh6c4a22f_1    conda-forge
     sphinx-click              3.1.0              pyhd8ed1ab_0    conda-forge
     sphinx-copybutton         0.5.0              pyhd8ed1ab_0    conda-forge
     sphinx-markdown-tables    0.0.15             pyhd3deb0d_0    conda-forge
     sphinx_rtd_theme          1.0.0              pyhd8ed1ab_0    conda-forge
     sphinxcontrib-applehelp   1.0.2                      py_0    conda-forge
     sphinxcontrib-devhelp     1.0.2                      py_0    conda-forge
     sphinxcontrib-htmlhelp    2.0.0              pyhd8ed1ab_0    conda-forge
     sphinxcontrib-jsmath      1.0.1                      py_0    conda-forge
     sphinxcontrib-qthelp      1.0.3                      py_0    conda-forge
     sphinxcontrib-serializinghtml 1.1.5              pyhd8ed1ab_1    conda-forge
     sphinxcontrib-websupport  1.2.4              pyhd8ed1ab_1    conda-forge
     sqlite                    3.37.1               h4ff8645_0    conda-forge
     sshpubkeys                3.1.0                      py_0    conda-forge
     statsmodels               0.13.2           py39hce5d2b2_0    conda-forge
     streamz                   0.6.3              pyh6c4a22f_0    conda-forge
     sysroot_linux-64          2.12                he073ed8_15    conda-forge
     tbb                       2021.5.0             h4bd325d_0    conda-forge
     tblib                     1.7.0              pyhd8ed1ab_0    conda-forge
     terminado                 0.13.3           py39hf3d152e_0    conda-forge
     testpath                  0.6.0              pyhd8ed1ab_0    conda-forge
     threadpoolctl             3.1.0              pyh8a188c0_0    conda-forge
     tifffile                  2021.11.2          pyhd8ed1ab_0    conda-forge
     tiledb                    2.3.4                he87e0bf_0    conda-forge
     tk                        8.6.12               h27826a3_0    conda-forge
     tokenizers                0.10.3           py39hd6d55de_1    conda-forge
     toml                      0.10.2             pyhd8ed1ab_0    conda-forge
     tomli                     2.0.1              pyhd8ed1ab_0    conda-forge
     toolz                     0.11.2             pyhd8ed1ab_0    conda-forge
     tornado                   6.1              py39h3811e60_2    conda-forge
     tqdm                      4.63.1             pyhd8ed1ab_0    conda-forge
     traitlets                 5.1.1              pyhd8ed1ab_0    conda-forge
     transformers              4.6.1              pyhd8ed1ab_0    conda-forge
     treelite                  2.3.0            py39heaea588_0    conda-forge
     treelite-runtime          2.3.0                    pypi_0    pypi
     twine                     3.8.0              pyhd8ed1ab_0    conda-forge
     typed-ast                 1.4.3            py39h3811e60_1    conda-forge
     typing-extensions         4.1.1                hd8ed1ab_0    conda-forge
     typing_extensions         4.1.1              pyha770c72_0    conda-forge
     tzcode                    2022a                h166bdaf_0    conda-forge
     tzdata                    2022a                h191b570_0    conda-forge
     ucx                       1.12.0+gd367332      cuda11.2_0    rapidsai-nightly
     ucx-proc                  1.0.0                       gpu    rapidsai-nightly
     ucx-py                    0.25.00a220328  py39_gd367332_13    rapidsai-nightly
     ukkonen                   1.0.1            py39h1a9c180_1    conda-forge
     umap-learn                0.5.2            py39hf3d152e_1    conda-forge
     unicodedata2              14.0.0           py39h3811e60_0    conda-forge
     urllib3                   1.26.9             pyhd8ed1ab_0    conda-forge
     virtualenv                20.14.0          py39hf3d152e_0    conda-forge
     wcwidth                   0.2.5              pyh9f0ad1d_2    conda-forge
     webencodings              0.5.1                      py_1    conda-forge
     websocket-client          1.3.1              pyhd8ed1ab_0    conda-forge
     websockets                10.2             py39hb9d737c_0    conda-forge
     werkzeug                  2.0.3              pyhd8ed1ab_1    conda-forge
     wheel                     0.37.1             pyhd8ed1ab_0    conda-forge
     widgetsnbextension        3.6.0            py39hf3d152e_0    conda-forge
     wrapt                     1.14.0           py39hb9d737c_0    conda-forge
     xarray                    2022.3.0           pyhd8ed1ab_0    conda-forge
     xerces-c                  3.2.3                h9d8b166_3    conda-forge
     xgboost                   1.5.2                    pypi_0    pypi
     xmltodict                 0.12.0                     py_0    conda-forge
     xorg-kbproto              1.0.7             h7f98852_1002    conda-forge
     xorg-libice               1.0.10               h7f98852_0    conda-forge
     xorg-libsm                1.2.3             hd9c2040_1000    conda-forge
     xorg-libx11               1.7.2                h7f98852_0    conda-forge
     xorg-libxau               1.0.9                h7f98852_0    conda-forge
     xorg-libxdmcp             1.1.3                h7f98852_0    conda-forge
     xorg-libxext              1.3.4                h7f98852_1    conda-forge
     xorg-libxrender           0.9.10            h7f98852_1003    conda-forge
     xorg-renderproto          0.11.1            h7f98852_1002    conda-forge
     xorg-xextproto            7.3.0             h7f98852_1002    conda-forge
     xorg-xproto               7.0.31            h7f98852_1007    conda-forge
     xz                        5.2.5                h516909a_1    conda-forge
     yaml                      0.2.5                h7f98852_2    conda-forge
     yarl                      1.7.2            py39h3811e60_1    conda-forge
     zeromq                    4.3.4                h9c3ff4c_1    conda-forge
     zfp                       0.5.5                h9c3ff4c_8    conda-forge
     zict                      2.1.0              pyhd8ed1ab_0    conda-forge
     zipp                      3.7.0              pyhd8ed1ab_1    conda-forge
     zlib                      1.2.11            h166bdaf_1014    conda-forge
     zstd                      1.5.2                ha95c52a_0    conda-forge
     
</pre></details>",2022-03-31T14:45:42Z,0,0,Charles Blackmon-Luca,@rapidsai,True
267,[FEA] Provide a way to specify the maximum allowable precision for integers/floats,"**Is your feature request related to a problem? Please describe.**

GPU memory is a valuable resource, and using int64/float64 columns where int32/float32 would suffice means using 2x as much memory unnecessarily. As opposed to scientific computing, 32-bit data types (or lower) are sufficient for many data science applications.

Even only 32-bit data types as inputs, the resulting output can be a 64-bit type:

```python
>>> cudf.Series([1, 2, 3], dtype=""int32"") + cudf.Scalar(1, dtype=""float32"")
0    2.0
1    3.0
2    4.0
dtype: float64
```

(this is consistent with Pandas and NumPy)

**Describe the solution you'd like**

It would be nice to be able to specify a maximum bitwidth for integer/floating types. If an operation would result in a value greater than could be accommodated, simply overflowing would be acceptable.

This could be another use case for [cudf.config](https://github.com/rapidsai/cudf/issues/5311).

**Describe alternatives you've considered**

The user can carefully cast results back from 64bit to 32bit to reduce memory usage, but this is tedious and does not help with peak memory usage.
",2022-03-31T18:28:33Z,0,0,Ashwin Srinath,Voltron Data,False
268,[BUG] Parsing string to float is inconsistent between CSV reader and to_numeric,"**Describe the bug**
The parsing of strings to floats has different results in different contexts, as discussed in https://github.com/NVIDIA/spark-rapids/issues/5035.

## Parsing floats from CSV

Given the following input file:

```
0.1
0.2
0.3
```

The following code parses the floats as follows.

``` python
>>> df  = cudf.read_csv(""floats.csv"", names=[""a""], dtype=[""float64""])
>>> df['a'][0]
0.1
>>> df['a'][1]
0.2
>>> df['a'][2]
0.30000000000000004
```

## Parsing floats with to_numeric

Parsing the same values using `to_numeric` produces different results.

``` python
>>> s = cudf.Series(['0.1', '0.2', '0.3'])
>>> x = cudf.to_numeric(s)
>>> x[0]
0.09999999999999999
>>> x[1]
0.19999999999999998
>>> x[2]
0.3
```

**Steps/Code to reproduce bug**
See above code examples.

**Expected behavior**
These two paths should produce consistent results.

**Environment overview (please complete the following information)**
Bare metal (desktop PC).

**Environment details**
<details><summary>Click here to see environment details</summary><pre>
     
     **git***
     commit 9932a03894f12f9c3f83ae635792c7d79eb430a1 (HEAD, sperlingxx/enable_zero_backref)
     Author: sperlingxx <lovedreamf@gmail.com>
     Date:   Wed Mar 30 08:44:00 2022 +0800
     
     update
     **git submodules***
     
     ***OS Information***
     DISTRIB_ID=Ubuntu
     DISTRIB_RELEASE=20.04
     DISTRIB_CODENAME=focal
     DISTRIB_DESCRIPTION=""Ubuntu 20.04.3 LTS""
     NAME=""Ubuntu""
     VERSION=""20.04.3 LTS (Focal Fossa)""
     ID=ubuntu
     ID_LIKE=debian
     PRETTY_NAME=""Ubuntu 20.04.3 LTS""
     VERSION_ID=""20.04""
     HOME_URL=""https://www.ubuntu.com/""
     SUPPORT_URL=""https://help.ubuntu.com/""
     BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
     PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
     VERSION_CODENAME=focal
     UBUNTU_CODENAME=focal
     Linux ripper 5.13.0-39-generic #44~20.04.1-Ubuntu SMP Thu Mar 24 16:43:35 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
     
     ***GPU Information***
     Tue Apr  5 14:53:32 2022
     +-----------------------------------------------------------------------------+
     | NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |
     |-------------------------------+----------------------+----------------------+
     | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
     | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
     |                               |                      |               MIG M. |
     |===============================+======================+======================|
     |   0  NVIDIA GeForce ...  On   | 00000000:42:00.0  On |                  N/A |
     |  0%   50C    P5    26W / 320W |   1086MiB / 10014MiB |      5%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     
     +-----------------------------------------------------------------------------+
     | Processes:                                                                  |
     |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
     |        ID   ID                                                   Usage      |
     |=============================================================================|
     |    0   N/A  N/A      1714      G   /usr/lib/xorg/Xorg                102MiB |
     |    0   N/A  N/A      2459      G   /usr/lib/xorg/Xorg                605MiB |
     |    0   N/A  N/A      2595      G   /usr/bin/gnome-shell              109MiB |
     |    0   N/A  N/A      3028      G   ...AAAAAAAAA= --shared-files        8MiB |
     |    0   N/A  N/A      3409      G   /usr/lib/firefox/firefox          168MiB |
     |    0   N/A  N/A      9759      G   ...veSuggestionsOnlyOnDemand       51MiB |
     |    0   N/A  N/A     39765      G   ..._20525.log --shared-files        4MiB |
     |    0   N/A  N/A    126249      G   ./jetbrains-toolbox                12MiB |
     |    0   N/A  N/A    147354      G   /usr/lib/firefox/firefox            3MiB |
     |    0   N/A  N/A    190794      G   /usr/lib/firefox/firefox            3MiB |
     +-----------------------------------------------------------------------------+
     
     ***CPU***
     Architecture:                    x86_64
     CPU op-mode(s):                  32-bit, 64-bit
     Byte Order:                      Little Endian
     Address sizes:                   43 bits physical, 48 bits virtual
     CPU(s):                          48
     On-line CPU(s) list:             0-47
     Thread(s) per core:              2
     Core(s) per socket:              24
     Socket(s):                       1
     NUMA node(s):                    4
     Vendor ID:                       AuthenticAMD
     CPU family:                      23
     Model:                           8
     Model name:                      AMD Ryzen Threadripper 2970WX 24-Core Processor
     Stepping:                        2
     Frequency boost:                 enabled
     CPU MHz:                         2166.041
     CPU max MHz:                     3000.0000
     CPU min MHz:                     2200.0000
     BogoMIPS:                        5988.02
     Virtualization:                  AMD-V
     L1d cache:                       768 KiB
     L1i cache:                       1.5 MiB
     L2 cache:                        12 MiB
     L3 cache:                        64 MiB
     NUMA node0 CPU(s):               0-5,24-29
     NUMA node1 CPU(s):               12-17,36-41
     NUMA node2 CPU(s):               6-11,30-35
     NUMA node3 CPU(s):               18-23,42-47
     Vulnerability Itlb multihit:     Not affected
     Vulnerability L1tf:              Not affected
     Vulnerability Mds:               Not affected
     Vulnerability Meltdown:          Not affected
     Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp
     Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
     Vulnerability Spectre v2:        Mitigation; LFENCE, IBPB conditional, STIBP disabled, RSB filling
     Vulnerability Srbds:             Not affected
     Vulnerability Tsx async abort:   Not affected
     Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb hw_pstate ssbd ibpb vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 xsaves clzero irperf xsaveerptr arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif overflow_recov succor smca sme sev sev_es
     
     ***CMake***
     /usr/bin/cmake
     cmake version 3.16.3
     
     CMake suite maintained and supported by Kitware (kitware.com/cmake).
     
     ***g++***
     /usr/bin/g++
     g++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
     Copyright (C) 2019 Free Software Foundation, Inc.
     This is free software; see the source for copying conditions.  There is NO
     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
     
     
     ***nvcc***
     /usr/local/cuda/bin/nvcc
     nvcc: NVIDIA (R) Cuda compiler driver
     Copyright (c) 2005-2021 NVIDIA Corporation
     Built on Thu_Nov_18_09:45:30_PST_2021
     Cuda compilation tools, release 11.5, V11.5.119
     Build cuda_11.5.r11.5/compiler.30672275_0
     
     ***Python***
     /home/andy/miniconda3/envs/rapids-22.04/bin/python
     Python 3.8.13
     
     ***Environment Variables***
     PATH                            : /home/andy/miniconda3/envs/rapids-22.04/bin:/home/andy/miniconda3/condabin:/home/andy/.cargo/bin:/home/andy/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/opt/apache-maven-3.8.4/bin:mvnd-0.5.2-linux-amd64/bin:/usr/local/cuda/bin
     LD_LIBRARY_PATH                 : :/usr/local/cuda/targets/x86_64-linux/lib/:/usr/local/cuda/lib64
     NUMBAPRO_NVVM                   :
     NUMBAPRO_LIBDEVICE              :
     CONDA_PREFIX                    : /home/andy/miniconda3/envs/rapids-22.04
     PYTHON_PATH                     :
     
     ***conda packages***
     /home/andy/miniconda3/condabin/conda
     # packages in environment at /home/andy/miniconda3/envs/rapids-22.04:
     #
     # Name                    Version                   Build  Channel
     _libgcc_mutex             0.1                 conda_forge    conda-forge
     _openmp_mutex             4.5                       1_gnu    conda-forge
     abseil-cpp                20210324.2           h9c3ff4c_0    conda-forge
     aiohttp                   3.8.1            py38h0a891b7_1    conda-forge
     aiosignal                 1.2.0              pyhd8ed1ab_0    conda-forge
     alsa-lib                  1.2.3                h516909a_0    conda-forge
     anyio                     3.5.0            py38h578d9bd_0    conda-forge
     appdirs                   1.4.4              pyh9f0ad1d_0    conda-forge
     argon2-cffi               21.3.0             pyhd8ed1ab_0    conda-forge
     argon2-cffi-bindings      21.2.0           py38h497a2fe_1    conda-forge
     arrow-cpp                 6.0.1           py38h4dc56cc_5_cuda    conda-forge
     arrow-cpp-proc            3.0.0                      cuda    conda-forge
     asgiref                   3.5.0              pyhd8ed1ab_0    conda-forge
     asttokens                 2.0.5              pyhd8ed1ab_0    conda-forge
     async-timeout             4.0.2              pyhd8ed1ab_0    conda-forge
     attrs                     21.4.0             pyhd8ed1ab_0    conda-forge
     aws-c-auth                0.6.8                hadad3cd_1    conda-forge
     aws-c-cal                 0.5.12               h70efedd_7    conda-forge
     aws-c-common              0.6.17               h7f98852_0    conda-forge
     aws-c-compression         0.2.14               h7c7754b_7    conda-forge
     aws-c-event-stream        0.2.7               hd2be095_32    conda-forge
     aws-c-http                0.6.10               h416565a_3    conda-forge
     aws-c-io                  0.10.14              he836878_0    conda-forge
     aws-c-mqtt                0.7.10               h885097b_0    conda-forge
     aws-c-s3                  0.1.29               h8d70ed6_0    conda-forge
     aws-c-sdkutils            0.1.1                h7c7754b_4    conda-forge
     aws-checksums             0.1.12               h7c7754b_6    conda-forge
     aws-crt-cpp               0.17.10              h6ab17b9_5    conda-forge
     aws-sdk-cpp               1.9.160              h36ff4c5_0    conda-forge
     backcall                  0.2.0              pyh9f0ad1d_0    conda-forge
     backports                 1.0                        py_2    conda-forge
     backports.functools_lru_cache 1.6.4              pyhd8ed1ab_0    conda-forge
     backports.zoneinfo        0.2.1            py38h497a2fe_4    conda-forge
     beautifulsoup4            4.10.0             pyha770c72_0    conda-forge
     bleach                    4.1.0              pyhd8ed1ab_0    conda-forge
     blosc                     1.21.0               h9c3ff4c_0    conda-forge
     bokeh                     2.4.2            py38h578d9bd_0    conda-forge
     boost                     1.74.0           py38h2b96118_5    conda-forge
     boost-cpp                 1.74.0               h312852a_4    conda-forge
     brotli                    1.0.9                h166bdaf_7    conda-forge
     brotli-bin                1.0.9                h166bdaf_7    conda-forge
     brotlipy                  0.7.0           py38h0a891b7_1004    conda-forge
     brunsli                   0.1                  h9c3ff4c_0    conda-forge
     bzip2                     1.0.8                h7f98852_4    conda-forge
     c-ares                    1.18.1               h7f98852_0    conda-forge
     c-blosc2                  2.0.4                h5f21a17_1    conda-forge
     ca-certificates           2021.10.8            ha878542_0    conda-forge
     cachetools                5.0.0              pyhd8ed1ab_0    conda-forge
     cairo                     1.16.0            h6cf1ce9_1008    conda-forge
     certifi                   2021.10.8        py38h578d9bd_2    conda-forge
     cffi                      1.15.0           py38h3931269_0    conda-forge
     cfitsio                   3.470                hb418390_7    conda-forge
     charls                    2.2.0                h9c3ff4c_0    conda-forge
     charset-normalizer        2.0.12             pyhd8ed1ab_0    conda-forge
     click                     8.0.4            py38h578d9bd_0    conda-forge
     click-plugins             1.1.1                      py_0    conda-forge
     cligj                     0.7.2              pyhd8ed1ab_1    conda-forge
     cloudpickle               2.0.0              pyhd8ed1ab_0    conda-forge
     colorama                  0.4.4              pyh9f0ad1d_0    conda-forge
     colorcet                  3.0.0              pyhd8ed1ab_0    conda-forge
     cryptography              36.0.2           py38h2b5fc30_1    conda-forge
     cucim                     22.04.00a220405 cuda_11_py38_g7602774_34    rapidsai-nightly
     cuda-python               11.6.1           py38h3fd9d12_0    nvidia
     cudatoolkit               11.5.1               hcf5317a_9    nvidia
     cudf                      22.04.00a220405 cuda_11_py38_g4c84184a3f_304    rapidsai-nightly
     cudf_kafka                22.04.00a220405 py38_g4c84184a3f_304    rapidsai-nightly
     cugraph                   22.04.00a220405 cuda11_py38_g38be932f_101    rapidsai-nightly
     cuml                      22.04.00a220405 cuda11_py38_g5feaf7b74_107    rapidsai-nightly
     cupy                      9.6.0            py38h177b0fd_0    conda-forge
     curl                      7.82.0               h7bff187_0    conda-forge
     cusignal                  22.04.00a220405 py39_g8878bf7_15    rapidsai-nightly
     cuspatial                 22.04.00a220405 py38_g7709a43_19    rapidsai-nightly
     custreamz                 22.04.00a220405 py38_g4c84184a3f_304    rapidsai-nightly
     cuxfilter                 22.04.00a220405 py38_gf9a106c_18    rapidsai-nightly
     cycler                    0.11.0             pyhd8ed1ab_0    conda-forge
     cyrus-sasl                2.1.27               h230043b_5    conda-forge
     cytoolz                   0.11.2           py38h497a2fe_1    conda-forge
     dask                      2022.3.0           pyhd8ed1ab_1    conda-forge
     dask-core                 2022.3.0           pyhd8ed1ab_0    conda-forge
     dask-cuda                 22.04.00a220405         py38_29    rapidsai-nightly
     dask-cudf                 22.04.00a220405 cuda_11_py38_g4c84184a3f_304    rapidsai-nightly
     dask-sql                  2022.1.1a220405  py_gab2aa5a_33    dask/label/dev
     datashader                0.13.1a                    py_0    rapidsai-nightly
     datashape                 0.5.4                      py_1    conda-forge
     debugpy                   1.5.1            py38h709712a_0    conda-forge
     decorator                 5.1.1              pyhd8ed1ab_0    conda-forge
     defusedxml                0.7.1              pyhd8ed1ab_0    conda-forge
     distributed               2022.3.0           pyhd8ed1ab_0    conda-forge
     dlpack                    0.5                  h9c3ff4c_0    conda-forge
     entrypoints               0.4                pyhd8ed1ab_0    conda-forge
     executing                 0.8.3              pyhd8ed1ab_0    conda-forge
     expat                     2.4.8                h27087fc_0    conda-forge
     faiss-proc                1.0.0                      cuda    conda-forge
     fastapi                   0.75.1             pyhd8ed1ab_0    conda-forge
     fastavro                  1.4.10           py38h0a891b7_0    conda-forge
     fastrlock                 0.8              py38hfa26641_1    conda-forge
     fiona                     1.8.20           py38hbb147eb_2    conda-forge
     flit-core                 3.7.1              pyhd8ed1ab_0    conda-forge
     font-ttf-dejavu-sans-mono 2.37                 hab24e00_0    conda-forge
     font-ttf-inconsolata      3.000                h77eed37_0    conda-forge
     font-ttf-source-code-pro  2.038                h77eed37_0    conda-forge
     font-ttf-ubuntu           0.83                 hab24e00_0    conda-forge
     fontconfig                2.14.0               h8e229c2_0    conda-forge
     fonts-conda-ecosystem     1                             0    conda-forge
     fonts-conda-forge         1                             0    conda-forge
     fonttools                 4.31.2           py38h0a891b7_0    conda-forge
     freetype                  2.10.4               h0708190_1    conda-forge
     freexl                    1.0.6                h7f98852_0    conda-forge
     frozenlist                1.3.0            py38h0a891b7_1    conda-forge
     fsspec                    2022.3.0           pyhd8ed1ab_0    conda-forge
     gdal                      3.3.2            py38h81a01a0_3    conda-forge
     geopandas                 0.9.0              pyhd8ed1ab_1    conda-forge
     geopandas-base            0.9.0              pyhd8ed1ab_1    conda-forge
     geos                      3.9.1                h9c3ff4c_2    conda-forge
     geotiff                   1.7.0                h08e826d_2    conda-forge
     gettext                   0.19.8.1          h73d1719_1008    conda-forge
     gflags                    2.2.2             he1b5a44_1004    conda-forge
     giflib                    5.2.1                h36c2ea0_2    conda-forge
     glog                      0.5.0                h48cff8f_0    conda-forge
     graphite2                 1.3.13            h58526e2_1001    conda-forge
     grpc-cpp                  1.42.0               ha1441d3_1    conda-forge
     h11                       0.13.0             pyhd8ed1ab_0    conda-forge
     harfbuzz                  2.9.1                h83ec7ef_1    conda-forge
     hdf4                      4.2.15               h10796ff_3    conda-forge
     hdf5                      1.12.1          nompi_h2386368_104    conda-forge
     heapdict                  1.0.1                      py_0    conda-forge
     holoviews                 1.14.6             pyhd8ed1ab_0    conda-forge
     icu                       68.2                 h9c3ff4c_0    conda-forge
     idna                      3.3                pyhd8ed1ab_0    conda-forge
     imagecodecs               2021.8.26        py38hb5ce8f7_1    conda-forge
     imageio                   2.16.1             pyhcf75d05_0    conda-forge
     importlib-metadata        4.11.3           py38h578d9bd_1    conda-forge
     importlib_metadata        4.11.3               hd8ed1ab_1    conda-forge
     importlib_resources       5.6.0              pyhd8ed1ab_0    conda-forge
     ipykernel                 6.12.1           py38h7f3c49e_0    conda-forge
     ipython                   8.2.0            py38h578d9bd_0    conda-forge
     ipython_genutils          0.2.0                      py_1    conda-forge
     ipywidgets                7.7.0              pyhd8ed1ab_0    conda-forge
     jbig                      2.1               h7f98852_2003    conda-forge
     jedi                      0.18.1           py38h578d9bd_1    conda-forge
     jinja2                    3.1.1              pyhd8ed1ab_0    conda-forge
     joblib                    1.1.0              pyhd8ed1ab_0    conda-forge
     jpeg                      9e                   h7f98852_0    conda-forge
     jpype1                    1.3.0            py38h1fd1430_2    conda-forge
     json-c                    0.15                 h98cffda_0    conda-forge
     jsonschema                4.4.0              pyhd8ed1ab_0    conda-forge
     jupyter-server-proxy      3.2.1              pyhd8ed1ab_0    conda-forge
     jupyter_client            7.2.1              pyhd8ed1ab_0    conda-forge
     jupyter_core              4.9.2            py38h578d9bd_0    conda-forge
     jupyter_server            1.16.0             pyhd8ed1ab_1    conda-forge
     jupyterlab_pygments       0.1.2              pyh9f0ad1d_0    conda-forge
     jupyterlab_widgets        1.1.0              pyhd8ed1ab_0    conda-forge
     jxrlib                    1.1                  h7f98852_2    conda-forge
     kealib                    1.4.14               h87e4c3c_3    conda-forge
     keyutils                  1.6.1                h166bdaf_0    conda-forge
     kiwisolver                1.4.2            py38h43d8883_1    conda-forge
     krb5                      1.19.3               h3790be6_0    conda-forge
     lcms2                     2.12                 hddcbb42_0    conda-forge
     ld_impl_linux-64          2.36.1               hea4e1c9_2    conda-forge
     lerc                      3.0                  h9c3ff4c_0    conda-forge
     libaec                    1.0.6                h9c3ff4c_0    conda-forge
     libblas                   3.9.0           13_linux64_openblas    conda-forge
     libbrotlicommon           1.0.9                h166bdaf_7    conda-forge
     libbrotlidec              1.0.9                h166bdaf_7    conda-forge
     libbrotlienc              1.0.9                h166bdaf_7    conda-forge
     libcblas                  3.9.0           13_linux64_openblas    conda-forge
     libcucim                  22.04.00a220405 cuda11_g7602774_34    rapidsai-nightly
     libcudf                   22.04.00a220405 cuda11_g4c84184a3f_304    rapidsai-nightly
     libcudf_kafka             22.04.00a220405 g4c84184a3f_304    rapidsai-nightly
     libcugraph                22.04.00a220405 cuda11_g38be932f_101    rapidsai-nightly
     libcugraph_etl            22.04.00a220405 cuda11_g38be932f_101    rapidsai-nightly
     libcugraphops             22.04.00a220405 cuda11_ga9f323f_32    rapidsai-nightly
     libcuml                   22.04.00a220405 cuda11_g5feaf7b74_107    rapidsai-nightly
     libcumlprims              22.04.00a220324 cuda11_g99e8d8f_15    rapidsai-nightly
     libcurl                   7.82.0               h7bff187_0    conda-forge
     libcusolver               11.3.4.124           h33c3c4e_0    nvidia
     libcuspatial              22.04.00a220405 cuda11_g7709a43_19    rapidsai-nightly
     libdap4                   3.20.6               hd7c4107_2    conda-forge
     libdeflate                1.8                  h7f98852_0    conda-forge
     libedit                   3.1.20191231         he28a2e2_2    conda-forge
     libev                     4.33                 h516909a_1    conda-forge
     libevent                  2.1.10               h9b69904_4    conda-forge
     libfaiss                  1.7.0           cuda112h5bea7ad_8_cuda    conda-forge
     libffi                    3.4.2                h7f98852_5    conda-forge
     libgcc-ng                 11.2.0              h1d223b6_14    conda-forge
     libgcrypt                 1.10.0               h7f98852_0    conda-forge
     libgdal                   3.3.2                h6acdded_3    conda-forge
     libgfortran-ng            11.2.0              h69a702a_14    conda-forge
     libgfortran5              11.2.0              h5c6108e_14    conda-forge
     libglib                   2.70.2               h174f98d_4    conda-forge
     libgomp                   11.2.0              h1d223b6_14    conda-forge
     libgpg-error              1.44                 h9eb791d_0    conda-forge
     libgsasl                  1.10.0               h5b4c23d_0    conda-forge
     libhwloc                  2.3.0                h5e5b7d1_1    conda-forge
     libiconv                  1.16                 h516909a_0    conda-forge
     libkml                    1.3.0             h238a007_1014    conda-forge
     liblapack                 3.9.0           13_linux64_openblas    conda-forge
     libllvm11                 11.1.0               hf817b99_3    conda-forge
     libnetcdf                 4.8.1           nompi_hb3fd0d9_101    conda-forge
     libnghttp2                1.47.0               h727a467_0    conda-forge
     libnsl                    2.0.0                h7f98852_0    conda-forge
     libntlm                   1.4               h7f98852_1002    conda-forge
     libopenblas               0.3.18          pthreads_h8fe5266_0    conda-forge
     libpng                    1.6.37               h21135ba_2    conda-forge
     libpq                     13.5                 hd57d9b9_1    conda-forge
     libprotobuf               3.19.4               h780b84a_0    conda-forge
     libraft-distance          22.04.00a220405 cuda11_gc509483_112    rapidsai-nightly
     libraft-headers           22.04.00a220405 cuda11_gc509483_112    rapidsai-nightly
     libraft-nn                22.04.00a220405 cuda11_gc509483_112    rapidsai-nightly
     librdkafka                1.7.0                hc49e61c_1    conda-forge
     librmm                    22.04.00a220405 cuda11_g1420689_48    rapidsai-nightly
     librttopo                 1.1.0                h1185371_6    conda-forge
     libsodium                 1.0.18               h36c2ea0_1    conda-forge
     libspatialindex           1.9.3                h9c3ff4c_4    conda-forge
     libspatialite             5.0.1                h5cf074c_8    conda-forge
     libssh2                   1.10.0               ha56f1ee_2    conda-forge
     libstdcxx-ng              11.2.0              he4da1e4_14    conda-forge
     libthrift                 0.15.0               he6d91bd_1    conda-forge
     libtiff                   4.3.0                h6f004c6_2    conda-forge
     libutf8proc               2.7.0                h7f98852_0    conda-forge
     libuuid                   2.32.1            h7f98852_1000    conda-forge
     libuv                     1.43.0               h7f98852_0    conda-forge
     libwebp                   1.2.2                h3452ae3_0    conda-forge
     libwebp-base              1.2.2                h7f98852_1    conda-forge
     libxcb                    1.13              h7f98852_1004    conda-forge
     libxgboost                1.5.2dev.rapidsai22.04       cuda_11_0    rapidsai-nightly
     libxml2                   2.9.12               h72842e0_0    conda-forge
     libzip                    1.8.0                h4de3113_1    conda-forge
     libzlib                   1.2.11            h166bdaf_1014    conda-forge
     libzopfli                 1.0.3                h9c3ff4c_0    conda-forge
     llvmlite                  0.38.0           py38h38d86a4_1    conda-forge
     locket                    0.2.0                      py_2    conda-forge
     lz4                       4.0.0            py38h1bf946c_1    conda-forge
     lz4-c                     1.9.3                h9c3ff4c_1    conda-forge
     mapclassify               2.4.3              pyhd8ed1ab_0    conda-forge
     markdown                  3.3.6              pyhd8ed1ab_0    conda-forge
     markupsafe                2.1.1            py38h0a891b7_1    conda-forge
     matplotlib-base           3.5.1            py38hf4fb855_0    conda-forge
     matplotlib-inline         0.1.3              pyhd8ed1ab_0    conda-forge
     mistune                   0.8.4           py38h497a2fe_1005    conda-forge
     msgpack-python            1.0.3            py38h43d8883_1    conda-forge
     multidict                 6.0.2            py38h0a891b7_1    conda-forge
     multipledispatch          0.6.0                      py_0    conda-forge
     munch                     2.5.0                      py_0    conda-forge
     munkres                   1.1.4              pyh9f0ad1d_0    conda-forge
     nbclient                  0.5.13             pyhd8ed1ab_0    conda-forge
     nbconvert                 6.4.5              pyhd8ed1ab_2    conda-forge
     nbconvert-core            6.4.5              pyhd8ed1ab_2    conda-forge
     nbconvert-pandoc          6.4.5              pyhd8ed1ab_2    conda-forge
     nbformat                  5.3.0              pyhd8ed1ab_0    conda-forge
     nccl                      2.12.7.1             h0800d71_0    conda-forge
     ncurses                   6.3                  h9c3ff4c_0    conda-forge
     nest-asyncio              1.5.5              pyhd8ed1ab_0    conda-forge
     networkx                  2.6.3              pyhd8ed1ab_1    conda-forge
     nodejs                    14.18.3              h92b4a50_1    conda-forge
     notebook                  6.4.10             pyha770c72_0    conda-forge
     nspr                      4.32                 h9c3ff4c_1    conda-forge
     nss                       3.77                 h2350873_0    conda-forge
     numba                     0.55.1           py38h4bf6c61_0    conda-forge
     numpy                     1.21.5           py38h87f13fb_0    conda-forge
     nvtx                      0.2.3            py38h497a2fe_1    conda-forge
     openjdk                   11.0.9.1             h5cc2fde_1    conda-forge
     openjpeg                  2.4.0                hb52868f_1    conda-forge
     openssl                   1.1.1n               h166bdaf_0    conda-forge
     orc                       1.7.1                h1be678f_1    conda-forge
     packaging                 21.3               pyhd8ed1ab_0    conda-forge
     pandas                    1.3.5            py38h43a58ef_0    conda-forge
     pandoc                    2.17.1.1             ha770c72_0    conda-forge
     pandocfilters             1.5.0              pyhd8ed1ab_0    conda-forge
     panel                     0.12.7             pyhd8ed1ab_0    conda-forge
     param                     1.12.1             pyh6c4a22f_0    conda-forge
     parquet-cpp               1.5.1                         2    conda-forge
     parso                     0.8.3              pyhd8ed1ab_0    conda-forge
     partd                     1.2.0              pyhd8ed1ab_0    conda-forge
     pcre                      8.45                 h9c3ff4c_0    conda-forge
     pexpect                   4.8.0              pyh9f0ad1d_2    conda-forge
     pickleshare               0.7.5                   py_1003    conda-forge
     pillow                    9.1.0            py38h0ee0e06_0    conda-forge
     pip                       22.0.4             pyhd8ed1ab_0    conda-forge
     pixman                    0.40.0               h36c2ea0_0    conda-forge
     poppler                   21.09.0              ha39eefc_3    conda-forge
     poppler-data              0.4.11               hd8ed1ab_0    conda-forge
     postgresql                13.5                 h2510834_1    conda-forge
     proj                      8.1.0                h277dcde_1    conda-forge
     prometheus_client         0.13.1             pyhd8ed1ab_0    conda-forge
     prompt-toolkit            3.0.29             pyha770c72_0    conda-forge
     protobuf                  3.19.4           py38h709712a_0    conda-forge
     psutil                    5.9.0            py38h0a891b7_1    conda-forge
     pthread-stubs             0.4               h36c2ea0_1001    conda-forge
     ptxcompiler               0.2.0            py38h98f4b32_0    rapidsai-nightly
     ptyprocess                0.7.0              pyhd3deb0d_0    conda-forge
     pure_eval                 0.2.2              pyhd8ed1ab_0    conda-forge
     py-xgboost                1.5.2dev.rapidsai22.04  cuda_11_py38_0    rapidsai-nightly
     pyarrow                   6.0.1           py38ha746e9d_5_cuda    conda-forge
     pycparser                 2.21               pyhd8ed1ab_0    conda-forge
     pyct                      0.4.6                      py_0    conda-forge
     pyct-core                 0.4.6                      py_0    conda-forge
     pydantic                  1.9.0            py38h0a891b7_1    conda-forge
     pydeck                    0.5.0              pyh9f0ad1d_0    conda-forge
     pyee                      8.1.0              pyhd8ed1ab_0    conda-forge
     pygments                  2.11.2             pyhd8ed1ab_0    conda-forge
     pylibcugraph              22.04.00a220405 cuda11_py38_g38be932f_101    rapidsai-nightly
     pynvml                    11.4.1             pyhd8ed1ab_0    conda-forge
     pyopenssl                 22.0.0             pyhd8ed1ab_0    conda-forge
     pyparsing                 3.0.7              pyhd8ed1ab_0    conda-forge
     pyppeteer                 1.0.2              pyhd8ed1ab_0    conda-forge
     pyproj                    3.1.0            py38h3701b11_4    conda-forge
     pyraft                    22.04.00a220405 cuda11_py38_gc509483_112    rapidsai-nightly
     pyrsistent                0.18.1           py38h0a891b7_1    conda-forge
     pysocks                   1.7.1            py38h578d9bd_5    conda-forge
     python                    3.8.13          h582c2e5_0_cpython    conda-forge
     python-confluent-kafka    1.7.0            py38h497a2fe_2    conda-forge
     python-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge
     python-fastjsonschema     2.15.3             pyhd8ed1ab_0    conda-forge
     python-tzdata             2022.1             pyhd8ed1ab_0    conda-forge
     python_abi                3.8                      2_cp38    conda-forge
     pytz                      2022.1             pyhd8ed1ab_0    conda-forge
     pytz-deprecation-shim     0.1.0.post0      py38h578d9bd_1    conda-forge
     pyviz_comms               2.2.0              pyhd8ed1ab_0    conda-forge
     pywavelets                1.3.0            py38h3ec907f_0    conda-forge
     pyyaml                    6.0              py38h0a891b7_4    conda-forge
     pyzmq                     22.3.0           py38hfc09fa9_2    conda-forge
     rapids                    22.04.00a220405 cuda11_py38_g5d1fba5_121    rapidsai-nightly
     rapids-xgboost            22.04.00a220405 cuda11_py38_g5d1fba5_121    rapidsai-nightly
     re2                       2021.11.01           h9c3ff4c_0    conda-forge
     readline                  8.1                  h46c0cb4_0    conda-forge
     requests                  2.27.1             pyhd8ed1ab_0    conda-forge
     rmm                       22.04.00a220405 cuda11_py38_g1420689_48    rapidsai-nightly
     rtree                     0.9.7            py38h02d302b_3    conda-forge
     s2n                       1.3.0                h9b69904_0    conda-forge
     scikit-image              0.19.2           py38h43a58ef_0    conda-forge
     scikit-learn              1.0.2            py38h1561384_0    conda-forge
     scipy                     1.8.0            py38h56a6a73_1    conda-forge
     send2trash                1.8.0              pyhd8ed1ab_0    conda-forge
     setuptools                59.8.0           py38h578d9bd_1    conda-forge
     shapely                   1.8.0            py38hb7fe4a8_0    conda-forge
     simpervisor               0.4                pyhd8ed1ab_0    conda-forge
     six                       1.16.0             pyh6c4a22f_0    conda-forge
     snappy                    1.1.8                he1b5a44_3    conda-forge
     sniffio                   1.2.0            py38h578d9bd_3    conda-forge
     sortedcontainers          2.4.0              pyhd8ed1ab_0    conda-forge
     soupsieve                 2.3.1              pyhd8ed1ab_0    conda-forge
     spdlog                    1.8.5                h4bd325d_1    conda-forge
     sqlite                    3.37.1               h4ff8645_0    conda-forge
     stack_data                0.2.0              pyhd8ed1ab_0    conda-forge
     starlette                 0.17.1             pyhd8ed1ab_0    conda-forge
     streamz                   0.6.3              pyh6c4a22f_0    conda-forge
     tabulate                  0.8.9              pyhd8ed1ab_0    conda-forge
     tblib                     1.7.0              pyhd8ed1ab_0    conda-forge
     terminado                 0.13.3           py38h578d9bd_1    conda-forge
     testpath                  0.6.0              pyhd8ed1ab_0    conda-forge
     threadpoolctl             3.1.0              pyh8a188c0_0    conda-forge
     tifffile                  2021.11.2          pyhd8ed1ab_0    conda-forge
     tiledb                    2.3.4                he87e0bf_0    conda-forge
     tk                        8.6.12               h27826a3_0    conda-forge
     toolz                     0.11.2             pyhd8ed1ab_0    conda-forge
     tornado                   6.1              py38h0a891b7_3    conda-forge
     tqdm                      4.64.0             pyhd8ed1ab_0    conda-forge
     traitlets                 5.1.1              pyhd8ed1ab_0    conda-forge
     treelite                  2.3.0            py38hdd725b4_0    conda-forge
     treelite-runtime          2.3.0                    pypi_0    pypi
     typing-extensions         4.1.1                hd8ed1ab_0    conda-forge
     typing_extensions         4.1.1              pyha770c72_0    conda-forge
     tzcode                    2022a                h166bdaf_0    conda-forge
     tzdata                    2022a                h191b570_0    conda-forge
     tzlocal                   4.2              py38h578d9bd_0    conda-forge
     ucx                       1.12.0+gd367332      cuda11.2_0    rapidsai-nightly
     ucx-proc                  1.0.0                       gpu    rapidsai-nightly
     ucx-py                    0.25.00a220405  py38_gd367332_13    rapidsai-nightly
     unicodedata2              14.0.0           py38h0a891b7_1    conda-forge
     urllib3                   1.26.9             pyhd8ed1ab_0    conda-forge
     uvicorn                   0.17.6           py38h578d9bd_0    conda-forge
     wcwidth                   0.2.5              pyh9f0ad1d_2    conda-forge
     webencodings              0.5.1                      py_1    conda-forge
     websocket-client          1.3.2              pyhd8ed1ab_0    conda-forge
     websockets                10.2             py38h0a891b7_0    conda-forge
     wheel                     0.37.1             pyhd8ed1ab_0    conda-forge
     widgetsnbextension        3.6.0            py38h578d9bd_0    conda-forge
     xarray                    2022.3.0           pyhd8ed1ab_0    conda-forge
     xerces-c                  3.2.3                h9d8b166_3    conda-forge
     xgboost                   1.5.2dev.rapidsai22.04  cuda_11_py38_0    rapidsai-nightly
     xorg-fixesproto           5.0               h7f98852_1002    conda-forge
     xorg-inputproto           2.3.2             h7f98852_1002    conda-forge
     xorg-kbproto              1.0.7             h7f98852_1002    conda-forge
     xorg-libice               1.0.10               h7f98852_0    conda-forge
     xorg-libsm                1.2.3             hd9c2040_1000    conda-forge
     xorg-libx11               1.7.2                h7f98852_0    conda-forge
     xorg-libxau               1.0.9                h7f98852_0    conda-forge
     xorg-libxdmcp             1.1.3                h7f98852_0    conda-forge
     xorg-libxext              1.3.4                h7f98852_1    conda-forge
     xorg-libxfixes            5.0.3             h7f98852_1004    conda-forge
     xorg-libxi                1.7.10               h7f98852_0    conda-forge
     xorg-libxrender           0.9.10            h7f98852_1003    conda-forge
     xorg-libxtst              1.2.3             h7f98852_1002    conda-forge
     xorg-recordproto          1.14.2            h7f98852_1002    conda-forge
     xorg-renderproto          0.11.1            h7f98852_1002    conda-forge
     xorg-xextproto            7.3.0             h7f98852_1002    conda-forge
     xorg-xproto               7.0.31            h7f98852_1007    conda-forge
     xz                        5.2.5                h516909a_1    conda-forge
     yaml                      0.2.5                h7f98852_2    conda-forge
     yarl                      1.7.2            py38h0a891b7_2    conda-forge
     zeromq                    4.3.4                h9c3ff4c_1    conda-forge
     zfp                       0.5.5                h9c3ff4c_8    conda-forge
     zict                      2.1.0              pyhd8ed1ab_0    conda-forge
     zipp                      3.8.0              pyhd8ed1ab_0    conda-forge
     zlib                      1.2.11            h166bdaf_1014    conda-forge
     zstd                      1.5.2                ha95c52a_0    conda-forge
     
</pre></details>

**Additional context**
See https://github.com/NVIDIA/spark-rapids/issues/5035
",2022-04-05T20:54:06Z,0,0,Andy Grove,@Apple,False
269,[BUG] MultiIndex.difference returns a pandas MultiIndex,"**Describe the bug**
[The implementation of `MultiIndex.difference`](https://github.com/rapidsai/cudf/blob/branch-22.06/python/cudf/cudf/core/multiindex.py#L1487) converts to pandas and calls that object's method rather than properly implementing it.

**Expected behavior**
This method should be implemented internally and return a cudf MultiIndex object.",2022-04-05T23:55:14Z,0,0,Vyas Ramasubramani,@rapidsai,True
270,[BUG] Passing `fill_value` in `groupby.shift` with a mismatching dtype must result in error,"**Describe the bug**
When `fill_values`'s dtype actually is mismatching with a column's dtype, we should raise an error as we don't support mixed types(`'object'`) like in pandas. Pandas is returning a mixed `object` dtypes when there is a mismatch of column dtype & `fill_value` dtype in `1.4.x`

**Steps/Code to reproduce bug**
```python
>>> import pandas as pd
>>> import pandas as pd
>>> s = pd.DataFrame()
>>> s['a'] = [1, 2, 3, 4, 5]
>>> s['b'] = pd.Series([10, 11, 12, 13, 14], dtype='datetime64[ns]')
>>> s.groupby('a').shift(1500, fill_value=10)
    b
0  10
1  10
2  10
3  10
4  10
>>> s.groupby('a').shift(1500, fill_value=10).dtypes
b    object
dtype: object
>>> import cudf
>>> gs = cudf.from_pandas(s)
>>> gs.groupby('a').shift(1500, fill_value=10)
                              b
0 1970-01-01 00:00:00.000000010
1 1970-01-01 00:00:00.000000010
2 1970-01-01 00:00:00.000000010
3 1970-01-01 00:00:00.000000010
4 1970-01-01 00:00:00.000000010
>>> gs.groupby('a').shift(1500, fill_value=10).dtypes
b    datetime64[ns]
dtype: object
```
**Expected behavior**
Few possible solutions:
1. Raise error that mixed types are not possible like we do in other places of code-base.
2. If we do 1, users will want a way fill values based on dtypes or column names. So expanding `fill_value` to be a dictionary of fill values that can be used to fill values based on dtype like: {""str"": ""abc"", ""int64"": 10, ....} or based on columns like: {""a"":""abc"", ""b"": 10, ...}

In a way, this is both a bug & feature request.

**Environment overview (please complete the following information)**
 - Environment location: [Bare-metal]
 - Method of cuDF install: [from source]

",2022-04-06T16:40:14Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
271,"[FEA] Define unary operations like `__neg__`, `__pos__`, `__abs__` and `__invert__` for `Column`.","Currently, invoking Python's builtin unary operators on columns raises:

```python
>>> s = cudf.Series([1, 2, 3])
>>> -s._column  # TypeError
```

This came up in https://github.com/rapidsai/cudf/pull/10564.

A workaround is to do for example, `s._column.unary_operator(""abs"")` or `0 - s._column`, but it would be nice to do this in a more Pythonic way.",2022-04-06T20:23:25Z,0,0,Ashwin Srinath,Voltron Data,False
272,[FEA] Support for approx_count_distinct,"**Is your feature request related to a problem? Please describe.**
I would like to be able to implement a GPU version of Spark's `approx_count_distinct` function, which uses the [HyperLogLog++](https://en.wikipedia.org/wiki/HyperLogLog) cardinality estimation algorithm. 

cuDF does not appear to provide any features today that would allow me to do this.

**Describe the solution you'd like**
I would like cuDF to implement this capability and expose an API that is likely similar to `approx_percentile` in that there would be methods both for computing and merging the underlying data structure, whether that is based on HyperLogLog++ or some other algorithm.

**Describe alternatives you've considered**
None

**Additional context**
None
",2022-04-13T18:16:08Z,0,0,Andy Grove,@Apple,False
273,Remove cudautils.py,"Currently the contents of cudautils.py can be grouped into three sets of functionality:
1. The first set of 7 functions are all targeted at supporting `find_first` and `find_last`, which in turn are used respectively by the `find_first_value` and `find_last_value` methods of Column objects.
2. The second set of functions 4 around window sizes are used by the rolling calculations in rolling.py.
3. The third pair of 2 functions is used for UDF compilation.

The functions in group 3 are necessary but should probably be moved to `core/udf/utils.py`. The remaining functions should be possible to remove altogether, but they may require some additional functionality to be added to libcudf. For instance, `find_first_value` and `find_last_value` may be possible to implement using libcudf's `lower_bound` followed by an equality check, and similarly `find_last_value` could be replaced with `upper_bound`, accounting for all of group 1. For group 2 functions I am not certain if the necessary functionality exists in libcudf's roling aggregations, so we may need to do a little more engineering there first.",2022-04-21T17:21:52Z,1,0,Vyas Ramasubramani,@rapidsai,True
274,[FEA] Lint all Python docstrings using ruff's pydocstyle rules,"**Is your feature request related to a problem? Please describe.**
Our Python docstrings have various style violations when compared against standards like [pep257](https://peps.python.org/pep-0257/). Not only does this impact readability (which may be subjective), it also reduces the effectiveness of tools like Sphinx or numpydoc that rely on specific formatting in order to parse docstrings.

**Describe the solution you'd like**
We should lint all of our docstrings using `pydocstyle`, the standard for docstring linting in the Python ecosystem. I added support for pydocstyle in #7985 and have since added support for a few additional modules, but many of our most prominent public-facing APIs are not yet linted. My reticence to lint those files has stemmed in large part from the degree to which these files have been refactored recently and the number of docstrings that were rendered obsolete in the process. However, that process is largely complete now, at least for the most important public APIs, so I think we are at a good stage to start working through the remaining changes systematically.

While we have added pydocstyle on a per-file basis up to now, not all of pydocstyle's rules make sense for us since matching pandas is a higher priority. As a result, going forward we will enable rules on a per-rule rather than per-file basis so that we can get the most value out of pydocstyle without conflicting with preexisting pandas conventions. For more extensive discussion of different rules and why we choose to enable some or not, see the [excellent table compiled by @bdice a few comments below.](https://github.com/rapidsai/cudf/issues/10711#issuecomment-1108886629).",2022-04-21T23:27:15Z,0,0,Vyas Ramasubramani,@rapidsai,True
275,Additional tests for cudf::distinct using the new row hasher,"#10641 adds new a new row hasher capable of natively handling nested struct and list types. The new hasher was tested by incorporating it into the implementation of `cudf::distinct`. Now that the functionality for `cudf::distinct` has been expanded by the use of this new hasher, we should add tests for some new cases, as was pointed out in some discussion threads on that PR:
- [X] We should add tests of tests with sliced input for both structs and lists input columns (https://github.com/rapidsai/cudf/pull/10641#discussion_r858037209)
- [X] We should add tests for simple nested cases like `List<Struct<...>>` and `Struct<List<...>>` to ensure that the new nesting behavior really works as expected (https://github.com/rapidsai/cudf/pull/10641#discussion_r853494994)
- [ ] We should add at least one test of a more complex nested case (e.g. `List<List<Struct<List<...>>>>`) to potentially help catch unexpected errors (https://github.com/rapidsai/cudf/pull/10641#discussion_r853494994)

Adding these tests will be helpful to validate the behavior of the new row hasher.",2022-04-26T20:06:35Z,0,0,Vyas Ramasubramani,@rapidsai,True
276,[FEA] Make cudf::ast::operation own its subexpressions,"**Is your feature request related to a problem? Please describe.**
Currently users are responsible for keeping all subexpressions of an `expression` alive. In particular, `operation`s do not own their operands, which are stored as a `std::vector<std::reference_wrapper<expression const>> const`. This approach is cumbersome because it forces client code to keep track of all previously created expressions to avoid their premature destruction. 

**Describe the solution you'd like**
Instead of storing a vector of references to its operands, an `operation` should instead store a vector of unique pointers to copies of the operands. While this change would create duplicate `expression`s, they are cheap to carry around, and this change would simplify the management of `expression` lifetimes in all client code.

**Describe alternatives you've considered**
We could use `std::shared_ptr` instead of using `std::unique_ptr`, which would allow subexpression reuse. Reuse isn't that important here though because `expression`s are relatively small and only use host memory. The primary goal is to simplify the management of `expression` objects, which is simpler with `unique_ptr` than `shared_ptr`.

We could also retain the status quo, which is not unworkable, just more complex and less intuitive than necessary. The fact that higher-level clients built on top of libcudf likely to build different ownerships models into their wrappers of expression APIs indicates that the libcudf approach is not the friendliest API for programmatic expression construction.

**Additional context**
This change would not change that a`literal` only stores a reference to the underlying `scalar`. Creating a `scalar` involves a device memory allocation, an expensive operation that we should not hide from a user. Additionally, it is reasonably straightforward for client code to wrap a literal in some owning container that handles scalar allocation if needed, whereas maintaining all subexpressions of potentially arbitrarily complex expressions is a significantly more onerous task.",2022-04-26T20:58:28Z,0,0,Vyas Ramasubramani,@rapidsai,True
277,[FEA] Support cudf.DataFrame.corrwith ,"**Is your feature request related to a problem? Please describe.**
Developer request to use 'corrwith' in cudf.DataFrame and it is the basic block for their business. 

**Describe the solution you'd like**
Similar in pandas.DataFrame.corrwith(https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corrwith.html#pandas.DataFrame.corrwith), understand its realization in pandas and try to make it happen in cudf. ",2022-04-28T13:53:32Z,0,0,,,False
278,[FEA] Additional cloud storage support in `ParquetDatasetWriter`,"**Feature request**
As part of https://github.com/rapidsai/cudf/pull/10769, `s3` support was added to `ParquetDatasetWriter`. This is a request to add other generic cloud provider support such that `gcfs` too is handled.

",2022-05-04T15:39:40Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
279,[FEA] Story - Supporting Approximate Count Distinct,"This issue tracks the dependencies for supporting approximate count distinct using [HyperLogLog](https://www.wikiwand.com/en/HyperLogLog) algorithm.

 * Design doc (WIP): https://docs.google.com/document/d/1ezAXhH7GejuaNDvIyqMBYfmsSypcDWinl2_tmLJd-O4",2022-05-04T20:04:24Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
280,[ENH]: serialization schema cleanup,Followup from #10784. Hyphens and underscores are used inconsistently when separating names in metadata keys in `serialize`; go through and standardise on one choice (hyphens seem more popular).,2022-05-05T16:45:28Z,0,0,Lawrence Mitchell,,False
281,Reduce usage of `import cudf` internally,"Came up during https://github.com/rapidsai/cudf/pull/10791, we should work towards an architecture where we don't need to fully import the top level `cudf` namespace in internal areas of the code. Doing so creates a state that is a little prone to circular import issues especially in python and goes against the general onion model of dependencies. 

We do this kind of [everywhere](https://github.com/rapidsai/cudf/search?q=%22import+cudf%22) so I'd suggest doing this kind of from the inside out starting with the most base level classes and then working towards dataframe and series. If we find it's not possible to decouple things at some point we can revisit. ",2022-05-10T14:33:45Z,0,0,,NVIDIA,True
282,[BUG] can't set groupby transform output to a new column,"**Describe the bug**
I try to use groupby count transform for frequency encoding of a column. I get an error when I do it:

> ValueError: Cannot align indices with non-unique values


**Steps/Code to reproduce bug**
`df[""a_freq""] = df.groupby([""a""])[""b""].transform(""count"")
`
This throws an error.
Setting the column in advance with zero or using values work as workarounds.
```
df[""a_freq""] = 0
df[""a_freq""] = df.groupby([""a""])[""b""].transform(""count"")
```
`df[""a_freq""] = df.groupby([""a""])[""b""].transform(""count"").values`

**Expected behavior**
I expect it to work as it works in pandas.

**Environment overview (please complete the following information)**
 - Environment location: local
 - Method of cuDF install: conda

**Environment details**
conda environment with RAPIDS 22.02
",2022-05-11T10:01:53Z,0,0,Ahmet Erdem,,False
283,[FEA] Disable CuFileTest.java tests automatically on systems without GDS,"**Is your feature request related to a problem? Please describe.**
I wish I don't need to remember to exclude CuFIleTest as discussed in https://github.com/NVIDIA/spark-rapids-jni/issues/238#issuecomment-1124612977 on a system without GDS

**Describe the solution you'd like**
Implement a Maven profile activated on a file pattern indicating a system with GDS: e.g  `gdsio` or `gdscheck` https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#verify-suc-install that adds CuFileTest .

**Describe alternatives you've considered**
Keep replicating CLI override `-Dtest=*,!CuFileTest`

**Additional context**
NVIDIA/spark-rapids-jni#238",2022-05-12T21:53:09Z,0,0,Gera Shegalov,@NVIDIA,True
284,[BUG] CuFileJni.cpp should include file path to Exception messages for diagnostics,"**Describe the bug**
Error messages in CuFileJni.cpp don't include path leave user wondering what filesystem causes issues

**Steps/Code to reproduce bug**
Run CuFileTest on a system without GDS NVIDIA/spark-rapids-jni#238


**Expected behavior**
Expect to see the file path to which the error reported pertains

**Environment overview (please complete the following information)**
 - docker in spark-rapids-jni, bare metal

**Environment details**
N/A

**Additional context**
NVIDIA/spark-rapids-jni#238",2022-05-12T22:01:52Z,0,0,Gera Shegalov,@NVIDIA,True
285,[ENH] Support more input data layouts in `cudf.from_dlpack`,"Related to #10754, the current implementation of `from_dlpack` requires unit-stride fortran order, and produces appropriate error messages in the unsupported cases

Consider

```python
import cudf
import cupy
a = cupy.arange(10)
b = a[::2]
c = cudf.from_dlpack(b.__dlpack__())
=> RuntimeError: from_dlpack of 1D DLTensor only for unit-stride data
b = cupy.broadcast_to(a[1], (10,)) # b is stride-0
=> RuntimeError: from_dlpack of 1D DLTensor only for unit-stride data

a = cupy.arange(12).reshape(3, 4).copy(order=""F"")
b = a[::2, :]
c = cudf.from_dlpack(b.__dlpack__())
=> RuntimeError: from_dlpack of 2D DLTensor only for column-major unit-stride data
```

Since `from_dlpack` copies in all cases right now, I think that things can be handled like so:

1. Non-fortran-order: useful error
2. unit-stride: current `cudaMemcpyAsync` one column at a time
3. fastest-dimension is stride-0 (broadcasted arrays): `std::fill` for the 1D case, just getting the strides right for the 2D case
4. fastest-dimension is stride-N (sliced arrays): `cudaMemcpy2DAsync` with appropriate choices of pitch and stride for the source array

However, I'm not really sure of the performance implications of these choices, and if the current approach of producing an error and requiring that the caller copy to contiguous fortran-order first before calling `from_dlpack` is not better. For example, for case 4 is it faster to copy to a contiguous buffer first rather than copying column by column?",2022-05-13T13:56:20Z,0,0,Lawrence Mitchell,,False
286,[FEA] Ability to control the amount of temporary memory used for regex expressions,"**Is your feature request related to a problem? Please describe.**
Regular expression processing can require a significant amount of temporary memory.  The RAPIDS Accelerator for Apache Spark needs the ability to control how much GPU memory is used for these operations in order to avoid excessive spilling or GPU out of memory errors when the user provides a particularly complicated regex pattern and/or large input data.

**Describe the solution you'd like**
The libcudf regular expression APIs accept an optional parameter to be specified which is an upper bound on the amount of temporary GPU memory to use for regular expression processing.  If the value is below the ""natural"" size for full concurrency, the algorithm would reduce the concurrency to fit within the memory bound.  I would expect there would be a lower-limit below which regex processing would not be possible within the requested memory limit.

**Describe alternatives you've considered**
Instead of APIs focused on limiting memory there could be APIs to report what will be used without the ability to control it, such as the one implemented in #10808.  This type of API does not allow the caller to tradeoff between GPU memory usage and GPU performance, as it either will fit in GPU memory or it won't.  If reported as too big the RAPIDS Accelerator would be forced to fallback to the CPU to perform the regex processing (with the requisite columnar to row formatted data transform and back).

The RAPIDS Accelerator currently does not support falling back to the CPU after query planning has completed on the Spark driver (which does not have a GPU), and the query planning does not have access to the string data to search (only the regex pattern to use).  Even with a memory size reporting API, without the input data the API would have to be a worst-case estimate that could cause an unnecessary fallback to the CPU.
",2022-05-13T19:12:00Z,0,0,Jason Lowe,NVIDIA,True
287,[BUG] OOM when invoking normalize_characters on a relatively small dataframe,"**Describe the bug**
Hi, I am facing the following OOM when invoking normalize_characters() on a relatively small dataset, which uses less than 2 GB of VRAM on a 16GB v100.

In case it helps, normalize_spaces() works without issues in the same dataframe.

Please, find below how to reproduce it.
 
![image](https://user-images.githubusercontent.com/26169771/168416343-b712b4bf-e92b-44b4-babb-6f303130a87e.png)

**Steps/Code to reproduce bug**

> import cudf
> import pandas as pd
> 
> df = cudf.DataFrame({""text"": pd.util.testing.rands_array(256, 5000000)})
> #df[""text""] = df[""text""].str.normalize_spaces()
> df[""text""] = df[""text""].str.normalize_characters(do_lower=False)
> 

**Expected behavior**
No OOM

**Environment overview (please complete the following information)**
 - Environment location: [Bare-metal, Docker, Cloud(specify cloud provider)]
 - Method of cuDF install: [conda, Docker, or from source]
   - If method of install is [Docker], provide `docker pull` & `docker run` commands used

**Environment details**
DGX-v100, cudf 22.04
",2022-05-14T07:50:56Z,0,0,Miguel Martínez,NVIDIA,True
288,[BUG] Performance difference between cudf and dask_cudf when reading jsonl files,"**Describe the bug**
Hi, I have noticed a difference in performance when reading a jsonl file with cudf and dask_cudf.

In both cases, I will be using only 1 GPU.

I have the following files (see details below):
 - jsonl_cudf.py 
 - jsonl_dask_cudf.py 

Please find below the execution time when I run them on a DGX1 v100 (16GBs):
```
(rapids) root@6ccf9a94ad0e:/rapids/notebooks/host# python jsonl_cudf.py 
4.183666706085205
(rapids) root@6ccf9a94ad0e:/rapids/notebooks/host# python jsonl_dask_cudf.py
6.8754589557647705
```

The scripts content is as follows:
`json_cudf.py`

```
import cudf
import time

start = time.time()
df = cudf.read_json(""x00_002GB.jsonl"", lines=True)
end = time.time()
print(end - start)
```

and
`jsonl_dask_cudf.py`

```
import dask_cudf
import time

start = time.time()
df = dask_cudf.read_json(""x00_002GB.jsonl"", lines=True)
end = time.time()
print(end - start)
```

**Steps/Code to reproduce bug**
Hi @shwina , as discussed in the Slack channel, I will send you an email with the link to the dataset used. Thanks!

**Expected behavior**
Not such a huge difference in performance.

**Environment overview (please complete the following information)**
DGX-A100, cuda 11.5, rapids 22.04",2022-05-16T18:07:28Z,0,0,Miguel Martínez,NVIDIA,True
289,[FEA] Don't copy data in to/from_dlpack when unnecessary,"To `.to_dlpack()` and `.from_dlpack()` methods in cuDF currently always perform a copy to/from the DLTensor. This is reasonable for DataFrames, as the columns of a dataframe in cuDF are not contiguous in memory, nor are they always of the same data type.

For a Series however, I believe we should be able to zero-copy to and from DLPack. That is not the case today:

```python
>>> import cudf
>>> import cupy as cp
>>> s = cudf.Series([1, 2, 3])
>>> arr = cp.from_dlpack(s.to_dlpack())
>>> s._column.data.ptr
139742968545280
>>> arr.data.ptr
139742968545792
```",2022-05-17T13:03:27Z,0,0,Ashwin Srinath,Voltron Data,False
290,[FEA] Add overloads for row comparator that automatically handle comparisons between rows of a table with a scalar,"Sometimes, we want to compare rows of a table with a single element given as a scalar. In order to do that, at the caller site, we have to convert the scalar into a column of one row, then convert that column into a table, then pass both tables into the comparator. This is repetitive and tedious.

We should add overloads for the row comparator that can automatically do this. The caller just needs to pass in the input table and input scalar.",2022-05-19T04:00:28Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
291,[FEA] Add support to str.normalize_spaces and str.normalize_characters in dask_cudf,"**Is your feature request related to a problem? Please describe.**
`str.normalize_spaces` and `str.normalize_characters` are not supported in dask_cudf . It would be great to expose them via dask_cudf.

**Describe the solution you'd like**
To have those methods available.

**Describe alternatives you've considered**
None

**Additional context**
Using RAPIDS 22.04 in a DGX-1 server",2022-05-19T22:35:59Z,0,0,Miguel Martínez,NVIDIA,True
292,"[FEA] Introduce distributed computing stages, then combine groupby and reduction aggregations","**Background.**
Distributed computing aggregations are typically performed in 3 stages:
1. Update: Computes intermediate results at each node.
2. Merge: Merge multiple intermediate results of the update stages from different nodes.
3. Evaluate: Compute the final result of the aggregation. 

Only the result of the last stage is what the users want to get. The intermediate results are typically used internally by the library and do not need to be exposed to the users.

However, currently in libcudf, for several aggregations, we have implemented separate public aggregations for each of these stages. Let's look at several aggregations: 
 * `M2` and `MERGE_M2`
 * `TDIGEST` and `MERGE_TDIGEST`
 * `COLLECT_LIST` and `MERGE_LISTS`
 * `COLLECT_SET` and `MERGE_SETS`

These aggregations generate only (intermediate) results that must be used together to generate the final result. Thus, it makes more sense to unify them together so the intermediate results of one aggregation class can be processed by the same class in the next stage.

**Describe the solution**
We should only provide just one public aggregation for each kind of operation that has the right and meaningful name. For example, just `STANDARD_DEVIATION` aggregation that can perform all the `Update`, `Merge`, and `Evaluate` stages. Upon constructing an instance of the aggregation, we pass in a parameter specifying which stage the aggregation should do its job. Such parameter can be something like this:
```
enum class distributed_computing_stage {
UPDATE,
MERGE,
EVALUATE,
ALL_IN_ONE // Generate the final result directly in just one pass (no distributed computing supported)
};
```

So we will construct the aggregation like this:
```
template <typename Base = aggregation>
std::unique_ptr<Base> make_std_aggregation(distributed_computing_stage stage = ALL_IN_ONE, size_type ddof = 1);
```

**Benefits**
The architecture I propose here can make the aggregations sound more meaningful. For example, we have a `STANDARD_DEVIATION` aggregation that will produce its own intermediate results, which will be merged by the same `STANDARD_DEVIATION` aggregation class, and the final result can be computed by the same `STANDARD_DEVIATION` aggregation class. It makes much more sense than computing the intermediate results by calling `M2` aggregation, then calling `MERGE_M2` aggregation, then evaluating the final result.

It also can simplify the implementation of aggregations a lot. It allows to reduce the number of classes, reducing the number of factory methods (`make_xxx_aggregation`), reducing the number of related methods (like `std::vector<std::unique_ptr<aggregation>> simple_aggregations_collector::visit` and `void aggregation_finalizer::visit`) etc.",2022-05-21T04:38:00Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
293,[FEA] Allow SQL-like 3VL for segmented_reduce,"**Is your feature request related to a problem? Please describe.**
I wish I could use libcudf's segmented_reduce without much pre/post-processing to implement SQL 3VL 

Current implementation is ""null begets null"" with https://docs.rapids.ai/api/libcudf/stable/group__aggregation__reduction.html#gae36b126703c20e1836f5eb02adaa965d
> <html>
<body>
<!--StartFragment-->

null_handling | If INCLUDE, the reduction is valid if all elements in a segment are valid, otherwise null
-- | --


<!--EndFragment-->
</body>
</html>

**Describe the solution you'd like**
In SQL 3VL, a valid result is returned when the result does not depend on the ""unknowns"" .  E.g. given in Spark
```
>>> sql(""select null or true"").show()
+--------------+
|(NULL OR true)|
+--------------+
|          true|
+--------------+
```
by extension one would expect `ANY` aggregation which is just OR-ing to be valid `true` if one of the segment values is 1 regardless of presence of NULLs.

Similar argument can be made for min/max aggregations if the segment contains a minimum/maximum value for the type such as INT_MIN/INT_MAX  

**Describe alternatives you've considered**
Run multiple reductions https://github.com/NVIDIA/spark-rapids/blob/branch-22.06/sql-plugin/src/main/scala/com/nvidia/spark/rapids/higherOrderFunctions.scala#L371-L384 to implement 3VL

**Additional context**
[spark.sql.legacy.followThreeValuedLogicInArrayExists](https://spark.apache.org/docs/latest/sql-migration-guide.html)",2022-05-26T02:14:43Z,0,0,Gera Shegalov,@NVIDIA,True
294,[FEA]Support for list columns,"**PROBLEM DESCRIPTION**

cudF doesn't support applymap or applying using defined functions to columns containing list as cell values. Also, cudf can not save to_csv when dataframe has list columns. Both of these are supported in Pandas. However, switching back and forth between pandas and cudf is time-consuming for huge amount of datas, and it will be great if we can bring more support for list column dataframe

**SOLUTION**
I want to apply user defined functions to cudF columns with lists. The functions are not simple function with one arguement but more complex operation. Moreover I want to save cudf dataframe to csv or hdf5 without switching to pandas for the same type of dataframe.

**Alternatives**
One alternative is to switch to pandas which is not ideal as I want to do everything in GPU. Another alternate solution I tried is by loading the list columns into numpy array and then apply the user defined function and then write them back to the dataframe which is again time-consuming. It will be way easier if I can just apply the function to the column directly.


",2022-05-31T12:20:12Z,0,0,Arpan Das,The École polytechnique fédérale de Lausanne,False
295,[FEA] Split `experimental/row_operators.cuh`,"Currently, `experimental/row_operators.cuh` file is huge: over 1200LOC and is growing. Including the entire file into a source file that only uses a small part of it is burdensome. I also feel dizzy while navigating it trying to find a struct/function of interest.

We should split it up (and re-organize files) into separate files like:
 * `experimental/row_operators/common.cuh`
 * `experimental/row_operators/lexicographic.cuh`
 * `experimental/row_operators/equality.cuh`
 * `experimental/row_operators/hashing.cuh`
 * ...",2022-05-31T18:05:23Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
296,[FEA] Experiment try to unify implementation of non-nested types and nested types in `cudf::contains`,"Currently, `cudf::contains` implements two separate code paths for non-nested types and nested types. That requires more effort to maintain all of them, comparing to just have one code path that can handle both.

Recently, there was an attempt to use just one code path to handle all data type (ref: https://github.com/rapidsai/cudf/pull/10770#discussion_r880980883) in groupby key hashing, which reported no performance regression on basic data types. As such, we should also test and try to unify the two code paths of `cudf::contains`.

Having just one code path that handles everything, maintenance will be much easier.",2022-06-01T01:45:49Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
297,[FEA] Alternate design for owning/non-owning comparators ,"**Is your feature request related to a problem? Please describe.**

The new experimental row operators require non-trivial preprocessing that involves new allocations whose lifetime must be maintained while attempting to do any row-wise operations on the specified data. 

To manage this, we introduced owning and non-owning comparator types.

For example, `self_comparator` is an owning type that handles doing row-wise operations on a single table. 

`self_comparator` is not a binary callable object (it doesn't have an `operator()`). Creating the actual non-owning callable object is currently done through a member factory function of the owning type (renamed in https://github.com/rapidsai/cudf/pull/10870).

```
table_view input_table{...};
self_comparator s{input_table}; // ""Expensive"" construction that does necessary pre-processing and allocations
auto callable = s.less(); // ""Cheap"" factory that returns a binary callable suitable for passing to algorithms like thrust::sort
thrust::sort(..., callable);
```
After reviewing code using this functionality I've noticed that the callable being returned from a member of the owning type is a bit awkward. For instance the `s.less()` call above isn't immediately obvious that this is actually a factory returning a callable function object. One way to remedy that could be to call it `s.make_less()` instead, but I think there's an all together better way. 

**Describe the solution you'd like**
Inspired by `std::` function objects like `std::less/std::equal_to`, I think we should make the callable objects currently being returned from functions like `less()/less_equivalent()/equal_to()` to instead be freestanding types that are _constructible from_ the owning types instead of being returned from a factory of the owning type. 

I think this would simplify the owning type as well as make the owner/viewer relationship more clear and explicit.

For example, the code above would become:
```
table_view input_table{...};
self_comparator s{input_table}; // ""Expensive"" construction that does necessary pre-processing and allocations
auto callable = cudf::row::less{s}; // ""Cheap"" construction that _views_ the internals of self_comparator
thrust::sort(..., callable);
```

Here's a high level sketch of how this idea could be implemented: https://godbolt.org/z/5fbK7PTb6

Salient points:
- `less` is a standalone type constructible from the `owning` type
- `less` is a `friend` of the owning type to access internals
- `less` deletes constructions from an r-value ref of the owning type to prevent construction from a temporary of the owning type. If allowed, this would lead to dangling references. 
- For simplicity, this sketch has `less::operator()` just invoke the `PhysicalComparator`. The actual implementation would have more layers. It would be roughly equivalent to what the internals of the `self_comparator::less` factory above does today. 


**Additional Thoughts**

I've come to believe that ""comparator"" is probably an inappropriate name for the owning types. It's not a comparator (not invokable), it just preprocesses and holds data needed by the actual comparators.

I don't have a good suggestion for a different name yet. ",2022-06-03T15:24:33Z,0,0,Jake Hemstad,@NVIDIA,True
298,Improve aggregations through an acyclic visitor pattern,"This issue is a longer description for a project originally proposed here: https://github.com/rapidsai/cudf/issues/10432#issuecomment-1067560695

## High-level overview

Several features in libcudf are implemented using “aggregations”: operations that act on multiple pieces of data. Aggregations enable a wide range of computations, including scans over a column of data, reductions over a column to find quantiles or standard deviations, reductions over grouped data, rolling (windowed) averages, or segmented reductions that can find the minimum value of each list in a column. Real-world examples include computing a rolling average of sales through the year, or average sales grouped by the day of the week. Some aggregations are simple to implement (like a sum), while others require data pre-processing (like sum-of-squares, which squares each element before performing the sum across elements) or data post-processing (like a mean, which sums across elements and then divides by the number of elements). Currently, the code in libcudf implements a [visitor pattern](https://en.wikipedia.org/wiki/Visitor_pattern) for the groupby and rolling code paths using aggregation, but not for other paths such as reductions and segmented reductions.

## Proposed changes

This issue proposes to:
- implement the visitor pattern style of groupby/rolling for additional forms of aggregation such as reductions and segmented reductions
- improve consistency and code reuse for aggregations across the library

The unified design should be benchmarked. Existing tests should be adapted to ensure complete coverage. Aggregation design documentation should be written for the libcudf developer docs so these patterns can be recognized and reused where appropriate.

Ideally, we would like to use an _acyclic visitor pattern_ to reduce the amount of boilerplate code needed. References:
- https://condor.depaul.edu/dmumaugh/OOT/Design-Principles/acv.pdf
- https://codecrafter.blogspot.com/2012/12/the-acyclic-visitor-pattern.html

See also this table overview of [aggregation algorithms and operations](https://gist.github.com/codereport/b747c8cad4026fa1defc422411afbcbc).",2022-06-06T21:48:35Z,0,0,Bradley Dice,@NVIDIA @rapidsai,True
299,Use CPU decompression for ORC footer with Zstandard compression,"Currently, nvCOMP is called to decompress ORC file footers that are compressed with ZSTD compression.
This is very likely slower than CPU decompression, especially with additional H2D and D2H copies.
We should investigate the perf gain from using a thrid party CPU library for this instead.",2022-06-07T19:30:28Z,0,0,Vukasin Milovanovic,NVIDIA,True
300,[FEA] Implement more efficient null check for `list_device_view`,"Currently `list_device_view` only has `is_null()` and `is_null(size_type)` API which do a full check for nulls. This may be expensive and inefficient if the input lists column doesn't contain any null elements.

We should implement a better way for that, shortcircuiting the computation if the input doesn't have nulls.",2022-06-07T21:06:17Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
301,[BUG] Using `.iloc[]` to set a single value in a Series does not do so in-place,"```python
>>> import  cudf
>>> s = cudf.Series([1, 2, 3])
>>> s2 = s.iloc[:]  # s2 is a view into s
>>> s.iloc[0] = -1  # this line does not behave as expected
>>> s
0   -1
1    2
2    3
dtype: int64
>>> s2
0    1
1    2
2    3
dtype: int64
```

Contrast with Pandas:

```python
>>> import pandas as pd
>>> s = pd.Series([1, 2, 3])
>>> s2 = s.iloc[:]  # s2 is a view into s
>>> s.iloc[0] = -1
>>> s
0   -1
1    2
2    3
dtype: int64
>>> s2
0   -1
1    2
2    3
dtype: int64
```

Note that setting  by slice works as expected:

```python
>>> import cudf
>>> s = cudf.Series([1, 2, 3])
>>> s2 = s.iloc[:]
>>> s.iloc[0:1] = -1
>>> s
0   -1
1    2
2    3
dtype: int64
>>> s2
0   -1
1    2
2    3
dtype: int64
```",2022-06-08T20:02:53Z,0,0,Ashwin Srinath,Voltron Data,False
302,[FEA] Support strong index types in other `cudf::experimental::row::` comparators and hashers,"When we have multiple tables to compare, we need strong index types. On the other hand, when we just do one table operations like comparing or hashing rows of the same table, we typically don't need these strong types.

However, sometimes we actually need to support strong types in these self- operations. For example, we create a hash map with strong index types. In such cases, row comparator and hasher need to operate on strong index types.",2022-06-10T15:42:37Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
303,[FEA] Replace `std::vector` by `host_span` in APIs parameter,"cudf currently has many APIs that take in `std::vector` as one of its input parameters. For example: https://github.com/rapidsai/cudf/blob/c01a2a41b7d57a1360324270101d9304fbc9515f/cpp/include/cudf/table/table_view.hpp#L235

We should replace the `std::vector` parameter by `host_span` wherever possible.",2022-06-15T23:47:37Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
304,[BUG] libcudf tests warn when linking when building in a conda environment without conda compiler metapackages,"When building libcudf and tests in a conda environment without having the compiler metapackages installed (`c-compiler`, `cxx-compiler`, etc.), it produces warnings about not finding `libz.so.1` needed by `libcudf.so`:
```
[406/674] Linking CXX executable gtests/COLUMN_TEST
/home/keith/miniconda3/envs/dev/bin/../lib/gcc/x86_64-conda-linux-gnu/10.3.0/../../../../x86_64-conda-linux-gnu/bin/ld: warning: libz.so.1, needed by libcudf.so, not found (try using -rpath or -rpath-link)
```

I believe this is due to both the `conda_env` target (https://github.com/rapidsai/cudf/blob/branch-22.08/cpp/CMakeLists.txt#L628) and the zlib target (https://github.com/rapidsai/cudf/blob/branch-22.08/cpp/CMakeLists.txt#L622) both being private links for the libcudf target. zlib is just one example of this, but this could happen for any other private target of libcudf that comes from the conda environment.

This isn't a problem for the compiler metapackages because they set a slew of compiler CLI flags, notably an `rpath-link` flag into the conda environment. I do not use these metapackages because those CLI flags set will point into the conda environment before pointing into the local build directory, which makes having both an install and a working directory a pain.

I believe the fix is either changing the `conda_env` link to be a `PUBLIC` link of the libcudf target, or adding a `conda_env` link to the tests.",2022-06-17T19:32:26Z,0,0,Keith Kraus,@VoltronData,False
305,[FEA] Get Series.list offsets / Construct Series of lists from offsets and values,"**Is your feature request related to a problem? Please describe.**
I would like to be able to access the offsets of a Series of lists. That would allow me to implement a function like `list_add` that takes two ""awkward arrays,"" Series of lists of numbers that have the same list shape, and adds them together. The binary operation can be straightforwardly applied to the ""leaves"" of each list column, which is the child column containing the data. However, to do this, I need a way to access indices and rebuild the list structure. For example, if `Series.list.offsets` and `cudf.Series.list.from_arrays(offsets, values)` existed, I could run something like:

```python
def list_add(s1, s2):
    """"""Take two Series of lists of numerical data and add them.""""""
    # Ignore nested lists for simplicity -- this only works for a single level of lists
    if s1.list.offsets!= s2.list.offsets:
        raise ValueError(""List columns must have corresponding offsets."")
    return cudf.Series.list.from_arrays(s1.list.offsets, s1.list.leaves + s2.list.leaves)
```

**Describe the solution you'd like**
- Implement a property `Series.list.offsets` that exposes the offset array, similar to [PyArrow's `pyarrow.ListArray.offsets`](https://arrow.apache.org/docs/python/generated/pyarrow.ListArray.html#pyarrow.ListArray.offsets) but returning a GPU-resident array.

- Implement a constructor `Series.list.from_arrays(offsets, values)` that builds a Series of lists from input offsets and values, similar to [PyArrow's `pyarrow.ListArray.from_arrays`](https://arrow.apache.org/docs/python/generated/pyarrow.ListArray.html#pyarrow.ListArray.from_arrays) but enabling construction from GPU-resident arrays.

**Describe alternatives you've considered**
I strongly prefer this approach over implementing binops directly on list types because it allows for precise control of what APIs are exposed and how they behave. Implementing binops for lists would allow for operators like `+` to be used, which is prone to error because it overloads the Python-like list semantics of ""adding is list concatenation"" with the array-like semantics of normal addition.

**Additional context**
It's not clear to me where the name ""leaves"" came from. To align with PyArrow, we would rename ""leaves"" to `Series.list.values`.",2022-06-21T21:05:08Z,0,0,Bradley Dice,@NVIDIA @rapidsai,True
306,[FEA] read_csv support header param with list input,"cudf.read_csv 's header param only accepts an int. This limits the use case where the DataFrame has a MultiIndex.
i.e.

`pd.read_csv('myfile.csv', index_col=0, header=[0,1])`
vs
`cudf.read_csv('myfile.csv', index_col=0, header=[0,1]) #fails since header is an int`

rapids 22.04+
",2022-06-22T21:07:06Z,0,0,,@NVIDIA,True
307,[FEA] first and last as hash based aggregates,"**Is your feature request related to a problem? Please describe.**
In a recent customer query I found that it was rather slow because they were doing a first aggregation along with a number of other small aggregations.  First is translated into an NTH_ELEMENT aggregation, either with or without null handling.  But NTH_ELEMENT is implemented as a sort based aggregation, not a hash based one.  This slows down the entire query. If I replace `first` with `max` the entire query gets to be 18% faster.

I know in the general case NTH_ELEMENT cannot be a hash based aggregation, but for common SQL operations like `first` and `last` we should be able to make it a hash aggregate.

**Describe the solution you'd like**
I personally would love for some magic to happen behind the scenes with NTH_ELEMENT where it can become a hash aggregate if `n` is `0` or `-1`.  But I would be happy to have some new FIRST and LAST aggregations instead, if that is simpler.

The algorithm I have been thinking about for first, is to do a min aggregation on a counting sequence starting at 0. Then we use the result of that as a gather map to pull in the first value from the original input column.  If we don't want to include nulls, then instead of using the counting iterator directly we also pull in the null mask from the original input column, and we replace nulls in the gather map with -1 before doing the gather.

For last we would switch it over to doing max aggregation instead of a min.

**Describe alternatives you've considered**
We could writ this ourselves, but the current Spark aggregation code does not make it simple to get access to the original input column after doing the aggregation. I can change that, but I didn't want to do that until I head from CUDF about how hard this might be.  Also the CUDF version is going to be more efficient, because I might not have to materialize as much data depending on how much can use thrust iterators to manipulate the original input data.

**Additional context**
Add any other context, code examples, or references to existing implementations about the feature request here.
",2022-06-23T15:57:14Z,0,0,Robert (Bobby) Evans,Nvidia,True
308,[BUG] cudf.DataFrame.all error on pivot-generated data frame,"**Describe the bug**
Getting an `UnboundLocalError: local variable 'index_class_type' referenced before assignment` when calling the cudf.DataFrame.all function. The data frame was created from comparing two data frames with the same columns and index attributes, one of which is generated from a call to cudf.DataFrame.pivot.

**Steps/Code to reproduce bug**
```Python
import cupy
import cudf

x = cupy.array([[1,2,3],[4,2,5]])
df = cudf.DataFrame(x)
pivot_table = df.pivot(index=[0], columns=[1], values=[2])
df2 = cudf.DataFrame(cupy.broadcast_to(pivot_table.iloc[:,0].values[:,None], pivot_table.shape), index=pivot_table.index, columns=pivot_table.columns)

print((df2 == pivot_table).all())
```

**Expected behavior**
Expected output:
```
2    True
dtype: bool
```

**Environment overview (please complete the following information)**
 - Environment location: Docker
 - Method of cuDF install: Docker
   - `docker run -it --rm --gpus all --ipc=host --network=host -v .`

**Environment details**
Version: 22.4.0a0+306.g0cb75a4913

**Additional context**
This snippet works as expected in cuDF 22.2.0 but does not work on cuDF 22.4.0.
",2022-06-24T20:37:57Z,0,0,Alex Xu,,False
309,[BUG] Timestamps displayed incorrectly due to Pandas not supporting >ns resolution,"The most minimal reproducer of the type of issues I'm seeing are:

```python
In [13]: cudf.Series([""2499-11-01 01:00:00""], dtype=""datetime64[s]"")
Out[13]:
0   1915-04-14 01:25:26.290448384
dtype: datetime64[s]

# indexing into a single element seems to work
In [14]: cudf.Series([""2499-11-01 01:00:00""], dtype=""datetime64[s]"").iloc[0]
Out[14]: numpy.datetime64('2499-11-01T01:00:00')
```

This is  most likely due to Pandas not being able to represent the timestamp accurately overflow occurring somewhere along the way.

Perhaps we should not rely on conversion to Pandas for displaying timestamps?",2022-06-27T19:15:24Z,1,0,Ashwin Srinath,Voltron Data,False
310,[FEA] Support segmented_sorted_order using an AST function for comparison or something similar,"**Is your feature request related to a problem? Please describe.**
Spark supports an `array_sort` SQL method that will sort arrays, but it takes a lambda function that calculates and returns the comparison between the elements of the array instead of going off of a config for doing the ordering.

The docs for this are at https://spark.apache.org/docs/latest/api/sql/index.html#array_sort

```
The comparator will take two arguments representing two elements of the array. It returns -1, 0, or 1 as the first element is less than, equal to, or greater than the second element. If the comparator function returns other values (including null), the function will fail and raise an error.
```

**Describe the solution you'd like**
I would love a segmented_sorted_order that takes an AST function similar to an inequality join and follows the rules described above. I am fine if it is more lenient than Spark requires and does not throw exceptions if the values do not fully conform to exactly what is requested here.

We may also need some extensions to what the AST can do.  Because of the tight rules that Spark has the implementations often use case/when and if/else expressions. We probably need to file a separate issue for this, but I wanted to call it out here because without it, there isn't a lot that we can do.

**Describe alternatives you've considered**
We don't really have any other alternative. We could do some code analysis of the function and see if it matches some known patterns for ascending/descending and nulls preceding or following. But this is very limiting, and we would prefer to do it as a performance improvement instead.
",2022-06-28T14:00:41Z,0,0,Robert (Bobby) Evans,Nvidia,True
311,[FEA] Support if/else in AST,"**Is your feature request related to a problem? Please describe.**
As a part of https://github.com/rapidsai/cudf/issues/11162 we really could use the ability to do if/else statements in AST.

**Describe the solution you'd like**
Just like I said. It would be great to have a way to do if/else statements in AST.  Takes a boolean value, a true expression and a false expression as input. If the boolean predicate is true then the true expression is evaluated and returned. If boolean predicate is false, then the false expression is evaluated and returned. For performance/consistency reasons I am fine if we evaluate both paths and just pick the one needed at the end.

**Describe alternatives you've considered**
We really have no good way to simulate this with existing operators, except in some very rare corner cases.
",2022-06-28T14:05:22Z,0,0,Robert (Bobby) Evans,Nvidia,True
312,[FEA] Extract some utilities function from `join_common_utils.cuh` into `cuco_helpers.cuh`,"In `join_common_utils.cuh`, there are several utilities functions that were created to work with cuco hash tables data structure. They are useful not just in join implementation but also somewhere else, like in search implementation. As such, we should extract there functions into a new file named `cuco_helpers.cuh`.",2022-06-30T16:13:35Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
313,[BUG] Dask_cudf merge function returns too few rows,"**Describe the bug**
The dask_cudf merge functions returns too few rows when both the dtype of the column being merged on is mismatched (eg: `int64` on the left and `int32` on the right) _and_ when `npartitions>1`
 
**Steps/Code to reproduce bug**
Here's a reproducer showing that when the dtype is mismatched the number of rows returned is dependent on the number of partitions in the dataframes being merged:
```python
import cupy as cp
import cudf
import dask_cudf

dfa = cudf.DataFrame({""a"":cp.random.randint(0,100,100000), ""b"":cp.random.normal(size=100000)})
dfb = cudf.DataFrame({""a"":cp.random.randint(0,100,100000), ""c"":cp.random.normal(size=100000)})

dfa[""a""] = dfa[""a""].astype(""int32"")
dfb[""a""] = dfb[""a""].astype(""int64"")

ddfa = dask_cudf.from_cudf(dfa, npartitions=4)
ddfb = dask_cudf.from_cudf(dfb, npartitions=4)
print(""npartitions:"")
print(""left: {}"".format(ddfa.npartitions))
print(""right: {}"".format(ddfb.npartitions))

print(""Number of rows in merge result:"")
print(len(ddfa.merge(ddfb, how=""inner"", on=""a"")))
print(""*""*30)

ddfa = ddfa.repartition(npartitions=3)
ddfb = ddfb.repartition(npartitions=3)
print(""npartitions:"")
print(""left: {}"".format(ddfa.npartitions))
print(""right: {}"".format(ddfb.npartitions))

print(""Number of rows in merge result:"")
print(len(ddfa.merge(ddfb, how=""inner"", on=""a"")))
print(""*""*30)

ddfa = ddfa.repartition(npartitions=2)
ddfb = ddfb.repartition(npartitions=2)
print(""npartitions:"")
print(""left: {}"".format(ddfa.npartitions))
print(""right: {}"".format(ddfb.npartitions))

print(""Number of rows in merge result:"")
print(len(ddfa.merge(ddfb, how=""inner"", on=""a"")))
print(""*""*30)

ddfa = ddfa.repartition(npartitions=1)
ddfb = ddfb.repartition(npartitions=1)
print(""npartitions:"")
print(""left: {}"".format(ddfa.npartitions))
print(""right: {}"".format(ddfb.npartitions))

print(""Number of rows in merge result:"")
print(len(ddfa.merge(ddfb, how=""inner"", on=""a"")))
print(""*""*30)
```
This returns:
```
npartitions:
left: 4
right: 4
Number of rows in merge result:
16083716
******************************
npartitions:
left: 3
right: 3
Number of rows in merge result:
35342145
******************************
npartitions:
left: 2
right: 2
Number of rows in merge result:
46959990
******************************
npartitions:
left: 1
right: 1
Number of rows in merge result:
100006687
******************************
```

**Expected behavior**
If we perform the same operation with just cudf we can see the expected result:
```python
import cupy as cp
import cudf

dfa = cudf.DataFrame({""a"":cp.random.randint(0,100,100000), ""b"":cp.random.normal(size=100000)})
dfb = cudf.DataFrame({""a"":cp.random.randint(0,100,100000), ""c"":cp.random.normal(size=100000)})

dfa[""a""] = dfa[""a""].astype(""int32"")
dfb[""a""] = dfb[""a""].astype(""int64"")

print(len(ddfa.merge(ddfb, how=""inner"", on=""a"")))
```
which returns:
```
100006687
```
which is the same as the dask_cudf version when `npartitions=1`

**Environment overview (please complete the following information)**
 - Environment location: Bare-metal
 - Method of cuDF install: conda


**Environment details**
```
cudf                      22.08.00a220629 cuda_11_py38_gff63c0a745_173    rapidsai-nightly
dask-cudf                 22.08.00a220629 cuda_11_py38_gff63c0a745_173    rapidsai-nightly
libcudf                   22.08.00a220629 cuda11_gff63c0a745_173    rapidsai-nightly
dask                      2022.6.1           pyhd8ed1ab_0    conda-forge
dask-core                 2022.6.1           pyhd8ed1ab_0    conda-forge
dask-cuda                 22.08.00a220630         py38_21    rapidsai-nightly
distributed               2022.6.1           pyhd8ed1ab_0    conda-forge
```

**Note**
The same thing occurs when `how={""left"", ""right"", ""outer""}`",2022-07-01T17:22:10Z,0,0,,,False
314,[BUG] cuIO benchmarks generate larger files than expected,"cuIO ORC and Parquet benchmarks generate larger files than previously (e.g. GDS blog data).
Some observations:

- [x] Only the cases where both cardinality and run length are set lead to small files.
- [ ] Cases where the data does not encode/compress well now generate files larger by ~20% (based on integral columns).",2022-07-06T13:41:01Z,0,0,Vukasin Milovanovic,NVIDIA,True
315,"[FEA] Add overload of cudf::slice that accepts host_span<pair<size_type, size_type>> ","**Is your feature request related to a problem? Please describe.**
Currently building up a list of selected indices for a slice requires doing something like:
```
std::vector<cudf::size_type> selected_ranges;
....
selected_ranges.push_back(slice_start);
selected_ranges.push_back(cur_row);
...
auto slices = cudf::slice(table_v, selected_ranges);
```

The problem is I need to use the selected ranges for other purposes. 
Specifically I have several related tensors that I want to slice the same rows from,
which requires me to do something like:
```
// todo assert ranges.size() is even
for (std::size_t i=0; i < ranges.size()-1; i += 2) 
{
    start = ranges[i];
    end = ranges[i+1];
}
```

**Describe the solution you'd like**
It would be nice to instead be able to do something like:
```
std::vector<std::pair<cudf::size_type, cudf::size_type>> selected_ranges;
selected_ranges.push_back(std::make_pair(slice_start, cur_row));
```",2022-07-07T23:57:56Z,0,0,David Gardner,@Nvidia,True
316,Enable numpydoc validation,"numpydoc supports [docstring validation](https://numpydoc.readthedocs.io/en/latest/validation.html) that we should enable to supplement things like pydocstyle checks. The numpydoc validation will help us with more semantic checking, such as ensuring that docstrings actually align with the function signatures that they are attached to.",2022-07-12T20:37:24Z,0,0,Vyas Ramasubramani,@rapidsai,True
317,[BUG] Loading a missing column from a Parquet file results in ArrayIndexOutOfBoundsException,"**Describe the bug**
Attempting to load a non-existent column from a Parquet file throws ArrayIndexOutOfBoundsException instead of a more specific error.

**Steps/Code to reproduce bug**
Try to load a single column name that does not exist from a Parquet file.  For example:
```
Table.readParquet(ParquetOptions.builder().includeColumn(""doesnotexist"").build(), new java.io.File(""data.parquet""))
```
throws the following exception:
```
java.lang.ArrayIndexOutOfBoundsException: 0
  at ai.rapids.cudf.Table.<init>(Table.java:96)
  at ai.rapids.cudf.Table.readParquet(Table.java:974)
  ... 49 elided
```

**Expected behavior**
A more useful error exception/message should be used than an array index error.
",2022-07-15T19:08:37Z,0,0,Jason Lowe,NVIDIA,True
318,[FEA] Potential missing performance in `partitioning.partition` (compared to `hash.hash_partition`),"A common pattern in dask is to shuffle distributed data around by some hash-based index. For example, this comes up in merging dataframes. Since the determination of index buckets is typically carried out independently from the splitting of the dataframe, this turns into calls to `libcudf.partitioning.partition`. The other option for this particular case would be to call `libcudf.hash.hash_partition`. The latter appears  to be signficantly (~5x) faster for large dataframes (code attached below, which partitions a dataframe with row columns and 100_000_000 rows on the first column into a configurable number of partitions, for the results below I used 10). Typical numbers of partitions for this use case are likely O(10-1000). Although this performance difference is not the order one cost in a distributed shuffle, flipping the switch from partition by index to partition by hash in dask-cuda provides a 10% speedup in some benchmarks (see rapidsai/dask-cuda#952).

```
$ python scatter-test.py
partition-by-indices: 52ms
partition-by-hash: 9.5ms
```

To help the timings for the `partition-by-indices` case, I only compute the indices to partition on once. Profiling with nsight shows this takes ~2.7ms. The `partition` call takes 52 ms (of which `scatter` takes 22ms), in contrast `hash-and-scatter` in one go via `hash_partition` takes 9.5ms. Since `partition` by indices needs to read an extra column (the indices), I might expect things to be a bit slower, but this large difference was a bit surprising.

There's a note in the partitioning code that it might make sense to avoid atomics:

https://github.com/rapidsai/cudf/blob/ec0b32bf73fc725982f62b0932782718d3886125/cpp/src/partitioning/partitioning.cu#L631

Aside: the pathological case of `npartitions == 1` is a factor of 2x slower for the partition-by-indices case, and 10x slower for partition-by-hash (probably worthwhile dispatching into a fast-path copy for that).

```python
import rmm
import cudf
import cudf._lib as libcudf
import cupy
import time


def build_dataframe(nrows):
    return cudf.DataFrame({""key"": cupy.arange(nrows),
                           ""value"": cupy.arange(nrows)})


def partition_by_indices(df, indices, npartitions):
    cols, offsets = libcudf.partitioning.partition(
        list(df._columns),
        indices,
        npartitions
    )
    return cols, offsets


def partition_by_hash(df, key, npartitions):
    cols, offsets = libcudf.hash.hash_partition(
        list(df._columns),
        [df._column_names.index(key)],
        npartitions
    )
    return cols, offsets


def run(*, nrows=10**8, with_pool=True, npartitions=10):
    rmm.reinitialize(pool_allocator=with_pool)
    df = build_dataframe(nrows)
    key = ""key""
    indices = (libcudf.hash.hash([df[key]._column], ""murmur3"") % npartitions)
    start = time.time()
    for _ in range(100):
        _ = partition_by_indices(df, indices, npartitions)
    end = time.time()
    print(f""partition-by-indices: {(end - start)*10:.3g}ms"")
    start = time.time()
    for _ in range(100):
        _ = partition_by_hash(df, key, npartitions)
    end = time.time()
    print(f""partition-by-hash: {(end - start)*10:.3g}ms"")


if __name__ == ""__main__"":
    run(nrows=10**8, with_pool=True, npartitions=10)
```

cc: @bdice, @shwina 

",2022-07-18T11:46:41Z,1,0,Lawrence Mitchell,,False
319,[BUG] Assignment of string columns using `df.loc` fails with type error.,"**Describe the bug**
Assignment of string columns using `df.loc` fails with type error.

**Steps/Code to reproduce bug**
```python
import cudf

values = list(['1', '2', '3', '4', '5', '6', '7', '8', '9', '0'])

df = cudf.DataFrame({
    'existing_column': values[0:10]
})

# succeeds when creating new string column
df.loc[df.index[0:5], 'letters'] = values[0:5]
print(df)

# fails with `ValueError: Unsupported dtype <U1` for existing string column
df.loc[df.index[5:10], 'letters'] = values[5:10]
print(df)
```

**Expected behavior**
expected string support when assigning slices of a DataFrame using `loc`.

**Environment overview**
 - Environment location: Bare-metal
 - Method of cuDF install: conda

**Additional context**
```python
$ python -c \
> ""
> import cudf
> 
> values = list(['1', '2', '3', '4', '5', '6', '7', '8', '9', '0'])
> 
> df = cudf.DataFrame({
>     'some_existing_column': values[0:10]
> })
> 
> df.loc[df.index[0:5], 'letters'] = values[0:5]
> print(df)
> 
> df.loc[df.index[5:10], 'letters'] = values[5:10]
> print(df)
> ""
  some_existing_column letters
0                    1       1
1                    2       2
2                    3       3
3                    4       4
4                    5       5
5                    6    <NA>
6                    7    <NA>
7                    8    <NA>
8                    9    <NA>
9                    0    <NA>
Traceback (most recent call last):
  File ""<string>"", line 13, in <module>
  File ""/home/charris/miniconda3/envs/morpheus/lib/python3.8/site-packages/cudf/core/dataframe.py"", line 143, in __setitem__
    return self._setitem_tuple_arg(key, value)
  File ""/home/charris/miniconda3/envs/morpheus/lib/python3.8/contextlib.py"", line 75, in inner
    return func(*args, **kwds)
  File ""/home/charris/miniconda3/envs/morpheus/lib/python3.8/site-packages/cudf/core/dataframe.py"", line 372, in _setitem_tuple_arg
    value = cupy.asarray(value)
  File ""/home/charris/miniconda3/envs/morpheus/lib/python3.8/site-packages/cupy/_creation/from_data.py"", line 66, in asarray
    return _core.array(a, dtype, False, order)
  File ""cupy/_core/core.pyx"", line 2164, in cupy._core.core.array
  File ""cupy/_core/core.pyx"", line 2243, in cupy._core.core.array
  File ""cupy/_core/core.pyx"", line 2315, in cupy._core.core._send_object_to_gpu
ValueError: Unsupported dtype <U1
```
",2022-07-19T16:52:10Z,0,0,Christopher Harris,Nvidia,True
320,[BUG] Interchange `Column.get_buffers()` erroneously raises for empty string columns,"When using `get_buffers()` on an [interchange protocol](https://data-apis.org/dataframe-protocol/latest/API.html) string column of size 0, an error raises. <sup>Mind I might be constructing string columns incorrectly?</sup>

```python
>>> df = cudf.DataFrame({""foo"": cudf.Series([], dtype=""U8"")})
>>> interchange_df = df.__dataframe__()
>>> interchange_col = interchange_df.get_column_by_name(""foo"")
>>> interchange_col.get_buffers()
Traceback (most recent call last)

  File .../cudf/lib/python3.9/site-packages/cudf/core/df_protocol.py:390, in _CuDFColumn.get_buffers(self)
      387     buffers[""validity""] = None
      389 try:
  --> 390     buffers[""offsets""] = self._get_offsets_buffer()
      391 except RuntimeError:
      392     buffers[""offsets""] = None

  File .../cudf/lib/python3.9/site-packages/cudf/core/df_protocol.py:453, in _CuDFColumn._get_offsets_buffer(self)
      444 """"""
      445 Return the buffer containing the offset values for
      446 variable-size binary data (e.g., variable-length strings)
    (...)
      450 offsets buffer.
      451 """"""
      452 if self.dtype[0] == _DtypeKind.STRING:
  --> 453     offsets = self._col.children[0]
      454     assert (offsets is not None) and (offsets.data is not None), "" ""
      455     ""offsets(.data) should not be None for string column""

IndexError: tuple index out of range
```

Columns with 1+ elements seem to work just fine. I'm assuming the bug stems from an assumption of non-empty columns?

EDIT: Thinking about it, maybe `get_buffers()` doesn't make sense for 0-sided dataframes? Need to see if there's been any previous discussion on the subject.

**Environment overview**
 - Environment location: locally, Ubuntu 20.04.4
 - Method of cuDF install: conda via `rapidsai-nightly`
 - cudf version: `22.08.00a+213.g002cb1c45b`

<details><summary>Click here to see environment details</summary><pre>
     
     **git***
     commit 8b3ae33c2874025c29c3747de1c3254eac516f9f (HEAD -> main, origin/main)
     Author: Matthew Barber <quitesimplymatt@gmail.com>
     Date:   Tue Jul 12 09:58:38 2022 +0100
     
     `--max-examples` flag, skip specific flaky cases on CI
     **git submodules***
     
     ***OS Information***
     DISTRIB_ID=Ubuntu
     DISTRIB_RELEASE=20.04
     DISTRIB_CODENAME=focal
     DISTRIB_DESCRIPTION=""Ubuntu 20.04.4 LTS""
     NAME=""Ubuntu""
     VERSION=""20.04.4 LTS (Focal Fossa)""
     ID=ubuntu
     ID_LIKE=debian
     PRETTY_NAME=""Ubuntu 20.04.4 LTS""
     VERSION_ID=""20.04""
     HOME_URL=""https://www.ubuntu.com/""
     SUPPORT_URL=""https://help.ubuntu.com/""
     BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
     PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
     VERSION_CODENAME=focal
     UBUNTU_CODENAME=focal
     Linux honno-pc 5.13.0-52-generic #59~20.04.1-Ubuntu SMP Thu Jun 16 21:21:28 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
     
     ***GPU Information***
     Tue Jul 12 14:43:34 2022
     +-----------------------------------------------------------------------------+
     | NVIDIA-SMI 515.48.07    Driver Version: 515.48.07    CUDA Version: 11.7     |
     |-------------------------------+----------------------+----------------------+
     | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
     | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
     |                               |                      |               MIG M. |
     |===============================+======================+======================|
     |   0  NVIDIA GeForce ...  On   | 00000000:01:00.0  On |                  N/A |
     | 40%   54C    P8    20W / 170W |   1312MiB / 12288MiB |      2%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     
     +-----------------------------------------------------------------------------+
     | Processes:                                                                  |
     |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
     |        ID   ID                                                   Usage      |
     |=============================================================================|
     |    0   N/A  N/A      1180      G   /usr/lib/xorg/Xorg                 35MiB |
     |    0   N/A  N/A      1865      G   /usr/lib/xorg/Xorg                135MiB |
     |    0   N/A  N/A      2019      G   /usr/bin/gnome-shell               66MiB |
     |    0   N/A  N/A      2480      G   /usr/lib/firefox/firefox          130MiB |
     |    0   N/A  N/A     10214      G   ...veSuggestionsOnlyOnDemand       68MiB |
     |    0   N/A  N/A     62041      C   ...3/envs/cudf/bin/python3.9      431MiB |
     |    0   N/A  N/A     65091      C   ..._playground/bin/python3.9      431MiB |
     +-----------------------------------------------------------------------------+
     
     ***CPU***
     Architecture:                    x86_64
     CPU op-mode(s):                  32-bit, 64-bit
     Byte Order:                      Little Endian
     Address sizes:                   39 bits physical, 48 bits virtual
     CPU(s):                          12
     On-line CPU(s) list:             0-11
     Thread(s) per core:              2
     Core(s) per socket:              6
     Socket(s):                       1
     NUMA node(s):                    1
     Vendor ID:                       GenuineIntel
     CPU family:                      6
     Model:                           165
     Model name:                      Intel(R) Core(TM) i5-10400F CPU @ 2.90GHz
     Stepping:                        5
     CPU MHz:                         2900.000
     CPU max MHz:                     4300.0000
     CPU min MHz:                     800.0000
     BogoMIPS:                        5799.77
     L1d cache:                       192 KiB
     L1i cache:                       192 KiB
     L2 cache:                        1.5 MiB
     L3 cache:                        12 MiB
     NUMA node0 CPU(s):               0-11
     Vulnerability Itlb multihit:     KVM: Mitigation: VMX unsupported
     Vulnerability L1tf:              Not affected
     Vulnerability Mds:               Not affected
     Vulnerability Meltdown:          Not affected
     Vulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable
     Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp
     Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
     Vulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling
     Vulnerability Srbds:             Mitigation; Microcode
     Vulnerability Tsx async abort:   Not affected
     Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx rdseed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp pku ospke md_clear flush_l1d arch_capabilities
     
     ***CMake***
     /snap/bin/cmake
     cmake version 3.23.2
     
     CMake suite maintained and supported by Kitware (kitware.com/cmake).
     
     ***g++***
     /usr/bin/g++
     g++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
     Copyright (C) 2019 Free Software Foundation, Inc.
     This is free software; see the source for copying conditions.  There is NO
     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
     
     
     ***nvcc***
     /usr/local/cuda-11.7/bin/nvcc
     nvcc: NVIDIA (R) Cuda compiler driver
     Copyright (c) 2005-2022 NVIDIA Corporation
     Built on Tue_May__3_18:49:52_PDT_2022
     Cuda compilation tools, release 11.7, V11.7.64
     Build cuda_11.7.r11.7/compiler.31294372_0
     
     ***Python***
     /home/honno/anaconda3/envs/cudf_playground/bin/python
     Python 3.9.13
     
     ***Environment Variables***
     PATH                            : /home/honno/anaconda3/envs/cudf_playground/bin:/home/honno/anaconda3/condabin:/usr/local/cuda-11.7/bin:/home/honno/.pyenv/shims:/home/honno/.pyenv/bin:/home/honno/go/bin:/usr/local/go/bin:/home/linuxbrew/.linuxbrew/bin:/home/linuxbrew/.linuxbrew/sbin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/honno/.dotnet/tools
     LD_LIBRARY_PATH                 :
     NUMBAPRO_NVVM                   :
     NUMBAPRO_LIBDEVICE              :
     CONDA_PREFIX                    : /home/honno/anaconda3/envs/cudf_playground
     PYTHON_PATH                     :
     
     ***conda packages***
     /home/honno/anaconda3/condabin/conda
     # packages in environment at /home/honno/anaconda3/envs/cudf_playground:
     #
     # Name                    Version                   Build  Channel
     _libgcc_mutex             0.1                 conda_forge    conda-forge
     _openmp_mutex             4.5                       2_gnu    conda-forge
     abseil-cpp                20210324.2           h9c3ff4c_0    conda-forge
     arrow-cpp                 8.0.0           py39h2f48f8a_4_cuda    conda-forge
     arrow-cpp-proc            3.0.0                      cuda    conda-forge
     asttokens                 2.0.5                    pypi_0    pypi
     aws-c-cal                 0.5.11               h95a6274_0    conda-forge
     aws-c-common              0.6.2                h7f98852_0    conda-forge
     aws-c-event-stream        0.2.7               h3541f99_13    conda-forge
     aws-c-io                  0.10.5               hfb6a706_0    conda-forge
     aws-checksums             0.1.11               ha31a3da_7    conda-forge
     aws-sdk-cpp               1.8.186              hb4091e7_3    conda-forge
     backcall                  0.2.0                    pypi_0    pypi
     bzip2                     1.0.8                h7f98852_4    conda-forge
     c-ares                    1.18.1               h7f98852_0    conda-forge
     ca-certificates           2022.6.15            ha878542_0    conda-forge
     cachetools                5.0.0              pyhd8ed1ab_0    conda-forge
     cuda-python               11.7.0           py39h5a03fae_0    conda-forge
     cudatoolkit               11.7.0              hd8887f6_10    conda-forge
     cudf                      22.08.00a220712 cuda_11_py39_g002cb1c45b_213    rapidsai-nightly
     cupy                      10.6.0           py39hc3c280e_0    conda-forge
     decorator                 5.1.1                    pypi_0    pypi
     dlpack                    0.5                  h9c3ff4c_0    conda-forge
     executing                 0.8.3                    pypi_0    pypi
     fastavro                  1.5.2            py39hb9d737c_0    conda-forge
     fastrlock                 0.8              py39h5a03fae_2    conda-forge
     fsspec                    2022.5.0           pyhd8ed1ab_0    conda-forge
     gflags                    2.2.2             he1b5a44_1004    conda-forge
     glog                      0.6.0                h6f12383_0    conda-forge
     grpc-cpp                  1.45.2               h3b8df00_4    conda-forge
     ipython                   8.4.0                    pypi_0    pypi
     jedi                      0.18.1                   pypi_0    pypi
     keyutils                  1.6.1                h166bdaf_0    conda-forge
     krb5                      1.19.3               h3790be6_0    conda-forge
     ld_impl_linux-64          2.38                 h1181459_1
     libblas                   3.9.0           15_linux64_openblas    conda-forge
     libbrotlicommon           1.0.9                h166bdaf_7    conda-forge
     libbrotlidec              1.0.9                h166bdaf_7    conda-forge
     libbrotlienc              1.0.9                h166bdaf_7    conda-forge
     libcblas                  3.9.0           15_linux64_openblas    conda-forge
     libcrc32c                 1.1.2                h9c3ff4c_0    conda-forge
     libcudf                   22.08.00a220712 cuda11_g002cb1c45b_213    rapidsai-nightly
     libcurl                   7.83.1               h7bff187_0    conda-forge
     libedit                   3.1.20210910         h7f8727e_0
     libev                     4.33                 h516909a_1    conda-forge
     libevent                  2.1.10               h9b69904_4    conda-forge
     libffi                    3.4.2                h7f98852_5    conda-forge
     libgcc-ng                 12.1.0              h8d9b700_16    conda-forge
     libgfortran-ng            12.1.0              h69a702a_16    conda-forge
     libgfortran5              12.1.0              hdcd56e2_16    conda-forge
     libgomp                   12.1.0              h8d9b700_16    conda-forge
     libgoogle-cloud           1.40.2               habd0e3a_0    conda-forge
     liblapack                 3.9.0           15_linux64_openblas    conda-forge
     libllvm11                 11.1.0               hf817b99_3    conda-forge
     libnghttp2                1.47.0               h727a467_0    conda-forge
     libnsl                    2.0.0                h7f98852_0    conda-forge
     libopenblas               0.3.20          pthreads_h78a6416_0    conda-forge
     libprotobuf               3.20.1               h6239696_0    conda-forge
     librmm                    22.08.00a220712 cuda11_g5ac89d17_57    rapidsai-nightly
     libssh2                   1.10.0               ha56f1ee_2    conda-forge
     libstdcxx-ng              12.1.0              ha89aaad_16    conda-forge
     libthrift                 0.16.0               h519c5ea_1    conda-forge
     libutf8proc               2.7.0                h7f98852_0    conda-forge
     libuuid                   2.32.1            h7f98852_1000    conda-forge
     libzlib                   1.2.12               h166bdaf_1    conda-forge
     llvmlite                  0.38.1           py39h7d9a04d_0    conda-forge
     lz4-c                     1.9.3                h9c3ff4c_1    conda-forge
     matplotlib-inline         0.1.3                    pypi_0    pypi
     ncurses                   6.3                  h27087fc_1    conda-forge
     numba                     0.55.2           py39h66db6d7_0    conda-forge
     numpy                     1.22.4           py39hc58783e_0    conda-forge
     nvtx                      0.2.3            py39h3811e60_1    conda-forge
     openssl                   1.1.1q               h166bdaf_0    conda-forge
     orc                       1.7.5                h6c59b99_0    conda-forge
     packaging                 21.3               pyhd8ed1ab_0    conda-forge
     pandas                    1.4.3            py39h1832856_0    conda-forge
     parquet-cpp               1.5.1                         2    conda-forge
     parso                     0.8.3                    pypi_0    pypi
     pexpect                   4.8.0                    pypi_0    pypi
     pickleshare               0.7.5                    pypi_0    pypi
     pip                       22.1.2             pyhd8ed1ab_0    conda-forge
     prompt-toolkit            3.0.30                   pypi_0    pypi
     protobuf                  3.20.1           py39h5a03fae_0    conda-forge
     ptxcompiler               0.4.0            py39h1eff087_0    conda-forge
     ptyprocess                0.7.0                    pypi_0    pypi
     pure-eval                 0.2.2                    pypi_0    pypi
     pyarrow                   8.0.0           py39h1ed2e5d_4_cuda    conda-forge
     pyflakes                  2.4.0                    pypi_0    pypi
     pyflyby                   1.7.7                    pypi_0    pypi
     pygments                  2.12.0                   pypi_0    pypi
     pyparsing                 3.0.9              pyhd8ed1ab_0    conda-forge
     python                    3.9.13          h9a8a25e_0_cpython    conda-forge
     python-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge
     python_abi                3.9                      2_cp39    conda-forge
     pytz                      2022.1             pyhd8ed1ab_0    conda-forge
     re2                       2022.06.01           h27087fc_0    conda-forge
     readline                  8.1.2                h0f457ee_0    conda-forge
     rmm                       22.08.00a220712 cuda11_py39_g5ac89d17_57    rapidsai-nightly
     s2n                       1.0.10               h9b69904_0    conda-forge
     setuptools                63.1.0           py39hf3d152e_0    conda-forge
     six                       1.16.0             pyh6c4a22f_0    conda-forge
     snappy                    1.1.9                hbd366e4_1    conda-forge
     spdlog                    1.8.5                h4bd325d_1    conda-forge
     sqlite                    3.39.0               h4ff8645_0    conda-forge
     stack-data                0.3.0                    pypi_0    pypi
     tk                        8.6.12               h27826a3_0    conda-forge
     traitlets                 5.3.0                    pypi_0    pypi
     typing_extensions         4.3.0              pyha770c72_0    conda-forge
     tzdata                    2022a                h191b570_0    conda-forge
     wcwidth                   0.2.5                    pypi_0    pypi
     wheel                     0.37.1             pyhd8ed1ab_0    conda-forge
     xz                        5.2.5                h516909a_1    conda-forge
     zlib                      1.2.12               h166bdaf_1    conda-forge
     zstd                      1.5.2                h8a70e8d_2    conda-forge
     
</pre></details>",2022-07-20T12:49:41Z,0,0,Matthew Barber,Hexegic,False
321,"[BUG] Interchange `Column.describe_categorical` is a tuple, not a dict","In the [interchange protocol](https://data-apis.org/dataframe-protocol/latest/API.html), `describe_categorical` should return a dict, but cuDF returns a tuple

```python
>>> df = cudf.DataFrame({""foo"": cudf.Series([0, 1], dtype=""category"")})
>>> interchange_df = df.__dataframe__()
>>> interchange_col = interchange_df.get_column_by_name(""foo"")
>>> interchange_col.describe_categorical
(False, True, {0: 0, 1: 1})
>>> type(interchange_col.describe_categorical)
tuple  # should be dict
```

Relevant code returning a tuple as opposed to dict

https://github.com/rapidsai/cudf/blob/edc5062bdcc3e12755603b0ad07a4d271fe95261/python/cudf/cudf/core/df_protocol.py#L295

This prevents interchanging dataframes with categorical columns, e.g. with pandas
```python
>>> from pandas.api.exchange import from_dataframe
>>> from_dataframe(df)
.../pandas/core/exchange/from_dataframe.py:184, in categorical_column_to_series(col)
    169 """"""
    170 Convert a column holding categorical data to a pandas Series.
    171 
   (...)
    180     that keeps the memory alive.
    181 """"""
    182 categorical = col.describe_categorical
--> 184 if not categorical[""is_dictionary""]:
    185     raise NotImplementedError(""Non-dictionary categoricals not supported yet"")
    187 mapping = categorical[""mapping""]
TypeError: tuple indices must be integers or slices, not str
```


pandas and modin are compliant here, but interestingly vaex currently returns a tuple too https://github.com/vaexio/vaex/issues/2113",2022-07-22T09:20:25Z,0,0,Matthew Barber,Hexegic,False
322,[FEA] Ability to write multiple files in `to_orc` in a similar file naming pattern to that of `to_csv`,"**What is your question?**
Hello guys,

I wonder if we can rename the files when we use to_orc in dask_cudf,  such as  `ddf.to_orc('name-*.orc')`? I think we are able to do it in `ddf.to_csv()`. Thanks",2022-07-25T15:31:59Z,0,0,Cg Lai,,False
323,Constructing `Column` objects from `cudf::column` should always take ownership of the null mask,This came out of discussions in #11354. The current approach that `Column.from_unique_ptr` drops a reference to the `cudf::column`s `null_mask` if `null_count == 0` can result in use-after-free if there are also `column_view`s referencing the `column`. See discussion in https://github.com/rapidsai/cudf/pull/11354#discussion_r930808913,2022-07-28T08:44:43Z,0,0,Lawrence Mitchell,,False
324,"[BUG] Interchange `Column.dtype` returns format strings in NumPy-style, instead of Arrow-style","In the [interchange protocol](https://data-apis.org/dataframe-protocol/latest/API.html), `Column.dtype` should return an [Arrow-style](https://arrow.apache.org/docs/format/CDataInterface.html#data-type-description-format-strings) format string, but instead a NumPy-styled one is returned

```python
>>> df = cudf.DataFrame({""foo"": cudf.Series([0, 1], dtype=""int8"")})
>>> interchange_df = df.__dataframe__()
>>> interchange_col = interchange_df.get_column_by_name(""foo"")
>>> interchange_col.dtype
(<_DtypeKind.INT: 0>, 8, '|i1', '|')  # 3rd element (format string) should be ""c""
```

It looks like currently the `.str` attribute of the dtype objects (i.e. `np.dtype(...)`) is returned as-is

https://github.com/rapidsai/cudf/blob/edc5062bdcc3e12755603b0ad07a4d271fe95261/python/cudf/cudf/core/df_protocol.py#L260",2022-07-28T13:08:05Z,0,0,Matthew Barber,Hexegic,False
325,[FEA] Change cudf::io::detail::make_column() to have a more verbose name.,"```
std::unique_ptr<column> make_column(column_buffer& buffer,
                                    column_name_info* schema_info,
                                    rmm::cuda_stream_view stream,
                                    rmm::mr::device_memory_resource* mr)
```

This is kind of a weakly named global function.   Because its name is so generic, it doesn't play nice with users who might want to create their own, more appropriately named `make_column` that does some other work along the way.  Changing this to `column_from_column_buffer()` or `make_column_from_buffer()`, or maybe making it a member of `column_buffer` would clean things up a bit

 Eg.

```
// user function (in parquet code for example)
make_column(column_buffer &b)
{
    // random preprocessing work
    auto x = column_from_column_buffer(b);
    // random postprocessing work
}
```",2022-07-28T16:10:13Z,0,0,,,False
326,Investigate need for output as binary configuration option,"It is possible that the output as binary options are not necessary as mentioned in this comment. Initial testing in the course of the PR showed list tests failing when using just physical type, so more investigation is needed.

_Originally posted by @vuule in https://github.com/rapidsai/cudf/pull/11328#discussion_r932696159_",2022-07-28T21:58:26Z,0,0,Mike Wilson,,False
327,`schema_tree_node` needs a constructor.,"Nitpick: Looks like `schema_tree_node` needs a constructor. :]

I would've recommended aggregate initialization, but I'm not sure how appropriate it would be, since it's initializing parent members:
```suggestion
        schema_tree_node col_schema {
          .type            = Type::BYTE_ARRAY,
          .converted_type  = ConvertedType::UNKNOWN,
          .stats_dtype     = statistics_dtype::dtype_byte_array,
          .repetition_type = col_nullable ? OPTIONAL : REQUIRED,
          .name = (schema[parent_idx].name == ""list"") ? ""element"" : col_meta.get_name(),
          .parent_idx  = parent_idx,
          .leaf_column = col
        };
```

My personal preference would be to avoid construct-then-initialize. But this is purely stylistic. Please feel free to ignore.

_Originally posted by @mythrocks in https://github.com/rapidsai/cudf/pull/11328#discussion_r932746165_",2022-07-28T23:26:36Z,0,0,Mike Wilson,,False
328,The example in the documentation for `get_dremel_data()` seems incorrect at line#1764,"I know this isn't part of the review, but the example in the documentation for `get_dremel_data()` seems incorrect at line#1738 (now [#1764](https://github.com/rapidsai/cudf/pull/11328/files#diff-e3891cd717ca174e53dd11bbf812ad3f13034d4c1a4626ea751f05bbed8470d5R1764)):
```c++
 * Given a LIST column of type `List<List<int>>` like so:
 * ```
 * col = {
 *    [],
 *    [[], [1, 2, 3], [4, 5]],
 *    [[]]
 * }
 * ```
 * We can represent it in cudf format with two level of offsets like this:
 * ```
 * Level 0 offsets = {0, 0, 3, 5, 6}
 * Level 1 offsets = {0, 0, 3, 5, 5}
 * Values          = {1, 2, 3, 4, 5}
 * ```
```
The `Level 0` offset values can't exceed `4`, since `Level 1` has only 4 ranges (i.e. 5 offsets).
I'll try make sense of this at a later date, but I'm not sure I could follow along.

_Originally posted by @mythrocks in https://github.com/rapidsai/cudf/pull/11328#discussion_r932727013_",2022-07-28T23:27:58Z,0,0,Mike Wilson,,False
329,[FEA] GPU-accelerated Feather support.,"**Is your feature request related to a problem? Please describe.**
A user with existing Feather data files asked about GPU-accelerated Feather support. cuDF currently supports CPU-based Feather reading but does not use GPU acceleration.

**Describe the solution you'd like**
`cudf.read_feather` should be GPU accelerated.

**Describe alternatives you've considered**
Converting files to another format like Parquet is probably ideal for performance and can leverage existing GPU accelerated I/O. This conversion can be done with pandas.

**Additional context**
Feather format description: https://github.com/wesm/feather/blob/master/doc/FORMAT.md",2022-07-29T20:32:10Z,0,0,Bradley Dice,@NVIDIA @rapidsai,True
330,[QST] Should byte_array_view in parquet reader/writer change,"**What is your question?**
Should [`byte_array_view`](https://github.com/rapidsai/cudf/blob/branch-22.08/cpp/src/io/statistics/byte_array_view.cuh) change to a different implementation method or even go away completely.

### Motivation
When reviewing the `byte_array_view` PR it was brought up in [review comments](https://github.com/rapidsai/cudf/pull/11322#discussion_r928012252) that things could be done differently and possibly better. This issue is an attempt to bring this design out in the light and get some discourse going so we can build it the best way possible. Jake was, rightfully, concerned about the cognitive overload of having another object type that has to be understood, no matter how minimal the type turns out to be.

### Backstory and origin
The original thought was that it would be nice to leverage the existing templates in the statistics code to get elements and compute max/min just like everything else. This meant that `.element` on a column would be able to return a type that represents a `list<uint8>`. This is almost identical to a string column, so the thought was to have something analogous to `string_view` that could be used. This was quickly dismissed due to the issue of not having all list columns comprised of this thing and it felt like we were forcing something. All string columns are lists of chars, but not all list columns are lists of bytes.

### Requirements
The requirements in the statistics code are the ability to get an element from a table, compare elements, and compose an element from a pointer and a length. The statistics code goes to great length to type-erase the statistics blobs so they can be easily consumed at a large scale on the GPU and the reconstructs them later. It also uses `thrust::min` and `cub::reduceBlock` to process them, so comparison operators are needed.

### Slippery issues to understand
We can't use the same statistics types as strings because `string_view::max()` is actually not the same as a max byte or a max `byte_array_view`. The distinction is subtle, but important between all of them.

- The max UTF8 string is actually just 5 bytes long and [defined inside the `string_view` header](https://github.com/rapidsai/cudf/blob/03f1c1c5c5fcf90bd594aabd41b6e15f54690777/cpp/include/cudf/strings/string_view.cuh#L75). No UTF8 string can have a higher value, so comparisons work even though it isn't an infinitely-long character string as one would initially think.
- Maximum value for an unsigned byte is obviously 255, but this isn't the what is intended when one asks for the max byte array view. Instead, the goal is to know the ""biggest"" one. This includes the length and the internal bytes. `0xff, 0x05` is less than `0xff, 0x15` and `0xff` is less than `0x00, 0x00`.
- Maximum `byte_array_view` is defined conceptually as an infinite array of 0xff. This isn't possible to statically define for comparison like the `string_view` class, so some [magic values](https://github.com/rapidsai/cudf/blob/03f1c1c5c5fcf90bd594aabd41b6e15f54690777/cpp/src/io/statistics/byte_array_view.cuh#L173) were used of a nullptr and max length. These then have to be [explicitly compared](https://github.com/rapidsai/cudf/blob/03f1c1c5c5fcf90bd594aabd41b6e15f54690777/cpp/src/io/statistics/byte_array_view.cuh#L101) later in the comparison function to achieve the proper results.

Lots of places required special handling for `byte_array_view` and potentially get worse with the different possible solutions. The goal of course is to make these areas as clean as possible, so I thought it would be good to point some of them out here.

 - [Here](https://github.com/rapidsai/cudf/blob/03f1c1c5c5fcf90bd594aabd41b6e15f54690777/cpp/src/io/statistics/column_statistics.cuh#L115) is where the code grabs the data from the column. There is conversion in here for types, which is used for things like duration and timestamps. Originally it was thought this could be a good spot to convert from a `list_view`, which can be returned from `.element` calls on a list column. This didn't end up being a great solution, but I can't remember the details.
 - min/max calculations and block reduce happens down in [typed_statistics_chunk](https://github.com/rapidsai/cudf/blob/03f1c1c5c5fcf90bd594aabd41b6e15f54690777/cpp/src/io/statistics/typed_statistics_chunk.cuh#L207). This code is responsible for figuring out min, max, null counts, and aggregations like sum. It has to pick up this new type and operate on it.
 - Actual data writing in parquet looks [something like this](https://github.com/rapidsai/cudf/blob/03f1c1c5c5fcf90bd594aabd41b6e15f54690777/cpp/src/io/parquet/page_enc.cu#L1094) where an element is grabbed and written into place.

### Possible solutions
1. Use `device_span` directly. This requires passing comparison functions to cub and thrust for the calculations, but is completely doable. This was [attempted](https://github.com/hyperbolic2346/cudf/tree/mwilson/test_byte_array_view_removal), potentially poorly, with not great looking results.
2. Composition vs inheritance. This came up multiple times as to why it was built with composition, holding a `device_span` inside, vs inheriting from `device_span` either publicly or privately. There isn't a great answer here to argue against inheritance. I originally thought that this would be a very small subset of `device_span` and I didn't want to muddy the waters with all the accessors and iterators, but after further inspection, I don't see anything that I would want to remove from `device_span`, so this would be a viable path. It does still hold the issue of cognitive overload of yet another type someone encounters.
3. Continues to live on as it is now.
4. Your amazing idea that didn't come up in development or review.",2022-07-29T21:19:39Z,0,0,Mike Wilson,,False
331,[FEA] ClusterfuzzLite integration,"**Is your feature request related to a problem? Please describe.**
cuDF has an extensive fuzzing suite that could benefit the project by running in the CI.

**Describe the solution you'd like**
In this issue I suggest [ClusterfuzzLite](https://google.github.io/clusterfuzzlite/) integration for cuDF. This would require:

1. Adding a `.github/workflows/cflite.yml` file.
2. Adding a .clusterfuzzlite directory with build files.
3. Rewriting the fuzzers into coverage-guided fuzzers via [Atheris](https://github.com/google/atheris).

**Describe alternatives you've considered**


**Additional context**
ClusterfuzzLite handles the management of running fuzzers in the CI when PRs are made. It has a number of features that are useful for projects with multiple fuzzers:

1. CFL reuses corpus so that the fuzzers don't start from scratch every run.
2. [Batch fuzzing](https://google.github.io/clusterfuzzlite/running-clusterfuzzlite/github-actions/#batch-fuzzing) will run the fuzzers periodically to look for harder-to-find bugs and build of the corpus.
3. Only fuzzers that cover code that is changed in PRs run in the CI.

I will be glad to take the lead on this one if it is of interest to cuDF.
",2022-08-01T11:07:28Z,0,0,,Adam@adalogics.com,False
332,[BUG] Multiple DataFrame.loc operations gives confusing error message upon compute on Dask-cuDF,"**Describe the bug**
After creating a Dask-cuDF data frame, if I perform multiple .loc operations on it using boolean Dask-cuDF series, then when I compute the data frame, it produces a runtime error with the message `cuDF failure at: ../src/stream_compaction/apply_boolean_mask.cu:73: Column size mismatch`. A similar snippet works as expected on cuDF.

**Steps/Code to reproduce bug**
```python
import dask_cudf
import cudf
ddf1 = dask_cudf.from_cudf(cudf.DataFrame({'a':[1,2,3], 'b':[4,5,6]}), npartitions=2)
f1 = dask_cudf.from_cudf(cudf.Series([False, True, True]), npartitions=2)
f2 = dask_cudf.from_cudf(cudf.Series([True, False]), npartitions=2)
ddf2 = ddf1.loc[f1]
ddf3 = ddf2.loc[f2]
print(ddf2.compute())
print(ddf3.compute())
```
The above code produces the following output:
```
   a  b                        
1  2  5
2  3  6                                                       
Traceback (most recent call last):     
  File ""temp.py"", line 9, in <module>
    print(ddf3.compute())
  File ""/opt/conda/lib/python3.8/site-packages/dask/base.py"", line 292, in compute
    (result,) = compute(self, traverse=False, **kwargs)
  File ""/opt/conda/lib/python3.8/site-packages/dask/base.py"", line 575, in compute
    results = schedule(dsk, keys, **kwargs)    
  File ""/opt/conda/lib/python3.8/site-packages/dask/local.py"", line 554, in get_sync
    return get_async(                                         
  File ""/opt/conda/lib/python3.8/site-packages/dask/local.py"", line 497, in get_async                                       
    for key, res_info, failed in queue_get(queue).result():
  File ""/opt/conda/lib/python3.8/concurrent/futures/_base.py"", line 437, in result
    return self.__get_result()
  File ""/opt/conda/lib/python3.8/concurrent/futures/_base.py"", line 389, in __get_result
    raise self._exception
  File ""/opt/conda/lib/python3.8/site-packages/dask/local.py"", line 539, in submit
    fut.set_result(fn(*args, **kwargs))
  File ""/opt/conda/lib/python3.8/site-packages/dask/local.py"", line 235, in batch_execute_tasks
    return [execute_task(*a) for a in it]
  File ""/opt/conda/lib/python3.8/site-packages/dask/local.py"", line 235, in <listcomp>
    return [execute_task(*a) for a in it]
  File ""/opt/conda/lib/python3.8/site-packages/dask/local.py"", line 226, in execute_task
    result = pack_exception(e, dumps)
  File ""/opt/conda/lib/python3.8/site-packages/dask/local.py"", line 221, in execute_task
    result = _execute_task(task, data)
  File ""/opt/conda/lib/python3.8/site-packages/dask/core.py"", line 119, in _execute_task
    return func(*(_execute_task(a, cache) for a in args))
  File ""/opt/conda/lib/python3.8/site-packages/dask/optimization.py"", line 990, in __call__
    return core.get(self.dsk, self.outkey, dict(zip(self.inkeys, args)))
  File ""/opt/conda/lib/python3.8/site-packages/dask/core.py"", line 149, in get
    result = _execute_task(task, cache)
  File ""/opt/conda/lib/python3.8/site-packages/dask/core.py"", line 119, in _execute_task
    return func(*(_execute_task(a, cache) for a in args))
  File ""/opt/conda/lib/python3.8/site-packages/dask/utils.py"", line 39, in apply
    return func(*args, **kwargs)
  File ""/opt/conda/lib/python3.8/site-packages/dask/dataframe/core.py"", line 6330, in apply_and_enforce
    df = func(*args, **kwargs)
  File ""/opt/conda/lib/python3.8/site-packages/dask/dataframe/methods.py"", line 37, in loc
    return df.loc[iindexer]
  File ""/opt/conda/lib/python3.8/site-packages/cudf/core/dataframe.py"", line 127, in __getitem__
    return self._getitem_tuple_arg(arg)
  File ""/opt/conda/lib/python3.8/site-packages/nvtx/nvtx.py"", line 101, in inner
    result = func(*args, **kwargs)
  File ""/opt/conda/lib/python3.8/site-packages/cudf/core/dataframe.py"", line 267, in _getitem_tuple_arg
    df = columns_df._apply_boolean_mask(tmp_arg[0])
  File ""/opt/conda/lib/python3.8/site-packages/cudf/core/indexed_frame.py"", line 1696, in _apply_boolean_mask
    libcudf.stream_compaction.apply_boolean_mask(
  File ""cudf/_lib/stream_compaction.pyx"", line 101, in cudf._lib.stream_compaction.apply_boolean_mask
RuntimeError: cuDF failure at: ../src/stream_compaction/apply_boolean_mask.cu:73: Column size mismatch
```

**Expected behavior**
Expected output (verified with cudf instead of dask-cudf):
```
   a  b
1  2  5
2  3  6
   a  b
1  2  5
```

**Environment overview (please complete the following information)**
 - Environment location: Docker
 - Method of cuDF install: Docker
   - `docker run -it --rm --gpus all --ipc=host --network=host -v .`

**Environment details**
cuDF version 22.4.0a0+306.g0cb75a4913",2022-08-02T17:46:45Z,0,0,Alex Xu,,False
333,[BUG] `cats` argument does not behave correctly for `cudf.get_dummies`,"**Describe the bug**
`cudf.get_dummies` does not function correctly when specified with `cats` 🐱.

**Steps/Code to reproduce bug**
```python
In [1]: import cudf
In [2]: df = cudf.DataFrame({'col': list('abcdef')})
In [4]: cudf.get_dummies(df, cats={""a"": ['a', 'c', 'f']})
Out[4]: 
   col_a  col_b  col_c  col_d  col_e  col_f
0      1      0      0      0      0      0
1      0      1      0      0      0      0
2      0      0      1      0      0      0
3      0      0      0      1      0      0
4      0      0      0      0      1      0
5      0      0      0      0      0      1
```

**Expected behavior**
Per documentation, `get_dummies` should only encode `a, c, f`. However, since [`pd.get_dummies`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) does not support this argument. My suggestion is that we should remove it.

**Additional Information**
Since libcudf one-hot-encoding API encodes the column in a single contiguous buffer. The size of the return buffer is limited by the maximum addressable column size of libcudf, `std::numeric_limit<cudf::size_type>::max`. The removal of this argument may prevent user from doing manual batching. Instead, Python API should gracefully handle this situation internally.",2022-08-02T20:22:20Z,0,0,Michael Wang,Nvidia Rapids,True
334,[FEA] Mark `DataFrame.dtypes` as `_external_only_api`,"**Is your feature request related to a problem? Please describe.**
`DataFrame.dtypes` is used in many places in the code. For pandas compatibility, this method constructs a `pd.Series` from the column dtypes. This construction introduces unnecessary overhead that could be avoided, especially because in many cases the output is immediately converted back to a list or a `{colname: dtype}` dict.

**Describe the solution you'd like**
`DataFrame.dtypes` should be decorated with `_external_only_api`. All usage should be switched to instead use `Frame._dtypes`, which simply constructs a dict and avoids the unnecessary overhead. Here's a quick indication of the benefits:
```
In [1]: import cudf

In [2]: df = cudf.DataFrame({f""{i}"": [i] for i in range(10)})

In [3]: %timeit df._dtypes
2.31 µs ± 15.8 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)

In [4]: %timeit df.dtypes
165 µs ± 314 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)

In [5]: df = cudf.DataFrame({f""{i}"": [i] for i in range(100)})

In [6]: %timeit df._dtypes
13.9 µs ± 47.3 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)

In [7]: %timeit df.dtypes
316 µs ± 3.54 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)

```

**Describe alternatives you've considered**
None

**Additional context**
If there is any internal functionality that is actually relying on the output of `dtypes` being a `Series`, we should carefully consider whether that method should be reimplemented. There is almost no reason that a `Series` should be preferable to a `dict` internally.
",2022-08-03T23:47:14Z,0,0,Vyas Ramasubramani,@rapidsai,True
335,[FEA] Mark `DataFrame.insert` as `_external_only_api`,"**Is your feature request related to a problem? Please describe.**
`DataFrame.insert` is an expensive operation because it triggers index equality checking and potentially index realignment, both of which are typically unnecessary when this function is used internally because the items are either already aligned or we know exactly what needs to be done to align them.

**Describe the solution you'd like**
`DataFrame.insert` should be decorated with `_external_only_api`, and any code using it should be converted to use `_insert`.",2022-08-03T23:52:48Z,0,0,Vyas Ramasubramani,@rapidsai,True
336,"[FEA] Add `stream` parameter for groupby operations (aggregate, scan, shift, etc.)","Currently, groupby operations do not have a `stream` parameter. As such, we can't pass in the current `stream` value but the operations will use the default stream instead.

We should allow passing in `stream` parameter in groupby aggregation.",2022-08-04T02:21:34Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
337,Need an address alignment utility function in cudf,"As found in review of code, there are alignment functions scattered around. We should have a single device-accessible alignment function that everything uses instead of doing the bit operations themselves or having a static local function for it.

_Originally posted by @etseidl in https://github.com/rapidsai/cudf/pull/11403#discussion_r938132622_",2022-08-04T18:44:25Z,0,0,Mike Wilson,,False
338,[BUG] `DataFrame.iloc` returns the wrong type of object when a string column is present,"**Describe the bug**
In the cases described in the title, we are getting a `DataFrame` rather than a `Series` object like we should.


**Steps/Code to reproduce bug**
```python
>>> df1 = cudf.DataFrame({
...     'a': [1, 2, 3],
...     'b': ['a', 'b', 'c']
... })
>>> df2 = cudf.DataFrame({
...     'a': [1, 2, 3],
...     'b': [4, 5, 6]
... })
>>> type(df1.iloc[0])
<class 'cudf.core.dataframe.DataFrame'>
>>> type(df2.iloc[0]) 
<class 'cudf.core.series.Series'>
>>> type(df1.to_pandas().iloc[0])
<class 'pandas.core.series.Series'>
>>> type(df2.to_pandas().iloc[0])
<class 'pandas.core.series.Series'>
```

**Expected behavior**
We should get the same object as pandas (a series)

**Additional Information**
Branch-22.10",2022-08-05T00:34:53Z,0,0,,NVIDIA,True
339,[FEA] Support nested struct columns in ORC fuzz tests,"**Is your feature request related to a problem? Please describe.**
Fuzz testing support for nested struct columns in Orc is incomplete. Some patches were required as shown in #9395 discussion (https://github.com/rapidsai/cudf/issues/9395#issuecomment-1206776202.)

**Describe the solution you'd like**
Add support for nested struct columns in the Orc fuzz testing

**Describe alternatives you've considered**
Adhoc patches that bring side-effects to other fuzzers and unit tests in cuDF.

**Additional context**
We would benefit from a way to limit the overall depth of the generated struct columns - I propose maximum nesting depth of 256. Setting `max_structs_nesting_depth` in the `IOFuzz` class does not seem sufficient to address this limit. Here is a traceback for the recursion error:
```
  File ""/home/gregorykimball/Repo/cudf/python/cudf/cudf/testing/dataset_generator.py"", line 387, in rand_dataframe
    structDtype = create_nested_struct_type(
...
  File ""/home/gregorykimball/Repo/rapids-compose/etc/conda/cuda_11.5/envs/rapids/lib/python3.8/site-packages/numpy/core/fromnumeric.py"", line 70, in _wrapreduction
    passkwargs = {k: v for k, v in kwargs.items()
RecursionError: maximum recursion depth exceeded while calling a Python object

```
",2022-08-05T19:11:48Z,1,0,Gregory Kimball,,False
340,[FEA] Interchange protocol between processes on the same device.,"A related issue: https://github.com/rapidsai/cudf/issues/11462

We would like to transfer a cuDF dataframe between a JVM process and a Python process without data copy . This is primarily used in PySpark environment where Spark can execute user-defined Python functions in JVM processes. The current solution in Spark is to serialize the dataframe into arrow format and perform an inter-process transfer on host. This is not efficient for both memory usage and computation time. Our proposed solution at the moment is to use the CUDA IPC mechanism for transferring metadata between 2 processes without actually copying data between host and device or between different processes. The message sent between 2 processes in our proposed method is a JSON document that describes some properties of the dataframe along with an encoded CUDA IPC handle.

**Describe the solution you'd like**
We would like to add two roundtrip methods for generating an IPC message that describes the dataframe and reconstructs the dataframe from that message at the C++/C level along with wrappers in java/python.  

``` python
df = cudf.DataFrame({""a"": [1, 2, 3]})
msg = df.to_ipc()

# in a different process but on the same CUDA device
df = cudf.DataFrame.from_ipc(msg)
```

As for a quick design of the message, for the lack of a better term, we can jsonify the `__dataframe__` protocol along with the use of encoded CUDA IPC handle. The message sent between processes can be something similar to:
``` json
{
  ""columns"": [
    {
      ""describe_categorical"": {
        ""is_dictionary"": true,
        ""is_ordered"": true,
        ""mapping"": {
          ""a"": 0
        }
      },
      ""describe_null"": [
        3,
        null
      ],
      ""dtype"": [
        0,
        64,
        ""i"",
        ""=""
      ],
      ""buffers"": [{""ipc_handle"": ""aeb6df622e4d6ed84dac526c8815c52c5bb2a34855a765270d6e1a502dc6574b""}],
      ""metadata"": null,
      ""null_count"": null,
      ""offset"": 0,
      ""size"": 128
    },
    {
      ...
    }
  ]
}
```

We might be able to reuse some logic in the `df_protocol.py` module for implementing this feature.

**Describe alternatives you've considered**
- Suggested by @jakirkham , I looked into the serialize and deserialize methods. They are good starting points but not suitable for transferring ownership between 2 different language environments due to the use of pickle in type serialization.
- Suggested by @shwina , I looked into the dataframe protocol, which is very close to what we want, except for its Python only interface and the use of a pointer.
- `mapInArrow` method from PySpark, which still requires a full data transfer.",2022-08-11T09:52:09Z,0,0,Jiaming Yuan,NVIDIA,True
341,[BUG] Some custom dask aggregations fail with dask_cudf dataframes,"**Describe the bug**
Some dask custom aggregations (ex: a custom sum of squares aggregation) fail with dask_cudf.

**Steps/Code to reproduce bug**
```
import cudf
import dask_cudf
import dask.dataframe as dd

df = cudf.DataFrame({""a"":[1,2,3], ""b"":[1,1,2]})
ddf = dask_cudf.from_cudf(df, npartitions=1)

sum_of_squares = dd.Aggregation(
    name='sum_of_squares',
    chunk=lambda s: s.agg(lambda x: (x**2).sum()),
    agg=lambda s0: s0.sum()
)

ddf.groupby(""b"").agg(sum_of_squares).compute()
```
returns:
```
TypeError: unsupported operand type(s) for ** or pow(): 'type' and 'int'
```
however when running with a pandas backed dask dataframe:
```
ddf.to_dask_dataframe().groupby(""b"").agg(sum_of_squares).compute()
```
the expected result is returned:
```
   a
b   
1  5
2  9
​
```

cc: @randerzander 
",2022-08-11T16:04:18Z,0,0,,,False
342,[FEA] Add JNI for `replace_nans`,"**Is your feature request related to a problem? Please describe.**
There is an issue in spark-rapids that needs the JNI of `replace_nans` in cuDF: https://github.com/NVIDIA/spark-rapids/issues/6276

This is **not** a blocker of https://github.com/NVIDIA/spark-rapids/issues/5320.

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context, code examples, or references to existing implementations about the feature request here.
",2022-08-15T04:32:22Z,0,0,Remzi Yang,@nvidia,True
343,[FEA] Extend tests to ensure no memory is allocated outside of RMM  ,"**Is your feature request related to a problem? Please describe.**

It is important to ensure that all device memory allocated inside of cuDF functions is done through RMM. 

It is easy to overlook this, e.g., by forgetting to pass the `rmm::exec_policy` to a Thrust algorithm that allocates temporary memory. 

**Describe the solution you'd like**

It would be fairly easy to add this to our CI testing by writing a LD_PRELOAD library that overloads `cudaMalloc` to throw an error if it is called more than once.

This would ensure that there is only a single `cudaMalloc` call for the pool allocation.

There are some things to be aware of with this solution:
- We'd need to ensure the pool is sized such that it won't need to grow for the tests
- It would assume we're using cudaMalloc as the upstream resource for the pool (and not cudaMallocManaged)
",2022-08-16T20:54:52Z,0,0,Jake Hemstad,@NVIDIA,True
344,"[PERF] Improve ""isin"" performance by only sorting once","**Is your feature request related to a problem? Please describe.**
While benchmarking cuDF-python, I noticed that [bench_isin](https://github.com/rapidsai/cudf/blob/65a782112f4b76941483adf17f9a30a6824f6164/python/cudf/benchmarks/API/bench_dataframe.py#L50) has low end-to-end data throughput (<10GB/s). A closer look at the profiles showed that the data is being sorted twice, first with `.sort_values()` and then as part of `drop_duplicates()`. The following profile is for a test dataframe with 1 col and 100K rows, and is uses the `isin` argument `range(1000)` in the `bench_isin` benchmark.

<img width=""851"" alt=""image"" src=""https://user-images.githubusercontent.com/12725111/185016040-8c8b70ab-a8fd-4cc5-8294-2fb9ea5acc5e.png"">

When calling `isin` with a dataframe or dict argument, the profile shows two calls to `Frame.argsort`.

<img width=""841"" alt=""image"" src=""https://user-images.githubusercontent.com/12725111/185018010-a4e5789e-cf0a-4a7f-8291-a9fdd52adbbe.png"">





**Describe the solution you'd like**
For a performance improvement, I'd like to refactor `isin` to only sort the data once. We should prefer the libcudf `unique` function to `drop_duplicates` for pre-sorted data.

**Describe alternatives you've considered**
n/a

**Additional context**
Add any other context, code examples, or references to existing implementations about the feature request here.
",2022-08-17T01:48:17Z,0,0,Gregory Kimball,,False
345,"[DOC] `.mode()` incorrectly states that axis=1 is supported as it throws ""NotImplementedError"" when you try","## Report incorrect documentation

**Location of incorrect documentation**
https://docs.rapids.ai/api/cudf/stable/api_docs/api/cudf.DataFrame.mode.html?highlight=mode#cudf.DataFrame.mode

**Describe the problems or issues found in the documentation**
Documentation says that mode can be applied over columns (axis = 0) or rows (axis = 1), but when trying axis = 1, it says `Only axis=0 is currently supported`

**Steps taken to verify documentation is incorrect**

```
import cudf
df = cudf.DataFrame(np.random.randint(0,5,size=(15, 10)), columns=list('ABCDEFGHIJ'))
df.mode(axis=1)
```
Stack trace
```
---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
/tmp/ipykernel_2243/3323872345.py in <module>
----> 1 df.mode(axis=1)

/opt/conda/envs/rapids/lib/python3.8/contextlib.py in inner(*args, **kwds)
     73         def inner(*args, **kwds):
     74             with self._recreate_cm():
---> 75                 return func(*args, **kwds)
     76         return inner
     77 

/opt/conda/envs/rapids/lib/python3.8/site-packages/cudf/core/dataframe.py in mode(self, axis, numeric_only, dropna)
   5678         """"""
   5679         if axis not in (0, ""index""):
-> 5680             raise NotImplementedError(""Only axis=0 is currently supported"")
   5681 
   5682         if numeric_only:

NotImplementedError: Only axis=0 is currently supported
```
**Suggested fix for documentation**
Either 
1. state that axis = 1 is not supported
2. remove references for axis = 1 in docs
3. add the functionality
",2022-08-19T15:42:58Z,0,0,Taurean Dyer,,False
346,[FEA] Remove code duplication between orc writer and chunked orc writer,"**Is your feature request related to a problem? Please describe.**
There is some code-duplication happening in the regular orc writer and chunked orc writers. This can be reduced to an extent by re-using the regular orc writers methods in chunked orc writer: https://github.com/rapidsai/cudf/blob/branch-22.10/python/cudf/cudf/_lib/orc.pyx


**Additional context**

https://github.com/rapidsai/cudf/pull/11568#discussion_r950593454",2022-08-22T20:37:26Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
347,[FEA] Handle selected functions that don't have bytecode in `Series.apply`,"**Is your feature request related to a problem? Please describe.**
During discussion in https://github.com/rapidsai/cudf/pull/11578 we realized that users might try and pass several commonly used python functions that don't necessarily have bytecode directly to `Series.apply`. For instance, using `float` dispatches to a C-level `__init__` method and thus has no bytecode for numba to build a function out of. 

**Describe the solution you'd like**
It would be nice if this worked:
```python
cudf.Series([1,2,3]).apply(float)
# [1.0, 2.0, 3.0]
```
We should be able to use 
- int
- float
- bool

We could also add:
- Numpy dtypes
- string?


**Describe alternatives you've considered**
Users should just call `astype` but this seems like plausible usage. 

**Additional context**
Came up during https://github.com/rapidsai/cudf/pull/11578#pullrequestreview-1082926062",2022-08-25T14:38:33Z,0,0,,NVIDIA,True
348,[BUG] Series accessors aren't being doctested.,"**Describe the bug**
It appears that the special type accessors for `Series.str` (`StringMethods`) are not being doctested. See https://github.com/rapidsai/cudf/pull/11575#discussion_r953925683. I suspect this is also true for list and struct accessors but have not verified that.

**Expected behavior**
Doctests should run for type-accessors inheriting from `ColumnMethods` like `Series.str.findall`.

This will probably require some updates to [test_doctests.py](https://github.com/rapidsai/cudf/blob/main/python/cudf/cudf/tests/test_doctests.py) to ensure that these accessor methods are found and tested. It may require a special set of tests explicitly designed for finding and testing all `ColumnMethods` objects.",2022-08-25T21:12:24Z,0,0,Bradley Dice,@NVIDIA @rapidsai,True
349,Optimize `to_cupy` and `values`,"Currently `series.values` and especially `series.to_cupy()` are substantially slower than `cupy.asarray(series)`. 
```
In [2]: s = cudf.Series(range(10000))

In [3]: %timeit s.values
81.4 µs ± 1.68 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)

In [4]: %timeit cp.asarray(s)
19.1 µs ± 168 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)

In [5]: %timeit s.to_cupy()
349 µs ± 75.2 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

There are at least two obvious potential culprits in `Frame._to_array` (the underlying method for `to_cupy`):
- [It always performs an extra allocation](https://github.com/rapidsai/cudf/blob/branch-22.10/python/cudf/cudf/core/frame.py#L484), even when `copy=False`.
- [It performs dtype inference using `find_common_dtype`](https://github.com/rapidsai/cudf/blob/branch-22.10/python/cudf/cudf/core/frame.py#L479), which is _slow_ (and slower for `DataFrame`s with many columns):
```
In [11]: df = cudf.DataFrame({'a': [1], 'b': [3.], 'c': ['a']})

In [12]: %timeit cudf.utils.dtypes.find_common_type([col.dtype for col in df._data.values()])
53.6 µs ± 530 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)

In [13]: df = cudf.DataFrame({'a': [1], 'b': [3.]})

In [14]: %timeit cudf.utils.dtypes.find_common_type([col.dtype for col in df._data.values()])
39.8 µs ± 1.01 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)
```

The implementation of `values` drops down to `ColumnBase.values` and requires some deeper consideration. However, since we use `.values` frequently internally (and we occasionally use `to_cupy`) we are likely giving up a lot of performance. We should profile these functions to determine the bottlenecks, and if there are valid reasons for them we should establish some policies on how to select the right function to use when performing these conversions to arrays internally. While this exact analogy does not hold for `DataFrame` (because that doesn't support the conversion to an array), any optimization that we make for `Series` will likely also help speed up `DataFrame` operations.",2022-09-02T23:24:22Z,0,0,Vyas Ramasubramani,@rapidsai,True
350,[ENH] More type-stubs in the mypy pre-commit environment?,"Following on from #11640 (specifically https://github.com/rapidsai/cudf/pull/11640#discussion_r961551760) a question arises as to how complete we should make the mypy pre-commit environment in terms of supported typestubs.

Status quo:

The version of mypy installed via pre-commit only depends on the stdlib typeshed stubs (and after #11640 `types-cachetools`). We set `ignore_missing_imports = True` which means that mypy doesn't complain if it sees an import for something (say numpy) that it can't find. Consequently, any non-importable modules are typed as `Any` (as are all objects, methods, functions, etc... from that module); this is a type that always satisfies any type constraint.

In the development environment, all cudf modules _are_ importable, and so a type-checking run using that environment will deduce narrower types for many function calls in cudf. Many of these currently do not type-check, being of the following form: https://mypy-play.net/?mypy=latest&python=3.10&gist=8d3ba6046bb8ca39c6d6b71b442b432c
```python
from typing import Any

def foo(x: Any) -> Any:
    if isinstance(x, bool):
        y = x
    elif isinstance(x, int):
        y = (True, x)
    else:
        y = x

    return y
```

mypy complains about the assignment `y = (True, x)` ""error: Incompatible types in assignment (expression has type ""Tuple[bool, int]"", variable has type ""bool"")"" because it deduces the type of `y` from the first assignment as `bool`.

There are various places in the codebase where we do this kind of untagged union dispatch, this could be fixed by explicitly typing all the variables as `Union[a, b, c, ...]` but I am not sure that in the end it would be worth it. If we install `numpy` and the `pandas-stubs` package (which provides type stubs for pandas) then we get about 120 errors of this nature.

I think that fixing these things is rather difficult, the right approach is to use tagged unions, but there's no support for that in python and any workaround would (I think) make the code unnecessarily non-idiomatic.

If we think it's worthwhile pursuing this, I can prepare a draft patch for some of the uncovered typing issues so we can discuss more concretely.",2022-09-07T13:15:29Z,0,0,Lawrence Mitchell,,False
351,[FEA] Reduce disparity between nested and non-nested column handling in lexicographic comparator,"**Is your feature request related to a problem? Please describe.**
As part of #11129 the row lexicographic comparator became templated on whether or not the tables being compared contain any nested columns. This templating was necessary to guarantee that nested and non-nested code paths were compiled separately. That, in turn, was required because the compiler fails to completely optimize the non-nested code path due to the complexity of the nested code path, which slows down code even when no nested data is present. Unfortunately, this now means that calling code must dispatch to separate paths depending on whether the table being operated on has nested data. In addition to being cumbersome in and of itself, this requirement makes code using the lexicographic comparator different from code using the equality comparator.

**Describe the solution you'd like**
We should consider alternative APIs for the comparator that abstract the nested column dispatch away from the call-site. One option that I considered is to define a wrapper function that accepts a callable to apply (e.g. `thrust::sort`) that requires the comparator as an argument. This wrapper function could instantiate the appropriate comparator and then call the provided callable. @jrhemstad suggested that [this could be accomplished using a visitor pattern](https://godbolt.org/z/ra76jbM78). We should prototype this approach.

**Describe alternatives you've considered**
I have not yet come up with any alternative solutions, but it would be nice to try to find an even less invasive approach if possible since the visitor pattern does add an extra level of indirection that would be nice to avoid.

**Additional context**
N/A
",2022-09-08T17:34:57Z,0,0,Vyas Ramasubramani,@rapidsai,True
352,[BUG] ORC read/write is wrong in `day` values in pre-1582 datetime values,"Similar to https://github.com/rapidsai/cudf/issues/11525 that has just been fixed, I discovered new failures with ORC reader/writer. 

Note that these failures are wrong days, not wrong seconds like previously reported in https://github.com/rapidsai/cudf/issues/11525.

```
cpu = datetime.datetime(740, 7, 19, 10, 16, 58, 929621)
gpu = datetime.datetime(740, 7, 23, 10, 16, 58, 929621)

cpu = datetime.datetime(487, 4, 15, 11, 13, 37, 361058)
gpu = datetime.datetime(487, 4, 16, 11, 13, 37, 361058)

cpu = datetime.datetime(740, 7, 19, 10, 16, 58, 929621)
gpu = datetime.datetime(740, 7, 23, 10, 16, 58, 929621)

cpu = datetime.datetime(1132, 8, 4, 22, 50, 7, 153267)
gpu = datetime.datetime(1132, 8, 11, 22, 50, 7, 153267)

cpu = datetime.datetime(1348, 1, 31, 3, 21, 2, 422651)
gpu = datetime.datetime(1348, 2, 8, 3, 21, 2, 422651)

cpu = datetime.datetime(487, 4, 15, 11, 13, 37, 361058)
gpu = datetime.datetime(487, 4, 16, 11, 13, 37, 361058)

cpu = datetime.datetime(1348, 1, 31, 3, 21, 2, 422651)
gpu = datetime.datetime(1348, 2, 8, 3, 21, 2, 422651)

cpu = datetime.datetime(1132, 8, 4, 22, 50, 7, 153267)
gpu = datetime.datetime(1132, 8, 11, 22, 50, 7, 153267)

cpu = datetime.datetime(487, 4, 15, 11, 13, 37, 361058)
gpu = datetime.datetime(487, 4, 16, 11, 13, 37, 361058)

cpu = datetime.datetime(740, 7, 19, 10, 16, 58, 929621)
gpu = datetime.datetime(740, 7, 23, 10, 16, 58, 929621)

cpu = datetime.datetime(1132, 8, 4, 22, 50, 7, 153267)
gpu = datetime.datetime(1132, 8, 11, 22, 50, 7, 153267)

cpu = datetime.datetime(1348, 1, 31, 3, 21, 2, 422651)
gpu = datetime.datetime(1348, 2, 8, 3, 21, 2, 422651)

cpu = datetime.datetime(740, 7, 19, 10, 16, 58, 929621)
gpu = datetime.datetime(740, 7, 23, 10, 16, 58, 929621)

cpu = datetime.datetime(740, 7, 19, 10, 16, 58, 929621)
gpu = datetime.datetime(740, 7, 23, 10, 16, 58, 929621)

cpu = datetime.datetime(1132, 8, 4, 22, 50, 7, 153267)
gpu = datetime.datetime(1132, 8, 11, 22, 50, 7, 153267)

cpu = datetime.datetime(740, 7, 19, 10, 16, 58, 929621)
gpu = datetime.datetime(740, 7, 23, 10, 16, 58, 929621)

cpu = datetime.datetime(487, 4, 15, 11, 13, 37, 361058)
gpu = datetime.datetime(487, 4, 16, 11, 13, 37, 361058)

cpu = datetime.datetime(740, 7, 19, 10, 16, 58, 929621)
gpu = datetime.datetime(740, 7, 23, 10, 16, 58, 929621)

cpu = datetime.datetime(1348, 1, 31, 3, 21, 2, 422651)
gpu = datetime.datetime(1348, 2, 8, 3, 21, 2, 422651)

cpu = datetime.datetime(487, 4, 15, 11, 13, 37, 361058)
gpu = datetime.datetime(487, 4, 16, 11, 13, 37, 361058)

cpu = datetime.datetime(487, 4, 15, 11, 13, 37, 361058)
gpu = datetime.datetime(487, 4, 16, 11, 13, 37, 361058)

cpu = datetime.datetime(487, 4, 15, 11, 13, 37, 361058)
gpu = datetime.datetime(487, 4, 16, 11, 13, 37, 361058)

cpu = datetime.datetime(1348, 1, 31, 3, 21, 2, 422651)
gpu = datetime.datetime(1348, 2, 8, 3, 21, 2, 422651)

cpu = datetime.datetime(740, 7, 19, 10, 16, 58, 929621)
gpu = datetime.datetime(740, 7, 23, 10, 16, 58, 929621)

cpu = datetime.datetime(833, 6, 4, 10, 18, 10, 135672)
gpu = datetime.datetime(833, 6, 8, 10, 18, 10, 135672)

cpu = datetime.datetime(1348, 1, 31, 3, 21, 2, 422651)
gpu = datetime.datetime(1348, 2, 8, 3, 21, 2, 422651)

cpu = datetime.datetime(740, 7, 19, 10, 16, 58, 929621)
gpu = datetime.datetime(740, 7, 23, 10, 16, 58, 929621)

cpu = datetime.datetime(487, 4, 15, 11, 13, 37, 361058)
gpu = datetime.datetime(487, 4, 16, 11, 13, 37, 361058)

cpu = datetime.datetime(487, 4, 15, 11, 13, 37, 361058)
gpu = datetime.datetime(487, 4, 16, 11, 13, 37, 361058)

cpu = datetime.datetime(740, 7, 19, 10, 16, 58, 929621)
gpu = datetime.datetime(740, 7, 23, 10, 16, 58, 929621)

cpu = datetime.datetime(487, 4, 15, 11, 13, 37, 361058)
gpu = datetime.datetime(487, 4, 16, 11, 13, 37, 361058)

cpu = datetime.datetime(833, 6, 4, 10, 18, 10, 135672)
gpu = datetime.datetime(833, 6, 8, 10, 18, 10, 135672)

cpu = datetime.datetime(833, 6, 4, 10, 18, 10, 135672)
gpu = datetime.datetime(833, 6, 8, 10, 18, 10, 135672)

cpu = datetime.datetime(1348, 1, 31, 3, 21, 2, 422651)
gpu = datetime.datetime(1348, 2, 8, 3, 21, 2, 422651)

cpu = datetime.datetime(833, 6, 4, 10, 18, 10, 135672)
gpu = datetime.datetime(833, 6, 8, 10, 18, 10, 135672)

cpu = datetime.datetime(1132, 8, 4, 22, 50, 7, 153267)
gpu = datetime.datetime(1132, 8, 11, 22, 50, 7, 153267)

cpu = datetime.datetime(740, 7, 19, 10, 16, 58, 929621)
gpu = datetime.datetime(740, 7, 23, 10, 16, 58, 929621)

cpu = datetime.datetime(1348, 1, 31, 3, 21, 2, 422651)
gpu = datetime.datetime(1348, 2, 8, 3, 21, 2, 422651)

cpu = datetime.datetime(1348, 1, 31, 3, 21, 2, 422651)
gpu = datetime.datetime(1348, 2, 8, 3, 21, 2, 422651)

cpu = datetime.datetime(1132, 8, 4, 22, 50, 7, 153267)
gpu = datetime.datetime(1132, 8, 11, 22, 50, 7, 153267)

cpu = datetime.datetime(487, 4, 15, 11, 13, 37, 361058)
gpu = datetime.datetime(487, 4, 16, 11, 13, 37, 361058)

cpu = datetime.datetime(1348, 1, 31, 3, 21, 2, 422651)
gpu = datetime.datetime(1348, 2, 8, 3, 21, 2, 422651)

cpu = datetime.datetime(1348, 1, 31, 3, 21, 2, 422651)
gpu = datetime.datetime(1348, 2, 8, 3, 21, 2, 422651)

cpu = datetime.datetime(833, 6, 4, 10, 18, 10, 135672)
gpu = datetime.datetime(833, 6, 8, 10, 18, 10, 135672)

cpu = datetime.datetime(740, 7, 19, 10, 16, 58, 929621)
gpu = datetime.datetime(740, 7, 23, 10, 16, 58, 929621)

cpu = datetime.datetime(487, 4, 15, 11, 13, 37, 361058)
gpu = datetime.datetime(487, 4, 16, 11, 13, 37, 361058)

cpu = datetime.datetime(833, 6, 4, 10, 18, 10, 135672)
gpu = datetime.datetime(833, 6, 8, 10, 18, 10, 135672)

cpu = datetime.datetime(1348, 1, 31, 3, 21, 2, 422651)
gpu = datetime.datetime(1348, 2, 8, 3, 21, 2, 422651)
```",2022-09-13T05:01:13Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
353,[BUG] Java `Scalar` does not consider Decimal scale for `.hashCode()`/`.equals()`,"In its current implementation, the `Scalar` Java class does not consider the `scale` of a scalar value, when comparing two `DECIMAL` scalars.

Here is the section of `Scalar.equals()` that compares `DECIMAL64` values:
```java
    case DECIMAL64:
      return getLong() == other.getLong();
```
`getLong()` does not rescale the representative value of the scalar, based on a common scale. This implementation will equate two `DECIMAL64` scalars of different scales, if their `rep` values are equal.

A similar argument could be made for `Scalar.hashCode()`:
```java
      case DECIMAL64:
      // ...
        valueHash = Long.hashCode(getLong());
        break;
```

AFAICT, the problem applies to `DECIMAL32` and `DECIMAL64`, but not `DECIMAL128`. In adding support for `DECIMAL128` in #11645, the comparisons are made using `BigDecimal`, rather than `BigInteger`.",2022-09-13T18:42:01Z,0,0,MithunR,NVIDIA,True
354,[FEA] `read_csv` context-passing interface for distributed/segmented parsing,"**Is your feature request related to a problem? Please describe.**
To parse files from a regular language like CSV, we can only safely parse data inside a byte range if we know the parsing context from before this range. Without this information, we may accidentally interpret a record delimiter (newline) inside a quoted field as an actual delimiter.

More formally, when starting from a byte range, we don't know what state the DFA of the token language is in, so we need to store the transition vector starting from every possible state, and combine the vectors by function composition in the associative scan operation. This is pretty much identical to what is happening in [the finite state transducer](https://github.com/rapidsai/cudf/pull/11242).

Instead of just running the scan on a full file, we can run it only on a byte range, and combine the resulting transition vectors in an exclusive scan over all byte ranges to establish local parsing context.

**Describe the solution you'd like**
I want to propose a slight extension to the `read_csv` interface to handle this case:

1. add a `class csv_parse_context` that opaquely wraps `packed_rowctx_t`, only exposing the `merge_row_contexts` functionality to combine the parsing context from adjacent byte ranges, and a `finalize` operation that turns the transition vector into its value starting from the initial DFA state.
2. add a `read_csv_context` function that only scans the local byte range to compute its `csv_parse_context` transition vector. It can probably take the same parameters as `read_csv`
3. add a `csv_parse_context initial_parsing_state` parameter to `csv_reader_options` that defaults to the initial state. The `read_csv` function can then use this initial state to determine record offsets and do the actual parsing.

**Describe alternatives you've considered**
Alternatively, we could implement backtracking by reading chunks before the byte range until we figured out an unambiguous parser state (that is not the error state). This could in the worst case lead to reading the entire prefix up to the byte range.

**Additional context**
This is relevant if we want `dask.read_csv` to be able to handle quoted record delimiters (i.e. newlines) where the opening quote occurs before the byte range.

The interface has the advantage that it can be tested in isolation on a single node, without having to rely on dask.

The same kind of pattern could also apply to `read_json`, where on top of the regular parsing state, we also need to pass the stack transition operations from the beginning to the end of the byte range.
",2022-09-21T10:22:51Z,0,0,Tobias Ribizel,,False
355,[FEA] Support New Median / Median-Approximate in Dask-cuDF,"In https://github.com/dask/dask/pull/9483 Dask now has an implementation of `median` and `median_approximate`.  These should available with dask_cudf.  cuDF currently raise a NotImplementedError with `mean(axis=1)`:


```python
cdf = cudf.datasets.timeseries()
cdf = dd.from_pandas(cdf, npartitions=2)
cdf.median(axis=1).compute()

File /datasets/bzaitlen/miniconda3/envs/tpcds-20220906/lib/python3.9/site-packages/cudf/core/dataframe.py:5225, in DataFrame.quantile(self, q, axis, numeric_only, interpolation, columns, exact)
   5164 """"""
   5165 Return values at the given quantile.
   5166
   (...)
   5222 0.5  2.5  55.0
   5223 """"""  # noqa: E501
   5224 if axis not in (0, None):
-> 5225     raise NotImplementedError(""axis is not implemented yet"")
   5227 data_df = self
   5228 if numeric_only:
```

As for `mean_approximate`, ValueError is thrown unexpectedly:


```python
In [32]: cdf.compute()
Out[32]:
   x    y
0  1  1.1
1  2  2.2
2  3  3.3
3  4  4.4
4  5  5.5

In [33]: cdf.median_approximate().compute()
...
File /datasets/bzaitlen/miniconda3/envs/tpcds-20220906/lib/python3.9/site-packages/cudf/core/frame.py:898, in Frame.fillna(self, value, method, axis, inplace, limit)
    892 should_fill = (
    893     col_name in value
    894     and col.contains_na_entries
    895     and not libcudf.scalar._is_null_host_scalar(replace_val)
    896 ) or method is not None
    897 if should_fill:
--> 898     filled_data[col_name] = col.fillna(replace_val, method)
    899 else:
    900     filled_data[col_name] = col.copy(deep=True)

File /datasets/bzaitlen/miniconda3/envs/tpcds-20220906/lib/python3.9/site-packages/cudf/core/column/numerical.py:503, in NumericalColumn.fillna(self, fill_value, method, dtype, fill_nan)
    499     return super(NumericalColumn, col).fillna(fill_value, method)
    501 if np.isscalar(fill_value):
    502     # cast safely to the same dtype as self
--> 503     fill_value_casted = col.dtype.type(fill_value)
    504     if not np.isnan(fill_value) and (fill_value_casted != fill_value):
    505         raise TypeError(
    506             f""Cannot safely cast non-equivalent ""
    507             f""{type(fill_value).__name__} to {col.dtype.name}""
    508         )

ValueError: cannot convert float NaN to integer
```",2022-09-21T13:47:13Z,0,0,Benjamin Zaitlen,,False
356,[FEA] Support passing scalar string args to string_udfs,"Using the newly merged strings_udf support, I'm trying to pass scalar arguments to a string UDF:
```
import cudf

df = cudf.DataFrame({""str_col"": [""a"", ""abb"", ""abc""]})

def delim_count(row, delim):
    return row[""str_col""].count(delim)

df.apply(delim_count, args=(""b"",), axis=1)
```

But I get udf compilation failed errors.

Trace:
```
---------------------------------------------------------------------------
NumbaNotImplementedError                  Traceback (most recent call last)
/opt/conda/envs/rapids/lib/python3.9/site-packages/numba/core/base.py in cast(self, builder, val, fromty, toty)
    711         try:
--> 712             impl = self._casts.find((fromty, toty))
    713             return impl(self, builder, fromty, toty, val)

/opt/conda/envs/rapids/lib/python3.9/site-packages/numba/core/base.py in find(self, sig)
     48         if out is None:
---> 49             out = self._find(sig)
     50             self._cache[sig] = out

/opt/conda/envs/rapids/lib/python3.9/site-packages/numba/core/base.py in _find(self, sig)
     57         else:
---> 58             raise errors.NumbaNotImplementedError(f'{self}, {sig}')
     59 

NumbaNotImplementedError: <numba.core.base.OverloadSelector object at 0x7f2498b3aca0>, (unicode_type, string_view)

During handling of the above exception, another exception occurred:

NumbaNotImplementedError                  Traceback (most recent call last)
/opt/conda/envs/rapids/lib/python3.9/site-packages/numba/core/base.py in cast(self, builder, val, fromty, toty)
    712             impl = self._casts.find((fromty, toty))
--> 713             return impl(self, builder, fromty, toty, val)
    714         except errors.NumbaNotImplementedError:

/opt/conda/envs/rapids/lib/python3.9/site-packages/cudf/core/udf/masked_lowering.py in cast_primitive_to_masked(context, builder, fromty, toty, val)
    336 def cast_primitive_to_masked(context, builder, fromty, toty, val):
--> 337     casted = context.cast(builder, val, fromty, toty.value_type)
    338     ext = cgutils.create_struct_proxy(toty)(context, builder)

/opt/conda/envs/rapids/lib/python3.9/site-packages/numba/core/base.py in cast(self, builder, val, fromty, toty)
    714         except errors.NumbaNotImplementedError:
--> 715             raise errors.NumbaNotImplementedError(
    716                 ""Cannot cast %s to %s: %s"" % (fromty, toty, val))

NumbaNotImplementedError: Cannot cast unicode_type to string_view: %""inserted.parent"" = insertvalue {i8*, i64, i32, i32, i64, i8*, i8*} %""inserted.meminfo"", i8* %""arg.delim.6"", 6

During handling of the above exception, another exception occurred:

NumbaNotImplementedError                  Traceback (most recent call last)
/opt/conda/envs/rapids/lib/python3.9/site-packages/cudf/core/indexed_frame.py in _apply(self, func, kernel_getter, *args, **kwargs)
   1817         try:
-> 1818             kernel, retty = _compile_or_get(
   1819                 self, func, args, kernel_getter=kernel_getter

/opt/conda/envs/rapids/lib/python3.9/contextlib.py in inner(*args, **kwds)
     78             with self._recreate_cm():
---> 79                 return func(*args, **kwds)
     80         return inner

/opt/conda/envs/rapids/lib/python3.9/site-packages/cudf/core/udf/utils.py in _compile_or_get(frame, func, args, kernel_getter)
    214 
--> 215     kernel, scalar_return_type = kernel_getter(frame, func, args)
    216     np_return_type = numpy_support.as_dtype(scalar_return_type)

/opt/conda/envs/rapids/lib/python3.9/site-packages/cudf/core/udf/row_function.py in _get_row_kernel(frame, func, args)
    132     )
--> 133     scalar_return_type = _get_udf_return_type(row_type, func, args)
    134     # this is the signature for the final full kernel compilation

/opt/conda/envs/rapids/lib/python3.9/contextlib.py in inner(*args, **kwds)
     78             with self._recreate_cm():
---> 79                 return func(*args, **kwds)
     80         return inner

/opt/conda/envs/rapids/lib/python3.9/site-packages/cudf/core/udf/utils.py in _get_udf_return_type(argty, func, args)
     55     # needed here.
---> 56     ptx, output_type = cudautils.compile_udf(func, compile_sig)
     57     if not isinstance(output_type, MaskedType):

/opt/conda/envs/rapids/lib/python3.9/site-packages/cudf/utils/cudautils.py in compile_udf(udf, type_signature)
    249     # compilation with Numba
--> 250     ptx_code, return_type = cuda.compile_ptx_for_current_device(
    251         udf, type_signature, device=True

/opt/conda/envs/rapids/lib/python3.9/site-packages/numba/cuda/compiler.py in compile_ptx_for_current_device(pyfunc, args, debug, lineinfo, device, fastmath, opt)
    289     cc = get_current_device().compute_capability
--> 290     return compile_ptx(pyfunc, args, debug=debug, lineinfo=lineinfo,
    291                        device=device, fastmath=fastmath, cc=cc, opt=True)

/opt/conda/envs/rapids/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs)
     34             with self:
---> 35                 return func(*args, **kwargs)
     36         return _acquire_compile_lock

/opt/conda/envs/rapids/lib/python3.9/site-packages/numba/cuda/compiler.py in compile_ptx(pyfunc, args, debug, lineinfo, device, fastmath, cc, opt)
    266 
--> 267     cres = compile_cuda(pyfunc, None, args, debug=debug, lineinfo=lineinfo,
    268                         nvvm_options=nvvm_options)

/opt/conda/envs/rapids/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs)
     34             with self:
---> 35                 return func(*args, **kwargs)
     36         return _acquire_compile_lock

/opt/conda/envs/rapids/lib/python3.9/site-packages/numba/cuda/compiler.py in compile_cuda(pyfunc, return_type, args, debug, lineinfo, inline, fastmath, nvvm_options)
    201     # Run compilation pipeline
--> 202     cres = compiler.compile_extra(typingctx=typingctx,
    203                                   targetctx=targetctx,

/opt/conda/envs/rapids/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class)
    692                               args, return_type, flags, locals)
--> 693     return pipeline.compile_extra(func)
    694 

/opt/conda/envs/rapids/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func)
    428         self.state.lifted_from = None
--> 429         return self._compile_bytecode()
    430 

/opt/conda/envs/rapids/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self)
    496         assert self.state.func_ir is None
--> 497         return self._compile_core()
    498 

/opt/conda/envs/rapids/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self)
    475                     if is_final_pipeline:
--> 476                         raise e
    477             else:

/opt/conda/envs/rapids/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self)
    462                 try:
--> 463                     pm.run(self.state)
    464                     if self.state.cr is not None:

/opt/conda/envs/rapids/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state)
    352                 patched_exception = self._patch_error(msg, e)
--> 353                 raise patched_exception
    354 

/opt/conda/envs/rapids/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state)
    340                 if isinstance(pass_inst, CompilerPass):
--> 341                     self._runPass(idx, pass_inst, state)
    342                 else:

/opt/conda/envs/rapids/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs)
     34             with self:
---> 35                 return func(*args, **kwargs)
     36         return _acquire_compile_lock

/opt/conda/envs/rapids/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state)
    295         with SimpleTimer() as pass_time:
--> 296             mutated |= check(pss.run_pass, internal_state)
    297         with SimpleTimer() as finalize_time:

/opt/conda/envs/rapids/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state)
    268         def check(func, compiler_state):
--> 269             mangled = func(compiler_state)
    270             if mangled not in (True, False):

/opt/conda/envs/rapids/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state)
    393                                        metadata=metadata)
--> 394                 lower.lower()
    395                 if not flags.no_cpython_wrapper:

/opt/conda/envs/rapids/lib/python3.9/site-packages/numba/core/lowering.py in lower(self)
    195             self.genlower = None
--> 196             self.lower_normal_function(self.fndesc)
    197         else:

/opt/conda/envs/rapids/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc)
    249         self.extract_function_arguments()
--> 250         entry_block_tail = self.lower_function_body()
    251 

/opt/conda/envs/rapids/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self)
    278             self.builder.position_at_end(bb)
--> 279             self.lower_block(block)
    280         self.post_lower()

/opt/conda/envs/rapids/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block)
    292                                    loc=self.loc, errcls_=defaulterrcls):
--> 293                 self.lower_inst(inst)
    294         self.post_block(block)

/opt/conda/envs/rapids/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst)
    437             ty = self.typeof(inst.target.name)
--> 438             val = self.lower_assign(ty, inst)
    439             argidx = None

/opt/conda/envs/rapids/lib/python3.9/site-packages/numba/core/lowering.py in lower_assign(self, ty, inst)
    623         elif isinstance(value, ir.Expr):
--> 624             return self.lower_expr(ty, value)
    625 

/opt/conda/envs/rapids/lib/python3.9/site-packages/numba/core/lowering.py in lower_expr(self, resty, expr)
   1158         elif expr.op == 'call':
-> 1159             res = self.lower_call(resty, expr)
   1160             return res

/opt/conda/envs/rapids/lib/python3.9/site-packages/numba/core/lowering.py in lower_call(self, resty, expr)
    888         else:
--> 889             res = self._lower_call_normal(fnty, expr, signature)
    890 

/opt/conda/envs/rapids/lib/python3.9/site-packages/numba/core/lowering.py in _lower_call_normal(self, fnty, expr, signature)
   1111         else:
-> 1112             argvals = self.fold_call_args(
   1113                 fnty, signature, expr.args, expr.vararg, expr.kws,

/opt/conda/envs/rapids/lib/python3.9/site-packages/numba/core/lowering.py in fold_call_args(self, fnty, signature, pos_args, vararg, kw_args)
    810                                           ""when calling %s"" % (fnty,))
--> 811             argvals = [self._cast_var(var, sigty)
    812                        for var, sigty in zip(pos_args, signature.args)]

/opt/conda/envs/rapids/lib/python3.9/site-packages/numba/core/lowering.py in <listcomp>(.0)
    810                                           ""when calling %s"" % (fnty,))
--> 811             argvals = [self._cast_var(var, sigty)
    812                        for var, sigty in zip(pos_args, signature.args)]

/opt/conda/envs/rapids/lib/python3.9/site-packages/numba/core/lowering.py in _cast_var(self, var, ty)
    793             val = self.loadvar(var.name)
--> 794         return self.context.cast(self.builder, val, varty, ty)
    795 

/opt/conda/envs/rapids/lib/python3.9/site-packages/numba/core/base.py in cast(self, builder, val, fromty, toty)
    714         except errors.NumbaNotImplementedError:
--> 715             raise errors.NumbaNotImplementedError(
    716                 ""Cannot cast %s to %s: %s"" % (fromty, toty, val))

NumbaNotImplementedError: Failed in cuda mode pipeline (step: native lowering)
Cannot cast unicode_type to Masked(string_view): %""inserted.parent"" = insertvalue {i8*, i64, i32, i32, i64, i8*, i8*} %""inserted.meminfo"", i8* %""arg.delim.6"", 6
During: lowering ""$12call_method.5 = call $8load_method.3(delim, func=$8load_method.3, args=[Var(delim, 3005899295.py:6)], kws=(), vararg=None, target=None)"" at /tmp/ipykernel_2334/3005899295.py (6)

The above exception was the direct cause of the following exception:

ValueError                                Traceback (most recent call last)
/tmp/ipykernel_2334/3005899295.py in <module>
      6     return row[""str_col""].count(delim)
      7 
----> 8 df.apply(delim_count, args=(""b"",), axis=1)

/opt/conda/envs/rapids/lib/python3.9/contextlib.py in inner(*args, **kwds)
     77         def inner(*args, **kwds):
     78             with self._recreate_cm():
---> 79                 return func(*args, **kwds)
     80         return inner
     81 

/opt/conda/envs/rapids/lib/python3.9/site-packages/cudf/core/dataframe.py in apply(self, func, axis, raw, result_type, args, **kwargs)
   4089             raise ValueError(""The `result_type` kwarg is not yet supported."")
   4090 
-> 4091         return self._apply(func, _get_row_kernel, *args, **kwargs)
   4092 
   4093     def applymap(

/opt/conda/envs/rapids/lib/python3.9/contextlib.py in inner(*args, **kwds)
     77         def inner(*args, **kwds):
     78             with self._recreate_cm():
---> 79                 return func(*args, **kwds)
     80         return inner
     81 

/opt/conda/envs/rapids/lib/python3.9/site-packages/cudf/core/indexed_frame.py in _apply(self, func, kernel_getter, *args, **kwargs)
   1820             )
   1821         except Exception as e:
-> 1822             raise ValueError(
   1823                 ""user defined function compilation failed.""
   1824             ) from e

ValueError: user defined function compilation failed.
```

numba version: '0.55.2'
cudf version: '22.10.00a+242.g387c5ff96d' (built from source)

cc @brandon-b-miller ",2022-09-21T16:17:00Z,0,0,Randy Gelhausen,,False
357,[BUG] Grouping by a nonexistent key gives `ValueError` rather than `KeyError`,"I would expect the code below to raise a `KeyError`, but it raises a `ValueError` instead:

```python
>>> import cudf
>>> df = cudf.DataFrame({'a': [1, 1, 2], 'b': [1, 2, 3]})
>>> df.groupby('cd').sum() 
...
ValueError: Grouper and object must have same length
```",2022-09-27T18:15:34Z,0,0,Ashwin Srinath,Voltron Data,False
358,"[FEA] Parquet reader code cleanup, re:  nested columns vs columns with lists.","In the parquet reader there are two similar-sounding but distinct pieces of terminology:

- Nested columns.  This is the same as in the cudf sense.  Anything involving structs or lists at any level.
- Nested hierarchies.  This only involves columns (or _parts_ of columns) that contain lists (represented via repetition levels).

This causes confusion and bugs for a couple of reasons. A given (cudf) output column can contain both nested and non-nested hierarchies.  For example:
```
         A (struct)
       /   \
      B     C (list)
            |
            D (int)
```
This single output column contains two separate input column hierarchies.  A->B and A->C->D.  A->B does not contain repetition data and therefore is not a nested hierarchy.  A->C->D does contain repetition data and does constitute a nested hierarchy.  However they are _both_ nested in the cudf sense (more than 1 level deep).

We handle these two fundamental situations differently during the decoding process.  So if the two concepts get confused it can easily cause bugs.   

It would be great to do a pass that cleans this up in a comprehensive way.

",2022-09-27T19:07:05Z,0,0,,,False
359,[BUG] sort_values on categorical column fails in dask_cudf,"**Describe the bug**
`ddf.sort_values(col)` does not work with a `dask_cudf` DataFrame when `col` is categorical.

**Steps/Code to reproduce bug**
```python
import cudf
import dask_cudf
df = cudf.DataFrame({""a"": list(""caba""), ""b"": list(range(4))})
df[""a""] = df[""a""].astype(""category"")
ddf = dask_cudf.from_cudf(df, npartitions=2)
df.sort_values(""a"")  # <-- works as expected
ddf.sort_values(""a"")  # raises
```
<details>
<summary>Traceback</summary>

```python-traceback
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In [7], line 1
----> 1 ddf.sort_values(""a"")

File ~/miniconda3/envs/cugraph_dev15/lib/python3.9/contextlib.py:79, in ContextDecorator.__call__.<locals>.inner(*args, **kwds)
     76 @wraps(func)
     77 def inner(*args, **kwds):
     78     with self._recreate_cm():
---> 79         return func(*args, **kwds)

File ~/miniconda3/envs/cugraph_dev15/lib/python3.9/site-packages/dask_cudf/core.py:256, in DataFrame.sort_values(self, by, ignore_index, max_branch, divisions, set_divisions, ascending, na_position, sort_function, sort_function_kwargs, **kwargs)
    251 if kwargs:
    252     raise ValueError(
    253         f""Unsupported input arguments passed : {list(kwargs.keys())}""
    254     )
--> 256 df = sorting.sort_values(
    257     self,
    258     by,
    259     max_branch=max_branch,
    260     divisions=divisions,
    261     set_divisions=set_divisions,
    262     ignore_index=ignore_index,
    263     ascending=ascending,
    264     na_position=na_position,
    265     sort_function=sort_function,
    266     sort_function_kwargs=sort_function_kwargs,
    267 )
    269 if ignore_index:
    270     return df.reset_index(drop=True)

File ~/miniconda3/envs/cugraph_dev15/lib/python3.9/contextlib.py:79, in ContextDecorator.__call__.<locals>.inner(*args, **kwds)
     76 @wraps(func)
     77 def inner(*args, **kwds):
     78     with self._recreate_cm():
---> 79         return func(*args, **kwds)

File ~/miniconda3/envs/cugraph_dev15/lib/python3.9/site-packages/dask_cudf/sorting.py:266, in sort_values(df, by, max_branch, divisions, set_divisions, ignore_index, ascending, na_position, sort_function, sort_function_kwargs)
    264 # Step 1 - Calculate new divisions (if necessary)
    265 if divisions is None:
--> 266     divisions = quantile_divisions(df, by, npartitions)
    268 # Step 2 - Perform repartitioning shuffle
    269 meta = df._meta._constructor_sliced([0])

File ~/miniconda3/envs/cugraph_dev15/lib/python3.9/contextlib.py:79, in ContextDecorator.__call__.<locals>.inner(*args, **kwds)
     76 @wraps(func)
     77 def inner(*args, **kwds):
     78     with self._recreate_cm():
---> 79         return func(*args, **kwds)

File ~/miniconda3/envs/cugraph_dev15/lib/python3.9/site-packages/dask_cudf/sorting.py:213, in quantile_divisions(df, by, npartitions)
    211 dtype = df[col].dtype
    212 if dtype != ""object"":
--> 213     divisions[col] = divisions[col].astype(""int64"")
    214     divisions[col].iloc[-1] += 1
    215     divisions[col] = divisions[col].astype(dtype)

File ~/miniconda3/envs/cugraph_dev15/lib/python3.9/contextlib.py:79, in ContextDecorator.__call__.<locals>.inner(*args, **kwds)
     76 @wraps(func)
     77 def inner(*args, **kwds):
     78     with self._recreate_cm():
---> 79         return func(*args, **kwds)

File ~/miniconda3/envs/cugraph_dev15/lib/python3.9/site-packages/cudf/core/series.py:1857, in Series.astype(self, dtype, copy, errors, **kwargs)
   1855 else:
   1856     dtype = {self.name: dtype}
-> 1857 return super().astype(dtype, copy, errors, **kwargs)

File ~/miniconda3/envs/cugraph_dev15/lib/python3.9/site-packages/cudf/core/indexed_frame.py:3264, in IndexedFrame.astype(self, dtype, copy, errors, **kwargs)
   3262 except Exception as e:
   3263     if errors == ""raise"":
-> 3264         raise e
   3265     return self
   3267 return self._from_data(data, index=self._index)

File ~/miniconda3/envs/cugraph_dev15/lib/python3.9/site-packages/cudf/core/indexed_frame.py:3261, in IndexedFrame.astype(self, dtype, copy, errors, **kwargs)
   3258     raise ValueError(""invalid error value specified"")
   3260 try:
-> 3261     data = super().astype(dtype, copy, **kwargs)
   3262 except Exception as e:
   3263     if errors == ""raise"":

File ~/miniconda3/envs/cugraph_dev15/lib/python3.9/contextlib.py:79, in ContextDecorator.__call__.<locals>.inner(*args, **kwds)
     76 @wraps(func)
     77 def inner(*args, **kwds):
     78     with self._recreate_cm():
---> 79         return func(*args, **kwds)

File ~/miniconda3/envs/cugraph_dev15/lib/python3.9/site-packages/cudf/core/frame.py:328, in Frame.astype(self, dtype, copy, **kwargs)
    326 dt = dtype.get(col_name, col.dtype)
    327 if not is_dtype_equal(dt, col.dtype):
--> 328     result[col_name] = col.astype(dt, copy=copy, **kwargs)
    329 else:
    330     result[col_name] = col.copy() if copy else col

File ~/miniconda3/envs/cugraph_dev15/lib/python3.9/site-packages/cudf/core/column/column.py:857, in ColumnBase.astype(self, dtype, **kwargs)
    851 dtype = (
    852     pandas_dtypes_alias_to_cudf_alias.get(dtype, dtype)
    853     if isinstance(dtype, str)
    854     else pandas_dtypes_to_np_dtypes.get(dtype, dtype)
    855 )
    856 if _is_non_decimal_numeric_dtype(dtype):
--> 857     return self.as_numerical_column(dtype, **kwargs)
    858 elif is_categorical_dtype(dtype):
    859     return self.as_categorical_column(dtype, **kwargs)

File ~/miniconda3/envs/cugraph_dev15/lib/python3.9/site-packages/cudf/core/column/categorical.py:1236, in CategoricalColumn.as_numerical_column(self, dtype, **kwargs)
   1235 def as_numerical_column(self, dtype: Dtype, **kwargs) -> NumericalColumn:
-> 1236     return self._get_decategorized_column().as_numerical_column(dtype)

File ~/miniconda3/envs/cugraph_dev15/lib/python3.9/site-packages/cudf/core/column/string.py:5312, in StringColumn.as_numerical_column(self, dtype, **kwargs)
   5310 if out_dtype.kind in {""i"", ""u""}:
   5311     if not libstrings.is_integer(string_col).all():
-> 5312         raise ValueError(
   5313             ""Could not convert strings to integer ""
   5314             ""type due to presence of non-integer values.""
   5315         )
   5316 elif out_dtype.kind == ""f"":
   5317     if not libstrings.is_float(string_col).all():

ValueError: Could not convert strings to integer type due to presence of non-integer values.
```

</details>

**Expected behavior**
I expect it to work--that is, match the result of cudf and dask.dataframe.

**Environment overview (please complete the following information)**
 - Environment location: Bare-metal
 - Method of cuDF install: conda

**Environment details**
<details><summary>Click here to see environment details</summary><pre>

     **git***
     commit 82b9922cc1635f6f0923f633a17a7c12f93ebdbe (HEAD -> pg_set_index_and_categorical, eriknw/pg_set_index_and_categorical)
     Author: Erik Welch <erik.n.welch@gmail.com>
     Date:   Tue Sep 27 11:28:04 2022 -0700

     workaround dask_cudf issue with `sort_values` on categorical column
     **git submodules***

     ***OS Information***
     DGX_NAME=""DGX Server""
     DGX_PRETTY_NAME=""NVIDIA DGX Server""
     DGX_SWBUILD_DATE=""2020-03-04""
     DGX_SWBUILD_VERSION=""4.4.0""
     DGX_COMMIT_ID=""ee09ebc""
     DGX_PLATFORM=""DGX Server for DGX-1""
     DGX_SERIAL_NUMBER=""QTFCOU8220028""

     DGX_R418_REPO_ENABLED=20220727-142458

     DGX_OTA_VERSION=""4.13.0""
     DGX_OTA_DATE=""Wed Jul 27 14:38:05 PDT 2022""
     DISTRIB_ID=Ubuntu
     DISTRIB_RELEASE=18.04
     DISTRIB_CODENAME=bionic
     DISTRIB_DESCRIPTION=""Ubuntu 18.04.6 LTS""
     NAME=""Ubuntu""
     VERSION=""18.04.6 LTS (Bionic Beaver)""
     ID=ubuntu
     ID_LIKE=debian
     PRETTY_NAME=""Ubuntu 18.04.6 LTS""
     VERSION_ID=""18.04""
     HOME_URL=""https://www.ubuntu.com/""
     SUPPORT_URL=""https://help.ubuntu.com/""
     BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
     PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
     VERSION_CODENAME=bionic
     UBUNTU_CODENAME=bionic
     Linux dgx12 4.15.0-189-generic #200-Ubuntu SMP Wed Jun 22 19:53:37 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux

     ***GPU Information***
     Tue Sep 27 12:26:17 2022
     +-----------------------------------------------------------------------------+
     | NVIDIA-SMI 515.48.07    Driver Version: 515.48.07    CUDA Version: 11.7     |
     |-------------------------------+----------------------+----------------------+
     | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
     | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
     |                               |                      |               MIG M. |
     |===============================+======================+======================|
     |   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |
     | N/A   32C    P0    42W / 300W |      3MiB / 32768MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |
     | N/A   30C    P0    42W / 300W |      3MiB / 32768MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |
     | N/A   28C    P0    41W / 300W |      3MiB / 32768MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |
     | N/A   28C    P0    41W / 300W |      3MiB / 32768MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |
     | N/A   30C    P0    42W / 300W |      3MiB / 32768MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |
     | N/A   30C    P0    41W / 300W |      3MiB / 32768MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |
     | N/A   33C    P0    43W / 300W |      3MiB / 32768MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |
     | N/A   29C    P0    41W / 300W |      3MiB / 32768MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+

     +-----------------------------------------------------------------------------+
     | Processes:                                                                  |
     |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
     |        ID   ID                                                   Usage      |
     |=============================================================================|
     |  No running processes found                                                 |
     +-----------------------------------------------------------------------------+

     ***CPU***
     Architecture:        x86_64
     CPU op-mode(s):      32-bit, 64-bit
     Byte Order:          Little Endian
     CPU(s):              80
     On-line CPU(s) list: 0-79
     Thread(s) per core:  2
     Core(s) per socket:  20
     Socket(s):           2
     NUMA node(s):        2
     Vendor ID:           GenuineIntel
     CPU family:          6
     Model:               79
     Model name:          Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz
     Stepping:            1
     CPU MHz:             3343.606
     CPU max MHz:         3600.0000
     CPU min MHz:         1200.0000
     BogoMIPS:            4389.85
     Virtualization:      VT-x
     L1d cache:           32K
     L1i cache:           32K
     L2 cache:            256K
     L3 cache:            51200K
     NUMA node0 CPU(s):   0-19,40-59
     NUMA node1 CPU(s):   20-39,60-79
     Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap intel_pt xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts md_clear flush_l1d

     ***CMake***
     /home/nfs/erwelch/miniconda3/envs/cugraph_dev15/bin/cmake
     cmake version 3.24.2

     CMake suite maintained and supported by Kitware (kitware.com/cmake).

     ***g++***
     /home/nfs/erwelch/miniconda3/envs/cugraph_dev15/bin/g++
     g++ (conda-forge gcc 10.4.0-16) 10.4.0
     Copyright (C) 2020 Free Software Foundation, Inc.
     This is free software; see the source for copying conditions.  There is NO
     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


     ***nvcc***
     /home/nfs/erwelch/miniconda3/envs/cugraph_dev15/bin/nvcc
     nvcc: NVIDIA (R) Cuda compiler driver
     Copyright (c) 2005-2021 NVIDIA Corporation
     Built on Thu_Nov_18_09:45:30_PST_2021
     Cuda compilation tools, release 11.5, V11.5.119
     Build cuda_11.5.r11.5/compiler.30672275_0

     ***Python***
     /home/nfs/erwelch/miniconda3/envs/cugraph_dev15/bin/python
     Python 3.9.13

     ***Environment Variables***
     PATH                            : /home/nfs/erwelch/miniconda3/envs/cugraph_dev15/bin:/home/nfs/erwelch/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
     LD_LIBRARY_PATH                 :
     NUMBAPRO_NVVM                   :
     NUMBAPRO_LIBDEVICE              :
     CONDA_PREFIX                    : /home/nfs/erwelch/miniconda3/envs/cugraph_dev15
     PYTHON_PATH                     :

     ***conda packages***
     /home/nfs/erwelch/miniconda3/condabin/conda
     # packages in environment at /home/nfs/erwelch/miniconda3/envs/cugraph_dev15:
     #
     # Name                    Version                   Build  Channel
     _libgcc_mutex             0.1                 conda_forge    conda-forge
     _openmp_mutex             4.5                       2_gnu    conda-forge
     alabaster                 0.7.12                     py_0    conda-forge
     argon2-cffi               21.3.0             pyhd8ed1ab_0    conda-forge
     argon2-cffi-bindings      21.2.0           py39hb9d737c_2    conda-forge
     arrow-cpp                 9.0.0           py39hd3ccb9b_2_cpu    conda-forge
     asttokens                 2.0.8              pyhd8ed1ab_0    conda-forge
     asvdb                     0.4.2               g90e8f2c_40    rapidsai
     attrs                     22.1.0             pyh71513ae_1    conda-forge
     aws-c-cal                 0.5.11               h95a6274_0    conda-forge
     aws-c-common              0.6.2                h7f98852_0    conda-forge
     aws-c-event-stream        0.2.7               h3541f99_13    conda-forge
     aws-c-io                  0.10.5               hfb6a706_0    conda-forge
     aws-checksums             0.1.11               ha31a3da_7    conda-forge
     aws-sdk-cpp               1.8.186              hb4091e7_3    conda-forge
     babel                     2.10.3             pyhd8ed1ab_0    conda-forge
     backcall                  0.2.0              pyh9f0ad1d_0    conda-forge
     backports                 1.0                        py_2    conda-forge
     backports.functools_lru_cache 1.6.4              pyhd8ed1ab_0    conda-forge
     beautifulsoup4            4.11.1             pyha770c72_0    conda-forge
     binutils                  2.36.1               hdd6e379_2    conda-forge
     binutils_impl_linux-64    2.36.1               h193b22a_2    conda-forge
     binutils_linux-64         2.36                hf3e587d_10    conda-forge
     bleach                    5.0.1              pyhd8ed1ab_0    conda-forge
     bokeh                     2.4.3              pyhd8ed1ab_3    conda-forge
     boost                     1.80.0           py39hac2352c_1    conda-forge
     boost-cpp                 1.80.0               h75c5d50_0    conda-forge
     boto3                     1.24.81            pyhd8ed1ab_0    conda-forge
     botocore                  1.27.81            pyhd8ed1ab_0    conda-forge
     brotlipy                  0.7.0           py39hb9d737c_1004    conda-forge
     bzip2                     1.0.8                h7f98852_4    conda-forge
     c-ares                    1.18.1               h7f98852_0    conda-forge
     c-compiler                1.5.0                h166bdaf_0    conda-forge
     ca-certificates           2022.9.24            ha878542_0    conda-forge
     cachetools                5.2.0              pyhd8ed1ab_0    conda-forge
     certifi                   2022.9.24          pyhd8ed1ab_0    conda-forge
     cffi                      1.15.1           py39he91dace_0    conda-forge
     charset-normalizer        2.1.1              pyhd8ed1ab_0    conda-forge
     clang                     11.1.0               ha770c72_1    conda-forge
     clang-11                  11.1.0          default_ha53f305_1    conda-forge
     clang-tools               11.1.0          default_ha53f305_1    conda-forge
     clangxx                   11.1.0          default_ha53f305_1    conda-forge
     click                     8.1.3            py39hf3d152e_0    conda-forge
     cloudpickle               2.2.0              pyhd8ed1ab_0    conda-forge
     cmake                     3.24.2               h5432695_0    conda-forge
     colorama                  0.4.5              pyhd8ed1ab_0    conda-forge
     commonmark                0.9.1                      py_0    conda-forge
     coverage                  6.4.4            py39hb9d737c_0    conda-forge
     cryptography              37.0.4           py39hd97740a_0    conda-forge
     cuda-python               11.7.0           py39h3fd9d12_0    nvidia
     cudatoolkit               11.5.1               hcf5317a_9    nvidia
     cudf                      22.10.00a220920 cuda_11_py39_g0528b38f2b_241    rapidsai-nightly
     cugraph                   22.10.0a0+84.gc2f983f0          pypi_0    pypi
     cupy                      11.1.0           py39hc3c280e_0    conda-forge
     cxx-compiler              1.5.0                h924138e_0    conda-forge
     cython                    0.29.32          py39h5a03fae_0    conda-forge
     cytoolz                   0.12.0           py39hb9d737c_0    conda-forge
     dask                      2022.9.1           pyhd8ed1ab_0    conda-forge
     dask-core                 2022.9.1           pyhd8ed1ab_0    conda-forge
     dask-cuda                 22.10.00a220927 py39_g8de9ce3_19    rapidsai-nightly
     dask-cudf                 22.10.00a220920 cuda_11_py39_g0528b38f2b_241    rapidsai-nightly
     debugpy                   1.6.3            py39h5a03fae_0    conda-forge
     decorator                 5.1.1              pyhd8ed1ab_0    conda-forge
     defusedxml                0.7.1              pyhd8ed1ab_0    conda-forge
     distributed               2022.9.1           pyhd8ed1ab_0    conda-forge
     distro                    1.6.0              pyhd8ed1ab_0    conda-forge
     dlpack                    0.5                  h9c3ff4c_0    conda-forge
     docutils                  0.19             py39hf3d152e_0    conda-forge
     doxygen                   1.9.5                h583eb01_0    conda-forge
     entrypoints               0.4                pyhd8ed1ab_0    conda-forge
     executing                 1.1.0              pyhd8ed1ab_0    conda-forge
     expat                     2.4.9                h27087fc_0    conda-forge
     fastavro                  1.6.1            py39hb9d737c_0    conda-forge
     fastrlock                 0.8              py39h5a03fae_2    conda-forge
     flake8                    5.0.4              pyhd8ed1ab_0    conda-forge
     flit-core                 3.7.1              pyhd8ed1ab_0    conda-forge
     freetype                  2.12.1               hca18f0e_0    conda-forge
     fsspec                    2022.8.2           pyhd8ed1ab_0    conda-forge
     future                    0.18.2           py39hf3d152e_5    conda-forge
     gcc                       10.4.0              hb92f740_10    conda-forge
     gcc_impl_linux-64         10.4.0              h7ee1905_16    conda-forge
     gcc_linux-64              10.4.0              h9215b83_10    conda-forge
     gflags                    2.2.2             he1b5a44_1004    conda-forge
     gh                        2.16.1               ha8f183a_0    conda-forge
     glog                      0.6.0                h6f12383_0    conda-forge
     gmock                     1.10.0               h4bd325d_7    conda-forge
     grpc-cpp                  1.47.1               hbad87ad_6    conda-forge
     gtest                     1.10.0               h4bd325d_7    conda-forge
     gxx                       10.4.0              hb92f740_10    conda-forge
     gxx_impl_linux-64         10.4.0              h7ee1905_16    conda-forge
     gxx_linux-64              10.4.0              h6e491c6_10    conda-forge
     heapdict                  1.0.1                      py_0    conda-forge
     icecream                  2.1.3              pyhd8ed1ab_0    conda-forge
     icu                       70.1                 h27087fc_0    conda-forge
     idna                      3.4                pyhd8ed1ab_0    conda-forge
     imagesize                 1.4.1              pyhd8ed1ab_0    conda-forge
     importlib-metadata        4.11.4           py39hf3d152e_0    conda-forge
     importlib_resources       5.9.0              pyhd8ed1ab_0    conda-forge
     iniconfig                 1.1.1              pyh9f0ad1d_0    conda-forge
     ipykernel                 6.16.0             pyh210e3f2_0    conda-forge
     ipython                   8.5.0              pyh41d4057_1    conda-forge
     ipython_genutils          0.2.0                      py_1    conda-forge
     jedi                      0.18.1             pyhd8ed1ab_2    conda-forge
     jinja2                    3.1.2              pyhd8ed1ab_1    conda-forge
     jmespath                  1.0.1              pyhd8ed1ab_0    conda-forge
     joblib                    1.2.0              pyhd8ed1ab_0    conda-forge
     jpeg                      9e                   h166bdaf_2    conda-forge
     jsonschema                4.16.0             pyhd8ed1ab_0    conda-forge
     jupyter_client            7.3.4              pyhd8ed1ab_0    conda-forge
     jupyter_core              4.11.1           py39hf3d152e_0    conda-forge
     jupyterlab_pygments       0.2.2              pyhd8ed1ab_0    conda-forge
     kernel-headers_linux-64   2.6.32              he073ed8_15    conda-forge
     keyutils                  1.6.1                h166bdaf_0    conda-forge
     krb5                      1.19.3               h3790be6_0    conda-forge
     lcms2                     2.12                 hddcbb42_0    conda-forge
     ld_impl_linux-64          2.36.1               hea4e1c9_2    conda-forge
     lerc                      4.0.0                h27087fc_0    conda-forge
     libabseil                 20220623.0      cxx17_h48a1fff_4    conda-forge
     libblas                   3.9.0           16_linux64_openblas    conda-forge
     libbrotlicommon           1.0.9                h166bdaf_7    conda-forge
     libbrotlidec              1.0.9                h166bdaf_7    conda-forge
     libbrotlienc              1.0.9                h166bdaf_7    conda-forge
     libcblas                  3.9.0           16_linux64_openblas    conda-forge
     libclang-cpp11.1          11.1.0          default_ha53f305_1    conda-forge
     libcrc32c                 1.1.2                h9c3ff4c_0    conda-forge
     libcudf                   22.10.00a220920 cuda11_g0528b38f2b_241    rapidsai-nightly
     libcugraphops             22.10.00a220927 cuda11_g553bacf_29    rapidsai-nightly
     libcurl                   7.83.1               h7bff187_0    conda-forge
     libcusolver               11.4.0.1                      0    nvidia
     libcusparse               11.7.4.91                     0    nvidia
     libdeflate                1.14                 h166bdaf_0    conda-forge
     libedit                   3.1.20191231         he28a2e2_2    conda-forge
     libev                     4.33                 h516909a_1    conda-forge
     libevent                  2.1.10               h9b69904_4    conda-forge
     libffi                    3.4.2                h7f98852_5    conda-forge
     libgcc-devel_linux-64     10.4.0              h74af60c_16    conda-forge
     libgcc-ng                 12.1.0              h8d9b700_16    conda-forge
     libgfortran-ng            12.1.0              h69a702a_16    conda-forge
     libgfortran5              12.1.0              hdcd56e2_16    conda-forge
     libgomp                   12.1.0              h8d9b700_16    conda-forge
     libgoogle-cloud           2.1.0                h9ebe8e8_2    conda-forge
     libiconv                  1.17                 h166bdaf_0    conda-forge
     liblapack                 3.9.0           16_linux64_openblas    conda-forge
     libllvm11                 11.1.0               hf817b99_3    conda-forge
     libnghttp2                1.47.0               hdcd2b5c_1    conda-forge
     libnsl                    2.0.0                h7f98852_0    conda-forge
     libopenblas               0.3.21          pthreads_h78a6416_3    conda-forge
     libpng                    1.6.38               h753d276_0    conda-forge
     libprotobuf               3.20.1               h6239696_4    conda-forge
     libraft-distance          22.10.00a220927 cuda11_g1dd2feb1_54    rapidsai-nightly
     libraft-headers           22.10.00a220927 cuda11_g1dd2feb1_54    rapidsai-nightly
     librmm                    22.10.00a220927 cuda11_g6e0d65a9_20    rapidsai-nightly
     libsanitizer              10.4.0              hde28e3b_16    conda-forge
     libsodium                 1.0.18               h36c2ea0_1    conda-forge
     libsqlite                 3.39.3               h753d276_0    conda-forge
     libssh2                   1.10.0               haa6b8db_3    conda-forge
     libstdcxx-devel_linux-64  10.4.0              h74af60c_16    conda-forge
     libstdcxx-ng              12.1.0              ha89aaad_16    conda-forge
     libthrift                 0.16.0               h491838f_2    conda-forge
     libtiff                   4.4.0                h55922b4_4    conda-forge
     libutf8proc               2.7.0                h7f98852_0    conda-forge
     libuuid                   2.32.1            h7f98852_1000    conda-forge
     libuv                     1.44.2               h166bdaf_0    conda-forge
     libwebp-base              1.2.4                h166bdaf_0    conda-forge
     libxcb                    1.13              h7f98852_1004    conda-forge
     libxml2                   2.10.2               h4c7fe37_1    conda-forge
     libxslt                   1.1.35               h8affb1d_0    conda-forge
     libzlib                   1.2.12               h166bdaf_3    conda-forge
     llvmlite                  0.38.1           py39h7d9a04d_0    conda-forge
     locket                    1.0.0              pyhd8ed1ab_0    conda-forge
     lxml                      4.9.1            py39hb9d737c_0    conda-forge
     lz4                       4.0.0            py39h029007f_2    conda-forge
     lz4-c                     1.9.3                h9c3ff4c_1    conda-forge
     make                      4.3                  hd18ef5c_1    conda-forge
     markdown                  3.4.1              pyhd8ed1ab_0    conda-forge
     markupsafe                2.1.1            py39hb9d737c_1    conda-forge
     matplotlib-inline         0.1.6              pyhd8ed1ab_0    conda-forge
     mccabe                    0.7.0              pyhd8ed1ab_0    conda-forge
     mistune                   2.0.4              pyhd8ed1ab_0    conda-forge
     msgpack-python            1.0.4            py39hf939315_0    conda-forge
     nbclient                  0.6.8              pyhd8ed1ab_0    conda-forge
     nbconvert                 7.0.0              pyhd8ed1ab_0    conda-forge
     nbconvert-core            7.0.0              pyhd8ed1ab_0    conda-forge
     nbconvert-pandoc          7.0.0              pyhd8ed1ab_0    conda-forge
     nbformat                  5.6.1              pyhd8ed1ab_0    conda-forge
     nbsphinx                  0.8.9              pyhd8ed1ab_0    conda-forge
     nccl                      2.14.3.1             h0800d71_0    conda-forge
     ncurses                   6.3                  h27087fc_1    conda-forge
     nest-asyncio              1.5.5              pyhd8ed1ab_0    conda-forge
     networkx                  2.8.6              pyhd8ed1ab_0    conda-forge
     notebook                  6.4.12             pyha770c72_0    conda-forge
     numba                     0.55.2           py39h66db6d7_0    conda-forge
     numpy                     1.22.4           py39hc58783e_0    conda-forge
     numpydoc                  1.4.0              pyhd8ed1ab_1    conda-forge
     nvcc_linux-64             10.1                hcaf9a05_10
     nvtx                      0.2.3            py39h3811e60_1    conda-forge
     openjpeg                  2.5.0                h7d73246_1    conda-forge
     openssl                   1.1.1q               h166bdaf_0    conda-forge
     orc                       1.7.6                h6c59b99_0    conda-forge
     packaging                 21.3               pyhd8ed1ab_0    conda-forge
     pandas                    1.4.4            py39h1832856_0    conda-forge
     pandoc                    2.19.2               ha770c72_0    conda-forge
     pandocfilters             1.5.0              pyhd8ed1ab_0    conda-forge
     parquet-cpp               1.5.1                         2    conda-forge
     parso                     0.8.3              pyhd8ed1ab_0    conda-forge
     partd                     1.3.0              pyhd8ed1ab_0    conda-forge
     pexpect                   4.8.0              pyh9f0ad1d_2    conda-forge
     pickleshare               0.7.5                   py_1003    conda-forge
     pillow                    9.2.0            py39hd5dbb17_2    conda-forge
     pip                       22.2.2             pyhd8ed1ab_0    conda-forge
     pkgutil-resolve-name      1.3.10             pyhd8ed1ab_0    conda-forge
     pluggy                    1.0.0            py39hf3d152e_3    conda-forge
     prometheus_client         0.14.1             pyhd8ed1ab_0    conda-forge
     prompt-toolkit            3.0.31             pyha770c72_0    conda-forge
     protobuf                  3.20.1           py39h5a03fae_0    conda-forge
     psutil                    5.9.2            py39hb9d737c_0    conda-forge
     pthread-stubs             0.4               h36c2ea0_1001    conda-forge
     ptxcompiler               0.2.0            py39h107f55c_0    rapidsai
     ptyprocess                0.7.0              pyhd3deb0d_0    conda-forge
     pure_eval                 0.2.2              pyhd8ed1ab_0    conda-forge
     py                        1.11.0             pyh6c4a22f_0    conda-forge
     py-cpuinfo                8.0.0              pyhd8ed1ab_0    conda-forge
     pyarrow                   9.0.0           py39hc0775d8_2_cpu    conda-forge
     pycodestyle               2.9.1              pyhd8ed1ab_0    conda-forge
     pycparser                 2.21               pyhd8ed1ab_0    conda-forge
     pydata-sphinx-theme       0.10.1             pyhd8ed1ab_0    conda-forge
     pyflakes                  2.5.0              pyhd8ed1ab_0    conda-forge
     pygal                     2.4.0                      py_0    conda-forge
     pygments                  2.13.0             pyhd8ed1ab_0    conda-forge
     pylibcugraph              22.10.0a0+84.gc2f983f0           dev_0    <develop>
     pylibraft                 22.10.00a220927 cuda11_py39_g1dd2feb1_54    rapidsai-nightly
     pynvml                    11.4.1             pyhd8ed1ab_0    conda-forge
     pyopenssl                 22.0.0             pyhd8ed1ab_1    conda-forge
     pyparsing                 3.0.9              pyhd8ed1ab_0    conda-forge
     pyrsistent                0.18.1           py39hb9d737c_1    conda-forge
     pysocks                   1.7.1              pyha2e5f31_6    conda-forge
     pytest                    7.1.3            py39hf3d152e_0    conda-forge
     pytest-benchmark          3.2.3              pyh9f0ad1d_0    conda-forge
     pytest-cov                3.0.0              pyhd8ed1ab_0    conda-forge
     python                    3.9.13          h9a8a25e_0_cpython    conda-forge
     python-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge
     python-fastjsonschema     2.16.2             pyhd8ed1ab_0    conda-forge
     python_abi                3.9                      2_cp39    conda-forge
     pytz                      2022.2.1           pyhd8ed1ab_0    conda-forge
     pyyaml                    6.0              py39hb9d737c_4    conda-forge
     pyzmq                     24.0.1           py39headdf64_0    conda-forge
     raft-dask                 22.10.00a220927 cuda11_py39_g1dd2feb1_54    rapidsai-nightly
     rapids-pytest-benchmark   0.0.14                     py_0    rapidsai
     re2                       2022.06.01           h27087fc_0    conda-forge
     readline                  8.1.2                h0f457ee_0    conda-forge
     recommonmark              0.7.1              pyhd8ed1ab_0    conda-forge
     requests                  2.28.1             pyhd8ed1ab_1    conda-forge
     rhash                     1.4.3                h166bdaf_0    conda-forge
     rmm                       22.10.00a220927 cuda11_py39_g6e0d65a9_20    rapidsai-nightly
     s2n                       1.0.10               h9b69904_0    conda-forge
     s3transfer                0.6.0              pyhd8ed1ab_0    conda-forge
     scikit-build              0.15.0             pyhb871ab6_0    conda-forge
     scikit-learn              1.1.2            py39he5e8d7e_0    conda-forge
     scipy                     1.9.1            py39h8ba3f38_0    conda-forge
     send2trash                1.8.0              pyhd8ed1ab_0    conda-forge
     setuptools                65.4.0                   pypi_0    pypi
     six                       1.16.0             pyh6c4a22f_0    conda-forge
     snappy                    1.1.9                hbd366e4_1    conda-forge
     snowballstemmer           2.2.0              pyhd8ed1ab_0    conda-forge
     sortedcontainers          2.4.0              pyhd8ed1ab_0    conda-forge
     soupsieve                 2.3.2.post1        pyhd8ed1ab_0    conda-forge
     spdlog                    1.8.5                h4bd325d_1    conda-forge
     sphinx                    5.2.1              pyhd8ed1ab_0    conda-forge
     sphinx-copybutton         0.5.0              pyhd8ed1ab_0    conda-forge
     sphinx-markdown-tables    0.0.17             pyh6c4a22f_0    conda-forge
     sphinxcontrib-applehelp   1.0.2                      py_0    conda-forge
     sphinxcontrib-devhelp     1.0.2                      py_0    conda-forge
     sphinxcontrib-htmlhelp    2.0.0              pyhd8ed1ab_0    conda-forge
     sphinxcontrib-jsmath      1.0.1                      py_0    conda-forge
     sphinxcontrib-qthelp      1.0.3                      py_0    conda-forge
     sphinxcontrib-serializinghtml 1.1.5              pyhd8ed1ab_2    conda-forge
     sphinxcontrib-websupport  1.2.4              pyhd8ed1ab_1    conda-forge
     sqlite                    3.39.3               h4ff8645_0    conda-forge
     stack_data                0.5.1              pyhd8ed1ab_0    conda-forge
     sysroot_linux-64          2.12                he073ed8_15    conda-forge
     tblib                     1.7.0              pyhd8ed1ab_0    conda-forge
     terminado                 0.15.0           py39hf3d152e_0    conda-forge
     threadpoolctl             3.1.0              pyh8a188c0_0    conda-forge
     tinycss2                  1.1.1              pyhd8ed1ab_0    conda-forge
     tk                        8.6.12               h27826a3_0    conda-forge
     toml                      0.10.2             pyhd8ed1ab_0    conda-forge
     tomli                     2.0.1              pyhd8ed1ab_0    conda-forge
     toolz                     0.12.0             pyhd8ed1ab_0    conda-forge
     tornado                   6.1              py39hb9d737c_3    conda-forge
     traitlets                 5.4.0              pyhd8ed1ab_0    conda-forge
     typing_extensions         4.3.0              pyha770c72_0    conda-forge
     tzdata                    2022d                h191b570_0    conda-forge
     ucx                       1.13.1               h538f049_0    conda-forge
     ucx-proc                  1.0.0                       gpu    rapidsai
     ucx-py                    0.28.00a220926  py39_g8e07f67_25    rapidsai-nightly
     urllib3                   1.26.11            pyhd8ed1ab_0    conda-forge
     wcwidth                   0.2.5              pyh9f0ad1d_2    conda-forge
     webencodings              0.5.1                      py_1    conda-forge
     wheel                     0.37.1             pyhd8ed1ab_0    conda-forge
     xorg-libxau               1.0.9                h7f98852_0    conda-forge
     xorg-libxdmcp             1.1.3                h7f98852_0    conda-forge
     xz                        5.2.6                h166bdaf_0    conda-forge
     yaml                      0.2.5                h7f98852_2    conda-forge
     zeromq                    4.3.4                h9c3ff4c_1    conda-forge
     zict                      2.2.0              pyhd8ed1ab_0    conda-forge
     zipp                      3.8.1              pyhd8ed1ab_0    conda-forge
     zlib                      1.2.12               h166bdaf_3    conda-forge
     zstd                      1.5.2                h6239696_4    conda-forge

</pre></details>
**Additional context**
Encountered in ProperterGraph in cugraph.",2022-09-27T19:29:34Z,0,0,Erik Welch,,False
360,[QST] OOM issue while loading the 26GB twitter dataset into 128GB GPU memory,"Hey I try to load the twitter graph in a AWS `p3.16xlarge` instance, which has 8 16GB memory GPUs, in total 128GB. However, it is OOM. Could you please take a look if I missed anything? Thanks so much!

```python
import dask
from dask_cuda import LocalCUDACluster
from dask.distributed import Client
import dask_cudf
import cugraph
import cugraph.dask as dask_cugraph
from cugraph.dask.common.mg_utils import get_visible_devices
from cugraph.dask.comms import comms as Comms
import time

csv_file_name = ""twitter-2010.csv""

with dask.config.set(jit_unspill=True):
    with LocalCUDACluster(n_workers=8, device_memory_limit=""16GB"") as cluster:
        with Client(cluster) as client:
            client.wait_for_workers(len(get_visible_devices()))
            Comms.initialize(p2p=True)
            chunksize = dask_cugraph.get_chunksize(csv_file_name)
            ddf = dask_cudf.read_csv(csv_file_name, chunksize=chunksize, delimiter=' ', names=['src', 'dst'], dtype=['int32', 'int32'])
            ddf.compute()
            # G = cugraph.Graph(directed=True)
            # G.from_dask_cudf_edgelist(ddf, source='src', destination='dst')
```

I can't find similar issues,  this [one](https://github.com/rapidsai/cudf/issues/6087) got similar errors but it is because LocalCUDACluster is not used.

I used the docker approach to install the rapid frameworks:

```cmd
docker pull rapidsai/rapidsai-dev:22.08-cuda11.5-devel-ubuntu20.04-py3.9
docker run --gpus all --rm -it \
    --shm-size=10g --ulimit memlock=-1 \
    -p 8888:8888 -p 8787:8787 -p 8786:8786 \
    rapidsai/rapidsai-dev:22.08-cuda11.5-devel-ubuntu20.04-py3.9
```

The error log:

```
2022-09-27 13:03:21,520 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2022-09-27 13:03:21,520 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2022-09-27 13:03:21,524 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2022-09-27 13:03:21,524 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2022-09-27 13:03:21,544 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2022-09-27 13:03:21,544 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2022-09-27 13:03:21,554 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2022-09-27 13:03:21,554 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2022-09-27 13:03:21,555 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2022-09-27 13:03:21,555 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2022-09-27 13:03:21,556 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2022-09-27 13:03:21,556 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2022-09-27 13:03:21,597 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2022-09-27 13:03:21,597 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2022-09-27 13:03:21,673 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2022-09-27 13:03:21,673 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
---------------------------------------------------------------------------
MemoryError                               Traceback (most recent call last)
/tmp/ipykernel_5947/1798640855.py in <module>
      9             chunksize = dask_cugraph.get_chunksize(csv_file_name)
     10             ddf = dask_cudf.read_csv(csv_file_name, chunksize=chunksize, delimiter=' ', names=['src', 'dst'], dtype=['int32', 'int32'])
---> 11             ddf.compute()
     12             # G = cugraph.Graph(directed=True)
     13             # G.from_dask_cudf_edgelist(ddf, source='src', destination='dst')

/opt/conda/envs/rapids/lib/python3.9/site-packages/dask/base.py in compute(self, **kwargs)
    313         dask.base.compute
    314         """"""
--> 315         (result,) = compute(self, traverse=False, **kwargs)
    316         return result
    317 

/opt/conda/envs/rapids/lib/python3.9/site-packages/dask/base.py in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)
    597 
    598     results = schedule(dsk, keys, **kwargs)
--> 599     return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])
    600 
    601 

/opt/conda/envs/rapids/lib/python3.9/site-packages/dask/base.py in <listcomp>(.0)
    597 
    598     results = schedule(dsk, keys, **kwargs)
--> 599     return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])
    600 
    601 

/opt/conda/envs/rapids/lib/python3.9/site-packages/dask/dataframe/core.py in finalize(results)
    136 
    137 def finalize(results):
--> 138     return _concat(results)
    139 
    140 

/opt/conda/envs/rapids/lib/python3.9/site-packages/dask_cuda-22.8.0-py3.9.egg/dask_cuda/proxify_device_objects.py in wrapper(*args, **kwargs)
    167     @functools.wraps(func)
    168     def wrapper(*args, **kwargs):
--> 169         ret = func(*args, **kwargs)
    170         if dask.config.get(""jit-unspill-compatibility-mode"", default=False):
    171             ret = unproxify_device_objects(ret, skip_explicit_proxies=False)

/opt/conda/envs/rapids/lib/python3.9/site-packages/dask/dataframe/core.py in _concat(args, ignore_index)
    131         args[0]
    132         if not args2
--> 133         else methods.concat(args2, uniform=True, ignore_index=ignore_index)
    134     )
    135 

/opt/conda/envs/rapids/lib/python3.9/site-packages/dask/dataframe/dispatch.py in concat(dfs, axis, join, uniform, filter_warning, ignore_index, **kwargs)
     60     else:
     61         func = concat_dispatch.dispatch(type(dfs[0]))
---> 62         return func(
     63             dfs,
     64             axis=axis,

/opt/conda/envs/rapids/lib/python3.9/site-packages/dask_cuda-22.8.0-py3.9.egg/dask_cuda/proxy_object.py in wrapper(*args, **kwargs)
    900         args = [unproxy(d) for d in args]
    901         kwargs = {k: unproxy(v) for k, v in kwargs.items()}
--> 902         return func(*args, **kwargs)
    903 
    904     return wrapper

/opt/conda/envs/rapids/lib/python3.9/site-packages/dask/dataframe/dispatch.py in concat(dfs, axis, join, uniform, filter_warning, ignore_index, **kwargs)
     60     else:
     61         func = concat_dispatch.dispatch(type(dfs[0]))
---> 62         return func(
     63             dfs,
     64             axis=axis,

/opt/conda/envs/rapids/lib/python3.9/contextlib.py in inner(*args, **kwds)
     77         def inner(*args, **kwds):
     78             with self._recreate_cm():
---> 79                 return func(*args, **kwds)
     80         return inner
     81 

/opt/conda/envs/rapids/lib/python3.9/site-packages/dask_cudf/backends.py in concat_cudf(dfs, axis, join, uniform, filter_warning, sort, ignore_index, **kwargs)
    273         )
    274 
--> 275     return cudf.concat(dfs, axis=axis, ignore_index=ignore_index)
    276 
    277 

/opt/conda/envs/rapids/lib/python3.9/site-packages/cudf/core/reshape.py in concat(objs, axis, join, ignore_index, sort)
    397                 # don't filter out empty df's
    398                 objs = old_objs
--> 399             result = cudf.DataFrame._concat(
    400                 objs,
    401                 axis=axis,

/opt/conda/envs/rapids/lib/python3.9/contextlib.py in inner(*args, **kwds)
     77         def inner(*args, **kwds):
     78             with self._recreate_cm():
---> 79                 return func(*args, **kwds)
     80         return inner
     81 

/opt/conda/envs/rapids/lib/python3.9/site-packages/cudf/core/dataframe.py in _concat(cls, objs, axis, join, ignore_index, sort)
   1674         # Concatenate the Tables
   1675         out = cls._from_data(
-> 1676             *libcudf.concat.concat_tables(
   1677                 tables, ignore_index=ignore_index or are_all_range_index
   1678             )

concat.pyx in cudf._lib.concat.concat_tables()

concat.pyx in cudf._lib.concat.concat_tables()
```
",2022-09-27T20:05:54Z,0,0,zhao feng,,False
361,[FEA] Support DataFrame.div() in axis=0,"**Is your feature request related to a problem? Please describe.**
Hi!

While porting some Pandas code to cuDF, I have been making use of the new `cudf.crosstab()` feature. Unfortunately, the parameter `normalize='index'` is not implemented yet, so I decided to perform the normalization by index by hand.

I had planned to do it as follows, but `DataFrame.div()` is not supported in `axis = 0`:

```python
gdf = gdf.div(gdf.sum(axis=1), axis=0)

---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
/tmp/ipykernel_5103/715192400.py in <module>
----> 1 gdf = gdf.div(gdf.sum(axis=1), axis=0)

/opt/conda/envs/rapids/lib/python3.9/site-packages/cudf/core/dataframe.py in wrapper(self, other, axis, level, fill_value)
   6887     def wrapper(self, other, axis=""columns"", level=None, fill_value=None):
   6888         if axis not in (1, ""columns""):
-> 6889             raise NotImplementedError(""Only axis=1 supported at this time."")
   6890         output = wrapped_func(self, other, axis, level, fill_value)
   6891         if postprocess is None:

NotImplementedError: Only axis=1 supported at this time.

```

Therefore, I have performed the following workaround:

```python
tmp = gdf.sum(axis=1)
for col in gdf.columns:
    gdf[col] = gdf[col]/tmp
```

**Describe the solution you'd like**
I would like that `DataFrame.div()` would support `axis=0`. Thanks!!!!

**Additional context**
Please, find below a reproducer:

```python
import cudf
import pandas as pd

print(cudf.__version__, pd.__version__, '\n')

features = {'y1': [2, 1],
            'y2': [1, 1],
            'y3': [0, 2]}

pdf = pd.DataFrame(features)
gdf = cudf.DataFrame(features)

pdf = pdf.div(pdf.sum(axis=1), axis=0)

gdf = gdf.div(gdf.sum(axis=1), axis=0)
# 
# The code above fails as follows:
#     ---------------------------------------------------------------------------
#     NotImplementedError                       Traceback (most recent call last)
#     /tmp/ipykernel_5103/715192400.py in <module>
#     ----> 1 gdf = gdf.div(gdf.sum(axis=1), axis=0)

#     /opt/conda/envs/rapids/lib/python3.9/site-packages/cudf/core/dataframe.py in wrapper(self, other, axis, level, fill_value)
#        6887     def wrapper(self, other, axis=""columns"", level=None, fill_value=None):
#        6888         if axis not in (1, ""columns""):
#     -> 6889             raise NotImplementedError(""Only axis=1 supported at this time."")
#        6890         output = wrapped_func(self, other, axis, level, fill_value)
#        6891         if postprocess is None:

#     NotImplementedError: Only axis=1 supported at this time.

tmp = gdf.sum(axis=1)
for col in gdf.columns:
    gdf[col] = gdf[col]/tmp
    
print(gdf.to_pandas().equals(pdf))
```",2022-10-11T08:02:28Z,0,0,Miguel Martínez,NVIDIA,True
362,[FEA] Support 'normalize' parameter in cudf.crosstab,"**Is your feature request related to a problem? Please describe.**
Hi!

While porting some code to Pandas, I have noticed that `normalize` parameter is not supported in `cudf.crosstab`.

**Describe the solution you'd like**
I would like to have `normalize` parameter supported. Thanks!!!

**Describe alternatives you've considered**
I have performed the normalization by hand. While doing it, I have also found another feature missing. Please see issue https://github.com/rapidsai/cudf/issues/11894

**Additional context**
Please, find below the code that fails, and the workaround I have implemented:

```python
import cudf
import pandas as pd

print(cudf.__version__, pd.__version__, '\n')

features = {'x': ['x1', 'x1', 'x2', 'x2', 'x2', 'x1', 'x2'],
            'y': ['y1', 'y2', 'y1', 'y2', 'y3', 'y1', 'y3']}

pdf = pd.DataFrame(features)
pdf = pd.crosstab(index = pdf.x, columns = pdf.y, normalize='index')

gdf = cudf.DataFrame(features)
gdf = cudf.crosstab(index = gdf.x, columns = gdf.y, normalize='index')

# The code above fails as follows:
#     ---------------------------------------------------------------------------
#     NotImplementedError                       Traceback (most recent call last)
#     /tmp/ipykernel_1281/4238469792.py in <module>
#           1 gdf = cudf.DataFrame(features)
#     ----> 2 gdf = cudf.crosstab(index = gdf.x, columns = gdf.y, normalize='index')

#     /opt/conda/envs/rapids/lib/python3.9/site-packages/cudf/core/reshape.py in crosstab(index, columns, values, rownames, colnames, aggfunc, margins, margins_name, dropna, normalize)
#        1252     """"""
#        1253     if normalize is not False:
#     -> 1254         raise NotImplementedError(""normalize is not supported yet"")
#        1255 
#        1256     if values is None and aggfunc is not None:

#     NotImplementedError: normalize is not supported yet

gdf = cudf.crosstab(index = gdf.x, columns = gdf.y)

# The following code workarounds feature request https://github.com/rapidsai/cudf/issues/11894
tmp = gdf.sum(axis=1)
for col in gdf.columns:
    gdf[col] = gdf[col]/tmp

print(pdf, '\n', gdf)
```
",2022-10-11T08:19:37Z,0,0,Miguel Martínez,NVIDIA,True
363,[BUG] Column types after cudf.crosstab() does not match Pandas result,"**Describe the bug**
Hi

While porting some code from Pandas, I have noticed that the column types after `cudf.crosstab()` does not match Pandas result.

Please, see a reproducer below:

```python
> > import cudf
> > import pandas as pd
> > 
> > print(cudf.__version__, pd.__version__, '\n')
> > 
> > features = {'x': ['x1', 'x1', 'x2', 'x2', 'x2', 'x1', 'x2'],
> >             'y': ['y1', 'y2', 'y1', 'y2', 'y3', 'y1', 'y3']}
> > 
> > pdf = pd.DataFrame(features)
> > pdf = pd.crosstab(index = pdf.x, columns = pdf.y)
> > 
> > gdf = cudf.DataFrame(features)
> > gdf = cudf.crosstab(index = gdf.x, columns = gdf.y)
> > 
> > print(gdf.to_pandas().equals(pdf), '\n')
> > print(pdf.columns, '\n', gdf.columns)
> > 
> > 22.10.00a0+g17868b7 1.5.0 
> > 
> > False 
> > 
> > Index(['y1', 'y2', 'y3'], dtype='object', name='y') 
> >  MultiIndex([('y1',),
> >             ('y2',),
> >             ('y3',)],
> >            names=['y'])
> 
```

**Expected behavior**
I would like the results between cuDF and Pandas match.",2022-10-11T08:33:53Z,0,0,Miguel Martínez,NVIDIA,True
364,[TASK][JNI] Investigate train of `null_count` after `explode`,"While analyzing an nsys trace for a Spark job with deeply nested tables, we see an `explode` kernel call that is followed by a train of `null_count`, which end in `is_valid`. 

After we call `cudf::explode` we build up a table, and construct java `ColumnVector` objects. I think the construction of these objects is triggering it.

This task is to confirm that the columns with missing a null count are coming from the `explode` kernels. If they are coming from `explode`, it would be great if `explode` could compute null count as part of that kernel.

In this screenshot, it is the ~20ms at the end after `explode`:
![Screenshot from 2022-10-14 11-21-24](https://user-images.githubusercontent.com/1901059/195894495-ee65f7e6-0387-4bd6-990b-185bcc11a659.png)
",2022-10-14T16:22:33Z,0,0,Alessandro Bellina,NVIDIA,True
365,"[FEA] dataframe.corr() missing ""kendall"" method","**Is your feature request related to a problem? Please describe.**
Pandas has 4 options for the methods parameter in the corr() function: ""pearson"", ""spearman"", ""kendall"", and ""callable"" which accepts a callable object instead of a predetermined algorithm: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html

 CuDF currently only supports ""pearson"" and ""spearman"": https://docs.rapids.ai/api/cudf/stable/api_docs/api/cudf.DataFrame.corr.html

**Describe the solution you'd like**
Can we evaluate cudf and dask+cudf to implement the ""kendall"" correlation method?

**Context**
NVIDIA Solutions Architect, filing on behalf of customer

Related request for ""callable"" method: https://github.com/rapidsai/cudf/issues/11926
",2022-10-14T18:00:17Z,1,0,,,False
366,[FEA] dataframe.mode() axis parameter not supported,"**Is your feature request related to a problem? Please describe.**
The cudf documentation lists the mode function's {axis=0,1} parameter, but then has an additional note that ""axis parameter is currently not supported."":  https://docs.rapids.ai/api/cudf/stable/api_docs/api/cudf.DataFrame.mode.html 

**Describe the solution you'd like**
Is this feature possible to be implemented? Or is there an underlying challenge with replicating this pandas functionality in cudf?

**Additional context**
NVIDIA Solutions Architect, filing on behalf of customer
",2022-10-14T18:08:52Z,0,0,,,False
367,"[FEA] dataframe.corr() missing ""callable"" method","Is your feature request related to a problem? Please describe.
Pandas has 4 options for the methods parameter in the corr() function: ""pearson"", ""spearman"", ""kendall"", and ""callable"" which accepts a callable object instead of a predetermined algorithm: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html

CuDF currently only supports ""pearson"" and ""spearman"": https://docs.rapids.ai/api/cudf/stable/api_docs/api/cudf.DataFrame.corr.html

Describe the solution you'd like
Is it feasible to implement ""callable"" methods in cudf and dask+cudf?

Context
NVIDIA Solutions Architect, filing on behalf of customer

related request for ""kendall"" method: https://github.com/rapidsai/cudf/issues/11924 
",2022-10-14T18:52:08Z,0,0,,,False
368,[BUG] Assignment of string list to column doesn't work,"**Describe the bug**

This does not work:

df.loc[df['column'] =='value', 'column2'] = ['0','1']

TypeError: Implicit conversion to a host NumPy array via __array__ is not allowed, To explicitly construct a GPU matrix, consider using .to_cupy()
To explicitly construct a host matrix, consider using .to_numpy().

Integers do work:
df.loc[df['column']=='value', 'column2'] = [0,1]

Both work in pandas.

Rapids 22.08
Ubuntu 20
",2022-10-19T06:53:58Z,0,0,Üllar Kask,,False
369,[BUG] CSV reader cannot handle unquoted quote character appearing in a field,"**Describe the bug**
This is directly from https://github.com/NVIDIA/spark-rapids/issues/6435 If you have a field like `abc""""` in a CSV file the cudf CSV parser stops processing more data.

**Steps/Code to reproduce bug**
Create a file `test.csv` with the following data in it.

```
134324937434,#1991 N Grayhawk,"""",Menlo Park,89025,AB,United States,US
208564744937,""63,trevion Way"","""",st Lothian,h7f4h8,"""",United Kingdom,GB
132709376823,16 Oakland PARK RD,"""",ring,l1w1e4,South,Canada,CA
224867848652,7 kingwell Court,"""",United,s7jd9,South United,United Kingdom,GB
169636884295,30 cartuja Road,"""",Halifax,L0R 9p2,ON,Canada,CA
859473321609,Street,"""",Manchester,92220,OR,United States,US
141096112545,99 rue des,"""",Australia,jsd9je,"""",France,FR
160397658930,5 Rise,"""",walligshngton,RY6 8LT,FORT,United Kingdom,GB
726367494002,1852 Townsend st,666,Wallsend,90382,CA,United States,US
187644735867,Bärbel-HAMPDEN-Ping 37,"""",Miami,13355,"""",Kingdom,ZZ
948475348324,155 sw City ct,Rochdale,Germany,30864,FL,Australia,QQ
164083193213,abc"""","""",Jerez Fra.,11401,Cadiz,Spain,ES
198732413077,3p Grove Rochdale road,BAW,Fulifax,HX4 trW,"""",Israel,GB
227433927227,95 novem blvd,"""",RAW VILLAGE,3173,XYZ,Australia,IL
```

Now try to read it using CUDF.  The last two rows are skipped, and the `acb""""` is read back missing the last `""`

(From spark using the rapids plugin for apache spark)

```
+------------+--------------------+--------+-------------+-------+------------+--------------+---+
|         _c0|                 _c1|     _c2|          _c3|    _c4|         _c5|           _c6|_c7|
+------------+--------------------+--------+-------------+-------+------------+--------------+---+
|134324937434|    #1991 N Grayhawk|    null|   Menlo Park|  89025|          AB| United States| US|
|208564744937|      63,trevion Way|    null|   st Lothian| h7f4h8|        null|United Kingdom| GB|
|132709376823|  16 Oakland PARK RD|    null|         ring| l1w1e4|       South|        Canada| CA|
|224867848652|    7 kingwell Court|    null|       United|  s7jd9|South United|United Kingdom| GB|
|169636884295|     30 cartuja Road|    null|      Halifax|L0R 9p2|          ON|        Canada| CA|
|859473321609|              Street|    null|   Manchester|  92220|          OR| United States| US|
|141096112545|          99 rue des|    null|    Australia| jsd9je|        null|        France| FR|
|160397658930|              5 Rise|    null|walligshngton|RY6 8LT|        FORT|United Kingdom| GB|
|726367494002|    1852 Townsend st|     666|     Wallsend|  90382|          CA| United States| US|
|187644735867|Bärbel-HAMPDEN-Pi...|    null|        Miami|  13355|        null|       Kingdom| ZZ|
|948475348324|      155 sw City ct|Rochdale|      Germany|  30864|          FL|     Australia| QQ|
|164083193213|                abc""|    null|   Jerez Fra.|  11401|       Cadiz|         Spain| ES|
+------------+--------------------+--------+-------------+-------+------------+--------------+---+
```

Without the plugin I get back
```
+------------+--------------------+--------+-------------+-------+------------+--------------+---+
|         _c0|                 _c1|     _c2|          _c3|    _c4|         _c5|           _c6|_c7|
+------------+--------------------+--------+-------------+-------+------------+--------------+---+
|134324937434|    #1991 N Grayhawk|    null|   Menlo Park|  89025|          AB| United States| US|
|208564744937|      63,trevion Way|    null|   st Lothian| h7f4h8|        null|United Kingdom| GB|
|132709376823|  16 Oakland PARK RD|    null|         ring| l1w1e4|       South|        Canada| CA|
|224867848652|    7 kingwell Court|    null|       United|  s7jd9|South United|United Kingdom| GB|
|169636884295|     30 cartuja Road|    null|      Halifax|L0R 9p2|          ON|        Canada| CA|
|859473321609|              Street|    null|   Manchester|  92220|          OR| United States| US|
|141096112545|          99 rue des|    null|    Australia| jsd9je|        null|        France| FR|
|160397658930|              5 Rise|    null|walligshngton|RY6 8LT|        FORT|United Kingdom| GB|
|726367494002|    1852 Townsend st|     666|     Wallsend|  90382|          CA| United States| US|
|187644735867|Bärbel-HAMPDEN-Pi...|    null|        Miami|  13355|        null|       Kingdom| ZZ|
|948475348324|      155 sw City ct|Rochdale|      Germany|  30864|          FL|     Australia| QQ|
|164083193213|               abc""""|    null|   Jerez Fra.|  11401|       Cadiz|         Spain| ES|
|198732413077|3p Grove Rochdale...|     BAW|      Fulifax|HX4 trW|        null|        Israel| GB|
|227433927227|       95 novem blvd|    null|  RAW VILLAGE|   3173|         XYZ|     Australia| IL|
+------------+--------------------+--------+-------------+-------+------------+--------------+---+
```

Which is also what I get back from pandas.

```
>>> pd.read_csv(""./test.csv"", header=None)
               0                       1         2              3        4             5               6   7
0   134324937434        #1991 N Grayhawk       NaN     Menlo Park    89025            AB   United States  US
1   208564744937          63,trevion Way       NaN     st Lothian   h7f4h8           NaN  United Kingdom  GB
2   132709376823      16 Oakland PARK RD       NaN           ring   l1w1e4         South          Canada  CA
3   224867848652        7 kingwell Court       NaN         United    s7jd9  South United  United Kingdom  GB
4   169636884295         30 cartuja Road       NaN        Halifax  L0R 9p2            ON          Canada  CA
5   859473321609                  Street       NaN     Manchester    92220            OR   United States  US
6   141096112545              99 rue des       NaN      Australia   jsd9je           NaN          France  FR
7   160397658930                  5 Rise       NaN  walligshngton  RY6 8LT          FORT  United Kingdom  GB
8   726367494002        1852 Townsend st       666       Wallsend    90382            CA   United States  US
9   187644735867  Bärbel-HAMPDEN-Ping 37       NaN          Miami    13355           NaN         Kingdom  ZZ
10  948475348324          155 sw City ct  Rochdale        Germany    30864            FL       Australia  QQ
11  164083193213                   abc""""       NaN     Jerez Fra.    11401         Cadiz           Spain  ES
12  198732413077  3p Grove Rochdale road       BAW        Fulifax  HX4 trW           NaN          Israel  GB
13  227433927227           95 novem blvd       NaN    RAW VILLAGE     3173           XYZ       Australia  IL
```

**Expected behavior**
CUDF returns the same result as Pandas and Spark.",2022-10-19T20:45:58Z,0,0,Robert (Bobby) Evans,Nvidia,True
370,[FEA] cudf.DataFrame.filter,"**Is your feature request related to a problem? Please describe.**
rewriting code from pandas into cudf, trying to use `import cudf as pd`

**Describe the solution you'd like**
`cudf.DataFrame.filter` matching https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.filter.html",2022-10-21T13:55:58Z,0,0,Matthew Farrellee,,False
371,[BUG] semantic mismatch for Int64 and int64,"**Describe the bug**
rewriting code from pandas to cudf, using `import cudf as pd`

**Steps/Code to reproduce bug**

```
$ python
Python 3.9.13 (main, May 18 2022, 00:00:00) 
[GCC 11.3.1 20220421 (Red Hat 11.3.1-2)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pandas, cudf

>>> pandas.__version__
'1.5.1'

>>> cudf.__version__
'22.10.00a+392.g1558403753'

>>> pandas.Series([1, None], dtype=""Int64"")
0       1
1    <NA>
dtype: Int64

>>> pandas.Series([1, None], dtype=""int64"")
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File "".../python3.9/site-packages/pandas/core/series.py"", line 470, in __init__
    data = sanitize_array(data, index, dtype, copy)
  File "".../python3.9/site-packages/pandas/core/construction.py"", line 622, in sanitize_array
    subarr = _try_cast(data, dtype, copy, raise_cast_failure)
  File "".../python3.9/site-packages/pandas/core/construction.py"", line 835, in _try_cast
    subarr = maybe_cast_to_integer_array(arr, dtype)
  File "".../python3.9/site-packages/pandas/core/dtypes/cast.py"", line 1834, in maybe_cast_to_integer_array
    casted = np.array(arr, dtype=dtype, copy=copy)
TypeError: int() argument must be a string, a bytes-like object or a number, not 'NoneType'

>>> cudf.Series([1, None], dtype=""Int64"")
Traceback (most recent call last):
  File "".../python3.9/site-packages/cudf/core/column/column.py"", line 2038, in as_column
    memoryview(arbitrary), dtype=dtype, nan_as_null=nan_as_null
TypeError: memoryview: a bytes-like object is required, not 'list'
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File "".../python3.9/site-packages/cudf/core/column/column.py"", line 2127, in as_column
    np_type = np.dtype(dtype).type
TypeError: data type 'Int64' not understood
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File "".../python3.9/site-packages/cudf/core/column/column.py"", line 2199, in _construct_array
    arbitrary = cupy.asarray(arbitrary, dtype=dtype)
  File "".../python3.9/site-packages/cupy/_creation/from_data.py"", line 76, in asarray
    return _core.array(a, dtype, False, order)
  File ""cupy/_core/core.pyx"", line 2266, in cupy._core.core.array
  File ""cupy/_core/core.pyx"", line 2290, in cupy._core.core.array
  File ""cupy/_core/core.pyx"", line 2415, in cupy._core.core._array_default
TypeError: int() argument must be a string, a bytes-like object or a number, not 'NoneType'
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File "".../python3.9/site-packages/nvtx/nvtx.py"", line 101, in inner
    result = func(*args, **kwargs)
  File "".../python3.9/site-packages/cudf/core/series.py"", line 536, in __init__
    data = column.as_column(data, nan_as_null=nan_as_null, dtype=dtype)
  File "".../python3.9/site-packages/cudf/core/column/column.py"", line 2184, in as_column
    _construct_array(arbitrary, dtype),
  File "".../python3.9/site-packages/cudf/core/column/column.py"", line 2212, in _construct_array
    arbitrary = np.asarray(
TypeError: int() argument must be a string, a bytes-like object or a number, not 'NoneType'

>>> cudf.Series([1, None], dtype=""int64"")
0       1
1    <NA>
dtype: int64

```



**Expected behavior**
matching behavior for `Int64` and `int64`

ref https://pandas.pydata.org/pandas-docs/stable/user_guide/integer_na.html and https://pandas.pydata.org/docs/user_guide/basics.html#basics-dtypes",2022-10-24T11:25:25Z,0,0,Matthew Farrellee,,False
372,[BUG] Stop using detail APIs in tests and Java code,"**Describe the bug**
There are a large number of libcudf tests that rely on detail APIs. There are parts of the JNI that use detail APIs. detail APIs are internal functionality that should not be called externally.

**Expected behavior**
We should rewrite the offending code to not use detail APIs. If this proves too difficult, that probably indicates that we have functionality hidden in detail namespaces that should be exposed publicly. I suspect that in at least a few cases we will need to add new public APIs to support users, but hopefully not too many of them.",2022-10-24T18:40:29Z,0,0,Vyas Ramasubramani,@rapidsai,True
373,[FEA] Support imaginary numbers in cuDF dataframes,"Hello, I am trying to store some imaginary numbers as a cudf dataframe column. Each column cell is a list of imaginary numbers. I am wondering what would be the most efficient way to do it as cudf dataframe doesn't support imaginary numbers? Seperating the real and imaginary part is what I am doing now but this is a huge dataset and it is taking a lot of time.",2022-10-25T09:33:07Z,0,0,Arpan Das,The École polytechnique fédérale de Lausanne,False
374,[FEA] Add support for escape characters in CSV,"**Is your feature request related to a problem? Please describe.**
By default Spark uses an escape character `\` to escape things like double quotes within other double quotes i.e. `""\""""`. Pandas by default uses two double quotes right next to each other.  Spark does not support this method of escaping.

**Describe the solution you'd like**
We would like an option in the csv reader setting to allow setting an escape character. There should be an option to either disable the double double quote escaping, or have it automatically disabled if an escape character is provided.

**Describe alternatives you've considered**
write our own CSV parser.",2022-10-25T14:13:10Z,0,0,Robert (Bobby) Evans,Nvidia,True
375,[ENH/QST] actually inplace updates in `__setitem__` and friends,"## Context

As noted in #11085, in many cases (though inconsistently right now), obtaining a view on `Series` (probably a `DataFrame` as well) using `iloc[:]` _inadvertently_ behaves with pseudo-copy-on-write semantics

```python
import cudf
import numpy as np
s = cudf.Series([1, 2, 3])
sview = s.iloc[:]
s.iloc[[1, 2]] = [4, 5]
assert np.allclose(s.values, sview.values) # => False

sview = s.iloc[:]
s.iloc[0:2] = 3
assert np.allclose(s.values, sview.values) # => True
```

Note: pandas is moving towards _all_ indexing [behaving with copy semantics](https://docs.google.com/document/d/1ZCQ9mx3LBMy-nhwRl33_jgcvWo9IWdEfxDNQ2thyTb0), so for some of these cases we've already skated to the right answer :)

## Why does this happen?

Most (but not all) of the `__setitem__`-like calls into (e.g. `copy_range`, `scatter`) `libcudf` do not operate in place, but instead return a new `cudf::column` that must be wrapped up. As a consequence, to pretend like the operation was in place, we call `_mimic_inplace(...)` to switch out the backing data of the `Column` object we're doing `__setitem__` on:

```python
import cudf
s = cudf.Series([1, 2, 3])
old_data = s._column.data
s.iloc[1:3] = [4, 5]
new_data = s._column.data
assert old_data is new_data # => False
```

This is kind of fine as long as there's only one object holding on to the column data, but this breaks down as soon as we have views.

## Why is the status quo problematic?

1. The current inconsistencies make implementing copy-on-write rather delicate (and in many cases provoke more copies than needed).
2. Operations that to the user do not provoke a copy can overflow GPU memory:
     ```python
      # on a system with 32 GB gpu memory
      import cudf
      import cupy as cp
      import numpy as np
      df = cudf.DataFrame({f""{i}"": cp.ones(10**9, dtype=np.uint8) for i in range(20)}) # about 20GB
      # expectation: this behaves in place, so the operation should fit in memory.
      df.iloc[[0, 2]] = list(range(20)) # => MemoryError: std::bad_alloc: out_of_memory: CUDA error at: rmm/mr/device/cuda_memory_resource.hpp
    ````
3. If the scatter/copy_foo operations in libcudf had an in place then we would have lower memory pressure (as point 2) and in the (common) case where we have a target table view, could avoid a memcopy of the whole table.

## Possible solutions

I don't know the history as to why the libcudf generally tends to offer ""return a copy"" rather than ""modify in place"", but one could make an effort to offer in place versions of most functions. If these operations were available, then the Cython layer could switch to calling into them. In those cases where we really want a copy, we would allocate and copy into an empty table before calling into libcudf.

Edit: modification in place only works at the libcudf level for fixed-width column types (so no strings, lists), and having in- and out-of-place modification for every operation is too much work without some significant motivating use case.

Since we need a work-around that works for string/list columns that cannot by modified in-place _anyway_, I don't think this issue is a sufficiently motivating use case.

The above solution is a no-go, so what else could we do?

- Given that we're trying to move to copy-on-write, we could go the other way and audit all places where `__setitem__` really is in place, and break that connection. ~Note that this is not actually copy-on-write, but copy-on-read so it's not a great option.~ Something close to this probably is copy-on-write, so looks perhaps reasonable.
- ~Change the way `_mimic_inplace(self, other, inplace=True)` works: rather than rewriting where `self.data` points to, we could instead `memcopy` from `other.data` back into `self.data` and then drop `other`. This maintains the same memory footprint right now, at the cost of (another) full `memcopy`, and makes `__setitem__` really behave in place (even for views).~ As pointed out below, this doesn't work for non-fixed-width column dtypes.",2022-10-25T17:17:48Z,0,0,Lawrence Mitchell,,False
376,"[FEA] Support how=""cross"" in merge/join","**Is your feature request related to a problem? Please describe.**
Pandas has supported the `how=""cross""` option in `join`/`merge` [since `pandas-1.2`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html). Using this option creates the cartesian product from both frames, preserving the order of the left-hannd keys. I propose that we support this option in `cudf`.

**Describe the solution you'd like**
As discussed in https://github.com/rapidsai/cudf/issues/6755, this behavior can already be accomplished using a place-holder column and an inner merge. The quickest way to resolve this issue is to use this workaround automatically within the cudf merge/join API. The ""best"" solution is to add a primitive in libcudf (if one doesn't already exist?).
",2022-10-28T15:04:26Z,0,0,Richard (Rick) Zamora,@NVIDIA,True
377,[BUG] Orc reader not returning index columns always(if they exist),"**Describe the bug**
When an orc file consists of index columns, the reader seems to ignore reading the index columns if their name isn't in `columns`.

Required to reproduce: Changes in https://github.com/rapidsai/cudf/pull/12025/

**Steps/Code to reproduce bug**
```python
In [1]: import cudf

In [2]: df = cudf.DataFrame({""str_col"": [""a"", ""abb"", ""abc""], ""a"":[10, 1, 2]}, index=[10, 11, 12])

In [3]: df.to_orc(""a"", index=True)

In [4]: cudf.read_orc(""a"")
Out[4]: 
   str_col   a
10       a  10
11     abb   1
12     abc   2

In [5]: cudf.read_orc(""a"", columns=['a'])  
# At this stage metadata has 1 index col + 1 col, but the actual table has only 1 col,
# which leads to the following index error.
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
Cell In [5], line 1
----> 1 cudf.read_orc(""a"", columns=['a'])

File /nvme/0/pgali/envs/cudfdev/lib/python3.9/site-packages/cudf/io/orc.py:372, in read_orc(filepath_or_buffer, engine, columns, filters, stripes, skiprows, num_rows, use_index, timestamp_type, use_python_file_object, storage_options, bytes_per_thread)
    368         stripes = selected_stripes
    370 if engine == ""cudf"":
    371     return DataFrame._from_data(
--> 372         *liborc.read_orc(
    373             filepaths_or_buffers,
    374             columns,
    375             stripes,
    376             skiprows,
    377             num_rows,
    378             use_index,
    379             timestamp_type,
    380         )
    381     )
    382 else:
    384     def read_orc_stripe(orc_file, stripe, columns):

File orc.pyx:92, in cudf._lib.orc.read_orc()

File orc.pyx:138, in cudf._lib.orc.read_orc()

File utils.pyx:309, in cudf._lib.utils.data_from_unique_ptr()

IndexError: list index out of range

In [6]: df.to_orc(""a"", index=False)

In [7]: cudf.read_orc(""a"", columns=['a'])
Out[7]: 
    a
0  10
1   1
2   2
```

**Expected behavior**
Orc reader needs to behave like parquet reader where the index columns are read always irrespective of if their name is present in `columns`

**Environment overview (please complete the following information)**
 - Environment location: [Bare-metal]
 - Method of cuDF install: [from source]


**Environment details**
Please run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details
<details><summary>Click here to see environment details</summary><pre>
     
     **git***
     commit 99d5da156afdb3d8008ea67faedd884225f808db (HEAD -> 11780, origin/11780)
     Author: galipremsagar <sagarprem75@gmail.com>
     Date:   Fri Oct 28 13:39:58 2022 -0700
     
     refactor
     **git submodules***
     
     ***OS Information***
     DISTRIB_ID=Ubuntu
     DISTRIB_RELEASE=18.04
     DISTRIB_CODENAME=bionic
     DISTRIB_DESCRIPTION=""Ubuntu 18.04.4 LTS""
     NAME=""Ubuntu""
     VERSION=""18.04.4 LTS (Bionic Beaver)""
     ID=ubuntu
     ID_LIKE=debian
     PRETTY_NAME=""Ubuntu 18.04.4 LTS""
     VERSION_ID=""18.04""
     HOME_URL=""https://www.ubuntu.com/""
     SUPPORT_URL=""https://help.ubuntu.com/""
     BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
     PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
     VERSION_CODENAME=bionic
     UBUNTU_CODENAME=bionic
     Linux dt07 4.15.0-76-generic #86-Ubuntu SMP Fri Jan 17 17:24:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
     
     ***GPU Information***
     Fri Oct 28 14:03:21 2022
     +-----------------------------------------------------------------------------+
     | NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |
     |-------------------------------+----------------------+----------------------+
     | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
     | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
     |                               |                      |               MIG M. |
     |===============================+======================+======================|
     |   0  Tesla T4            On   | 00000000:3B:00.0 Off |                    0 |
     | N/A   49C    P0    27W /  70W |   6444MiB / 15109MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   1  Tesla T4            On   | 00000000:5E:00.0 Off |                    0 |
     | N/A   38C    P8     9W /  70W |      3MiB / 15109MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   2  Tesla T4            On   | 00000000:AF:00.0 Off |                    0 |
     | N/A   30C    P8     9W /  70W |      3MiB / 15109MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   3  Tesla T4            On   | 00000000:D8:00.0 Off |                    0 |
     | N/A   30C    P8     9W /  70W |      3MiB / 15109MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     
     +-----------------------------------------------------------------------------+
     | Processes:                                                                  |
     |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
     |        ID   ID                                                   Usage      |
     |=============================================================================|
     +-----------------------------------------------------------------------------+
     
     ***CPU***
     Architecture:        x86_64
     CPU op-mode(s):      32-bit, 64-bit
     Byte Order:          Little Endian
     CPU(s):              64
     On-line CPU(s) list: 0-63
     Thread(s) per core:  2
     Core(s) per socket:  16
     Socket(s):           2
     NUMA node(s):        2
     Vendor ID:           GenuineIntel
     CPU family:          6
     Model:               85
     Model name:          Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz
     Stepping:            4
     CPU MHz:             3425.591
     BogoMIPS:            4200.00
     Virtualization:      VT-x
     L1d cache:           32K
     L1i cache:           32K
     L2 cache:            1024K
     L3 cache:            22528K
     NUMA node0 CPU(s):   0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62
     NUMA node1 CPU(s):   1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63
     Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts pku ospke md_clear flush_l1d
     
     ***CMake***
     /nvme/0/pgali/envs/cudfdev/bin/cmake
     cmake version 3.24.2
     
     CMake suite maintained and supported by Kitware (kitware.com/cmake).
     
     ***g++***
     /nvme/0/pgali/envs/cudfdev/bin/g++
     g++ (conda-forge gcc 9.5.0-19) 9.5.0
     Copyright (C) 2019 Free Software Foundation, Inc.
     This is free software; see the source for copying conditions.  There is NO
     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
     
     
     ***nvcc***
     /nvme/0/pgali/envs/cudfdev/bin/nvcc
     nvcc: NVIDIA (R) Cuda compiler driver
     Copyright (c) 2005-2021 NVIDIA Corporation
     Built on Mon_Sep_13_19:13:29_PDT_2021
     Cuda compilation tools, release 11.5, V11.5.50
     Build cuda_11.5.r11.5/compiler.30411180_0
     
     ***Python***
     /nvme/0/pgali/envs/cudfdev/bin/python
     Python 3.9.13
     
     ***Environment Variables***
     PATH                            : /nvme/0/pgali/envs/cudfdev/bin:/nvme/0/pgali/anaconda3/bin:/nvme/0/pgali/.cargo/bin:/home/nfs/pgali/.vscode-server/bin/d045a5eda657f4d7b676dedbfa7aab8207f8a075/bin/remote-cli:/nvme/0/pgali/.cargo/bin:/nvme/0/pgali/envs/cudfdev/bin:/nvme/0/pgali/envs/myenv/condabin:/nvme/0/pgali/.cargo/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin
     LD_LIBRARY_PATH                 : /usr/local/cuda/lib64::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64
     NUMBAPRO_NVVM                   :
     NUMBAPRO_LIBDEVICE              :
     CONDA_PREFIX                    : /nvme/0/pgali/envs/cudfdev
     PYTHON_PATH                     :
     
     ***conda packages***
     /nvme/0/pgali/anaconda3/bin/conda
     # packages in environment at /nvme/0/pgali/envs/cudfdev:
     #
     # Name                    Version                   Build  Channel
     _libgcc_mutex             0.1                 conda_forge    conda-forge
     _openmp_mutex             4.5                       2_gnu    conda-forge
     _sysroot_linux-64_curr_repodata_hack 3                   h5bd9786_13    conda-forge
     abseil-cpp                20211102.0           h93e1e8c_3    conda-forge
     aiobotocore               2.4.0              pyhd8ed1ab_0    conda-forge
     aiohttp                   3.8.3            py39hb9d737c_1    conda-forge
     aioitertools              0.11.0             pyhd8ed1ab_0    conda-forge
     aiosignal                 1.2.0              pyhd8ed1ab_0    conda-forge
     alabaster                 0.7.12                     py_0    conda-forge
     anyio                     3.6.2              pyhd8ed1ab_0    conda-forge
     argon2-cffi               21.3.0             pyhd8ed1ab_0    conda-forge
     argon2-cffi-bindings      21.2.0           py39hb9d737c_3    conda-forge
     arrow-cpp                 9.0.0           py39h2531139_1_cpu    conda-forge
     asttokens                 2.0.8              pyhd8ed1ab_0    conda-forge
     async-timeout             4.0.2              pyhd8ed1ab_0    conda-forge
     attrs                     22.1.0             pyh71513ae_1    conda-forge
     aws-c-cal                 0.5.11               h95a6274_0    conda-forge
     aws-c-common              0.6.2                h7f98852_0    conda-forge
     aws-c-event-stream        0.2.7               h3541f99_13    conda-forge
     aws-c-io                  0.10.5               hfb6a706_0    conda-forge
     aws-checksums             0.1.11               ha31a3da_7    conda-forge
     aws-sam-translator        1.53.0             pyhd8ed1ab_0    conda-forge
     aws-sdk-cpp               1.8.186              hb4091e7_3    conda-forge
     aws-xray-sdk              2.10.0             pyhd8ed1ab_0    conda-forge
     babel                     2.10.3             pyhd8ed1ab_0    conda-forge
     backcall                  0.2.0              pyh9f0ad1d_0    conda-forge
     backports                 1.0                        py_2    conda-forge
     backports.functools_lru_cache 1.6.4              pyhd8ed1ab_0    conda-forge
     bcrypt                    3.2.2            py39hb9d737c_1    conda-forge
     beautifulsoup4            4.11.1             pyha770c72_0    conda-forge
     binutils                  2.39                 hdd6e379_0    conda-forge
     binutils_impl_linux-64    2.39                 h6ceecb4_0    conda-forge
     binutils_linux-64         2.39                h5fc0e48_11    conda-forge
     bleach                    5.0.1              pyhd8ed1ab_0    conda-forge
     bokeh                     2.4.3              pyhd8ed1ab_3    conda-forge
     boto3                     1.24.59            pyhd8ed1ab_0    conda-forge
     botocore                  1.27.59            pyhd8ed1ab_0    conda-forge
     brotlipy                  0.7.0           py39hb9d737c_1005    conda-forge
     bzip2                     1.0.8                h7f98852_4    conda-forge
     c-ares                    1.18.1               h7f98852_0    conda-forge
     c-compiler                1.3.0                h7f98852_0    conda-forge
     ca-certificates           2022.9.24            ha878542_0    conda-forge
     cachetools                5.2.0              pyhd8ed1ab_0    conda-forge
     certifi                   2022.9.24          pyhd8ed1ab_0    conda-forge
     cffi                      1.15.1           py39he91dace_2    conda-forge
     cfgv                      3.3.1              pyhd8ed1ab_0    conda-forge
     cfn-lint                  0.69.1             pyhd8ed1ab_0    conda-forge
     charset-normalizer        2.1.1              pyhd8ed1ab_0    conda-forge
     clang                     11.1.0               ha770c72_1    conda-forge
     clang-11                  11.1.0          default_ha53f305_1    conda-forge
     clang-tools               11.1.0          default_ha53f305_1    conda-forge
     clangxx                   11.1.0          default_ha53f305_1    conda-forge
     click                     8.1.3           unix_pyhd8ed1ab_2    conda-forge
     cloudpickle               2.2.0              pyhd8ed1ab_0    conda-forge
     cmake                     3.24.2               h5432695_0    conda-forge
     cmake_setuptools          0.1.3                      py_0    rapidsai
     colorama                  0.4.6              pyhd8ed1ab_0    conda-forge
     commonmark                0.9.1                      py_0    conda-forge
     cryptography              38.0.2           py39hd97740a_2    conda-forge
     cubinlinker               0.2.0            py39h11215e4_1    rapidsai
     cuda-python               11.7.0           py39h3fd9d12_0    nvidia
     cudatoolkit               11.5.0               h36ae40a_9    nvidia
     cudf                      22.12.0a0+206.g643cabea05.dirty          pypi_0    pypi
     cudnn                     8.4.1.50             hed8a83a_0    conda-forge
     cupy                      11.2.0           py39hc3c280e_0    conda-forge
     cxx-compiler              1.3.0                h4bd325d_0    conda-forge
     cyrus-sasl                2.1.27               h230043b_5    conda-forge
     cython                    0.29.32          py39h5a03fae_1    conda-forge
     cytoolz                   0.12.0           py39hb9d737c_1    conda-forge
     dask                      2022.10.1a221028 py_ge0669cdb_30    dask/label/dev
     dask-core                 2022.10.1a221027 py_g97c22be79_12    dask/label/dev
     dask-cuda                 22.12.00a221028 py39_g40bbfed_20    rapidsai-nightly
     dask-cudf                 22.12.0a0+206.g643cabea05.dirty          pypi_0    pypi
     dataclasses               0.8                pyhc8e2a94_3    conda-forge
     debugpy                   1.6.3            py39h5a03fae_1    conda-forge
     decopatch                 1.4.10             pyhd8ed1ab_0    conda-forge
     decorator                 5.1.1              pyhd8ed1ab_0    conda-forge
     defusedxml                0.7.1              pyhd8ed1ab_0    conda-forge
     distlib                   0.3.5              pyhd8ed1ab_0    conda-forge
     distributed               2022.10.1a221028 py_ge0669cdb_30    dask/label/dev
     distro                    1.6.0              pyhd8ed1ab_0    conda-forge
     dlpack                    0.5                  h9c3ff4c_0    conda-forge
     docker-py                 6.0.0              pyhd8ed1ab_0    conda-forge
     docutils                  0.19             py39hf3d152e_1    conda-forge
     double-conversion         3.2.0                h27087fc_1    conda-forge
     doxygen                   1.8.20               had0d8f1_0    conda-forge
     ecdsa                     0.18.0             pyhd8ed1ab_1    conda-forge
     entrypoints               0.4                pyhd8ed1ab_0    conda-forge
     exceptiongroup            1.0.0              pyhd8ed1ab_0    conda-forge
     execnet                   1.9.0              pyhd8ed1ab_0    conda-forge
     executing                 1.1.1              pyhd8ed1ab_0    conda-forge
     expat                     2.5.0                h27087fc_0    conda-forge
     fastavro                  1.7.0            py39hb9d737c_0    conda-forge
     fastrlock                 0.8              py39h5a03fae_2    conda-forge
     filelock                  3.8.0              pyhd8ed1ab_0    conda-forge
     flask                     2.1.3              pyhd8ed1ab_0    conda-forge
     flask_cors                3.0.10             pyhd3deb0d_0    conda-forge
     flit-core                 3.7.1              pyhd8ed1ab_0    conda-forge
     freetype                  2.12.1               hca18f0e_0    conda-forge
     frozenlist                1.3.1            py39hb9d737c_1    conda-forge
     fsspec                    2022.10.0          pyhd8ed1ab_0    conda-forge
     future                    0.18.2             pyhd8ed1ab_6    conda-forge
     gcc                       9.5.0               h1fea6ba_11    conda-forge
     gcc_impl_linux-64         9.5.0               h99780fb_19    conda-forge
     gcc_linux-64              9.5.0               h4258300_11    conda-forge
     gettext                   0.21.1               h27087fc_0    conda-forge
     gflags                    2.2.2             he1b5a44_1004    conda-forge
     glog                      0.6.0                h6f12383_0    conda-forge
     gmp                       6.2.1                h58526e2_0    conda-forge
     gmpy2                     2.1.2            py39h376b7d2_1    conda-forge
     graphql-core              3.2.3              pyhd8ed1ab_0    conda-forge
     greenlet                  1.1.3.post0      py39h5a03fae_0    conda-forge
     grpc-cpp                  1.46.4               h6fc47f4_3    conda-forge
     gxx                       9.5.0               h1fea6ba_11    conda-forge
     gxx_impl_linux-64         9.5.0               h99780fb_19    conda-forge
     gxx_linux-64              9.5.0               h43f449f_11    conda-forge
     heapdict                  1.0.1                      py_0    conda-forge
     huggingface_hub           0.10.1             pyhd8ed1ab_0    conda-forge
     hypothesis                6.56.3           py39hf3d152e_2    conda-forge
     identify                  2.5.8              pyhd8ed1ab_0    conda-forge
     idna                      3.4                pyhd8ed1ab_0    conda-forge
     imagesize                 1.4.1              pyhd8ed1ab_0    conda-forge
     importlib-metadata        5.0.0              pyha770c72_1    conda-forge
     importlib_metadata        5.0.0                hd8ed1ab_1    conda-forge
     importlib_resources       3.3.1              pyhd8ed1ab_1    conda-forge
     iniconfig                 1.1.1              pyh9f0ad1d_0    conda-forge
     intel-openmp              2022.1.0          h9e868ea_3769
     ipykernel                 6.16.2             pyh210e3f2_0    conda-forge
     ipython                   8.5.0              pyh41d4057_1    conda-forge
     ipython_genutils          0.2.0                      py_1    conda-forge
     itsdangerous              2.1.2              pyhd8ed1ab_0    conda-forge
     jedi                      0.18.1             pyhd8ed1ab_2    conda-forge
     jinja2                    3.1.2              pyhd8ed1ab_1    conda-forge
     jmespath                  1.0.1              pyhd8ed1ab_0    conda-forge
     joblib                    1.2.0              pyhd8ed1ab_0    conda-forge
     jpeg                      9e                   h166bdaf_2    conda-forge
     jschema-to-python         1.2.3              pyhd8ed1ab_0    conda-forge
     jsondiff                  2.0.0              pyhd8ed1ab_0    conda-forge
     jsonpatch                 1.32               pyhd8ed1ab_0    conda-forge
     jsonpickle                2.2.0              pyhd8ed1ab_0    conda-forge
     jsonpointer               2.0                        py_0    conda-forge
     jsonschema                3.2.0              pyhd8ed1ab_3    conda-forge
     junit-xml                 1.9                pyh9f0ad1d_0    conda-forge
     jupyter-cache             0.5.0              pyhd8ed1ab_0    conda-forge
     jupyter_client            7.3.4              pyhd8ed1ab_0    conda-forge
     jupyter_core              4.11.1           py39hf3d152e_1    conda-forge
     jupyter_server            1.21.0             pyhd8ed1ab_0    conda-forge
     jupyterlab_pygments       0.2.2              pyhd8ed1ab_0    conda-forge
     kernel-headers_linux-64   3.10.0              h4a8ded7_13    conda-forge
     keyutils                  1.6.1                h166bdaf_0    conda-forge
     krb5                      1.19.3               h3790be6_0    conda-forge
     lcms2                     2.12                 hddcbb42_0    conda-forge
     ld_impl_linux-64          2.39                 hc81fddc_0    conda-forge
     lerc                      4.0.0                h27087fc_0    conda-forge
     libabseil                 20211102.0      cxx17_h48a1fff_3    conda-forge
     libblas                   3.9.0            16_linux64_mkl    conda-forge
     libbrotlicommon           1.0.9                h166bdaf_8    conda-forge
     libbrotlidec              1.0.9                h166bdaf_8    conda-forge
     libbrotlienc              1.0.9                h166bdaf_8    conda-forge
     libcblas                  3.9.0            16_linux64_mkl    conda-forge
     libclang-cpp11.1          11.1.0          default_ha53f305_1    conda-forge
     libcrc32c                 1.1.2                h9c3ff4c_0    conda-forge
     libcurl                   7.86.0               h7bff187_0    conda-forge
     libdeflate                1.14                 h166bdaf_0    conda-forge
     libedit                   3.1.20191231         he28a2e2_2    conda-forge
     libev                     4.33                 h516909a_1    conda-forge
     libevent                  2.1.10               h9b69904_4    conda-forge
     libffi                    3.4.2                h7f98852_5    conda-forge
     libgcc-devel_linux-64     9.5.0               h0a57e50_19    conda-forge
     libgcc-ng                 12.2.0              h65d4601_19    conda-forge
     libgcrypt                 1.10.1               h166bdaf_0    conda-forge
     libgfortran-ng            12.2.0              h69a702a_19    conda-forge
     libgfortran5              12.2.0              h337968e_19    conda-forge
     libgomp                   12.2.0              h65d4601_19    conda-forge
     libgoogle-cloud           1.40.2               hefc27d0_0    conda-forge
     libgpg-error              1.45                 hc0c96e0_0    conda-forge
     libgsasl                  1.10.0               h5b4c23d_0    conda-forge
     libiconv                  1.17                 h166bdaf_0    conda-forge
     liblapack                 3.9.0            16_linux64_mkl    conda-forge
     libllvm11                 11.1.0               he0ac6c6_5    conda-forge
     libnghttp2                1.47.0               hdcd2b5c_1    conda-forge
     libnsl                    2.0.0                h7f98852_0    conda-forge
     libntlm                   1.4               h7f98852_1002    conda-forge
     libpng                    1.6.38               h753d276_0    conda-forge
     libprotobuf               3.20.1               h6239696_4    conda-forge
     librdkafka                1.7.0                hc49e61c_1    conda-forge
     librmm                    22.12.00a221028 cuda11_gca53a5ea_46    rapidsai-nightly
     libsanitizer              9.5.0               h2f262e1_19    conda-forge
     libsodium                 1.0.18               h36c2ea0_1    conda-forge
     libsqlite                 3.39.4               h753d276_0    conda-forge
     libssh2                   1.10.0               haa6b8db_3    conda-forge
     libstdcxx-devel_linux-64  9.5.0               h0a57e50_19    conda-forge
     libstdcxx-ng              12.2.0              h46fd767_19    conda-forge
     libthrift                 0.16.0               h491838f_2    conda-forge
     libtiff                   4.4.0                h55922b4_4    conda-forge
     libutf8proc               2.7.0                h7f98852_0    conda-forge
     libuuid                   2.32.1            h7f98852_1000    conda-forge
     libuv                     1.44.2               h166bdaf_0    conda-forge
     libwebp-base              1.2.4                h166bdaf_0    conda-forge
     libxcb                    1.13              h7f98852_1004    conda-forge
     libzlib                   1.2.13               h166bdaf_4    conda-forge
     livereload                2.6.3              pyh9f0ad1d_0    conda-forge
     llvmlite                  0.39.1           py39h7d9a04d_0    conda-forge
     locket                    1.0.0              pyhd8ed1ab_0    conda-forge
     lz4                       4.0.2            py39h029007f_0    conda-forge
     lz4-c                     1.9.3                h9c3ff4c_1    conda-forge
     magma                     2.5.4                hc72dce7_4    conda-forge
     make                      4.3                  hd18ef5c_1    conda-forge
     makefun                   1.15.0             pyhd8ed1ab_0    conda-forge
     markdown                  3.4.1              pyhd8ed1ab_0    conda-forge
     markdown-it-py            2.1.0              pyhd8ed1ab_0    conda-forge
     markupsafe                2.1.1            py39hb9d737c_2    conda-forge
     matplotlib-inline         0.1.6              pyhd8ed1ab_0    conda-forge
     mdit-py-plugins           0.3.1              pyhd8ed1ab_0    conda-forge
     mdurl                     0.1.0              pyhd8ed1ab_0    conda-forge
     mimesis                   6.1.1              pyhd8ed1ab_0    conda-forge
     mistune                   2.0.4              pyhd8ed1ab_0    conda-forge
     mkl                       2022.1.0           hc2b9512_224
     moto                      4.0.8              pyhd8ed1ab_0    conda-forge
     mpc                       1.2.1                h9f54685_0    conda-forge
     mpfr                      4.1.0                h9202a9a_1    conda-forge
     msgpack-python            1.0.4            py39hf939315_1    conda-forge
     multidict                 6.0.2            py39hb9d737c_2    conda-forge
     myst-nb                   0.17.1             pyhd8ed1ab_0    conda-forge
     myst-parser               0.18.1             pyhd8ed1ab_0    conda-forge
     nbclassic                 0.4.5              pyhd8ed1ab_0    conda-forge
     nbclient                  0.5.13             pyhd8ed1ab_0    conda-forge
     nbconvert                 7.2.3              pyhd8ed1ab_0    conda-forge
     nbconvert-core            7.2.3              pyhd8ed1ab_0    conda-forge
     nbconvert-pandoc          7.2.3              pyhd8ed1ab_0    conda-forge
     nbformat                  5.7.0              pyhd8ed1ab_0    conda-forge
     nbsphinx                  0.8.9              pyhd8ed1ab_0    conda-forge
     nccl                      2.14.3.1             h0800d71_0    conda-forge
     ncurses                   6.3                  h27087fc_1    conda-forge
     nest-asyncio              1.5.6              pyhd8ed1ab_0    conda-forge
     networkx                  2.8.7              pyhd8ed1ab_0    conda-forge
     ninja                     1.11.0               h924138e_0    conda-forge
     nodeenv                   1.7.0              pyhd8ed1ab_0    conda-forge
     notebook                  6.5.1              pyha770c72_0    conda-forge
     notebook-shim             0.2.0              pyhd8ed1ab_0    conda-forge
     numba                     0.56.3           py39h61ddf18_0    conda-forge
     numpy                     1.23.4           py39h3d75532_1    conda-forge
     numpydoc                  1.5.0              pyhd8ed1ab_0    conda-forge
     nvcc_linux-64             11.5                h44f499b_21    conda-forge
     nvtx                      0.2.3            py39h3811e60_1    conda-forge
     openapi-schema-validator  0.2.3              pyhd8ed1ab_0    conda-forge
     openapi-spec-validator    0.4.0              pyhd8ed1ab_1    conda-forge
     openjpeg                  2.5.0                h7d73246_1    conda-forge
     openssl                   1.1.1q               h166bdaf_1    conda-forge
     orc                       1.7.5                h6c59b99_0    conda-forge
     packaging                 21.3               pyhd8ed1ab_0    conda-forge
     pandas                    1.5.1            py39h4661b88_1    conda-forge
     pandoc                    1.19.2                        0    conda-forge
     pandocfilters             1.5.0              pyhd8ed1ab_0    conda-forge
     paramiko                  2.11.0             pyhd8ed1ab_0    conda-forge
     parquet-cpp               1.5.1                         2    conda-forge
     parso                     0.8.3              pyhd8ed1ab_0    conda-forge
     partd                     1.3.0              pyhd8ed1ab_0    conda-forge
     pbr                       5.11.0             pyhd8ed1ab_0    conda-forge
     pexpect                   4.8.0              pyh9f0ad1d_2    conda-forge
     pickleshare               0.7.5                   py_1003    conda-forge
     pillow                    9.2.0            py39hf3a2cdf_3    conda-forge
     pip                       22.3               pyhd8ed1ab_0    conda-forge
     platformdirs              2.5.2              pyhd8ed1ab_1    conda-forge
     pluggy                    1.0.0            py39hf3d152e_4    conda-forge
     pre-commit                2.20.0           py39hf3d152e_0    conda-forge
     prometheus_client         0.15.0             pyhd8ed1ab_0    conda-forge
     prompt-toolkit            3.0.31             pyha770c72_0    conda-forge
     protobuf                  3.20.1           py39h5a03fae_0    conda-forge
     psutil                    5.9.3            py39hb9d737c_1    conda-forge
     pthread-stubs             0.4               h36c2ea0_1001    conda-forge
     ptxcompiler               0.7.0           cuda_11_py39_gb86c990_1    rapidsai-nightly
     ptyprocess                0.7.0              pyhd3deb0d_0    conda-forge
     pure_eval                 0.2.2              pyhd8ed1ab_0    conda-forge
     py-cpuinfo                9.0.0              pyhd8ed1ab_0    conda-forge
     pyarrow                   9.0.0           py39h58137f1_1_cpu    conda-forge
     pyasn1                    0.4.8                      py_0    conda-forge
     pycparser                 2.21               pyhd8ed1ab_0    conda-forge
     pydata-sphinx-theme       0.11.0             pyhd8ed1ab_1    conda-forge
     pygments                  2.13.0             pyhd8ed1ab_0    conda-forge
     pynacl                    1.5.0            py39hb9d737c_2    conda-forge
     pynvml                    11.4.1             pyhd8ed1ab_0    conda-forge
     pyopenssl                 22.1.0             pyhd8ed1ab_0    conda-forge
     pyorc                     0.7.0            py39h3720fd5_0    conda-forge
     pyparsing                 3.0.9              pyhd8ed1ab_0    conda-forge
     pyrsistent                0.18.1           py39hb9d737c_2    conda-forge
     pysocks                   1.7.1              pyha2e5f31_6    conda-forge
     pytest                    7.2.0            py39hf3d152e_1    conda-forge
     pytest-benchmark          4.0.0              pyhd8ed1ab_0    conda-forge
     pytest-cases              3.6.13             pyhd8ed1ab_0    conda-forge
     pytest-xdist              3.0.2              pyhd8ed1ab_0    conda-forge
     python                    3.9.13          h9a8a25e_0_cpython    conda-forge
     python-confluent-kafka    1.7.0            py39h3811e60_2    conda-forge
     python-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge
     python-fastjsonschema     2.16.2             pyhd8ed1ab_0    conda-forge
     python-jose               3.3.0              pyh6c4a22f_1    conda-forge
     python-snappy             0.6.0            py39he8e2bb5_2    conda-forge
     python_abi                3.9                      2_cp39    conda-forge
     pytorch                   1.11.0          cuda112py39ha0cca9b_202    conda-forge
     pytz                      2022.5             pyhd8ed1ab_0    conda-forge
     pywin32-on-windows        0.1.0              pyh1179c8e_3    conda-forge
     pyyaml                    6.0              py39hb9d737c_5    conda-forge
     pyzmq                     24.0.1           py39headdf64_1    conda-forge
     rapidjson                 1.1.0             he1b5a44_1002    conda-forge
     re2                       2022.06.01           h27087fc_0    conda-forge
     readline                  8.1.2                h0f457ee_0    conda-forge
     recommonmark              0.7.1              pyhd8ed1ab_0    conda-forge
     regex                     2022.9.13        py39hb9d737c_1    conda-forge
     requests                  2.28.1             pyhd8ed1ab_1    conda-forge
     responses                 0.21.0             pyhd8ed1ab_0    conda-forge
     rhash                     1.4.3                h166bdaf_0    conda-forge
     rmm                       22.12.00a221026 cuda11_py39_g4de6eeca_43    rapidsai-nightly
     rsa                       4.9                pyhd8ed1ab_0    conda-forge
     s2n                       1.0.10               h9b69904_0    conda-forge
     s3fs                      2022.10.0          pyhd8ed1ab_0    conda-forge
     s3transfer                0.6.0              pyhd8ed1ab_0    conda-forge
     sacremoses                0.0.53             pyhd8ed1ab_0    conda-forge
     sarif-om                  1.0.4              pyhd8ed1ab_0    conda-forge
     scikit-build              0.15.0             pyhb871ab6_0    conda-forge
     scipy                     1.9.3            py39hddc5342_1    conda-forge
     sed                       4.8                  he412f7d_0    conda-forge
     send2trash                1.8.0              pyhd8ed1ab_0    conda-forge
     setuptools                65.5.0             pyhd8ed1ab_0    conda-forge
     six                       1.16.0             pyh6c4a22f_0    conda-forge
     sleef                     3.5.1                h9b69904_2    conda-forge
     snappy                    1.1.9                hbd366e4_1    conda-forge
     sniffio                   1.3.0              pyhd8ed1ab_0    conda-forge
     snowballstemmer           2.2.0              pyhd8ed1ab_0    conda-forge
     sortedcontainers          2.4.0              pyhd8ed1ab_0    conda-forge
     soupsieve                 2.3.2.post1        pyhd8ed1ab_0    conda-forge
     spdlog                    1.8.5                h4bd325d_1    conda-forge
     sphinx                    5.3.0              pyhd8ed1ab_0    conda-forge
     sphinx-autobuild          2021.3.14          pyhd8ed1ab_0    conda-forge
     sphinx-copybutton         0.5.0              pyhd8ed1ab_0    conda-forge
     sphinx-markdown-tables    0.0.17             pyh6c4a22f_0    conda-forge
     sphinxcontrib-applehelp   1.0.2                      py_0    conda-forge
     sphinxcontrib-devhelp     1.0.2                      py_0    conda-forge
     sphinxcontrib-htmlhelp    2.0.0              pyhd8ed1ab_0    conda-forge
     sphinxcontrib-jsmath      1.0.1                      py_0    conda-forge
     sphinxcontrib-qthelp      1.0.3                      py_0    conda-forge
     sphinxcontrib-serializinghtml 1.1.5              pyhd8ed1ab_2    conda-forge
     sphinxcontrib-websupport  1.2.4              pyhd8ed1ab_1    conda-forge
     sqlalchemy                1.4.42           py39hb9d737c_1    conda-forge
     sqlite                    3.39.4               h4ff8645_0    conda-forge
     sshpubkeys                3.3.1              pyhd8ed1ab_0    conda-forge
     stack_data                0.5.1              pyhd8ed1ab_0    conda-forge
     streamz                   0.6.4              pyh6c4a22f_0    conda-forge
     strings-udf               22.12.0a0+202.g05824fff6b          pypi_0    pypi
     sysroot_linux-64          2.17                h4a8ded7_13    conda-forge
     tabulate                  0.9.0              pyhd8ed1ab_1    conda-forge
     tblib                     1.7.0              pyhd8ed1ab_0    conda-forge
     terminado                 0.17.0             pyh41d4057_0    conda-forge
     tinycss2                  1.2.1              pyhd8ed1ab_0    conda-forge
     tk                        8.6.12               h27826a3_0    conda-forge
     tokenizers                0.10.3           py39hd6d55de_1    conda-forge
     toml                      0.10.2             pyhd8ed1ab_0    conda-forge
     tomli                     2.0.1              pyhd8ed1ab_0    conda-forge
     toolz                     0.12.0             pyhd8ed1ab_0    conda-forge
     tornado                   6.1              py39hb9d737c_3    conda-forge
     tqdm                      4.64.1             pyhd8ed1ab_0    conda-forge
     traitlets                 5.5.0              pyhd8ed1ab_0    conda-forge
     transformers              4.10.3             pyhd8ed1ab_0    conda-forge
     typing-extensions         4.4.0                hd8ed1ab_0    conda-forge
     typing_extensions         4.4.0              pyha770c72_0    conda-forge
     tzdata                    2022e                h191b570_0    conda-forge
     ukkonen                   1.0.1            py39hf939315_2    conda-forge
     urllib3                   1.26.11            pyhd8ed1ab_0    conda-forge
     virtualenv                20.16.5          py39hf3d152e_1    conda-forge
     wcwidth                   0.2.5              pyh9f0ad1d_2    conda-forge
     webencodings              0.5.1                      py_1    conda-forge
     websocket-client          1.4.1              pyhd8ed1ab_0    conda-forge
     werkzeug                  2.1.2              pyhd8ed1ab_1    conda-forge
     wheel                     0.37.1             pyhd8ed1ab_0    conda-forge
     wrapt                     1.14.1           py39hb9d737c_1    conda-forge
     xmltodict                 0.13.0             pyhd8ed1ab_0    conda-forge
     xorg-libxau               1.0.9                h7f98852_0    conda-forge
     xorg-libxdmcp             1.1.3                h7f98852_0    conda-forge
     xz                        5.2.6                h166bdaf_0    conda-forge
     yaml                      0.2.5                h7f98852_2    conda-forge
     yarl                      1.8.1            py39hb9d737c_0    conda-forge
     zeromq                    4.3.4                h9c3ff4c_1    conda-forge
     zict                      2.2.0              pyhd8ed1ab_0    conda-forge
     zipp                      3.10.0             pyhd8ed1ab_0    conda-forge
     zlib                      1.2.13               h166bdaf_4    conda-forge
     zstd                      1.5.2                h6239696_4    conda-forge
     
</pre></details>

**Additional context**
This issue surfaced while I was working on: https://github.com/rapidsai/cudf/pull/12025/
",2022-10-28T21:04:39Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
378,[FEA] Unused variable in JNI/Java binding for `readParquet`,"This is not a complete FEA but is closed to it. Currently, from Java we pass in a boolean variable `binaryToString` to JNI `readParquet` but it is unused after merging https://github.com/rapidsai/cudf/pull/11524. This variable may be used in the future thus we didn't remove it.

This issue is to track that unused variable.",2022-10-31T16:06:12Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
379,Refactor groupby to rely less on storing keys as `Index` objects,"https://github.com/rapidsai/cudf/pull/11792 introduces the ability to group on list columns. In the future, we can expect grouping by, e.g., structs and other types that are not supported by Pandas.

In https://github.com/rapidsai/cudf/issues/6932, we made the decision not to support creating an `Index` with elements of type `list`. 

Unfortunately, our groupby internals rely heavily on being able to store the key columns of a groupby as an `Index`. In particular, the internal [`_Grouping.keys`](https://github.com/rapidsai/cudf/blob/991c86b13acdbc28ab60609bee6eba2f9eac1ecc/python/cudf/cudf/core/groupby/groupby.py#L1836) method is heavily used.

We should rely less on storing keys as `Index` objects, which will make it much easier to support grouping by lists and structs. ",2022-11-01T14:51:06Z,0,0,Ashwin Srinath,Voltron Data,False
380,[ENH/QST]: Behaviour of type promotion in `__setitem__`,"# Summary

CUDF is not consistent with Pandas (under a bunch of circumstances) in
its behaviour when upcasting during `__setitem__`. In some cases, we
might want to mimic pandas behaviour (though they are very keen to use
value-based type promotion). In others, where we have more structured
dtypes than pandas, we need to decide what to do (current behaviour is
internally inconsistent and buggy in a bunch of cases).

I summarise what I think the current state is (by way of experiment),
and then discuss some options. Opinions welcome!

cc: @vyasr, @mroeschke, @shwina
# Pandas behaviour

Pandas version 1.5.1, MacOS (Apple Silicon)

Edit: updated code for generating more tables.

I should note that these tables are for single index `__setitem__` (`s.iloc[i] = value`). I should check if the same behaviour also occurs for:
- [x] slice-based `__setitem__` with single value `s.iloc[:1] = [value]`
- [x] slice-based `__setitem__` with list of values `s.iloc[:2] = [value for _ in range(2)]`
- [x] mask-based `__setitem__` with singleton value `s.iloc[[True, False]] = [value]`
- [x] mask-based `__setitem__` with multiple values `s.iloc[[True, False, True]] = [value, value]`
- [x] index-based `__setitem__` with single value `s.iloc[[1]] = value`
- [x] index-based `__setitem__` with multiple values `s.iloc[[1, 2]] = [value, value]`

<details>
<summary>Code to generate tables</summary>

```python
from __future__ import annotations

import os
from enum import Enum, IntEnum, auto
from itertools import filterfalse, repeat
from operator import not_
from pathlib import Path

import numpy as np
import pandas as pd
import typer

try:
    import cudf
    import cupy

    class Backend(str, Enum):
        PANDAS = ""pandas""
        CUDF = ""cudf""

except ImportError:

    class Backend(str, Enum):
        PANDAS = ""pandas""


def numeric_series(values, dtype, *, pandas):
    if pandas:
        return pd.Series(values, dtype=dtype)
    else:
        return cudf.Series(values, dtype=dtype)


def format_val(v):
    try:
        dt = v.dtype
        return f""np.{dt.type.__name__}({v})""
    except AttributeError:
        return f""{v}""


class IndexType(IntEnum):
    SINGLE_INT = auto()
    SINGLETON_SLICE = auto()
    CONTIG_SLICE = auto()
    STRIDED_SLICE = auto()
    SINGLETON_MASK = auto()
    GENERAL_MASK = auto()
    SINGLETON_SCATTER = auto()
    GENERAL_SCATTER = auto()


def indexing(index_type: IndexType, n: int) -> tuple[int | slice | list, slice | list]:
    assert n >= 3
    if index_type == IndexType.SINGLE_INT:
        return n - 1, slice(0, n - 1, None)
    elif index_type == IndexType.SINGLETON_SLICE:
        return slice(1, 2, 1), [0, *range(2, n)]
    elif index_type == IndexType.CONTIG_SLICE:
        return slice(1, n - 2, 1), [0, *range(n - 2, n)]
    elif index_type == IndexType.STRIDED_SLICE:
        return slice(0, n, 2), slice(1, n, 2)
    elif index_type == IndexType.SINGLETON_MASK:
        yes = [False, True, *repeat(False, n - 2)]
        no = list(map(not_, yes))
        return yes, no
    elif index_type == IndexType.GENERAL_MASK:
        yes = [True, False, True, *repeat(False, n - 3)]
        no = list(map(not_, yes))
        return yes, no
    elif index_type == IndexType.SINGLETON_SCATTER:
        yes = [1]
        # Oh for Haskell-esque sections
        no = list(filterfalse(yes.__contains__, range(n)))
        return yes, no
    elif index_type == IndexType.GENERAL_SCATTER:
        yes = [0, 2]
        no = list(filterfalse(yes.__contains__, range(n)))
        return yes, no
    else:
        raise ValueError(""Unhandled case"")


def generate_table(f, initial_values, values_to_try, dtype, *, index_type, pandas):
    initial_values = np.asarray(initial_values, dtype=object)
    f.write(""| Initial dtype | New value | Final dtype | Lossy? |\n"")
    f.write(""|---------------|-----------|-------------|--------|\n"")

    yes, no = indexing(index_type, len(initial_values))
    for value in values_to_try:
        s = numeric_series(initial_values, dtype=dtype, pandas=pandas)
        otype = f""np.{type(s.dtype).__name__}""
        try:
            if index_type == IndexType.SINGLETON_SLICE:
                value = cupy.asarray([value])
            s.iloc[yes] = value
        except BaseException as e:
            f.write(f""| `{otype}` | `{format_val(value)}` | N/A | {e} |\n"")
            continue
        ntype = f""np.{type(s.dtype).__name__}""
        expect = (np.asarray if pandas else cupy.asarray)(
            initial_values[no], dtype=dtype
        )
        original_lost_info = (s.iloc[no].astype(dtype) != expect).any()
        try:
            new_vals = s.iloc[yes].astype(value.dtype)
        except AttributeError:
            if pandas:
                new_vals = np.asarray(s.iloc[yes])
            else:
                new_vals = cupy.asarray(s.iloc[yes])
        new_lost_info = (new_vals != value).any()
        lossy = ""Yes"" if original_lost_info or new_lost_info else ""No""
        f.write(f""| `{otype}` | `{format_val(value)}` | `{ntype}` | {lossy} |\n"")


def generate_tables(output_directory: Path, backend: Backend, index_type: IndexType):
    integer_column_values_to_try = [
        10,
        np.int64(10),
        2**40,
        np.int64(2**40),
        2**80,
        10.5,
        np.float64(10),
        np.float64(10.5),
        np.float32(10),
        np.float32(10.5),
    ]
    float_column_values_to_try = [
        10,
        np.int64(10),
        2**40,
        np.int64(2**40),
        np.int32(2**31 - 100),
        np.int64(2**63 - 100),
        2**80 - 100,
        10.5,
        np.float64(10),
        np.float64(10.5),
        np.float64(np.finfo(np.float32).max.astype(np.float64) * 10),
        np.float32(10),
        np.float32(10.5),
    ]

    pandas = backend == Backend.PANDAS
    filename = f""{backend}-setitem-{index_type.name}.md""
    with open(output_directory / filename, ""w"") as f:
        if pandas:
            f.write(f""Pandas {pd.__version__} behaviour for {index_type!r}\n\n"")
        else:
            f.write(f""CUDF {cudf.__version__} behaviour for {index_type!r}\n\n"")

        generate_table(
            f,
            [2**31 - 10, 2**31 - 100, 3, 4, 5],
            integer_column_values_to_try,
            np.int32,
            index_type=index_type,
            pandas=pandas,
        )
        f.write(""\n"")
        generate_table(
            f,
            [2**63 - 10, 2**63 - 100, 3, 4, 5],
            integer_column_values_to_try,
            np.int64,
            index_type=index_type,
            pandas=pandas,
        )
        f.write(""\n"")
        generate_table(
            f,
            [np.finfo(np.float32).max, np.float32(np.inf), 3, 4, 5],
            float_column_values_to_try,
            np.float32,
            index_type=index_type,
            pandas=pandas,
        )
        f.write(""\n"")
        generate_table(
            f,
            [np.finfo(np.float64).max, np.float64(np.inf), 3, 4, 5],
            float_column_values_to_try,
            np.float64,
            index_type=index_type,
            pandas=pandas,
        )


def main(
    output_directory: Path = typer.Argument(Path("".""), help=""Output directory for results""),
    backend: Backend = typer.Option(""pandas"", help=""Dataframe backend to test""),
):
    os.makedirs(output_directory, exist_ok=True)
    for index_type in IndexType.__members__.values():
        generate_tables(output_directory, backend, index_type)


if __name__ == ""__main__"":
    typer.run(main)
```

</details>

## Numeric columns

### Integer column dtypes

#### dtype width < max integer width

Initial values `[2**31 - 10, 2**31 - 100, 3]`. `np.int32` is
representative of any integer type that is smaller than the max width.

| Initial dtype     | New value                   | Final dtype          | Lossy? |
|-------------------|-----------------------------|----------------------|--------|
| `np.dtype[int32]` | `10`                        | `np.dtype[int32]`    | No[^1] |
| `np.dtype[int32]` | `np.int64(10)`              | `np.dtype[int32]`    | No[^1] |
| `np.dtype[int32]` | `1099511627776`             | `np.dtype[longlong]` | No[^2] |
| `np.dtype[int32]` | `np.int64(1099511627776)`   | `np.dtype[longlong]` | No[^2] |
| `np.dtype[int32]` | `1208925819614629174706176` | `np.dtype[object_]`  | No[^3] |
| `np.dtype[int32]` | `10.5`                      | `np.dtype[float64]`  | No[^4] |
| `np.dtype[int32]` | `np.float64(10.0)`          | `np.dtype[int32]`    | No[^1] |
| `np.dtype[int32]` | `np.float64(10.5)`          | `np.dtype[float64]`  | No[^2] |
| `np.dtype[int32]` | `np.float32(10.0)`          | `np.dtype[int32]`    | No[^1] |
| `np.dtype[int32]` | `np.float32(10.5)`          | `np.dtype[float64]`  | No[^5] |

[^1]: value is exact in the initial dtype
[^2]: next largest numpy type that contains the value
[^3]: not representable in a numpy type, so coercion to object column
[^4]: default float type is float64
[^5]: `np.int32` is losslessly convertible to `np.float64`

#### dtype width == max integer width

Initial values `[2 ** 63 - 10, 2 ** 63 - 100, 3]`. These provoke edge
cases in upcasting because:
```python
import numpy as np
np.find_common_type([], [np.int64, np.float64])
# => np.float64 Noooooo! Hates it
# Yes, I know this is the same as the integer to float promotion in
# C/C++, I'm allowed to hate that too.
```

| Initial dtype     | New value                   | Final dtype         | Lossy?  |
|-------------------|-----------------------------|---------------------|---------|
| `np.dtype[int64]` | `10`                        | `np.dtype[int64]`   | No[^1]  |
| `np.dtype[int64]` | `np.int64(10)`              | `np.dtype[int64]`   | No[^1]  |
| `np.dtype[int64]` | `1099511627776`             | `np.dtype[int64]`   | No[^1]  |
| `np.dtype[int64]` | `np.int64(1099511627776)`   | `np.dtype[int64]`   | No[^1]  |
| `np.dtype[int64]` | `1208925819614629174706176` | `np.dtype[object_]` | No[^3]  |
| `np.dtype[int64]` | `10.5`                      | `np.dtype[float64]` | Yes[^6] |
| `np.dtype[int64]` | `np.float64(10.0)`          | `np.dtype[int64]`   | No[^1]  |
| `np.dtype[int64]` | `np.float64(10.5)`          | `np.dtype[float64]` | Yes[^6] |
| `np.dtype[int64]` | `np.float32(10.0)`          | `np.dtype[int64]`   | No[^1]  |
| `np.dtype[int64]` | `np.float32(10.5)`          | `np.dtype[float64]` | Yes[^6] |

[^6]: `np.int64` is _not_ losslessly convertible `np.float64`

### Float column dtypes

#### dtype width < max float width

Initial values `[np.finfo(np.float32).max, np.float32(np.inf), 3]`

| Initial dtype       | New value                            | Final dtype         | Lossy?   |
|---------------------|--------------------------------------|---------------------|----------|
| `np.dtype[float32]` | `10`                                 | `np.dtype[float32]` | No[^1]   |
| `np.dtype[float32]` | `np.int64(10)`                       | `np.dtype[float32]` | No[^1]   |
| `np.dtype[float32]` | `1099511627776`                      | `np.dtype[float32]` | No[^1]   |
| `np.dtype[float32]` | `np.int64(1099511627776)`            | `np.dtype[float32]` | No[^1]   |
| `np.dtype[float32]` | `np.int32(2147483548)`               | `np.dtype[float64]` | No[^1]   |
| `np.dtype[float32]` | `np.int64(9223372036854775708)`      | `np.dtype[float32]` | Yes[^7] |
| `np.dtype[float32]` | `1208925819614629174706076`          | `np.dtype[object_]` | No[^3]  |
| `np.dtype[float32]` | `10.5`                               | `np.dtype[float32]` | No[^1]   |
| `np.dtype[float32]` | `np.float64(10.0)`                   | `np.dtype[float32]` | No[^1]   |
| `np.dtype[float32]` | `np.float64(10.5)`                   | `np.dtype[float32]` | No[^1]   |
| `np.dtype[float32]` | `np.float64(3.4028234663852886e+39)` | `np.dtype[float64]` | No[^2]  |
| `np.dtype[float32]` | `np.float32(10.0)`                   | `np.dtype[float32]` | No[^1]   |
| `np.dtype[float32]` | `np.float32(10.5)`                   | `np.dtype[float32]` | No[^1]   |

[^7]: value is not losslessly representable, but also, expecting
    `np.float64`!

#### dtype width == max float width

Initial values `[np.finfo(np.float64).max, np.float64(np.inf), 3]`

| Initial dtype       | New value                            | Final dtype         | Lossy?   |
|---------------------|--------------------------------------|---------------------|----------|
| `np.dtype[float64]` | `10`                                 | `np.dtype[float64]` | No[^1]  |
| `np.dtype[float64]` | `np.int64(10)`                       | `np.dtype[float64]` | No[^1]  |
| `np.dtype[float64]` | `1099511627776`                      | `np.dtype[float64]` | No[^1]  |
| `np.dtype[float64]` | `np.int64(1099511627776)`            | `np.dtype[float64]` | No[^1]  |
| `np.dtype[float64]` | `np.int32(2147483548)`               | `np.dtype[float64]` | No[^1]  |
| `np.dtype[float64]` | `np.int64(9223372036854775708)`      | `np.dtype[float64]` | Yes[^6] |
| `np.dtype[float64]` | `1208925819614629174706076`          | `np.dtype[object_]` | No[^3]  |
| `np.dtype[float64]` | `10.5`                               | `np.dtype[float64]` | No[^1]  |
| `np.dtype[float64]` | `np.float64(10.0)`                   | `np.dtype[float64]` | No[^1]  |
| `np.dtype[float64]` | `np.float64(10.5)`                   | `np.dtype[float64]` | No[^1]  |
| `np.dtype[float64]` | `np.float64(3.4028234663852886e+39)` | `np.dtype[float64]` | No[^1]  |
| `np.dtype[float64]` | `np.float32(10.0)`                   | `np.dtype[float64]` | No[^1]  |
| `np.dtype[float64]` | `np.float32(10.5)`                   | `np.dtype[float64]` | No[^1]  |

## Everything else

Basically, you can put anything in a column and you get an object out,
but numpy types are converted to `object` first.

# CUDF behaviour

CUDF trunk, and state in #11904.

## Numeric columns

### Integer column dtypes

#### dtype width < max integer width

Initial values `[2**31 - 10, 2**31 - 100, 3]`. `np.int32` is
representative of any integer type that is smaller than the max width.

| Initial dtype     | New value                   | Final dtype (trunk)   | Final dtype (#11904)    | Lossy? (trunk) | Lossy? (#11904) |
|-------------------|-----------------------------|-----------------------|-------------------------|----------------|-----------------|
| `np.dtype[int32]` | `10`                        | `np.dtype[int32]`[^8] | `np.dtype[int64]`[^9]   | No             | No              |
| `np.dtype[int32]` | `np.int64(10)`              | `np.dtype[int32]`[^8] | `np.dtype[int64]`[^9]   | No             | No              |
| `np.dtype[int32]` | `1099511627776`             | `np.dtype[int32]`[^8] | `np.dtype[int64]`[^9]   | Yes            | No              |
| `np.dtype[int32]` | `np.int64(1099511627776)`   | `np.dtype[int32]`[^8] | `np.dtype[int64]`[^9]   | Yes            | No              |
| `np.dtype[int32]` | `1208925819614629174706176` | OverflowError         | OverflowError           | N/A            | N/A             |
| `np.dtype[int32]` | `10.5`                      | `np.dtype[int32]`[^8] | `np.dtype[float64]`[^9] | Yes            | No              |
| `np.dtype[int32]` | `np.float64(10.0)`          | `np.dtype[int32]`[^8] | `np.dtype[float64]`[^9] | No             | No              |
| `np.dtype[int32]` | `np.float64(10.5)`          | `np.dtype[int32]`[^8] | `np.dtype[float64]`[^9] | Yes            | No              |
| `np.dtype[int32]` | `np.float32(10.0)`          | `np.dtype[int32]`[^8] | `np.dtype[float64]`[^9] | No             | No              |
| `np.dtype[int32]` | `np.float32(10.5)`          | `np.dtype[int32]`[^8] | `np.dtype[float64]`[^9] | Yes            | No              |

[^8]: Bug fixed by #11904
[^9]: CUDF doesn't inspect values, so type-based promotion (difference
    from pandas)

#### dtype width == max integer width

Initial values `[2 ** 63 - 10, 2 ** 63 - 100, 3]`.

| Initial dtype     | New value                   | Final dtype (trunk)   | Final dtype (#11904)    | Lossy? (trunk) | Lossy? (#11904) |
|-------------------|-----------------------------|-----------------------|-------------------------|----------------|-----------------|
| `np.dtype[int64]` | `10`                        | `np.dtype[int64]`[^8] | `np.dtype[int64]`[^9]   | No             | No              |
| `np.dtype[int64]` | `np.int64(10)`              | `np.dtype[int64]`[^8] | `np.dtype[int64]`[^9]   | No             | No              |
| `np.dtype[int64]` | `1099511627776`             | `np.dtype[int64]`[^8] | `np.dtype[int64]`[^9]   | No             | No              |
| `np.dtype[int64]` | `np.int64(1099511627776)`   | `np.dtype[int64]`[^8] | `np.dtype[int64]`[^9]   | No             | No              |
| `np.dtype[int64]` | `1208925819614629174706176` | OverflowError         | OverflowError           | N/A            | N/A             |
| `np.dtype[int64]` | `10.5`                      | `np.dtype[int64]`[^8] | `np.dtype[float64]`[^9] | Yes            | Yes[^6]         |
| `np.dtype[int64]` | `np.float64(10.0)`          | `np.dtype[int64]`[^8] | `np.dtype[float64]`[^9] | No             | Yes[^6]         |
| `np.dtype[int64]` | `np.float64(10.5)`          | `np.dtype[int64]`[^8] | `np.dtype[float64]`[^9] | Yes            | Yes[^6]         |
| `np.dtype[int64]` | `np.float32(10.0)`          | `np.dtype[int64]`[^8] | `np.dtype[float64]`[^9] | No             | Yes[^6]         |
| `np.dtype[int64]` | `np.float32(10.5)`          | `np.dtype[int64]`[^8] | `np.dtype[float64]`[^9] | Yes            | Yes[^6]         |

### Float column dtypes

#### dtype width < max float width

Initial values `[np.finfo(np.float32).max, np.float32(np.inf), 3]`

| Initial dtype       | New value                            | Final dtype (trunk)     | Final dtype (#11904)    | Lossy? (trunk) | Lossy? (#11904) |
|---------------------|--------------------------------------|-------------------------|-------------------------|----------------|-----------------|
| `np.dtype[float32]` | `10`                                 | `np.dtype[float32]`[^8] | `np.dtype[float64]`[^9] | No             | No              |
| `np.dtype[float32]` | `np.int64(10)`                       | `np.dtype[float32]`[^8] | `np.dtype[float64]`[^9] | No             | No              |
| `np.dtype[float32]` | `1099511627776`                      | `np.dtype[float32]`[^8] | `np.dtype[float64]`[^9] | No             | No              |
| `np.dtype[float32]` | `np.int64(1099511627776)`            | `np.dtype[float32]`[^8] | `np.dtype[float64]`[^9] | No             | No              |
| `np.dtype[float32]` | `np.int32(2147483548)`               | `np.dtype[float32]`[^8] | `np.dtype[float64]`[^9] | Yes[^10]       | No              |
| `np.dtype[float32]` | `np.int64(9223372036854775708)`      | `np.dtype[float32]`[^8] | `np.dtype[float64]`[^9] | Yes[^10]       | Yes[^6]         |
| `np.dtype[float32]` | `1208925819614629174706076`          | OverflowError           | OverflowError           | N/A            | N/A             |
| `np.dtype[float32]` | `10.5`                               | `np.dtype[float32]`[^8] | `np.dtype[float64]`[^9] | No             | No              |
| `np.dtype[float32]` | `np.float64(10.0)`                   | `np.dtype[float32]`[^8] | `np.dtype[float64]`[^9] | No             | No              |
| `np.dtype[float32]` | `np.float64(10.5)`                   | `np.dtype[float32]`[^8] | `np.dtype[float64]`[^9] | No             | No              |
| `np.dtype[float32]` | `np.float64(3.4028234663852886e+39)` | `np.dtype[float32]`[^8] | `np.dtype[float64]`[^9] | Yes[^8]        | No              |
| `np.dtype[float32]` | `np.float32(10.0)`                   | `np.dtype[float32]`[^8] | `np.dtype[float32]`[^9] | No             | No              |
| `np.dtype[float32]` | `np.float32(10.5)`                   | `np.dtype[float32]`[^8] | `np.dtype[float32]`[^9] | No             | No              |

[^10]: As for [^6], but promotion from `np.int32` to `np.float32` is
    also not lossless.

#### dtype width == max float width

Initial values `[np.finfo(np.float64).max, np.float64(np.inf), 3]`

| Initial dtype       | New value                            | Final dtype (trunk) | Final dtype (#11904) | Lossy? (trunk) | Lossy? (#11904) |
|---------------------|--------------------------------------|---------------------|----------------------|----------------|-----------------|
| `np.dtype[float64]` | `10`                                 | `np.dtype[float64]` | `np.dtype[float64]`  | No             | No              |
| `np.dtype[float64]` | `np.int64(10)`                       | `np.dtype[float64]` | `np.dtype[float64]`  | No             | No              |
| `np.dtype[float64]` | `1099511627776`                      | `np.dtype[float64]` | `np.dtype[float64]`  | No             | No              |
| `np.dtype[float64]` | `np.int64(1099511627776)`            | `np.dtype[float64]` | `np.dtype[float64]`  | No             | No              |
| `np.dtype[float64]` | `np.int32(2147483548)`               | `np.dtype[float64]` | `np.dtype[float64]`  | No             | No              |
| `np.dtype[float64]` | `np.int64(9223372036854775708)`      | `np.dtype[float64]` | `np.dtype[float64]`  | Yes[^6]        | Yes[^6]         |
| `np.dtype[float64]` | `1208925819614629174706076`          | OverflowError       | OverflowError        | N/A            | N/A             |
| `np.dtype[float64]` | `10.5`                               | `np.dtype[float64]` | `np.dtype[float64]`  | No             | No              |
| `np.dtype[float64]` | `np.float64(10.0)`                   | `np.dtype[float64]` | `np.dtype[float64]`  | No             | No              |
| `np.dtype[float64]` | `np.float64(10.5)`                   | `np.dtype[float64]` | `np.dtype[float64]`  | No             | No              |
| `np.dtype[float64]` | `np.float64(3.4028234663852886e+39)` | `np.dtype[float64]` | `np.dtype[float64]`  | No             | No              |
| `np.dtype[float64]` | `np.float32(10.0)`                   | `np.dtype[float64]` | `np.dtype[float64]`  | No             | No              |
| `np.dtype[float64]` | `np.float32(10.5)`                   | `np.dtype[float64]` | `np.dtype[float64]`  | No             | No              |

## Everything else

This is where it starts to get _really_ messy. This section is a work
in progress. We should decide what we _want_ the semantics to be,
because in most cases pandas doesn't have the same dtypes that CUDF does.

### Inserting strings into numerical columns

This ""works"", for some value of ""works"" on #11904 if the string value
is parseable as the target dtype.

So

```python
s = cudf.Series([1, 2, 3], dtype=int)
s.iloc[2] = ""4"" # works
s.iloc[2] = ""0xf"" # => ValueError: invalid literal for int() with base 10: '0xf'
```

And similarly for float strings and float dtypes.

This is probably a nice feature.

### Inserting things into string columns

Works if the the ""thing"" is convertible to a string (so numbers work),
but Scalars with list or struct dtypes don't work.

I would argue that explicit casting from the user here is probably
better.

### List columns

The new value must have an identical dtype to that of the target column.

### Struct columns

The new value must have leaf dtypes that are considered compatible in
some sense, but then the leaves are downcast to the leaf dtypes of the
target column. So this is lossy and likely a bug:

```python
 sr = cudf.Series([{""a"": 1, ""b"": 2}])
 sr.iloc[0] = {""a"": 10.5, ""b"": 2}
 sr[0] # => {""a"": 10, ""b"": 2} (lost data in ""a"")
```
## What I think we want (for composite columns)

For composite columns, if the dtype shapes match, I think the casting
rule should be to traverse to the leaf dtypes and promote using the
rules for non-composite columns. If shapes don't match, `__setitem__`
should not be allowed.

This, to me, exhibits principle of least surprise.
",2022-11-01T18:01:16Z,0,0,Lawrence Mitchell,,False
381,[BUG] Special case Parquet LIST names appear to be ignored,"**Describe the bug**
The parquet specification at https://github.com/apache/parquet-format/blob/master/LogicalTypes.md when talking about backwards compatibility in lists says that 

> If the repeated field is a group with one field and is named either array or uses the LIST-annotated group's name with _tuple appended then the repeated type is the element type and elements are required.

The examples given for these are.

```
// List<OneTuple<String>> (nullable list, non-null elements)
optional group my_list (LIST) {
  repeated group array {
    required binary str (UTF8);
  };
}

// List<OneTuple<String>> (nullable list, non-null elements)
optional group my_list (LIST) {
  repeated group my_list_tuple {
    required binary str (UTF8);
  };
}
```

I implemented some tests based off of this and saw the CUDF is able to parse the data, but it is not returning the same types as Spark does, nor does it return what I would expect the examples to show.

In [files.zip](https://github.com/rapidsai/cudf/files/9913952/files.zip) there are two parquet files.

`SPECIAL_ARRAY_LIST_TEST.parquet` has a footer schema of

```
message spark {
  required group my_list (LIST) {
    repeated group array {
      required int32 item;
    }
  }
}
```

When I parse the data with CUDF I get back a table with types like `Table<LIST<INT32>>`, but Spark and expects the data to look like `Table<LIST<STRUCT<INT32>>>`.

Pandas appears to do the same thing, but I am not an expert on pandas to be 100% sure that it is the same thing.

```python
>>> pd.read_parquet(""SPECIAL_ARRAY_LIST_TEST.parquet"")
                      my_list
0  [{'item': 0}, {'item': 1}]
>>> pd.read_parquet(""SPECIAL_ARRAY_LIST_TEST.parquet"").info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 1 entries, 0 to 0
Data columns (total 1 columns):
 #   Column   Non-Null Count  Dtype 
---  ------   --------------  ----- 
 0   my_list  1 non-null      object
dtypes: object(1)
memory usage: 136.0+ bytes
```

The other file is essentially the same, but it is using the `_tuple` special case instead of `array`.

**Steps/Code to reproduce bug**
Try to read the attached files in CUDF and see if they match the desired types/schema.

**Expected behavior**
They should match, but it looks like they do not.

**Additional context**
This is probably not super critical because it is an odd corner case that is not likely to be very common, but technically it is returning the wrong data.
",2022-11-01T21:11:13Z,0,0,Robert (Bobby) Evans,Nvidia,True
382,[BUG] Backwards compatible parquet MAP_KEY_VALUE is not treated properly,"**Describe the bug**
The parquet specification at https://github.com/apache/parquet-format/blob/master/LogicalTypes.md when talking about backwards compatibility in maps says that

> Some existing data incorrectly used MAP_KEY_VALUE in place of MAP. For backward-compatibility, a group annotated with MAP_KEY_VALUE that is not contained by a MAP-annotated group should be handled as a MAP-annotated group.

The example schema given for this is.
```
// Map<String, Integer> (nullable map, nullable values)
optional group my_map (MAP_KEY_VALUE) {
  repeated group map {
    required binary key (UTF8);
    optional int32 value;
  }
}
```

I created a parquet file and put it in [file.zip](https://github.com/rapidsai/cudf/files/9914136/file.zip) that is very similar, but it uses `int32` for both the key and the value.

```
message spark {
  required group my_map (MAP_KEY_VALUE) {
    repeated group map {
      required int32 key;
      required int32 value;
    }
  }
}
```

When I read the data back using CUDF I get a schema like `TABLE<STRUCT<STRUCT<INT32, INT32>>>`, but what we want is `TABLE<LIST<STRUCT<INT32, INT32>>>`. Because that first column is a STRUCT and not a LIST only the first row in the LIST is returned.

It looks like panads is able to do this.

```python
>>> pd.read_parquet(""MAP_KEY_VALUE_TEST.parquet"")
             my_map
0  [(0, 2), (1, 3)]
>>> pd.read_parquet(""MAP_KEY_VALUE_TEST.parquet"").info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 1 entries, 0 to 0
Data columns (total 1 columns):
 #   Column  Non-Null Count  Dtype 
---  ------  --------------  ----- 
 0   my_map  1 non-null      object
dtypes: object(1)
memory usage: 136.0+ bytes
```

**Additional context**
This is probably not a super high priority. It is an odd/rare corner case. At least until a customer hit this.
",2022-11-01T21:46:34Z,0,0,Robert (Bobby) Evans,Nvidia,True
383,[BUG] cudf astype('int32') behavior does not match pandas,"**Steps/Code to reproduce bug**
```
$ python3.9 -m IPython
Python 3.9.14 (main, Sep  7 2022, 23:43:48) 
Type 'copyright', 'credits' or 'license' for more information
IPython 8.5.0 -- An enhanced Interactive Python. Type '?' for help.

In [1]: import pandas as pd, cudf

In [2]: pd.__version__, cudf.__version__
Out[2]: ('1.4.4', '22.10.00a+392.g1558403753')

In [3]: pdf = pd.DataFrame({'a': ['123_1']})

In [4]: pdf.a.astype('int32')
Out[4]: 
0    1231
Name: a, dtype: int32

In [5]: cdf = cudf.DataFrame({'a': ['123_1']})

In [6]: cdf.a.astype('int32')
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In [6], line 1
----> 1 cdf.a.astype('int32')

...

File ~/.local/lib/python3.9/site-packages/cudf/core/column/string.py:5314, in StringColumn.as_numerical_column(self, dtype, **kwargs)
   5312 if out_dtype.kind in {""i"", ""u""}:
   5313     if not libstrings.is_integer(string_col).all():
-> 5314         raise ValueError(
   5315             ""Could not convert strings to integer ""
   5316             ""type due to presence of non-integer values.""
   5317         )
   5318 elif out_dtype.kind == ""f"":
   5319     if not libstrings.is_float(string_col).all():

ValueError: Could not convert strings to integer type due to presence of non-integer values.
```

**Expected behavior**
`cudf` to produce the same result as `pandas`


**Environment overview**
```
$ nvidia-smi 
Wed Nov  2 16:48:53 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 510.85.02    Driver Version: 510.85.02    CUDA Version: 11.6     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |
| N/A   56C    P8    15W /  70W |      0MiB / 15360MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
```",2022-11-02T16:53:58Z,0,0,Matthew Farrellee,,False
384,[FEA] cudf.Timestamp,"**Is your feature request related to a problem? Please describe.**
rewriting code from pandas into cudf, using `import cudf as pd`

**Describe the solution you'd like**
`cudf.Timestamp` matching https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.html",2022-11-03T12:49:31Z,0,0,Matthew Farrellee,,False
385,[FEA] Migrate to Thrust `nosync` stream policy for performance.,"### Motivation & Description

Thrust 1.16 added an execution policy `thrust::cuda::par_nosync` that removes unnecessary internal stream synchronizations, except when required for correctness (e.g. if the algorithm returns a value to the host like `thrust::reduce`, a sync is required).

In PR #11577, I experimented with a bulk find-replace of `rmm::exec_policy` with `rmm::exec_policy_nosync`. This resulted in notable performance improvements, particularly for small data sizes. See: https://gist.github.com/bdice/bbeae4d28a45bedf0f53a13304714f70

However, a plain find-replace may leave issues with stream safety. For example, an internal detail API could be constructing host memory whose lifetime doesn't guarantee a safe copy to device without a stream sync before returning (thought experiment identified by @jrhemstad). Changing to `nosync` execution policies requires analysis of every use case individually, at both the detail and public API levels.

(As a reminder of the current stream policy: [libcudf APIs called on the host do not guarantee that the stream is synchronized before returning](https://github.com/rapidsai/cudf/blob/branch-22.12/cpp/doxygen/developer_guide/DEVELOPER_GUIDE.md#treat-libcudf-apis-as-if-they-were-asynchronous), but this does not mean we can always use `nosync` safely.)

### Tasks

I am planning to open PRs to use `nosync` across the libcudf codebase. Below is a list of benchmarks that showed improvements. I will need to analyze each benchmark to identify the primary algorithms being called that should be refactored to use `nosync`. This list is loosely sorted by largest impact for small data sizes (or whatever fixed data sizes are in the benchmark), which indicates overhead that we can systematically eliminate. Note that these performance improvements use `nosync` everywhere, which may not be stream-safe in all cases, so the real performance gains may be lower if not all executions can use `nosync`. Additionally, the performance improvements for a given algorithm may rely on improvements in other algorithms, so the full improvement may not be achieved until all tasks are complete.

- [x] Gather/scatter. #12038
  - [Gather is 42% faster for 1024 rows, 1 column.](https://gist.github.com/bdice/bbeae4d28a45bedf0f53a13304714f70#file-nosync_benchmarks_all-txt-L449)
  - [Scatter is 37% faster for 1024 rows, 1 column.](https://gist.github.com/bdice/bbeae4d28a45bedf0f53a13304714f70#file-nosync_benchmarks_all-txt-L2052)
- [ ] Search
  - [14% faster for Table, 1000 rows](https://gist.github.com/bdice/bbeae4d28a45bedf0f53a13304714f70#file-nosync_benchmarks_all-txt-L4315)
  - [48% faster for ColumnContains_AllValid, 1024 rows](https://gist.github.com/bdice/bbeae4d28a45bedf0f53a13304714f70#file-nosync_benchmarks_all-txt-L4339)
- [ ] ReductionScan
  - [39% faster for 10k rows of floats, no nulls](https://gist.github.com/bdice/bbeae4d28a45bedf0f53a13304714f70#file-nosync_benchmarks_all-txt-L2396)
- [ ] Rank
  - [37% faster for 1024 rows, no nulls](https://gist.github.com/bdice/bbeae4d28a45bedf0f53a13304714f70#file-nosync_benchmarks_all-txt-L2417)
  - [13% faster for 1024 rows, nulls](https://gist.github.com/bdice/bbeae4d28a45bedf0f53a13304714f70#file-nosync_benchmarks_all-txt-L2424)
- [ ] Sort
  - [31% faster for 1024 rows, unstable, no nulls](https://gist.github.com/bdice/bbeae4d28a45bedf0f53a13304714f70#file-nosync_benchmarks_all-txt-L2431)
- [ ] Repeat
  - [33% faster for 1024 rows, double, no nulls](https://gist.github.com/bdice/bbeae4d28a45bedf0f53a13304714f70#file-nosync_benchmarks_all-txt-L2508)
- [ ] Groupby
  - [28% faster for basic, 10k rows](https://gist.github.com/bdice/bbeae4d28a45bedf0f53a13304714f70#file-nosync_benchmarks_all-txt-L2776)
- [ ] Hash
  - [25% faster for Murmur3, no nulls, 16k rows](https://gist.github.com/bdice/bbeae4d28a45bedf0f53a13304714f70#file-nosync_benchmarks_all-txt-L2835)
- [ ] Compound reductions (like std, var)
  - [25% faster for std over 10k rows of floats](https://gist.github.com/bdice/bbeae4d28a45bedf0f53a13304714f70#file-nosync_benchmarks_all-txt-L2376)
- [ ] ReductionDictionary
  - [24% faster for 10k rows of floats.](https://gist.github.com/bdice/bbeae4d28a45bedf0f53a13304714f70#file-nosync_benchmarks_all-txt-L2226)
- [ ] Quantiles
  - [15% faster for 65k rows](https://gist.github.com/bdice/bbeae4d28a45bedf0f53a13304714f70#file-nosync_benchmarks_all-txt-L3642)
- [ ] Merge.
  - [8% faster for 512k rows, 2 tables.](https://gist.github.com/bdice/bbeae4d28a45bedf0f53a13304714f70#file-nosync_benchmarks_all-txt-L222)

Other notes:
- I excluded I/O benchmarks from the prioritized list of algorithms above, because I/O benchmarks are a little noisy on the system I used for benchmarking.

### Further work

After addressing the major tasks above where we have clearly identified speedups resulting from `nosync` policies, I will re-assess the rest of the code base to evaluate a broader replacement to use `nosync` policies everywhere in libcudf.",2022-11-07T16:55:03Z,0,0,Bradley Dice,@NVIDIA @rapidsai,True
386,[FEA] cudf::column needs a set_stream() function.,"
`cudf::column` uses `rmm::device_buffer` for internal storage. `rmm::device_buffer` internally stores the stream it was created on.  This can create issues when creating columns on temporary worker-style streams and then returning them to a primary stream.  When the column gets destroyed, rmm throws exceptions about invalid device contexts.

A `set_stream(rmm::cuda_stream_view)` function that recurses through all children would make it easier to hand off columns between streams. 

We initially encountered this issue with `cudf::io::column_buffer` which has a similar problem but it will also be an issue for columns themselves.",2022-11-08T17:39:15Z,0,0,,,False
387,dataframe.drop not working for multindex[QST],"I have a cudf dataframe which looks like this  
![Screen Shot 2022-11-09 at 3 24 26 PM](https://user-images.githubusercontent.com/23120837/200855560-cdf38efd-c095-42c2-9c8d-269dcac2515d.png)

I want to drop all the rows corresponding to some multiindex. For example I want to drop 

`index_to_drop= cudf.MultiIndex.from_tuples(zip([0,0], [1,2]), names=(""B_0"", ""B_1""))`

`df.drop(index=index_to_drop)` 

I am getting a NotImplementedError, however same thing is working in Pandas",2022-11-09T14:38:27Z,0,0,Arpan Das,The École polytechnique fédérale de Lausanne,False
388,[FEA] Public header and detail header files should have the same names,"As the title said, the public and detail headers should have the same names. Currently there are some inconsistencies between them. For example:
 * Public header: `cudf/copying.hpp`,
 * Detail header: `cudf/detail/copy.hpp`.

We should change the file names to make them consistent.",2022-11-09T22:05:01Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
389,[BUG] off-by-one errors in `cudf.date_range`,"**Describe the bug**

[As part of attempting to get to XPASS-zero in the test suite]

If the date range is long enough, and for some frequencies, `cudf.date_range` has a fencepost error in the number of dates it produces.

**Steps/Code to reproduce bug**
```python
import cudf
import pandas as pd
start = ""1831-05-08 15:23:21""
end = ""1996-11-21 04:05:30""
freq = ""110546789L""

cr = cudf.date_range(start=start, end=end, freq=freq)
pr = pd.date_range(start=start, end=end, freq=freq)

assert len(cr) == len(pr) # => False, len(cr) == len(pr) + 1
print(cr[-1])
# => 1996-11-21T14:14:21.984000000
# Which is _after_ the specified end
```

**Expected behavior**

No fencepost error.",2022-11-11T18:47:11Z,0,0,Lawrence Mitchell,,False
390,Behaviour of inplace for `copy_range` when range is empty.,"In `copying.copy_range` we have an `inplace=False/True` argument.

If `inplace` is `True`, then the column is modified in place, if `inplace` is False, a new (copied) column is created and then modified. If the case that the range we are copying is empty, at present `inplace=False` _still_ returns a new copy (even though `copy_range` is a no-op in this situation).

Should it be allowed that

```
copy_range(source, target, empty_range, inplace=False)
```

Can return `target` (rather than a copy of `target`)?

See discussion where this came p: https://github.com/rapidsai/cudf/pull/12075#discussion_r1015307155
      

",2022-11-14T11:17:05Z,0,0,Lawrence Mitchell,,False
391,[FEA] Refactor `Frame.astype`,"**Is your feature request related to a problem? Please describe.**
Improve `Frame.astype` in a way such that we either return correct `Frame` type or rename it internal utility such that it is inline with: https://github.com/rapidsai/cudf/pull/12048#discussion_r1021964390

",2022-11-14T20:32:16Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
392,[FEA] Follow up on refactoring possibility from parquet chunked reader PR,"There is a small refactoring that can be done to de-duplicate some code in the parquet decoder which needs to be done as a followup.

https://github.com/rapidsai/cudf/pull/11867#discussion_r1022137500

<img width=""770"" alt=""image"" src=""https://github.com/rapidsai/cudf/assets/12725111/e10d9380-c7bf-4c37-9dfc-c1c6d301b211"">


",2022-11-15T02:44:37Z,0,0,,,False
393,[FEA] JNI `CSVOptions` for skipping blank lines,"[`cudf::io::csv_reader_options::enable_skip_blank_lines()`](https://github.com/rapidsai/cudf/blob/branch-22.12/cpp/include/cudf/io/csv.hpp#L665) provides a configuration option to choose whether to skip blank lines in an input file. This decides whether a blank line constitutes another input row.

`enable_skip_blank_lines()` does not have a corresponding setter in Java via `ai.rapids.cudf.CSVOptions`. This would be valuable for data-formats like [Hive delimited text](https://github.com/NVIDIA/spark-rapids/pull/7068).",2022-11-15T07:41:09Z,0,0,MithunR,NVIDIA,True
394,"[FEA] `csv_reader_options` to read empty strings as blank (i.e. `""""`), not `null`.","When `cudf::io::read_csv()` encounters two consecutive field delimiters within a row, it deems the corresponding string column value as null. E.g.:
```
a,,c
d,,f
```
Reading the input above via `read_csv()` produces rows `{a,null,c}` and `{d,null,f}`. This is conformant with Spark's CSV reader (and presumably Pandas).

It would be useful if the column value could be optionally interpreted as an empty string (`""""`) instead. This would permit support for reading [Hive delimited text](https://github.com/NVIDIA/spark-rapids/issues/7069), where empty strings are empty by default.",2022-11-15T08:43:23Z,0,0,MithunR,NVIDIA,True
395,[FEA] Document how decimal columns work,"**Is your feature request related to a problem? Please describe.**
We need to add some user-facing documentation on how our decimal columns work and various binops with a few walk-through examples.

Some high-level pointers:

- [x] Explain what `scale` & `precision` are.
- [ ] Explain how binops calculate the results.
- [ ] Explain how dtypes are deduced for binops.

",2022-11-15T21:38:55Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
396,[BUG] Expose groupby rank via agg interface,"In pandas, I can compute groupby rank via the agg interface. In cuDF, I cannot.

This came up while looking into SQL window function coverage.

```python
import cudf

df = cudf.DataFrame({
    ""a"": [0, 0, 0, 1, 1, 1],
    ""val"": [5, 3, 7, 3, 9, 4],
})
print(df.groupby(""a"").val.rank())
print(df.to_pandas().groupby(""a"").agg({""val"":[""rank""]}))
df.groupby(""a"").agg({""val"":[""rank""]})
0    2.0
1    1.0
2    3.0
3    1.0
4    3.0
5    2.0
Name: val, dtype: float64
   val
  rank
0  2.0
1  1.0
2  3.0
3  1.0
4  3.0
5  2.0

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Input In [55], in <cell line: 9>()
      7 print(df.groupby(""a"").val.rank())
      8 print(df.to_pandas().groupby(""a"").agg({""val"":[""rank""]}))
----> 9 df.groupby(""a"").agg({""val"":[""rank""]})

File ~/miniconda3/envs/rapids-22.12/lib/python3.9/contextlib.py:79, in ContextDecorator.__call__.<locals>.inner(*args, **kwds)
     76 @wraps(func)
     77 def inner(*args, **kwds):
     78     with self._recreate_cm():
---> 79         return func(*args, **kwds)

File ~/miniconda3/envs/rapids-22.12/lib/python3.9/site-packages/cudf/core/groupby/groupby.py:460, in GroupBy.agg(self, func)
    451 column_names, columns, normalized_aggs = self._normalize_aggs(func)
    453 # Note: When there are no key columns, the below produces
    454 # a Float64Index, while Pandas returns an Int64Index
    455 # (GH: 6945)
    456 (
    457     result_columns,
    458     grouped_key_cols,
    459     included_aggregations,
--> 460 ) = self._groupby.aggregate(columns, normalized_aggs)
    462 result_index = self.grouping.keys._from_columns_like_self(
    463     grouped_key_cols,
    464 )
    466 multilevel = _is_multi_agg(func)

File groupby.pyx:302, in cudf._lib.groupby.GroupBy.aggregate()

File groupby.pyx:250, in cudf._lib.groupby.GroupBy.scan_internal()

File aggregation.pyx:897, in cudf._lib.aggregation.make_groupby_scan_aggregation()

File aggregation.pyx:573, in cudf._lib.aggregation.GroupbyScanAggregation.rank()

TypeError: rank() takes exactly 4 positional arguments (0 given)
````

```
conda list | grep ""rapids\|pandas""
# packages in environment at /home/nicholasb/miniconda3/envs/rapids-22.12:
cubinlinker               0.2.0            py39h11215e4_1    rapidsai-nightly
cucim                     22.12.00a221115 cuda_11_py39_gf085f29_10    rapidsai-nightly
cudf                      22.12.00a221115 cuda_11_py39_gbae9e39ffa_261    rapidsai-nightly
cudf_kafka                22.12.00a221115 py39_gbae9e39ffa_261    rapidsai-nightly
cugraph                   22.12.00a221115 cuda11_py39_g85c447b0_108    rapidsai-nightly
cuml                      22.12.00a221115 cuda11_py39_g65008974f_41    rapidsai-nightly
cusignal                  22.12.00a221115 py39_g5265ccb_7    rapidsai-nightly
cuspatial                 22.12.00a221115 py39_g337a92a_59    rapidsai-nightly
custreamz                 22.12.00a221115 py39_gbae9e39ffa_261    rapidsai-nightly
cuxfilter                 22.12.00a221115 py39_gbeb436a_4    rapidsai-nightly
dask-cuda                 22.12.00a221115 py39_gb3ed902_27    rapidsai-nightly
dask-cudf                 22.12.00a221115 cuda_11_py39_gbae9e39ffa_261    rapidsai-nightly
datashader                0.13.1a                    py_0    rapidsai-nightly
geopandas                 0.12.1             pyhd8ed1ab_1    conda-forge
geopandas-base            0.12.1             pyha770c72_1    conda-forge
libcucim                  22.12.00a221115 cuda11_gf085f29_10    rapidsai-nightly
libcudf                   22.12.00a221115 cuda11_gbae9e39ffa_261    rapidsai-nightly
libcudf_kafka             22.12.00a221115 gbae9e39ffa_261    rapidsai-nightly
libcugraph                22.12.00a221115 cuda11_g605e2b53_109    rapidsai-nightly
libcugraph_etl            22.12.00a221115 cuda11_g605e2b53_109    rapidsai-nightly
libcugraphops             22.12.00a221115 cuda11_gbbf866a_20    rapidsai-nightly
libcuml                   22.12.00a221115 cuda11_g65008974f_41    rapidsai-nightly
libcumlprims              22.12.00a221010 cuda11_geaadb5e_2    rapidsai-nightly
libcuspatial              22.12.00a221115 cuda11_g337a92a_59    rapidsai-nightly
libraft-distance          22.12.00a221115 cuda11_g355f693_114    rapidsai-nightly
libraft-headers           22.12.00a221115 cuda11_g355f693_114    rapidsai-nightly
libraft-nn                22.12.00a221115 cuda11_g355f693_114    rapidsai-nightly
librmm                    22.12.00a221115 cuda11_g4da70d53_54    rapidsai-nightly
libxgboost                1.6.2dev.rapidsai22.12       cuda_11_0    rapidsai-nightly
pandas                    1.5.1            py39h4661b88_1    conda-forge
ptxcompiler               0.7.0           cuda_11_py39_gb86c990_1    rapidsai-nightly
py-xgboost                1.6.2dev.rapidsai22.12  cuda_11_py39_0    rapidsai-nightly
pylibcugraph              22.12.00a221115 cuda11_py39_g85c447b0_108    rapidsai-nightly
pylibraft                 22.12.00a221115 cuda11_py39_g355f693_114    rapidsai-nightly
raft-dask                 22.12.00a221115 cuda11_py39_g355f693_114    rapidsai-nightly
rapids                    22.12.00a221115 cuda11_py39_g2d3c297_34    rapidsai-nightly
rapids-xgboost            22.12.00a221115 cuda11_py39_g2d3c297_34    rapidsai-nightly
rmm                       22.12.00a221115 cuda11_py39_g4da70d53_54    rapidsai-nightly
strings_udf               22.12.00a221115 cuda_11_py39_gbae9e39ffa_261    rapidsai-nightly
ucx-proc                  1.0.0                       gpu    rapidsai-nightly
ucx-py                    0.29.00a221114  py39_g5c7f2a3_19    rapidsai-nightly
xgboost                   1.6.2dev.rapidsai22.12  cuda_11_py39_0    rapidsai-nightly
```",2022-11-15T23:42:39Z,0,0,Nick Becker,@NVIDIA,True
397,[BUG/pandas-compat]: Handling of type promotion and division/mod by zero for boolean columns ,"After #12074, most type promotions between columns of mixed types (and non-mixed types) match pandas. The exception is columns with boolean dtypes.

Pandas have taken the decision to disallow division and exponentiation on boolean types when both operands are booleans (https://github.com/pandas-dev/pandas/blob/d13c9e034ce8a1d738766c4b1cec80c76f5523be/pandas/core/ops/array_ops.py#L503).

Aside: I kind of disagree with this since this is all perfectly well defined (excepting the usual caveat of division by zero).

When only one of the operands is `bool`, the status quo depends on the dtype of the other operand:

## Pandas behaviour:

For `a % b`, with `a == 1`, `b == 0` for various dtypes

|  dtype-a \ dtype-b  | bool | int | float |
|----------|------|------|-|
| bool | int8(0) (or ZeroDivisionError[^1]) | float64(NaN) | float64(NaN) |
| int | int64(0) (or ZeroDivisionError[^1]) | float64(NaN) | float64(NaN) |
| float | float64(NaN) (or ZeroDivisionError[^1])| float64(NaN) | float64(NaN) |

For `a / b` (or `a // b`) with `a == 1`, `b = 0`

|  dtype-a \ dtype-b  | bool | int | float |
|----------|------|------|-|
| bool | NotImplemented (or ZeroDivisionError[^1]) | float64(inf) | float64(inf) |
| int | float64(inf)(or ZeroDivisionError[^1]) | float64(inf) | float64(inf) |
| float | float64(inf) (or ZeroDivisionError[^1])| float64(inf) | float64(inf) |

For `a % b`, with `a == 0`, `b == 0` for various dtypes

|  dtype-a \ dtype-b  | bool | int | float |
|----------|------|------|-|
| bool | int8(0)(or ZeroDivisionError[^1]) | float64(NaN) | float64(NaN) |
| int | int64(0) (or ZeroDivisionError[^1])| float64(NaN) | float64(NaN) |
| float | float64(NaN) (or ZeroDivisionError[^1])| float64(NaN) | float64(NaN) |

For `a / b` (or `a // b`) with `a == 0`, `b = 0`

|  dtype-a \ dtype-b  | bool | int | float |
|----------|------|------|-|
| bool | NotImplemented (or ZeroDivisionError[^1]) | float64(NaN) | float64(NaN) |
| int | float64(NaN) (or ZeroDivisionError[^1]) | float64(NaN) | float64(NaN) |
| float | float64(NaN) (or ZeroDivisionError[^1]) | float64(NaN) | float64(NaN) |

[^1]: If the operands are different lengths, we get a ZeroDivisionError (see https://github.com/pandas-dev/pandas/issues/49699)

## cuDF behaviour:

For `a % b`, with `a == 1`, `b == 0` for various dtypes

|  dtype-a \ dtype-b  | bool | int | float |
|----------|------|------|-|
| bool | bool(0) | float64(NaN) | float64(NaN) |
| int | int64(2**32 - 1) | float64(NaN) | float64(NaN) |
| float | float64(NaN) | float64(NaN) | float64(NaN) |

For `a // b` with `a == 1`, `b = 0`

|  dtype-a \ dtype-b  | bool | int | float |
|----------|------|------|-|
| bool | bool(1) | float64(inf) | float64(inf) |
| int | int64(2**32 - 1) | float64(inf) | float64(inf) |
| float | float64(inf) | float64(inf) | float64(inf) |

For `a / b` with `a == 1`, `b = 0`

|  dtype-a \ dtype-b  | bool | int | float |
|----------|------|------|-|
| bool | float32(inf) | float64(inf) | float64(inf) |
| int | float64(inf) | float64(inf) | float64(inf) |
| float | float64(inf) | float64(inf) | float64(inf) |

For `a % b`, with `a == 0`, `b == 0` for various dtypes

|  dtype-a \ dtype-b  | bool | int | float |
|----------|------|------|-|
| bool | bool(0) | float64(NaN) | float64(NaN) |
| int | int64(2**32 - 1) | float64(NaN) | float64(NaN) |
| float | float64(NaN) | float64(NaN) | float64(NaN) |

For `a // b` with `a == 0`, `b = 0`

|  dtype-a \ dtype-b  | bool | int | float |
|----------|------|------|-|
| bool | bool(False) | float64(NaN) | float64(NaN) |
| int | int64(2**32 - 1) | float64(NaN) | float64(NaN) |
| float | float64(NaN) | float64(NaN) | float64(NaN) |

For `a / b` with `a == 0`, `b = 0`

|  dtype-a \ dtype-b  | bool | int | float |
|----------|------|------|-|
| bool | float32(NaN) | float64(NaN) | float64(NaN) |
| int | float64(NaN) | float64(NaN) | float64(NaN) |
| float | float64(NaN) | float64(NaN) | float64(NaN) |",2022-11-16T11:31:10Z,0,0,Lawrence Mitchell,,False
398,[PROPOSAL] Define fixed-point types in `cudf::` namespace,"Fixed-point (decimal) types are defined in a top-level namespace `numeric` and not in the namespace `cudf`.

This shows up in the stringified type name, and violates the expectation that packages should probably keep all their definitions in a namespace owned by the package, i.e. `cudf::`.

See additional discussion: https://github.com/rapidsai/cudf/pull/12158#discussion_r1023411485

I propose moving the fixed-point code to a namespace `cudf::fixed_point` and refactoring as needed.",2022-11-16T18:39:36Z,0,0,Bradley Dice,@NVIDIA @rapidsai,True
399,[FEA] Improve re-use of dlpack from libcudf build dir for python builds,"**Is your feature request related to a problem? Please describe.**
When using `FIND_CUDF_CPP` for python cudf builds don't reclone dlpack.

**Describe the solution you'd like**
When using `FIND_CUDF_CPP` for python cudf builds re-use the dlpack in the libcudf build directory instead of cloning a second copy.",2022-11-16T20:02:43Z,0,0,Robert Maynard,NVIDIA,True
400,[FEA] Support `duplicate_find_option::FIND_ALL` for `lists::index_of`,"Currently, the API `lists::index_of` can only search for the first or last index of the given key in the lists. In some cases, we need to search for all the positions where the key appears in the search space (lists). For example, we want to remove an element from the lists:
```
remove({1, 2, 3, 4, 5}, 3) ==> {1, 2, 4, 5}
```

In order to implement such API (`remove`), we need to support `duplicate_find_option::FIND_ALL` for `lists::index_of` so we can search for all indices of the search key as a gather map and generate the output using that gather map.",2022-11-21T16:56:31Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
401,[FEA] Support `lists::remove`,"We should have an API to remove an element from the given list, like this:
```
remove({1, 2, 3, 4, 5}, 3) ==> {1, 2, 4, 5}

remove({ {1, 2, 3}, {3, 4, 5}, {5}, ......}, {1, 3, 5})  ==> { { 2, 3}, {4, 5}, {}, .....}
```",2022-11-21T17:07:45Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
402,[BUG] Add test cases to check compression is enabled,"**Describe the bug**
There have been cases where compression has been enabled but files fail to compress properly.  Please add test cases to validate compression algorithms are in use.  Failing to compress a file in production can result in very slow reading or writing.  

**Steps/Code to reproduce bug**
Related issues https://github.com/rapidsai/cudf/issues/12066, https://github.com/rapidsai/cudf/issues/12170

**Expected behavior**
Test cases fail if a test file is not compressed by at least a certain %
Test cases cover all file types where compression is supported
Test cases cover different compression algorithms",2022-11-22T05:20:52Z,0,0,Sameer Raheja,Nvidia,True
403,[FEA] Use RMM memory pool by default,"We should move to using an RMM managed memory pool by default.

This was brought up before in https://github.com/rapidsai/cudf/issues/2676. In response to that issue, we implemented `set_allocator`, https://github.com/rapidsai/cudf/pull/2682, but we chose not to enable the RMM pool by default (likely because we didn't want to monopolize GPU memory away from other libraries). 

Since then, CuPy, Numba (and soon [PyTorch](https://github.com/pytorch/pytorch/pull/86786)) all can be configured to use RMM, and therefore share the same memory pool as cuDF.

## Proposal

Concretely, the proposal is that `import cudf` will:

* Set RMM's default memory resource to a [pool memory resource](https://docs.rapids.ai/api/rmm/stable/api.html#rmm.mr.PoolMemoryResource)
* Configure CuPy, Numba, (and PyTorch?) to all use RMM's default memory resource

## What should the initial and maximum pool size be? 

An RMM pool can be configured with an initial and maximum pool size. The pool grows according to an implementation-defined strategy (see [here](https://github.com/rapidsai/rmm/blob/d132e5236b444d2dcdae25e846c4fe4d5651ee79/include/rmm/mr/device/pool_memory_resource.hpp#L246-L249) for the current strategy).

- As we cannot assume that all (or any) GPU memory is available when cuDF is imported, the initial pool size should be 0 bytes.
- The only reasonable maximum pool size I can think of is the maximum available GPU memory. If the pool cannot expand to this size because of allocations made outside of RMM, so be it: we will OOM. 

## What happens if `import cudf` appears in the middle of the program?

All this works well if `import cudf` appears at the beginning of the program, i.e., before any device memory is actually allocated by any library). However, if it appears _after_ some device objects have already been allocated, it can lead to early out-of-memory errors. As an example, consider some code that uses both PyTorch and cuDF in the following way:

```python
import torch
 
# this part of the code uses PyTorch

import cudf

# this part of the code uses cudf
```

Because PyTorch [uses a caching allocator](https://pytorch.org/docs/stable/notes/cuda.html#memory-management), a memory pool already exists by the time we import cuDF. Importing cuDF initializes a second pool that all libraries (including PyTorch) will use going forward. The first pool essentially becomes a kind of dead space: no new device objects are ever allocated within the pool, and no device memory is ever freed from it. 

There's no perfect solution I can think of to this particular problem, but it's probably a good idea to call [`empty_cache()`](https://pytorch.org/docs/stable/generated/torch.cuda.empty_cache.html#torch.cuda.empty_cache) before resetting the PyTorch allocator to minimize the amount of wasted memory.

---

That's just one example of the kind of issues that can arise if `import cudf` appears later. I think it's fine to assume this will be less common than importing it at the beginning.",2022-11-23T16:49:48Z,2,0,Ashwin Srinath,Voltron Data,False
404,[FEA] Add a new cuDF option stable_sort that provides ordering guarantees for otherwise nondeterministic APIs,"**Is your feature request related to a problem? Please describe.**
Currently various cuDF APIs (examples include `merge`, `groupby`, and `drop_duplicates`) rely on libcudf APIs that do not promise stable ordering. libcudf will not (and should not) ever be forced to make such guarantees since requiring ordering hamstrings potential optimizations and can always be achieved by calling code with the addition of suitable index columns and sorts. However, the requirements of cuDF Python are different. In particular, for pandas users transitioning to cuDF the lack of stability may be confusing at best and workflow-breaking at worst. Since there are multiple APIs that may exhibit this API divergence, and these APIs may be invoked many times in any particular piece of code, the responsibility should not fall to users to wrap those calls in appropriate sorting logic every time.

**Describe the solution you'd like**
cuDF should leverage the options framework introduced in #11193 to add a new option `stable_sort` that, when True, will change the behavior of all cuDF APIs to guarantee that the input order will be preserve. The default value should be `False` to force users to opt in to performance-inhibiting changes. The implementation of this option will depend on the API; for instance, with `merge` it will require consistent ordering in both tables, whereas for `drop_duplicates` it only affects one. I would recommend implementing the option as `_stable_sort` at first to indicate that it is internal, then adding support to one algorithm at a time, then exposing the option publicly as `stable_sort` once we have sufficient algorithmic coverage.

**Describe alternatives you've considered**
We have rejected similar requests in the past, in part because of the lack of clarity on how this should be handled in cuDF Python vs. libcudf, as well as because we did not wish to slow down all calls to these APIs by default. The use of `cudf.options` at the Python level provides an elegant and well-scoped solution to the problem that avoids these problems.

**Additional context**
Addressing this issue will close #1781 and #5286.",2022-11-23T18:41:10Z,0,0,Vyas Ramasubramani,@rapidsai,True
405,[BUG] libcudf tests of binary operations (in `binop-compiled-test.cpp`) mostly do not test against a ground truth,"**Describe the bug**

Binary operations between columns are implemented by runtime typed dispatch to device functors in [`cpp/src/binaryop/compiled/operation.cuh`](https://github.com/rapidsai/cudf/blob/7426a06a4510280650df4cf54b76504d690c80b2/cpp/src/binaryop/compiled/operation.cuh).

Tests of this functionality compare to host-based compute, with an implementation of the functors in [`cpp/tests/binaryop/util/operation.h`](https://github.com/rapidsai/cudf/blob/7426a06a4510280650df4cf54b76504d690c80b2/cpp/tests/binaryop/util/operation.h)

This is a very weak test of correctness that really is testing:

1. Can the author of the test copy the device functor implementation into the host implementation?
2. Can the device and host compilers emit correct code for these cases?

These tests should more properly test against a ground truth (either manually constructed, or automatically).",2022-11-24T18:40:03Z,0,0,Lawrence Mitchell,,False
406,[FEA] Support device-side de/compression of CSV files,"I have a lot of gzip compressed CSV files. When I use cudf to read them, the host handles decompression before copying decompressed data to device.

For cudf, that's not a problem, since it'll at worst be no slower than CPU.

But when I read w/ dask_cudf, compared to CPU dask.dataframe, I will usually have <=8 workers in a LocalCUDACluster. If I'm reading a large number of compressed files, those 8 workers will be highly bottlenecked by decompression.

**Describe the solution you'd like**
Ideally, we could have fast device side decompression for gzip compressed CSVs.

**Describe alternatives you've considered**
Another solution for dask_cudf could be some logic to make more parallel use of host CPUs for decompression, which should increase throughput t device.

**Additional context**
Per file compression level can be high, such that doing device side decompression, even if faster than CPU, could easily lead to OOM scenarios.

An illustrative dataset for use in exploring this problem is NOAA's daily weather observations:
```
import urllib, os

data_dir = '/raid/weather/csv/'

# download weather observations
base_url = 'ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/'
years = list(range(1763, 2020))
for year in years:
    fn = str(year) + '.csv.gz'
    if not os.path.isfile(data_dir+fn):
        print(f'Downloading {base_url+fn} to {data_dir+fn}')
        urllib.request.urlretrieve(base_url+fn, data_dir+fn) 
```

cc @GregoryKimball ",2022-11-29T17:34:28Z,0,0,Randy Gelhausen,,False
407,[FEA] automatic construction of MultiIndex DataFrame,"**Is your feature request related to a problem? Please describe.**
rewriting code from pandas into cudf, using `import cudf as pd`

**Describe the solution you'd like**
```
>>> import pandas as pd
>>> import cudf
>>> cudf.__version__
'22.12.00a+281.gcc4b4dd27c'
>>> df = pd.DataFrame([[1],[2]], index=[['a', 'a'],['b','c']])
>>> df
     0
a b  1
  c  2
>>> cudf.DataFrame([[1],[2]], index=[['a', 'a'],['b','c']])
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/opt/conda/envs/rapids/lib/python3.9/contextlib.py"", line 79, in inner
    return func(*args, **kwds)
  File ""/opt/conda/envs/rapids/lib/python3.9/site-packages/cudf/core/dataframe.py"", line 710, in __init__
    self._init_from_list_like(
  File ""/opt/conda/envs/rapids/lib/python3.9/contextlib.py"", line 79, in inner
    return func(*args, **kwds)
  File ""/opt/conda/envs/rapids/lib/python3.9/site-packages/cudf/core/dataframe.py"", line 838, in _init_from_list_like
    index = as_index(index)
  File ""/opt/conda/envs/rapids/lib/python3.9/contextlib.py"", line 79, in inner
    return func(*args, **kwds)
  File ""/opt/conda/envs/rapids/lib/python3.9/site-packages/cudf/core/index.py"", line 3019, in as_index
    return as_index(
  File ""/opt/conda/envs/rapids/lib/python3.9/contextlib.py"", line 79, in inner
    return func(*args, **kwds)
  File ""/opt/conda/envs/rapids/lib/python3.9/site-packages/cudf/core/index.py"", line 3005, in as_index
    return _index_from_data({kwargs.get(""name"", None): arbitrary})
  File ""/opt/conda/envs/rapids/lib/python3.9/site-packages/cudf/core/index.py"", line 122, in _index_from_data
    raise NotImplementedError(
NotImplementedError: Unsupported column type passed to create an Index: <class 'cudf.core.column.lists.ListColumn'>
```
i would like the same functionality in `cudf` as is present in `pandas` (see https://pandas.pydata.org/docs/user_guide/advanced.html#hierarchical-indexing-multiindex)",2022-11-29T23:08:04Z,0,0,Matthew Farrellee,,False
408,[FEA] cudf.DataFrame.xs,"**Is your feature request related to a problem? Please describe.**
rewriting code from pandas to cudf, using `import cudf as pd`


**Describe the solution you'd like**
`cudf.DataFrame.xs` matching https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.xs.html",2022-11-29T23:18:59Z,0,0,Matthew Farrellee,,False
409,[FEA] Refactor hash-based algorithms with new cuco data structures,"**Is your feature request related to a problem? Please describe.**
`cuco::static_map` and `cuco::static_multimap` are used to perform hash-based operations in libcudf. Depending on https://github.com/NVIDIA/cuCollections/issues/110, a lot of existing use cases could be replaced with `cuco::static_set` or `cuco:static_multiset` since the payload part in the hash map is not used.

Moreover, some libcudf implementations are still using `concurrent_unordered_map` or `unordered_multiset` and they should be refactored with `cuco::static_set/multiset` as well.

**Describe the solution you'd like**
Replace existing implementations with new data structures like `cuco::static_set`, `cuco::static_multiset` or `cuco::static_map`.

There should also be benchmarks showing the performance changes after replacement for most works listed below.
Note: under ""Related APIs"", the `ref::` prefix refers to the cuco device-side API.

## Part 1: Updates ready to be addressed in libcudf
|Algorithm|Desired Data Structure|Related APIs|PRs|Notes|
|---|---|---|---|---|
|distinct_count|`cuco::static_set`|`insert`, `insert_if`| ✅#13343 | |
|orc dictionary encoding (issue #10495)|`cuco::static_map` (legacy) |`ref::insert`, `ref::find`| ✅#13580 | Similar to parquet dictionary encoding but the current implementation is still using a custom dictionary | |
|byte_pair_encoding|`cuco::static_map`|`insert_async`, `ref::find`| ✅#13807 | uses heterogeneous lookup |
|json_tree|`cuco::static_set`|`insert_if_async`, `ref::find`| ✅#13928 |  |
|search/contains_table|`cuco::static_set`|`insert_async`, `insert_if_async`, `contains_async`, `contains_if_async` |✅#14064 | Needs migration from `cudf::detail::unordered_multiset` |
|search/contains_column|`cuco::static_set`|`insert_async`, `insert_if_async`, `contains_async`| ✅#14238 | Needs migration from `cudf::detail::unordered_multiset`. `unordered_multiset` can be removed once this is done.|
|hash-based groupby (issue #10401)|`cuco::static_set`|`ref::insert_and_find`, `retrieve_all`| ✅#14813 | Needs migration from `concurrent_unordered_map`|
| legacy json reader | `cuco::static_map` |`insert_async`, `ref::find`| ✅ #15813 | ~Needs migration from `concurrent_unordered_map`. Looks like a rational use of hash map. To be replaced by `cuco::static_map`~ Deleted in #15813 | |
| distinct | `cuco::static_set`  | | #15285  | Needs migration from `cuco::static_map` (legacy), and then in Part 2, further migration to `cuco::reduction_map` |

## Part 2: Updates depending on upstream cuco changes
|Algorithm|Desired Data Structure|Related APIs|PRs|Notes|
|---|---|---|---|---|
| semi/anti joins | `cuco::static_set` ? could be ready today for migration, but might need `cuco::static_multiset` updates first |  | TBD | see #9973. currently using `cuco::static_multimap` (legacy) |
| joins |  `cuco::static_multiset` | `count`, `count_outer`, `retrieve`, `retrieve_outer`| `insert_async`, `insert_if_async` | Needs migration from `cuco::static_multimap` (legacy) Related issues: #11176, #13116 |
| mixed joins |  `cuco::static_multiset` | see ""joins"" | TBD | Needs migration from `cuco::static_multimap` (legacy) |
| parquet dictionary encoding | `cuco::static_map`  (experimental)  | probably unblocked? | `ref::insert`, `ref::find` | Needs migration from `cuco::static_map` (legacy) |
| orc dictionary encoding |`cuco::static_map` (experimental) | probably unblocked?  | `ref::insert`, `ref::find` | Needs migration from `cuco::static_map` (legacy)  | |
| distinct | `cuco::static_reduction_map` | | TBD  |  see #13157. Reduction map not yet implemented in cuco, could just have `cuco::static_map` specialization with a new API ""insert_or_apply"" |


",2022-11-29T23:23:28Z,0,0,Yunsong Wang,@NVIDIA @rapidsai,True
410,[BUG] DataFrame.groupby.describe differs between cudf and pandas,"**Describe the bug**
```
>>> import pandas as pd
>>> import cudf
>>> cudf.__version__
'22.12.00a+281.gcc4b4dd27c'
>>> data = {'a': ['b'], 'p': ['q'], 'n': [0]}
>>> pd.DataFrame(data).groupby('a').describe()
      n                                  
  count mean std  min  25%  50%  75%  max
a                                        
b   1.0  0.0 NaN  0.0  0.0  0.0  0.0  0.0
>>> cudf.DataFrame(data).groupby('a').describe()
      p             n                                  
  count min max count mean   std min  25%  50%  75% max
a                                                      
b     1   q   q     1  0.0  <NA>   0  0.0  0.0  0.0   0
```

**Environment overview (please complete the following information)**
rapidsai/rapidsai-nightly:22.12-cuda11.5-runtime-rockylinux8-py3.9 on 29 nov 2022",2022-11-30T00:51:25Z,0,0,Matthew Farrellee,,False
411,[FEA] Add ascending parameter to cumcount.,"**Is your feature request related to a problem? Please describe.**
cumcount misses `ascending` parameter present in pandas breaking any code exported from pandas.

**Describe the solution you'd like**
Add `ascending=[True/False]` parameter to cumcount to match pandas syntax. 

**Describe alternatives you've considered**
An alternative for `ascending=False` would be to pick the max of the group and subtract from cumcount like:
df['cumcount'] = df.groupby('group')['var'].transform('max') - df.groupby('group')['var'].cumcount()

**Additional context**
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.cumcount.html
https://docs.rapids.ai/api/cudf/stable/api_docs/api/cudf.core.groupby.groupby.GroupBy.cumcount.html
",2022-11-30T13:14:51Z,0,0,Gilberto Titericz Junior,NVIDIA,True
412,[BUG] cudf.DataFrame.equals reports False for inputs with NaNs,"**Describe the bug**

According to the [documentation](https://docs.rapids.ai/api/cudf/stable/api_docs/api/cudf.DataFrame.equals.html#cudf.DataFrame.equals), the result of `DataFrame.equals()` should be `True` when the two dataframes are equal, even when they contain NaNs:

> NaNs in the same location are considered equal.

However, this does not seem to be the case under certain circumstances.

**Steps/Code to reproduce bug**

The following snippet demonstrates the unexpected behavior:

```python
import numpy as np
import cudf

input_array = np.array([np.nan])

a = cudf.DataFrame(input_array)
assert a.equals(a.copy())
```
will fail with an `AssertionError`.

Curiously the following snippet does **not** fail:

```python
a = cudf.DataFrame([np.nan])
assert a.equals(a.copy())
```

**Expected behavior**

The result of `DataFrame.equals()` should be True in case that values are the same and NaNs are in the same place as described by the method's documentation.

**Environment overview (please complete the following information)**
 - Environment location: Bare-metal
 - Method of cuDF install: conda

**Environment details**
<details><summary>Click here to see environment details</summary><pre>
     
     **git***
     Not inside a git repository
     
     ***OS Information***
     DGX_NAME=""DGX Server""
     DGX_PRETTY_NAME=""NVIDIA DGX Server""
     DGX_SWBUILD_DATE=""2020-03-04""
     DGX_SWBUILD_VERSION=""4.4.0""
     DGX_COMMIT_ID=""ee09ebc""
     DGX_PLATFORM=""DGX Server for DGX-1""
     DGX_SERIAL_NUMBER=""QTFCOU6300082""
     DISTRIB_ID=Ubuntu
     DISTRIB_RELEASE=18.04
     DISTRIB_CODENAME=bionic
     DISTRIB_DESCRIPTION=""Ubuntu 18.04.4 LTS""
     NAME=""Ubuntu""
     VERSION=""18.04.4 LTS (Bionic Beaver)""
     ID=ubuntu
     ID_LIKE=debian
     PRETTY_NAME=""Ubuntu 18.04.4 LTS""
     VERSION_ID=""18.04""
     HOME_URL=""https://www.ubuntu.com/""
     SUPPORT_URL=""https://help.ubuntu.com/""
     BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
     PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
     VERSION_CODENAME=bionic
     UBUNTU_CODENAME=bionic
     Linux dgx06 4.15.0-76-generic #86-Ubuntu SMP Fri Jan 17 17:24:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
     
     ***GPU Information***
     Wed Nov 30 05:42:50 2022
     +-----------------------------------------------------------------------------+
     | NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |
     |-------------------------------+----------------------+----------------------+
     | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
     | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
     |                               |                      |               MIG M. |
     |===============================+======================+======================|
     |   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |
     | N/A   32C    P0    55W / 300W |    668MiB / 32510MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |
     | N/A   32C    P0    42W / 300W |      3MiB / 32510MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |
     | N/A   30C    P0    42W / 300W |      3MiB / 32510MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |
     | N/A   29C    P0    42W / 300W |      3MiB / 32510MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |
     | N/A   30C    P0    42W / 300W |      3MiB / 32510MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |
     | N/A   30C    P0    42W / 300W |      3MiB / 32510MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |
     | N/A   31C    P0    42W / 300W |      3MiB / 32510MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |
     | N/A   29C    P0    41W / 300W |      3MiB / 32510MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     
     +-----------------------------------------------------------------------------+
     | Processes:                                                                  |
     |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
     |        ID   ID                                                   Usage      |
     |=============================================================================|
     |    0   N/A  N/A    630188      C   python                            665MiB |
     +-----------------------------------------------------------------------------+
     
     ***CPU***
     Architecture:        x86_64
     CPU op-mode(s):      32-bit, 64-bit
     Byte Order:          Little Endian
     CPU(s):              80
     On-line CPU(s) list: 0-79
     Thread(s) per core:  2
     Core(s) per socket:  20
     Socket(s):           2
     NUMA node(s):        2
     Vendor ID:           GenuineIntel
     CPU family:          6
     Model:               79
     Model name:          Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz
     Stepping:            1
     CPU MHz:             3226.018
     CPU max MHz:         3600.0000
     CPU min MHz:         1200.0000
     BogoMIPS:            4390.02
     Virtualization:      VT-x
     L1d cache:           32K
     L1i cache:           32K
     L2 cache:            256K
     L3 cache:            51200K
     NUMA node0 CPU(s):   0-19,40-59
     NUMA node1 CPU(s):   20-39,60-79
     Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap intel_pt xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts md_clear flush_l1d
     
     ***CMake***
     /raid/sadorf/mambaforge/envs/cuml_dev/bin/cmake
     cmake version 3.24.3
     
     CMake suite maintained and supported by Kitware (kitware.com/cmake).
     
     ***g++***
     /raid/sadorf/mambaforge/envs/cuml_dev/bin/g++
     g++ (conda-forge gcc 9.5.0-19) 9.5.0
     Copyright (C) 2019 Free Software Foundation, Inc.
     This is free software; see the source for copying conditions.  There is NO
     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
     
     
     ***nvcc***
     /usr/local/cuda/bin/nvcc
     nvcc: NVIDIA (R) Cuda compiler driver
     Copyright (c) 2005-2021 NVIDIA Corporation
     Built on Thu_Nov_18_09:45:30_PST_2021
     Cuda compilation tools, release 11.5, V11.5.119
     Build cuda_11.5.r11.5/compiler.30672275_0
     
     ***Python***
     /raid/sadorf/mambaforge/envs/cuml_dev/bin/python
     Python 3.8.13
     
     ***Environment Variables***
     PATH                            : /home/nfs/sadorf/.vscode-server/bin/6261075646f055b99068d3688932416f2346dd3b/bin/remote-cli:/raid/sadorf/mambaforge/envs/cuml_dev/bin:/raid/sadorf/mambaforge/condabin:/usr/local/cuda/bin:/opt/bin:/usr/local/cuda/bin:/opt/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
     LD_LIBRARY_PATH                 :
     NUMBAPRO_NVVM                   :
     NUMBAPRO_LIBDEVICE              :
     CONDA_PREFIX                    : /raid/sadorf/mambaforge/envs/cuml_dev
     PYTHON_PATH                     :
     
     ***conda packages***
     /raid/sadorf/mambaforge/envs/cuml_dev/bin/conda
     # packages in environment at /raid/sadorf/mambaforge/envs/cuml_dev:
     #
     # Name                    Version                   Build  Channel
     _libgcc_mutex             0.1                 conda_forge    conda-forge
     _openmp_mutex             4.5                       2_gnu    conda-forge
     _sysroot_linux-64_curr_repodata_hack 3                   h5bd9786_13    conda-forge
     abseil-cpp                20210324.2           h9c3ff4c_0    conda-forge
     aiobotocore               2.4.0              pyhd8ed1ab_0    conda-forge
     aiohttp                   3.8.3            py38h0a891b7_1    conda-forge
     aioitertools              0.11.0             pyhd8ed1ab_0    conda-forge
     aiosignal                 1.3.1              pyhd8ed1ab_0    conda-forge
     alabaster                 0.7.12                     py_0    conda-forge
     anyio                     3.6.2              pyhd8ed1ab_0    conda-forge
     aom                       3.5.0                h27087fc_0    conda-forge
     appdirs                   1.4.4              pyh9f0ad1d_0    conda-forge
     argon2-cffi               21.3.0             pyhd8ed1ab_0    conda-forge
     argon2-cffi-bindings      21.2.0           py38h0a891b7_3    conda-forge
     arrow-cpp                 9.0.0           py38ha7276ea_1_cpu    conda-forge
     astroid                   2.12.13          py38h578d9bd_0    conda-forge
     asvdb                     0.4.2               g90e8f2c_40    rapidsai
     async-timeout             4.0.2              pyhd8ed1ab_0    conda-forge
     atk-1.0                   2.38.0               hd4edc92_1    conda-forge
     attrs                     22.1.0             pyh71513ae_1    conda-forge
     autoconf                  2.69            pl5321hd708f79_11    conda-forge
     automake                  1.16.5          pl5321ha770c72_0    conda-forge
     aws-c-cal                 0.5.11               h95a6274_0    conda-forge
     aws-c-common              0.6.2                h7f98852_0    conda-forge
     aws-c-event-stream        0.2.7               h3541f99_13    conda-forge
     aws-c-io                  0.10.5               hfb6a706_0    conda-forge
     aws-checksums             0.1.11               ha31a3da_7    conda-forge
     aws-sam-translator        1.53.0             pyhd8ed1ab_0    conda-forge
     aws-sdk-cpp               1.8.186              hecaee15_4    conda-forge
     aws-xray-sdk              2.10.0             pyhd8ed1ab_0    conda-forge
     babel                     2.11.0             pyhd8ed1ab_0    conda-forge
     backcall                  0.2.0              pyh9f0ad1d_0    conda-forge
     backports                 1.0                        py_2    conda-forge
     backports.functools_lru_cache 1.6.4              pyhd8ed1ab_0    conda-forge
     backports.zoneinfo        0.2.1            py38h0a891b7_7    conda-forge
     bcrypt                    3.2.2            py38h0a891b7_1    conda-forge
     beautifulsoup4            4.11.1             pyha770c72_0    conda-forge
     benchmark                 1.5.1                he1b5a44_2    conda-forge
     binutils                  2.39                 hdd6e379_0    conda-forge
     binutils_impl_linux-64    2.39                 h6ceecb4_0    conda-forge
     binutils_linux-64         2.39                h5fc0e48_11    conda-forge
     black                     22.3.0             pyhd8ed1ab_0    conda-forge
     blas                      1.1                    openblas    conda-forge
     bleach                    5.0.1              pyhd8ed1ab_0    conda-forge
     blinker                   1.5                pyhd8ed1ab_0    conda-forge
     blosc                     1.21.1               h83bc5f7_3    conda-forge
     bokeh                     2.4.3              pyhd8ed1ab_3    conda-forge
     boost                     1.74.0           py38h2b96118_5    conda-forge
     boost-cpp                 1.74.0               h75c5d50_8    conda-forge
     boto3                     1.24.59            pyhd8ed1ab_0    conda-forge
     botocore                  1.27.59            pyhd8ed1ab_0    conda-forge
     branca                    0.6.0              pyhd8ed1ab_0    conda-forge
     breathe                   4.34.0             pyhd8ed1ab_0    conda-forge
     brotli                    1.0.9                h166bdaf_8    conda-forge
     brotli-bin                1.0.9                h166bdaf_8    conda-forge
     brotlipy                  0.7.0           py38h0a891b7_1005    conda-forge
     brunsli                   0.1                  h9c3ff4c_0    conda-forge
     bzip2                     1.0.8                h7f98852_4    conda-forge
     c-ares                    1.18.1               h7f98852_0    conda-forge
     c-blosc2                  2.4.3                h7a311fb_0    conda-forge
     c-compiler                1.3.0                h7f98852_0    conda-forge
     ca-certificates           2022.9.24            ha878542_0    conda-forge
     cached-property           1.5.2                hd8ed1ab_1    conda-forge
     cached_property           1.5.2              pyha770c72_1    conda-forge
     cachetools                5.2.0              pyhd8ed1ab_0    conda-forge
     cairo                     1.16.0            ha61ee94_1014    conda-forge
     ccache                    4.7.3                h2599c5e_0    conda-forge
     certifi                   2022.9.24          pyhd8ed1ab_0    conda-forge
     cffi                      1.15.1           py38h4a40e3a_2    conda-forge
     cfgv                      3.3.1              pyhd8ed1ab_0    conda-forge
     cfitsio                   4.1.0                hd9d235c_0    conda-forge
     cfn-lint                  0.71.0             pyhd8ed1ab_0    conda-forge
     chardet                   5.0.0            py38h578d9bd_1    conda-forge
     charls                    2.3.4                h9c3ff4c_0    conda-forge
     charset-normalizer        2.1.1              pyhd8ed1ab_0    conda-forge
     clang                     11.1.0               ha770c72_1    conda-forge
     clang-11                  11.1.0          default_ha53f305_1    conda-forge
     clang-tools               11.1.0          default_ha53f305_1    conda-forge
     clangxx                   11.1.0          default_ha53f305_1    conda-forge
     click                     8.1.3           unix_pyhd8ed1ab_2    conda-forge
     click-plugins             1.1.1                      py_0    conda-forge
     cligj                     0.7.2              pyhd8ed1ab_1    conda-forge
     cloudpickle               2.2.0              pyhd8ed1ab_0    conda-forge
     cmake                     3.24.3               h816a3e0_0    conda-forge
     cmake_setuptools          0.1.3                      py_0    rapidsai
     cmakelang                 0.6.13             pyhd8ed1ab_0    conda-forge
     cmarkgfm                  0.8.0            py38h0a891b7_2    conda-forge
     colorama                  0.4.6              pyhd8ed1ab_0    conda-forge
     colorcet                  3.0.1              pyhd8ed1ab_0    conda-forge
     commonmark                0.9.1                      py_0    conda-forge
     conda                     4.12.0           py38h578d9bd_0    conda-forge
     conda-build               3.22.0           py38h578d9bd_3    conda-forge
     conda-package-handling    1.9.0            py38h0a891b7_1    conda-forge
     conda-verify              3.1.1           py38h578d9bd_1006    conda-forge
     contourpy                 1.0.6            py38h43d8883_0    conda-forge
     coverage                  6.5.0            py38h0a891b7_1    conda-forge
     cryptography              38.0.3           py38h2b5fc30_0    conda-forge
     cubinlinker               0.2.0            py38h7144610_1    rapidsai
     cuda-python               11.7.1           py38h7525318_1    conda-forge
     cudatoolkit               11.5.1              h59c8dcf_11    conda-forge
     cudf                      23.02.00a221130 cuda_11_py38_g5f83a84916_111    rapidsai-nightly
     cuml                      22.12.0a0+56.gdd6c9d0e6          pypi_0    pypi
     cupy                      11.2.0           py38h405e1b6_0    conda-forge
     curl                      7.86.0               h7bff187_1    conda-forge
     cxx-compiler              1.3.0                h4bd325d_0    conda-forge
     cycler                    0.11.0             pyhd8ed1ab_0    conda-forge
     cyrus-sasl                2.1.27               h230043b_5    conda-forge
     cython                    0.29.32          py38hfa26641_1    conda-forge
     cytoolz                   0.12.0           py38h0a891b7_1    conda-forge
     dask                      2022.11.2a221129 py_gc23ee621_16    dask/label/dev
     dask-core                 2022.11.2a221125 py_g3ac3b8d6e_9    dask/label/dev
     dask-cuda                 23.02.00a221130 py38_gb087770_16    rapidsai-nightly
     dask-cudf                 23.02.00a221130 cuda_11_py38_g5f83a84916_111    rapidsai-nightly
     dask-glm                  0.2.1.dev52+g1daf4c5          pypi_0    pypi
     dask-labextension         6.0.0              pyhd8ed1ab_0    conda-forge
     dask-ml                   1.9.0              pyhd8ed1ab_0    conda-forge
     dataclasses               0.8                pyhc8e2a94_3    conda-forge
     datashader                0.13.1a                    py_0    rapidsai
     datashape                 0.5.4                      py_1    conda-forge
     dav1d                     1.0.0                h166bdaf_1    conda-forge
     dbus                      1.13.6               h5008d03_3    conda-forge
     debugpy                   1.6.3            py38hfa26641_1    conda-forge
     decopatch                 1.4.10             pyhd8ed1ab_0    conda-forge
     decorator                 5.1.1              pyhd8ed1ab_0    conda-forge
     defusedxml                0.7.1              pyhd8ed1ab_0    conda-forge
     dill                      0.3.6              pyhd8ed1ab_1    conda-forge
     distlib                   0.3.5              pyhd8ed1ab_0    conda-forge
     distributed               2022.11.2a221129 py_gc23ee621_16    dask/label/dev
     distro                    1.6.0              pyhd8ed1ab_0    conda-forge
     dlpack                    0.5                  h9c3ff4c_0    conda-forge
     docker-py                 6.0.0              pyhd8ed1ab_0    conda-forge
     docutils                  0.19             py38h578d9bd_1    conda-forge
     double-conversion         3.1.5                h9c3ff4c_2    conda-forge
     doxygen                   1.8.20               had0d8f1_0    conda-forge
     ecdsa                     0.18.0             pyhd8ed1ab_1    conda-forge
     entrypoints               0.4                pyhd8ed1ab_0    conda-forge
     exceptiongroup            1.0.1              pyhd8ed1ab_0    conda-forge
     execnet                   1.9.0              pyhd8ed1ab_0    conda-forge
     expat                     2.5.0                h27087fc_0    conda-forge
     faiss-proc                1.0.0                      cuda    rapidsai
     fastavro                  1.7.0            py38h0a891b7_0    conda-forge
     fastrlock                 0.8              py38hfa26641_3    conda-forge
     feather-format            0.4.1              pyh9f0ad1d_0    conda-forge
     filelock                  3.8.0              pyhd8ed1ab_0    conda-forge
     filterpy                  1.4.5                      py_1    conda-forge
     fiona                     1.8.22           py38hc72d8cd_0    conda-forge
     flake8                    3.8.4                      py_0    conda-forge
     flask                     2.1.3              pyhd8ed1ab_0    conda-forge
     flask_cors                3.0.10             pyhd3deb0d_0    conda-forge
     flit-core                 3.8.0              pyhd8ed1ab_0    conda-forge
     folium                    0.13.0             pyhd8ed1ab_0    conda-forge
     font-ttf-dejavu-sans-mono 2.37                 hab24e00_0    conda-forge
     font-ttf-inconsolata      3.000                h77eed37_0    conda-forge
     font-ttf-source-code-pro  2.038                h77eed37_0    conda-forge
     font-ttf-ubuntu           0.83                 hab24e00_0    conda-forge
     fontconfig                2.14.1               hc2a2eb6_0    conda-forge
     fonts-conda-ecosystem     1                             0    conda-forge
     fonts-conda-forge         1                             0    conda-forge
     fonttools                 4.38.0           py38h0a891b7_1    conda-forge
     freetype                  2.12.1               hca18f0e_0    conda-forge
     freexl                    1.0.6                h166bdaf_1    conda-forge
     fribidi                   1.0.10               h36c2ea0_0    conda-forge
     frozenlist                1.3.3            py38h0a891b7_0    conda-forge
     fsspec                    2022.10.0          pyhd8ed1ab_0    conda-forge
     future                    0.18.2             pyhd8ed1ab_6    conda-forge
     gcc                       9.5.0               h1fea6ba_11    conda-forge
     gcc_impl_linux-64         9.5.0               h99780fb_19    conda-forge
     gcc_linux-64              9.5.0               h4258300_11    conda-forge
     gcsfs                     2022.10.0          pyhd8ed1ab_0    conda-forge
     gdal                      3.5.2            py38h1f15b03_3    conda-forge
     gdk-pixbuf                2.42.8               hff1cb4f_1    conda-forge
     geopandas                 0.12.1             pyhd8ed1ab_1    conda-forge
     geopandas-base            0.12.1             pyha770c72_1    conda-forge
     geos                      3.11.0               h27087fc_0    conda-forge
     geotiff                   1.7.1                h4fc65e6_3    conda-forge
     gettext                   0.21.1               h27087fc_0    conda-forge
     gflags                    2.2.2             he1b5a44_1004    conda-forge
     giflib                    5.2.1                h36c2ea0_2    conda-forge
     git                       2.38.1          pl5321h5e804b7_1    conda-forge
     glib                      2.74.1               h6239696_1    conda-forge
     glib-tools                2.74.1               h6239696_1    conda-forge
     glob2                     0.7                        py_0    conda-forge
     glog                      0.6.0                h6f12383_0    conda-forge
     gmock                     1.10.0               h4bd325d_7    conda-forge
     gmp                       6.2.1                h58526e2_0    conda-forge
     gmpy2                     2.1.2            py38h793c122_1    conda-forge
     google-api-core           2.10.1             pyhd8ed1ab_0    conda-forge
     google-auth               2.14.0             pyh1a96a4e_0    conda-forge
     google-auth-oauthlib      0.7.1              pyhd8ed1ab_0    conda-forge
     google-cloud-core         2.3.2              pyhd8ed1ab_0    conda-forge
     google-cloud-storage      2.6.0              pyh1a96a4e_0    conda-forge
     google-crc32c             1.1.2            py38h57c428a_4    conda-forge
     google-resumable-media    2.4.0              pyhd8ed1ab_0    conda-forge
     googleapis-common-protos  1.56.4           py38h578d9bd_1    conda-forge
     graphite2                 1.3.13            h58526e2_1001    conda-forge
     graphql-core              3.2.3              pyhd8ed1ab_0    conda-forge
     graphviz                  6.0.1                h5abf519_0    conda-forge
     greenlet                  2.0.1            py38hfa26641_0    conda-forge
     grpc-cpp                  1.45.2               h9d3bbbb_5    conda-forge
     grpcio                    1.46.3           py38ha0cdfde_0    conda-forge
     gtest                     1.10.0               h4bd325d_7    conda-forge
     gtk2                      2.24.33              h90689f9_2    conda-forge
     gts                       0.7.6                h64030ff_2    conda-forge
     gxx                       9.5.0               h1fea6ba_11    conda-forge
     gxx_impl_linux-64         9.5.0               h99780fb_19    conda-forge
     gxx_linux-64              9.5.0               h43f449f_11    conda-forge
     h5py                      3.7.0           nompi_py38h7927eab_102    conda-forge
     harfbuzz                  5.3.0                h418a68e_0    conda-forge
     hdbscan                   0.8.29           py38h26c90d9_1    conda-forge
     hdf4                      4.2.15               h9772cbc_5    conda-forge
     hdf5                      1.12.2          nompi_h2386368_100    conda-forge
     heapdict                  1.0.1                      py_0    conda-forge
     holoviews                 1.14.6             pyhd8ed1ab_0    conda-forge
     html5lib                  1.1                pyh9f0ad1d_0    conda-forge
     httpretty                 1.1.4              pyhd8ed1ab_0    conda-forge
     huggingface_hub           0.10.1             pyhd8ed1ab_0    conda-forge
     hypothesis                6.56.4             pyha770c72_0    conda-forge
     icu                       70.1                 h27087fc_0    conda-forge
     identify                  2.5.8              pyhd8ed1ab_0    conda-forge
     idna                      3.4                pyhd8ed1ab_0    conda-forge
     imagecodecs               2022.9.26        py38h839e5d1_3    conda-forge
     imageio                   2.22.0             pyhfa7a67d_0    conda-forge
     imagesize                 1.4.1              pyhd8ed1ab_0    conda-forge
     importlib-metadata        5.0.0              pyha770c72_1    conda-forge
     importlib_metadata        5.0.0                hd8ed1ab_1    conda-forge
     importlib_resources       3.3.1              pyhd8ed1ab_1    conda-forge
     iniconfig                 1.1.1              pyh9f0ad1d_0    conda-forge
     ipykernel                 6.17.0             pyh210e3f2_0    conda-forge
     ipython                   7.31.1           py38h578d9bd_0    conda-forge
     ipython_genutils          0.2.0                      py_1    conda-forge
     ipywidgets                8.0.2              pyhd8ed1ab_1    conda-forge
     isort                     5.10.1             pyhd8ed1ab_0    conda-forge
     itsdangerous              2.1.2              pyhd8ed1ab_0    conda-forge
     jaraco.classes            3.2.3              pyhd8ed1ab_0    conda-forge
     jedi                      0.18.1             pyhd8ed1ab_2    conda-forge
     jeepney                   0.8.0              pyhd8ed1ab_0    conda-forge
     jinja2                    3.1.2              pyhd8ed1ab_1    conda-forge
     jmespath                  1.0.1              pyhd8ed1ab_0    conda-forge
     joblib                    1.2.0              pyhd8ed1ab_0    conda-forge
     jpeg                      9e                   h166bdaf_2    conda-forge
     jschema-to-python         1.2.3              pyhd8ed1ab_0    conda-forge
     json-c                    0.16                 hc379101_0    conda-forge
     json5                     0.9.5              pyh9f0ad1d_0    conda-forge
     jsondiff                  2.0.0              pyhd8ed1ab_0    conda-forge
     jsonpatch                 1.32               pyhd8ed1ab_0    conda-forge
     jsonpickle                2.2.0              pyhd8ed1ab_0    conda-forge
     jsonpointer               2.0                        py_0    conda-forge
     jsonschema                3.2.0              pyhd8ed1ab_3    conda-forge
     junit-xml                 1.9                pyh9f0ad1d_0    conda-forge
     jupyter-cache             0.5.0              pyhd8ed1ab_0    conda-forge
     jupyter-packaging         0.7.12             pyhd8ed1ab_0    conda-forge
     jupyter-server-proxy      3.2.2              pyhd8ed1ab_0    conda-forge
     jupyter_client            7.3.4              pyhd8ed1ab_0    conda-forge
     jupyter_core              4.11.2           py38h578d9bd_0    conda-forge
     jupyter_server            1.23.0             pyhd8ed1ab_0    conda-forge
     jupyter_sphinx            0.4.0            py38h578d9bd_1    conda-forge
     jupyterlab                3.5.0              pyhd8ed1ab_0    conda-forge
     jupyterlab-favorites      3.0.0              pyhd8ed1ab_0    conda-forge
     jupyterlab_pygments       0.2.2              pyhd8ed1ab_0    conda-forge
     jupyterlab_server         2.16.2             pyhd8ed1ab_0    conda-forge
     jupyterlab_widgets        3.0.3              pyhd8ed1ab_0    conda-forge
     jxrlib                    1.1                  h7f98852_2    conda-forge
     kealib                    1.4.15               ha7026e8_1    conda-forge
     kernel-headers_linux-64   3.10.0              h4a8ded7_13    conda-forge
     keyring                   23.11.0          py38h578d9bd_0    conda-forge
     keyutils                  1.6.1                h166bdaf_0    conda-forge
     kiwisolver                1.4.4            py38h43d8883_1    conda-forge
     krb5                      1.19.3               h3790be6_0    conda-forge
     lapack                    3.9.0                    netlib    conda-forge
     lazy-object-proxy         1.8.0            py38h0a891b7_0    conda-forge
     lcms2                     2.14                 h6ed2654_0    conda-forge
     ld_impl_linux-64          2.39                 hc81fddc_0    conda-forge
     lerc                      4.0.0                h27087fc_0    conda-forge
     libaec                    1.0.6                h9c3ff4c_0    conda-forge
     libarchive                3.6.1                hc6fc967_0    conda-forge
     libavif                   0.11.1               h5cdd6b5_0    conda-forge
     libblas                   3.9.0           16_linux64_openblas    conda-forge
     libbrotlicommon           1.0.9                h166bdaf_8    conda-forge
     libbrotlidec              1.0.9                h166bdaf_8    conda-forge
     libbrotlienc              1.0.9                h166bdaf_8    conda-forge
     libcblas                  3.9.0           16_linux64_openblas    conda-forge
     libclang-cpp11.1          11.1.0          default_ha53f305_1    conda-forge
     libcrc32c                 1.1.2                h9c3ff4c_0    conda-forge
     libcudf                   23.02.00a221130 cuda11_g5f83a84916_111    rapidsai-nightly
     libcumlprims              23.02.00a221129 cuda11_gad3dcef_4    rapidsai-nightly
     libcurl                   7.86.0               h7bff187_1    conda-forge
     libcusolver               11.4.1.48                     0    nvidia
     libcusparse               11.7.5.86                     0    nvidia
     libcypher-parser          0.6.2                         1    rapidsai
     libdap4                   3.20.6               hd7c4107_2    conda-forge
     libdeflate                1.14                 h166bdaf_0    conda-forge
     libedit                   3.1.20191231         he28a2e2_2    conda-forge
     libev                     4.33                 h516909a_1    conda-forge
     libevent                  2.1.10               h9b69904_4    conda-forge
     libfaiss                  1.7.0           cuda112h5bea7ad_8_cuda    conda-forge
     libffi                    3.4.2                h7f98852_5    conda-forge
     libgcc-devel_linux-64     9.5.0               h0a57e50_19    conda-forge
     libgcc-ng                 12.2.0              h65d4601_19    conda-forge
     libgcrypt                 1.10.1               h166bdaf_0    conda-forge
     libgd                     2.3.3                h18fbbfe_3    conda-forge
     libgdal                   3.5.2                hc23bfc3_3    conda-forge
     libgfortran-ng            12.2.0              h69a702a_19    conda-forge
     libgfortran5              12.2.0              h337968e_19    conda-forge
     libglib                   2.74.1               h606061b_1    conda-forge
     libgomp                   12.2.0              h65d4601_19    conda-forge
     libgoogle-cloud           1.40.2               habd0e3a_0    conda-forge
     libgpg-error              1.45                 hc0c96e0_0    conda-forge
     libgsasl                  1.10.0               h5b4c23d_0    conda-forge
     libhiredis                1.0.2                h2cc385e_0    conda-forge
     libiconv                  1.17                 h166bdaf_0    conda-forge
     libkml                    1.3.0             h238a007_1014    conda-forge
     liblapack                 3.9.0           16_linux64_openblas    conda-forge
     liblief                   0.12.3               h27087fc_0    conda-forge
     libllvm11                 11.1.0               he0ac6c6_5    conda-forge
     libnetcdf                 4.8.1           nompi_h261ec11_106    conda-forge
     libnghttp2                1.47.0               hdcd2b5c_1    conda-forge
     libnsl                    2.0.0                h7f98852_0    conda-forge
     libntlm                   1.4               h7f98852_1002    conda-forge
     libopenblas               0.3.21          pthreads_h78a6416_3    conda-forge
     libpng                    1.6.38               h753d276_0    conda-forge
     libpq                     14.5                 hd77ab85_1    conda-forge
     libprotobuf               3.20.1               h6239696_4    conda-forge
     libraft-distance          23.02.00a221130 cuda11_g0d76264_35    rapidsai-nightly
     libraft-headers           23.02.00a221130 cuda11_g0d76264_35    rapidsai-nightly
     libraft-nn                23.02.00a221130 cuda11_g0d76264_35    rapidsai-nightly
     librdkafka                1.7.0                hc49e61c_1    conda-forge
     librmm                    23.02.00a221117 cuda11_gd132e523_8    rapidsai-nightly
     librsvg                   2.54.4               h7abd40a_0    conda-forge
     librttopo                 1.1.0               hf730bdb_11    conda-forge
     libsanitizer              9.5.0               h2f262e1_19    conda-forge
     libsodium                 1.0.18               h36c2ea0_1    conda-forge
     libspatialindex           1.9.3                h9c3ff4c_4    conda-forge
     libspatialite             5.0.1               hd36657c_19    conda-forge
     libsqlite                 3.39.4               h753d276_0    conda-forge
     libssh2                   1.10.0               haa6b8db_3    conda-forge
     libstdcxx-devel_linux-64  9.5.0               h0a57e50_19    conda-forge
     libstdcxx-ng              12.2.0              h46fd767_19    conda-forge
     libthrift                 0.16.0               h491838f_2    conda-forge
     libtiff                   4.4.0                h55922b4_4    conda-forge
     libtool                   2.4.6             h9c3ff4c_1008    conda-forge
     libutf8proc               2.8.0                h166bdaf_0    conda-forge
     libuuid                   2.32.1            h7f98852_1000    conda-forge
     libuv                     1.44.2               h166bdaf_0    conda-forge
     libwebp                   1.2.4                h522a892_0    conda-forge
     libwebp-base              1.2.4                h166bdaf_0    conda-forge
     libxcb                    1.13              h7f98852_1004    conda-forge
     libxml2                   2.10.3               h7463322_0    conda-forge
     libzip                    1.9.2                hc869a4a_1    conda-forge
     libzlib                   1.2.13               h166bdaf_4    conda-forge
     libzopfli                 1.0.3                h9c3ff4c_0    conda-forge
     lightgbm                  3.3.3            py38hfa26641_1    conda-forge
     livereload                2.6.3              pyh9f0ad1d_0    conda-forge
     llvmlite                  0.39.1           py38h38d86a4_1    conda-forge
     locket                    1.0.0              pyhd8ed1ab_0    conda-forge
     lz4                       4.0.2            py38h1bf946c_0    conda-forge
     lz4-c                     1.9.3                h9c3ff4c_1    conda-forge
     lzo                       2.10              h516909a_1000    conda-forge
     m4                        1.4.18            h516909a_1001    conda-forge
     make                      4.3                  hd18ef5c_1    conda-forge
     makefun                   1.15.0             pyhd8ed1ab_0    conda-forge
     mapclassify               2.4.3              pyhd8ed1ab_0    conda-forge
     markdown                  3.4.1              pyhd8ed1ab_0    conda-forge
     markdown-it-py            2.1.0              pyhd8ed1ab_0    conda-forge
     markupsafe                2.1.1            py38h0a891b7_2    conda-forge
     matplotlib-base           3.6.2            py38hb021067_0    conda-forge
     matplotlib-inline         0.1.6              pyhd8ed1ab_0    conda-forge
     mccabe                    0.6.1                      py_1    conda-forge
     mdit-py-plugins           0.3.1              pyhd8ed1ab_0    conda-forge
     mdurl                     0.1.0              pyhd8ed1ab_0    conda-forge
     mimesis                   6.1.1              pyhd8ed1ab_0    conda-forge
     mistune                   2.0.4              pyhd8ed1ab_0    conda-forge
     mock                      4.0.3              pyhd8ed1ab_4    conda-forge
     more-itertools            9.0.0              pyhd8ed1ab_0    conda-forge
     moto                      4.0.9              pyhd8ed1ab_0    conda-forge
     mpc                       1.2.1                h9f54685_0    conda-forge
     mpfr                      4.1.0                h9202a9a_1    conda-forge
     msgpack-python            1.0.4            py38h43d8883_1    conda-forge
     multidict                 6.0.2            py38h0a891b7_2    conda-forge
     multipledispatch          0.6.0                      py_0    conda-forge
     munch                     2.5.0                      py_0    conda-forge
     munkres                   1.1.4              pyh9f0ad1d_0    conda-forge
     mypy                      0.971            py38h0a891b7_0    conda-forge
     mypy_extensions           0.4.3            py38h578d9bd_6    conda-forge
     myst-nb                   0.17.1             pyhd8ed1ab_0    conda-forge
     myst-parser               0.18.1             pyhd8ed1ab_0    conda-forge
     nbclassic                 0.4.8              pyhd8ed1ab_0    conda-forge
     nbclient                  0.5.13             pyhd8ed1ab_0    conda-forge
     nbconvert                 7.2.3              pyhd8ed1ab_0    conda-forge
     nbconvert-core            7.2.3              pyhd8ed1ab_0    conda-forge
     nbconvert-pandoc          7.2.3              pyhd8ed1ab_0    conda-forge
     nbformat                  5.7.0              pyhd8ed1ab_0    conda-forge
     nbsphinx                  0.8.9              pyhd8ed1ab_0    conda-forge
     nccl                      2.14.3.1             h0800d71_0    conda-forge
     ncurses                   6.3                  h27087fc_1    conda-forge
     nest-asyncio              1.5.6              pyhd8ed1ab_0    conda-forge
     networkx                  2.6.3              pyhd8ed1ab_1    conda-forge
     ninja                     1.11.0               h924138e_0    conda-forge
     nltk                      3.7                pyhd8ed1ab_0    conda-forge
     nodeenv                   1.7.0              pyhd8ed1ab_0    conda-forge
     nodejs                    18.11.0              h96d913c_0    conda-forge
     notebook                  6.5.2              pyha770c72_1    conda-forge
     notebook-shim             0.2.2              pyhd8ed1ab_0    conda-forge
     nspr                      4.32                 h9c3ff4c_1    conda-forge
     nss                       3.78                 h2350873_0    conda-forge
     numba                     0.56.3           py38h9a4aae9_0    conda-forge
     numpy                     1.23.4           py38h7042d01_1    conda-forge
     numpydoc                  1.5.0              pyhd8ed1ab_0    conda-forge
     nvtx                      0.2.3            py38h0a891b7_2    conda-forge
     oauthlib                  3.2.2              pyhd8ed1ab_0    conda-forge
     openapi-schema-validator  0.2.3              pyhd8ed1ab_0    conda-forge
     openapi-spec-validator    0.4.0              pyhd8ed1ab_1    conda-forge
     openblas                  0.3.21          pthreads_h320a7e8_3    conda-forge
     openjpeg                  2.5.0                h7d73246_1    conda-forge
     openslide                 3.4.1                h71beb9a_5    conda-forge
     openssl                   1.1.1s               h166bdaf_0    conda-forge
     orc                       1.7.5                h6c59b99_0    conda-forge
     packaging                 21.3               pyhd8ed1ab_0    conda-forge
     pandas                    1.5.1            py38h8f669ce_1    conda-forge
     pandoc                    1.19.2                        0    conda-forge
     pandocfilters             1.5.0              pyhd8ed1ab_0    conda-forge
     panel                     0.12.7             pyhd8ed1ab_0    conda-forge
     pango                     1.50.11              h382ae3d_0    conda-forge
     param                     1.12.2             pyh6c4a22f_0    conda-forge
     paramiko                  2.12.0             pyhd8ed1ab_0    conda-forge
     parquet-cpp               1.5.1                         2    conda-forge
     parso                     0.8.3              pyhd8ed1ab_0    conda-forge
     partd                     1.3.0              pyhd8ed1ab_0    conda-forge
     patch                     2.7.6             h7f98852_1002    conda-forge
     patchelf                  0.17.0               h58526e2_0    conda-forge
     pathspec                  0.10.1             pyhd8ed1ab_0    conda-forge
     patsy                     0.5.3              pyhd8ed1ab_0    conda-forge
     pbr                       5.11.0             pyhd8ed1ab_0    conda-forge
     pcre                      8.45                 h9c3ff4c_0    conda-forge
     pcre2                     10.40                hc3806b6_0    conda-forge
     perl                      5.32.1          2_h7f98852_perl5    conda-forge
     pexpect                   4.8.0              pyh1a96a4e_2    conda-forge
     pickleshare               0.7.5                   py_1003    conda-forge
     pillow                    9.2.0            py38h9eb91d8_3    conda-forge
     pip                       22.3.1             pyhd8ed1ab_0    conda-forge
     pixman                    0.40.0               h36c2ea0_0    conda-forge
     pkg-config                0.29.2            h36c2ea0_1008    conda-forge
     pkginfo                   1.8.3              pyhd8ed1ab_0    conda-forge
     platformdirs              2.5.2              pyhd8ed1ab_1    conda-forge
     pluggy                    1.0.0              pyhd8ed1ab_5    conda-forge
     poppler                   22.04.0              h8b295ee_2    conda-forge
     poppler-data              0.4.11               hd8ed1ab_0    conda-forge
     postgresql                14.5                 hdeef612_1    conda-forge
     pre-commit                2.20.0           py38h578d9bd_1    conda-forge
     proj                      9.0.1                h93bde94_1    conda-forge
     prometheus_client         0.15.0             pyhd8ed1ab_0    conda-forge
     prompt-toolkit            3.0.32             pyha770c72_0    conda-forge
     protobuf                  3.20.1           py38hfa26641_0    conda-forge
     psutil                    5.9.4            py38h0a891b7_0    conda-forge
     pthread-stubs             0.4               h36c2ea0_1001    conda-forge
     ptxcompiler               0.7.0            py38h7525318_2    conda-forge
     ptyprocess                0.7.0              pyhd3deb0d_0    conda-forge
     py-cpuinfo                9.0.0              pyhd8ed1ab_0    conda-forge
     py-lief                   0.12.3           py38hfa26641_0    conda-forge
     pyarrow                   9.0.0           py38he3cd574_1_cpu    conda-forge
     pyasn1                    0.4.8                      py_0    conda-forge
     pyasn1-modules            0.2.7                      py_0    conda-forge
     pycodestyle               2.6.0              pyh9f0ad1d_0    conda-forge
     pycosat                   0.6.4            py38h0a891b7_1    conda-forge
     pycparser                 2.21               pyhd8ed1ab_0    conda-forge
     pyct                      0.4.6                      py_0    conda-forge
     pyct-core                 0.4.6                      py_0    conda-forge
     pydata-sphinx-theme       0.11.0             pyhd8ed1ab_1    conda-forge
     pydeck                    0.5.0              pyh9f0ad1d_0    conda-forge
     pydocstyle                6.1.1              pyhd8ed1ab_0    conda-forge
     pyee                      8.1.0              pyhd8ed1ab_0    conda-forge
     pyflakes                  2.2.0              pyh9f0ad1d_0    conda-forge
     pygments                  2.13.0             pyhd8ed1ab_0    conda-forge
     pyjwt                     2.6.0              pyhd8ed1ab_0    conda-forge
     pylibraft                 23.02.00a221130 cuda11_py38_g0d76264_35    rapidsai-nightly
     pylint                    2.15.6             pyhd8ed1ab_0    conda-forge
     pynacl                    1.5.0            py38h0a891b7_2    conda-forge
     pynndescent               0.5.8              pyh1a96a4e_0    conda-forge
     pynvml                    11.4.1             pyhd8ed1ab_0    conda-forge
     pyopenssl                 22.1.0             pyhd8ed1ab_0    conda-forge
     pyorc                     0.7.0            py38hd2b0b8b_0    conda-forge
     pyparsing                 3.0.9              pyhd8ed1ab_0    conda-forge
     pyppeteer                 1.0.2              pyhd8ed1ab_0    conda-forge
     pyproj                    3.4.0            py38he1635e7_0    conda-forge
     pyrsistent                0.19.2           py38h0a891b7_0    conda-forge
     pysocks                   1.7.1              pyha2e5f31_6    conda-forge
     pytest                    7.2.0              pyhd8ed1ab_2    conda-forge
     pytest-asyncio            0.12.0           py38h32f6830_2    conda-forge
     pytest-benchmark          4.0.0              pyhd8ed1ab_0    conda-forge
     pytest-cases              3.6.13             pyhd8ed1ab_0    conda-forge
     pytest-cov                4.0.0              pyhd8ed1ab_0    conda-forge
     pytest-timeout            2.1.0              pyhd8ed1ab_0    conda-forge
     pytest-xdist              3.0.2              pyhd8ed1ab_0    conda-forge
     python                    3.8.13          h582c2e5_0_cpython    conda-forge
     python-confluent-kafka    1.7.0            py38h497a2fe_2    conda-forge
     python-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge
     python-fastjsonschema     2.16.2             pyhd8ed1ab_0    conda-forge
     python-jose               3.3.0              pyh6c4a22f_1    conda-forge
     python-libarchive-c       4.0              py38h578d9bd_2    conda-forge
     python-louvain            0.15               pyhd8ed1ab_1    conda-forge
     python-snappy             0.6.0            py38h1ddbb56_2    conda-forge
     python_abi                3.8                      2_cp38    conda-forge
     pytz                      2022.6             pyhd8ed1ab_0    conda-forge
     pyu2f                     0.1.5              pyhd8ed1ab_0    conda-forge
     pyviz_comms               2.2.1              pyhd8ed1ab_1    conda-forge
     pywavelets                1.3.0            py38h26c90d9_2    conda-forge
     pywin32-on-windows        0.1.0              pyh1179c8e_3    conda-forge
     pyyaml                    6.0              py38h0a891b7_5    conda-forge
     pyzmq                     24.0.1           py38hfc09fa9_1    conda-forge
     raft-dask                 23.02.00a221130 cuda11_py38_g0d76264_35    rapidsai-nightly
     rapidjson                 1.1.0             he1b5a44_1002    conda-forge
     rapids-build-env          23.02.00a221129 cuda11_py38_g284f34a_22    rapidsai-nightly
     rapids-doc-env            23.02.00a221130 py38_g9feda8d_25    rapidsai-nightly
     rapids-notebook-env       23.02.00a221130 cuda11_py38_g9feda8d_25    rapidsai-nightly
     re2                       2022.06.01           h27087fc_0    conda-forge
     readline                  8.1.2                h0f457ee_0    conda-forge
     readme_renderer           37.3               pyhd8ed1ab_0    conda-forge
     recommonmark              0.7.1              pyhd8ed1ab_0    conda-forge
     regex                     2022.10.31       py38h0a891b7_0    conda-forge
     requests                  2.28.1             pyhd8ed1ab_1    conda-forge
     requests-oauthlib         1.3.1              pyhd8ed1ab_0    conda-forge
     requests-toolbelt         0.10.1             pyhd8ed1ab_0    conda-forge
     responses                 0.21.0             pyhd8ed1ab_0    conda-forge
     rfc3986                   2.0.0              pyhd8ed1ab_0    conda-forge
     rhash                     1.4.3                h166bdaf_0    conda-forge
     rich                      12.6.0             pyhd8ed1ab_0    conda-forge
     ripgrep                   13.0.0               h2f28480_2    conda-forge
     rmm                       23.02.00a221117 cuda11_py38_gd132e523_8    rapidsai-nightly
     rsa                       4.9                pyhd8ed1ab_0    conda-forge
     rtree                     1.0.1            py38h02d302b_1    conda-forge
     ruamel_yaml               0.15.80         py38h0a891b7_1008    conda-forge
     s2n                       1.0.10               h9b69904_0    conda-forge
     s3fs                      2022.10.0          pyhd8ed1ab_0    conda-forge
     s3transfer                0.6.0              pyhd8ed1ab_0    conda-forge
     sacremoses                0.0.53             pyhd8ed1ab_0    conda-forge
     sarif-om                  1.0.4              pyhd8ed1ab_0    conda-forge
     scikit-build              0.13.1             pyhca92ed8_0    conda-forge
     scikit-image              0.19.3           py38h8f669ce_2    conda-forge
     scikit-learn              0.24.2           py38hacb3eff_1    conda-forge
     scipy                     1.6.0            py38hb2138dd_0    conda-forge
     seaborn                   0.12.1               hd8ed1ab_0    conda-forge
     seaborn-base              0.12.1             pyhd8ed1ab_0    conda-forge
     secretstorage             3.3.3            py38h578d9bd_1    conda-forge
     send2trash                1.8.0              pyhd8ed1ab_0    conda-forge
     setuptools                60.10.0          py38h578d9bd_0    conda-forge
     shapely                   1.8.5            py38hc9bb657_1    conda-forge
     shellcheck                0.8.0                ha770c72_0    conda-forge
     simpervisor               0.4                pyhd8ed1ab_0    conda-forge
     six                       1.16.0             pyh6c4a22f_0    conda-forge
     snappy                    1.1.9                hbd366e4_2    conda-forge
     sniffio                   1.3.0              pyhd8ed1ab_0    conda-forge
     snowballstemmer           2.2.0              pyhd8ed1ab_0    conda-forge
     sortedcontainers          2.4.0              pyhd8ed1ab_0    conda-forge
     soupsieve                 2.3.2.post1        pyhd8ed1ab_0    conda-forge
     sparse                    0.13.0                   pypi_0    pypi
     spdlog                    1.8.5                h4bd325d_1    conda-forge
     sphinx                    5.3.0              pyhd8ed1ab_0    conda-forge
     sphinx-autobuild          2021.3.14          pyhd8ed1ab_0    conda-forge
     sphinx-click              4.3.0              pyhd8ed1ab_0    conda-forge
     sphinx-copybutton         0.5.0              pyhd8ed1ab_0    conda-forge
     sphinx-markdown-tables    0.0.17             pyh6c4a22f_0    conda-forge
     sphinx_rtd_theme          0.5.2              pyhd8ed1ab_0    conda-forge
     sphinxcontrib-applehelp   1.0.2                      py_0    conda-forge
     sphinxcontrib-devhelp     1.0.2                      py_0    conda-forge
     sphinxcontrib-htmlhelp    2.0.0              pyhd8ed1ab_0    conda-forge
     sphinxcontrib-jsmath      1.0.1                      py_0    conda-forge
     sphinxcontrib-qthelp      1.0.3                      py_0    conda-forge
     sphinxcontrib-serializinghtml 1.1.5              pyhd8ed1ab_2    conda-forge
     sphinxcontrib-websupport  1.2.4              pyhd8ed1ab_1    conda-forge
     sqlalchemy                1.4.43           py38h0a891b7_0    conda-forge
     sqlite                    3.39.4               h4ff8645_0    conda-forge
     sshpubkeys                3.3.1              pyhd8ed1ab_0    conda-forge
     statsmodels               0.13.5           py38h26c90d9_2    conda-forge
     streamz                   0.6.4              pyh6c4a22f_0    conda-forge
     sysroot_linux-64          2.17                h4a8ded7_13    conda-forge
     tabulate                  0.9.0              pyhd8ed1ab_1    conda-forge
     tbb                       2021.6.0             h924138e_1    conda-forge
     tblib                     1.7.0              pyhd8ed1ab_0    conda-forge
     terminado                 0.17.0             pyh41d4057_0    conda-forge
     threadpoolctl             3.1.0              pyh8a188c0_0    conda-forge
     tifffile                  2022.10.10         pyhd8ed1ab_0    conda-forge
     tiledb                    2.11.3               h1e4a385_1    conda-forge
     tinycss2                  1.2.1              pyhd8ed1ab_0    conda-forge
     tk                        8.6.12               h27826a3_0    conda-forge
     tokenizers                0.10.3           py38hb63a372_1    conda-forge
     toml                      0.10.2             pyhd8ed1ab_0    conda-forge
     tomli                     2.0.1              pyhd8ed1ab_0    conda-forge
     tomlkit                   0.11.6             pyha770c72_0    conda-forge
     toolz                     0.12.0             pyhd8ed1ab_0    conda-forge
     tornado                   6.1              py38h0a891b7_3    conda-forge
     tqdm                      4.64.1             pyhd8ed1ab_0    conda-forge
     traitlets                 5.5.0              pyhd8ed1ab_0    conda-forge
     transformers              4.6.1              pyhd8ed1ab_0    conda-forge
     treelite                  3.0.0            py38h8e2129e_1    conda-forge
     treelite-runtime          3.0.0                    pypi_0    pypi
     twine                     4.0.1              pyhd8ed1ab_1    conda-forge
     typed-ast                 1.5.4            py38h0a891b7_1    conda-forge
     types-cachetools          5.2.1              pyhd8ed1ab_0    conda-forge
     typing                    3.10.0.0           pyhd8ed1ab_0    conda-forge
     typing-extensions         4.4.0                hd8ed1ab_0    conda-forge
     typing_extensions         4.4.0              pyha770c72_0    conda-forge
     tzcode                    2022f                h166bdaf_0    conda-forge
     tzdata                    2022f                h191b570_0    conda-forge
     ucx                       1.13.1               h538f049_0    conda-forge
     ucx-proc                  1.0.0                       gpu    rapidsai
     ucx-py                    0.30.00a221115  py38_gb080671_0    rapidsai-nightly
     ukkonen                   1.0.1            py38h43d8883_3    conda-forge
     umap-learn                0.5.3            py38h578d9bd_0    conda-forge
     unicodedata2              15.0.0           py38h0a891b7_0    conda-forge
     urllib3                   1.26.11            pyhd8ed1ab_0    conda-forge
     versioneer                0.28               pyhd8ed1ab_0    conda-forge
     virtualenv                20.16.5          py38h578d9bd_1    conda-forge
     wcwidth                   0.2.5              pyh9f0ad1d_2    conda-forge
     webencodings              0.5.1                      py_1    conda-forge
     websocket-client          1.4.2              pyhd8ed1ab_0    conda-forge
     websockets                10.4             py38h0a891b7_1    conda-forge
     werkzeug                  2.1.2              pyhd8ed1ab_1    conda-forge
     wheel                     0.38.3             pyhd8ed1ab_0    conda-forge
     widgetsnbextension        4.0.3              pyhd8ed1ab_0    conda-forge
     wrapt                     1.14.1           py38h0a891b7_1    conda-forge
     xarray                    2022.11.0          pyhd8ed1ab_0    conda-forge
     xerces-c                  3.2.4                h55805fa_1    conda-forge
     xmltodict                 0.13.0             pyhd8ed1ab_0    conda-forge
     xorg-kbproto              1.0.7             h7f98852_1002    conda-forge
     xorg-libice               1.0.10               h7f98852_0    conda-forge
     xorg-libsm                1.2.3             hd9c2040_1000    conda-forge
     xorg-libx11               1.7.2                h7f98852_0    conda-forge
     xorg-libxau               1.0.9                h7f98852_0    conda-forge
     xorg-libxdmcp             1.1.3                h7f98852_0    conda-forge
     xorg-libxext              1.3.4                h7f98852_1    conda-forge
     xorg-libxrender           0.9.10            h7f98852_1003    conda-forge
     xorg-renderproto          0.11.1            h7f98852_1002    conda-forge
     xorg-xextproto            7.3.0             h7f98852_1002    conda-forge
     xorg-xproto               7.0.31            h7f98852_1007    conda-forge
     xyzservices               2022.9.0           pyhd8ed1ab_0    conda-forge
     xz                        5.2.6                h166bdaf_0    conda-forge
     yaml                      0.2.5                h7f98852_2    conda-forge
     yarl                      1.8.1            py38h0a891b7_0    conda-forge
     zeromq                    4.3.4                h9c3ff4c_1    conda-forge
     zfp                       1.0.0                h27087fc_3    conda-forge
     zict                      2.2.0              pyhd8ed1ab_0    conda-forge
     zipp                      3.10.0             pyhd8ed1ab_0    conda-forge
     zlib                      1.2.13               h166bdaf_4    conda-forge
     zlib-ng                   2.0.6                h166bdaf_0    conda-forge
     zstd                      1.5.2                h6239696_4    conda-forge
     
</pre></details>",2022-11-30T13:45:53Z,0,0,Simon Adorf,NVIDIA,True
413,[BUG] fillna with dataframe containing list dtypes fails ,"**Describe the bug**

fillna with dataframe containing list dtypes fails . 

**Steps/Code to reproduce bug**
```python3
import cudf
s1 = cudf.Series(['game', 'game', None,None,None])
s2 = cudf.Series([[1], [1], None,None,None])
df = cudf.DataFrame({'_TYPE_':s1, 'h':s2})

c1 = cudf.Series([None,None, 'user', 'user', 'user'])
c2 = cudf.Series([None, None, [2], [2], [2]])
subdf = cudf.DataFrame({'_TYPE_':c1, 'h':c2})
df.fillna(subdf)
```

```python3
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In [23], line 9
      7 c2 = cudf.Series([None, None, [2], [2], [2]])
      8 subdf = cudf.DataFrame({'_TYPE_':c1, 'h':c2})
----> 9 df.fillna(subdf)

File /datasets/vjawa/miniconda3/envs/cugraph_dgl_dev/lib/python3.9/contextlib.py:79, in ContextDecorator.__call__.<locals>.inner(*args, **kwds)
     76 @wraps(func)
     77 def inner(*args, **kwds):
     78     with self._recreate_cm():
---> 79         return func(*args, **kwds)

File /datasets/vjawa/miniconda3/envs/cugraph_dgl_dev/lib/python3.9/site-packages/cudf/core/indexed_frame.py:1842, in IndexedFrame.fillna(self, value, method, axis, inplace, limit)
   1837 @_cudf_nvtx_annotate
   1838 def fillna(
   1839     self, value=None, method=None, axis=None, inplace=False, limit=None
   1840 ):  # noqa: D102
   1841     old_index = self._index
-> 1842     ret = super().fillna(value, method, axis, inplace, limit)
   1843     if inplace:
   1844         self._index = old_index

File /datasets/vjawa/miniconda3/envs/cugraph_dgl_dev/lib/python3.9/contextlib.py:79, in ContextDecorator.__call__.<locals>.inner(*args, **kwds)
     76 @wraps(func)
     77 def inner(*args, **kwds):
     78     with self._recreate_cm():
---> 79         return func(*args, **kwds)

File /datasets/vjawa/miniconda3/envs/cugraph_dgl_dev/lib/python3.9/site-packages/cudf/core/frame.py:905, in Frame.fillna(self, value, method, axis, inplace, limit)
    899 should_fill = (
    900     col_name in value
    901     and col.contains_na_entries
    902     and not libcudf.scalar._is_null_host_scalar(replace_val)
    903 ) or method is not None
    904 if should_fill:
--> 905     filled_data[col_name] = col.fillna(replace_val, method)
    906 else:
    907     filled_data[col_name] = col.copy(deep=True)

File /datasets/vjawa/miniconda3/envs/cugraph_dgl_dev/lib/python3.9/site-packages/cudf/core/column/column.py:609, in ColumnBase.fillna(self, value, method, dtype)
    599 def fillna(
    600     self: T,
    601     value: Any = None,
    602     method: str = None,
    603     dtype: Dtype = None,
    604 ) -> T:
    605     """"""Fill null values with ``value``.
    606 
    607     Returns a copy with null filled.
    608     """"""
--> 609     return libcudf.replace.replace_nulls(
    610         input_col=self, replacement=value, method=method, dtype=dtype
    611     )

File replace.pyx:139, in cudf._lib.replace.replace_nulls()

File /datasets/vjawa/miniconda3/envs/cugraph_dgl_dev/lib/python3.9/site-packages/cudf/core/single_column_frame.py:95, in SingleColumnFrame.__bool__(self)
     94 def __bool__(self):
---> 95     raise TypeError(
     96         f""The truth value of a {type(self)} is ambiguous. Use ""
     97         ""a.empty, a.bool(), a.item(), a.any() or a.all().""
     98     )

TypeError: The truth value of a <class 'cudf.core.series.Series'> is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
```

**Expected behavior**

I expect this to work like it does for non list columns. 

```python3
import cudf
s1 = cudf.Series(['game', 'game', None,None,None])
s2 = cudf.Series([1, 1, None,None,None])
df = cudf.DataFrame({'_TYPE_':s1, 'h':s2})

c1 = cudf.Series([None,None, 'user', 'user', 'user'])
c2 = cudf.Series([None, None, 2, 2, 2])
subdf = cudf.DataFrame({'_TYPE_':c1, 'h':c2})
df.fillna(subdf)
```
```python3
	_TYPE_	h
0	game	1
1	game	1
2	user	2
3	user	2
4	user	2
```

**Environment overview (please complete the following information)**
 - Method of cuDF install: [conda]
cudf                      22.12.00a221128 cuda_11_py39_g0c60819cf3_305    rapidsai-nightly


**Additional context**

Impacts property graph in cugraph where we use it for replacing fillna 

https://github.com/rapidsai/cugraph/blob/cb0d0923616f656ec816f999aa633ecbf3c57267/python/cugraph/cugraph/structure/property_graph.py#L758

https://github.com/rapidsai/cugraph/blob/cb0d0923616f656ec816f999aa633ecbf3c57267/python/cugraph/cugraph/structure/property_graph.py#L1155",2022-11-30T21:44:12Z,0,0,Vibhu Jawa,Nvidia,True
414,"[FEA] For cuDF `bench_merge`, make output size scale with input size","**Is your feature request related to a problem? Please describe.**
The benchmark [bench_merge](https://github.com/rapidsai/cudf/blob/ff3b64325d3ac48fc0e8e0e9e1cf6246dd4aa075/python/cudf/benchmarks/API/bench_dataframe.py#L32) in [bench_dataframe.py](https://github.com/rapidsai/cudf/blob/branch-23.02/python/cudf/benchmarks/API/bench_dataframe.py) yields geometrically-increasing output size when `num_key_cols=2` . As a result, this particular benchmark runs into out-of-memory failures long before any other in the benchmarking suite. 

**Describe the solution you'd like**
I would like to add a data generator with characteristics that make the output row count a roughly constant multiple of the input row count. 

**Describe alternatives you've considered**
One alternative is to disable this benchmark for row counts >1M. However, cuDF join performance is one of its strongest features and we don't want to restrict our benchmarking to smaller tables.

**Additional context**
Here is a code snippet that demonstrates the problem as well as a potential solution.
```
import string
import cupy
import cudf

random_state = cupy.random.RandomState(42)
column_generators = {
    ""int"": (lambda nr: random_state.randint(low=0, high=100, size=nr)),
    ""inthc"": (lambda nr: random_state.randint(low=0, high=nr ** 0.5, size=nr)),
}

for gen in ['int', 'inthc']:       
    print(f'using column generator key {gen}')
    for nr in [100, 10_000, 100_000, 1_000_000]:               
        df = cudf.DataFrame({f""{string.ascii_lowercase[i]}"": column_generators[gen](nr)  for i in range(6)})
        m = df.merge(df, on=['a', 'b'])
        print('for input size {}, output size is {} (the ratio is {:.2f})'.format(nr, len(m), len(m)/nr))
```

```
using column generator key int
for input size 100, output size is 100 (the ratio is 1.00)
for input size 10000, output size is 20234 (the ratio is 2.02)
for input size 100000, output size is 1100538 (the ratio is 11.01)
for input size 1000000, output size is 101002012 (the ratio is 101.00)
using column generator key inthc
for input size 100, output size is 214 (the ratio is 2.14)
for input size 10000, output size is 19868 (the ratio is 1.99)
for input size 100000, output size is 199970 (the ratio is 2.00)
for input size 1000000, output size is 2000782 (the ratio is 2.00)
```
",2022-12-01T05:43:36Z,0,0,Gregory Kimball,,False
415,Poor scaling for larger query/loc with cudf Dataframe[QST],"I am trying to extract the some columns of a cudf dataframe 

```
%%time
cudf_file = ""/work/ska/cudfoutput/lofar30MHz1_t201806301100_SBL153.parquet""
df = cudf.read_parquet(cudf_file, columns =['TIME','ANTENNA1','ANTENNA2','FLAG','DATA'])
df3 = df.loc[df['TIME'].isin(unique_time)]
for t in unique_time:   
    df2 = df3[df3.TIME == t]
    beam_id_0_cp = cp.asarray(df2['ANTENNA1'])
    beam_id_1_cp = cp.asarray(df2['ANTENNA2'])
    dfflag = df2['FLAG']
    data_flag_cp = cp.asarray(dfflag.list.leaves).reshape(len(dfflag),len(dfflag.iloc[0]),len(dfflag.iloc[0][0]))
    dfdata = df2['DATA']
    data_cp = cp.asarray(dfdata.list.leaves, dtype=np.float64).reshape(len(dfdata),len(dfdata.iloc[0]),len(dfdata.iloc[0][0])).view(np.complex128)
   ```

The scaling becomes worse if we have a bigger loop (>1000 steps), for smaller timesteps it is the faster but as we go higher the scaling is becoming an issue. However, I am wondering why as I am not reading the dataframe inside the loop. Is there a better way to do this? I have tried groupby as well and the results are similar",2022-12-02T11:13:41Z,0,0,Arpan Das,The École polytechnique fédérale de Lausanne,False
416,[FEA] Support multiple compressed inputs in `read_json`,"JSON reader currently concatenates data from all input sources before (optional) decompression and parsing.
This is not correct for compressed input files, as each file is compressed separately.
https://github.com/rapidsai/cudf/pull/12285 disables multiple inputs for compressed data.
`read_json` should decompress each input file before concatenating the decompressed data.",2022-12-03T01:23:44Z,0,0,Vukasin Milovanovic,NVIDIA,True
417,[FEA] Support segmented reductions (MIN/MAX/COUNT DISTINCT) in cuDF `list` accessor,"**Is your feature request related to a problem? Please describe.**

Currently, we use cuDF to implement distributed group aggregation operations. That is, GROUP BY + DISTINCT is performed on the local node, and GROUP BY + MIN/MAX/COUNT DISTINCT are performed on the merge node. The LIST is transferred as an intermediate format, but the aggregation operation cannot be implemented.

**Describe the solution you'd like**

Group +  Aggregation supports the list format.

![捕获2](https://user-images.githubusercontent.com/43532055/205540928-0359139e-03ea-4c71-bd8d-0daaf0d5f875.PNG)

**Describe alternatives you've considered**

Aggregation operators are supported in the list format. For example, the current list supports only `len`, and aggregation operators such as `count`, `min`, `max`, and `avg` are expected to be added.

![捕获](https://user-images.githubusercontent.com/43532055/205540951-3d8b4f09-7227-4cb1-836e-49e79c428e31.PNG)
",2022-12-05T03:11:31Z,0,0,Liu,,False
418,[FEA] Implement MultiIndex.set_levels,"**Is your feature request related to a problem? Please describe.**
`pandas.MultiIndex` supports a `set_levels` method that is currently not possible with cudf.MultiIndex.

**Describe the solution you'd like**
We should Implement `cudf.MultiIndex.set_levels`

**Describe alternatives you've considered**
None

**Additional context**
The `levels` parameter to `pandas.MultiIndex.copy` is deprecated, and `set_levels` is the recommended replacement, so we need to have this method in place to support the eventual removal of this deprecated code path. We should update the warning in `MultiIndex.copy` to recommend the usage of `set_levels` when this method is implemented",2022-12-05T18:58:46Z,0,0,Vyas Ramasubramani,@rapidsai,True
419,[FEA] Implement MultiIndex.set_codes,"**Is your feature request related to a problem? Please describe.**
`pandas.MultiIndex` supports a `set_codes` method that is currently not possible with cudf.MultiIndex.

**Describe the solution you'd like**
We should Implement `cudf.MultiIndex.set_codes`

**Describe alternatives you've considered**
None

**Additional context**
The `codes` parameter to `pandas.MultiIndex.copy` is deprecated, and `set_codes` is the recommended replacement, so we need to have this method in place to support the eventual removal of this deprecated code path. We should update the warning in `MultiIndex.copy` to recommend the usage of `set_codes` when this method is implemented",2022-12-05T18:58:51Z,0,0,Vyas Ramasubramani,@rapidsai,True
420,[FEA] `DataFrame.query` should be updated to use `eval` as a backend,"cuDF supports a [query](https://docs.rapids.ai/api/cudf/nightly/api_docs/api/cudf.DataFrame.query.html) function for evaluating expressions against dataframes. Thus far it has worked by using the `ast` _python_ package to build a python function that gets wrapped in a numba kernel and launched not unlike a UDF. Pandas seems to work by leveraging its own [eval function however](https://github.com/pandas-dev/pandas/blob/v1.5.2/pandas/core/frame.py#L4377-L4378), which I don't think we had at the time `query` was developed. We should investigate if we can make `cudf.DataFrame.query` work similarly. This would have a bunch of benefits including most likely faster performance, no need for runtime compilation or caching, I'm guessing code reduction, and anything new that we get in our internal ast framework that backs `eval`. 

cc @vyasr ",2022-12-06T16:42:33Z,0,0,,NVIDIA,True
421,[FEA] AST expression was provided non-matching operand types,"**Is your feature request related to a problem? Please describe.**
rewriting `pandas` code with `import cudf as pd`

**Describe the solution you'd like**
```
>>> import pandas as pd
>>> import cudf
>>> cudf.__version__
'22.10.01+2.gca9a422da9' <- rapidsai/rapidsai-nightly:cuda11.5-runtime-centos7-py3.9 (3b7d5d24867a) on 6 dec 2022
>>> df = cudf.DataFrame({'a': [1,2,3], 'b': [3.0,2.0,1.0]})
>>> df.to_pandas().eval('a + b')
0    4.0
1    4.0
2    4.0
dtype: float64
>>> df.eval('a + b')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/opt/conda/envs/rapids/lib/python3.9/contextlib.py"", line 79, in inner
    return func(*args, **kwds)
  File ""/opt/conda/envs/rapids/lib/python3.9/site-packages/cudf/core/dataframe.py"", line 6766, in eval
    None: libcudf.transform.compute_column(
  File ""transform.pyx"", line 190, in cudf._lib.transform.compute_column
RuntimeError: cuDF failure at: /workspace/.conda-bld/work/cpp/src/ast/expression_parser.cpp:149: An AST expression was provided non-matching operand types.
```

same behavior as `pandas`
",2022-12-06T18:09:14Z,0,0,Matthew Farrellee,,False
422,[FEA] Support inferring column names when using the `byte_range=` parameter in `read_csv`  ,"I cannot use the `byte_range` parameter with `read_csv` if the column names need to be inferred.

```python
>>> cudf.read_csv(StringIO(""1,2,3\n4,5,6""), dtype=int, byte_range=(3, 6))
RuntimeError: cuDF failure at: cudf/cpp/src/io/csv/reader_impl.cu:438: byte_range offset with header not supported
```

But this works:

```python
>>> cudf.read_csv(StringIO(""1,2,3\n4,5,6""), dtype=int, byte_range=(3, 6), names=['x', 'y', 'z'])
   x  y  z
0  4  5  6
```

And this works:

```python
>>> cudf.read_csv(StringIO(""1,2,3\n4,5,6""), dtype=int, byte_range=(3, 6), header=None)
   0  1  2
0  4  5  6
```
",2022-12-07T12:53:06Z,0,0,Ashwin Srinath,Voltron Data,False
423,[FEA] Support axis=None in reductions,"**Is your feature request related to a problem? Please describe.**
`DataFrame.(sum|mean|etc, axis=None)` should be a reduction over the entire `DataFrame` rather than imply `axis=0`.

See https://github.com/pandas-dev/pandas/issues/21597 for a full description. 

**Describe the solution you'd like**
This change would be a breaking change not just for us (which is less of an issue because of our short deprecation cycle) but for pandas. As a result, we cannot implement this until pandas 2.0 is imminent. When that time comes, we should also plan to support this.
",2022-12-07T18:59:28Z,0,0,Vyas Ramasubramani,@rapidsai,True
424,[BUG][JNI] Research reasons for the JNI pinned pool becoming very slow,"I have seen in jobs that have a lot of host memory usage, cases where executors looked to be hanging while trying to satisfy an allocation from the pinned pool in the java side of things:

```
Thread 25389: (state = IN_JAVA)
 - java.util.TreeMap.successor(java.util.TreeMap$Entry) @bci=30, line=2154 (Compiled frame; information may be imprecise)
 - java.util.TreeMap$KeySpliterator.tryAdvance(java.util.function.Consumer) @bci=45, line=2769 (Compiled frame)
 - java.util.stream.ReferencePipeline.forEachWithCancel(java.util.Spliterator, java.util.stream.Sink) @bci=11, line=126 (Compiled frame)
 - java.util.stream.AbstractPipeline.copyIntoWithCancel(java.util.stream.Sink, java.util.Spliterator) @bci=32, line=499 (Compiled frame)
 - java.util.stream.AbstractPipeline.copyInto(java.util.stream.Sink, java.util.Spliterator) @bci=49, line=486 (Compiled frame)
 - java.util.stream.AbstractPipeline.wrapAndCopyInto(java.util.stream.Sink, java.util.Spliterator) @bci=13, line=472 (Compiled frame)
 - java.util.stream.FindOps$FindOp.evaluateSequential(java.util.stream.PipelineHelper, java.util.Spliterator) @bci=14, line=152 (Compiled frame)
 - java.util.stream.AbstractPipeline.evaluate(java.util.stream.TerminalOp) @bci=88, line=234 (Compiled frame)
 - java.util.stream.ReferencePipeline.findFirst() @bci=5, line=531 (Compiled frame)
 - ai.rapids.cudf.PinnedMemoryPool.tryAllocateInternal(long) @bci=60, line=283 (Compiled frame)
 - ai.rapids.cudf.PinnedMemoryPool.tryAllocate(long) @bci=12, line=224 (Compiled frame)
```

```
Thread 22565: (state = IN_JAVA)
 - java.util.stream.ReferencePipeline.forEachWithCancel(java.util.Spliterator, java.util.stream.Sink) @bci=16, line=126 (Compiled frame; information may be imprecise)
 - java.util.stream.AbstractPipeline.copyIntoWithCancel(java.util.stream.Sink, java.util.Spliterator) @bci=32, line=499 (Compiled frame)
 - java.util.stream.AbstractPipeline.copyInto(java.util.stream.Sink, java.util.Spliterator) @bci=49, line=486 (Compiled frame)
 - java.util.stream.AbstractPipeline.wrapAndCopyInto(java.util.stream.Sink, java.util.Spliterator) @bci=13, line=472 (Compiled frame)
 - java.util.stream.FindOps$FindOp.evaluateSequential(java.util.stream.PipelineHelper, java.util.Spliterator) @bci=14, line=152 (Compiled frame)
 - java.util.stream.AbstractPipeline.evaluate(java.util.stream.TerminalOp) @bci=88, line=234 (Compiled frame)
 - java.util.stream.ReferencePipeline.findFirst() @bci=5, line=531 (Compiled frame)
 - ai.rapids.cudf.PinnedMemoryPool.tryAllocateInternal(long) @bci=60, line=283 (Compiled frame)
 - ai.rapids.cudf.PinnedMemoryPool.tryAllocate(long) @bci=12, line=224 (Compiled frame)

```

I don't have a repro case yet and that's part of this task. I have seen this happen after the spark-plugin has spilled a lot of memory to the host, and it seems we should have exhausted this pool. Theories could go from this pool structure being somehow corrupted, or there being an insane number of blocks in this pool that it is just taking a lot of time for it to find free blocks.",2022-12-08T15:36:30Z,0,0,Alessandro Bellina,NVIDIA,True
425,[BUG] Random sampling with cupy does not support these inputs,"Weighted sampling with cudf series is documented as supported but throws the following error

Repro:
```
a = cudf.Series(range(100))
a.sample(n=10, weights=a.values_host, random_state=0)

or

a = cudf.Series(range(100))
a.sample(n=10, weights=a.values_host, random_state=0)

```

Exception:
```
---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
File /conda/envs/rapids-22.12/lib/python3.9/site-packages/cudf/core/indexed_frame.py:2952, in IndexedFrame._sample_axis_0(self, n, weights, replace, random_state, ignore_index)
   2951 try:
-> 2952     gather_map_array = random_state.choice(
   2953         len(self), size=n, replace=replace, p=weights
   2954     )
   2955 except NotImplementedError as e:

File /conda/envs/rapids-22.12/lib/python3.9/site-packages/cupy/random/_generator.py:1066, in RandomState.choice(self, a, size, replace, p)
   1065 if not replace:
-> 1066     raise NotImplementedError
   1068 if p is not None:

NotImplementedError: 

The above exception was the direct cause of the following exception:

NotImplementedError                       Traceback (most recent call last)
Cell In [25], line 2
      1 a = cudf.Series(range(100))
----> 2 a.sample(n=10, weights=a, random_state=0)

File /conda/envs/rapids-22.12/lib/python3.9/contextlib.py:79, in ContextDecorator.__call__.<locals>.inner(*args, **kwds)
     76 @wraps(func)
     77 def inner(*args, **kwds):
     78     with self._recreate_cm():
---> 79         return func(*args, **kwds)

File /conda/envs/rapids-22.12/lib/python3.9/site-packages/cudf/core/indexed_frame.py:2930, in IndexedFrame.sample(self, n, frac, replace, weights, random_state, axis, ignore_index)
   2927     weights = weights / weights.sum()
   2929 if axis == 0:
-> 2930     return self._sample_axis_0(
   2931         n, weights, replace, random_state, ignore_index
   2932     )
   2933 else:
   2934     if isinstance(random_state, cp.random.RandomState):

File /conda/envs/rapids-22.12/lib/python3.9/site-packages/cudf/core/indexed_frame.py:2956, in IndexedFrame._sample_axis_0(self, n, weights, replace, random_state, ignore_index)
   2952     gather_map_array = random_state.choice(
   2953         len(self), size=n, replace=replace, p=weights
   2954     )
   2955 except NotImplementedError as e:
-> 2956     raise NotImplementedError(
   2957         ""Random sampling with cupy does not support these inputs.""
   2958     ) from e
   2960 return self._gather(
   2961     cudf.core.column.as_column(gather_map_array),
   2962     keep_index=not ignore_index,
   2963     check_bounds=False,
   2964 )

NotImplementedError: Random sampling with cupy does not support these inputs.
```",2022-12-08T17:08:17Z,0,0,Lahir Marni,,False
426,[BUG] Tests of ufuncs should ensure values are in the right domain,"**Describe the bug**
The `test_array_ufunc.py` module contains tests that I introduced in #10346 and #10217 that are intended to be generic for all ufuncs in `test_ufunc_(index|series|dataframe)`. However, the inputs are generated in exactly the same way for all ufuncs. This leads to some tests where all inputs are invalid, making some of the tests do nothing other than ensure that invalid inputs produce the same output. Some of the most egregious examples are inverse trigonometric functions arcsin and arccos, for which the domains are limited to (-1, 1) but all inputs are > 1.

**Steps/Code to reproduce bug**
Run the tests and check warnings.

**Expected behavior**
The tests should be rewritten to generate inputs within the domain of the functions of choice, with perhaps just a few values outside the domain so that we can continue to validate consistent behavior in both cases. In an ideal world we would also update cudf to throw the same warnings as pandas here, but that may be more work than it's worth since we would almost certainly have to introspect the data.",2022-12-09T18:21:18Z,0,0,Vyas Ramasubramani,@rapidsai,True
427,[FEA] support type float16,"**Is your feature request related to a problem? Please describe.**
rewriting code from pandas with `import cudf as pd` and managing memory usage

**Describe the solution you'd like**
```
df['src'] = df['src'].astype('float16')

cudf/core/dtypes.py:51, in dtype(arbitrary)
...
TypeError: Unsupported type float16
```",2022-12-12T16:29:14Z,0,0,Matthew Farrellee,,False
428,[FEA] Invalidate io::detail::column_buffer objects when they are converted to columns,"**Is your feature request related to a problem? Please describe.**
The `io::detail::make_column` method converts a `column_buffer` object into a `cudf::column`. The conversion moves data out from underneath the buffer, invalidating the buffer afterward. As a result, the object should not be used afterwards. At present, however, that information is something that the user has to know a priori. One such example is [this finalize_output method in the parquet reader](https://github.com/rapidsai/cudf/blob/branch-23.02/cpp/src/io/parquet/reader_impl.cpp#L300) that implicitly handles this logic by only interacting with buffers that would not have been previously modified. It would be better if that information were communicated to the developer more explicitly.

**Describe the solution you'd like**
After #12364 `make_column` can be made into a class method. This method should be modified to only be invokable on rvalue-references (see the ""member functions with ref-qualifier"" section of [the cppreference docs for member functions] on how to do this). That would force calling code to do something like `std::move(column_buffer).make_column`, making it much more obvious that the buffer is no longer usable afterwards.

**Describe alternatives you've considered**
None

**Additional context**
Making this change will also force careful reconsideration of all code currently using column buffers to ensure that usage patterns are safe. Some patterns may be valid at present (for instance, it looks like some code is currently checking the size of a buffer after its buffers have been moved because the size attribute is still accurate) but are antipatterns that should be refactored.
",2022-12-13T02:19:31Z,0,0,Vyas Ramasubramani,@rapidsai,True
429,Turn on `xfail_strict=true` in all subpackages,"Followup to #12244.

Can we do the same for other packages in the repo? I would expect that dask-cudf / custreamz / cudf-kafka / strings_udf won't have as many problems as cudf.

_Originally posted by @bdice in https://github.com/rapidsai/cudf/pull/12244#discussion_r1048803383_
      ",2022-12-15T11:11:31Z,0,0,Lawrence Mitchell,,False
430,Split out exceptional cases in heavily parametrized array ufunc tests,"Followup to #12244; A number of the parametrized array ufunc tests need to xfail (or skip) a large subsection of their parameters since the behaviour varies.

In many cases, this is not really a bug that we're intending to fix, and so the ""xfail""s pollute test output and hide what is really problematic from what is not.

_Originally posted by @vyasr in https://github.com/rapidsai/cudf/pull/12244#discussion_r1049220334_",2022-12-15T11:21:25Z,0,0,Lawrence Mitchell,,False
431,[BUG] Overflow potentially corrupting hashes in hash_vocab implementation,"**Describe the bug**
The hash vocab test in cudf currently warns about an overflow occurring. This can be easily observed by running the pytest with warnings set to raise errors.

**Steps/Code to reproduce bug**
Execute `pytest -W error python/cudf/cudf/tests/test_hash_vocab.py::test_correct_bert_base_vocab_hash` from the root of the repository.

The output should include a traceback like this:
```
test_correct_bert_base_vocab_hash ____________________________________________________________________________________

datadir = '/home/vyasr/local/rapids/cudf/python/cudf/cudf/tests/data/subword_tokenizer_data/bert_base_cased_sampled', tmpdir = local('/tmp/pytest-of-rapids/pytest-2/test_correct_bert_base_vocab_h0')

    def test_correct_bert_base_vocab_hash(datadir, tmpdir):
        # The vocabulary is drawn from bert-base-cased
        vocab_path = os.path.join(datadir, ""vocab.txt"")
    
        groundtruth_path = os.path.join(datadir, ""vocab-hash.txt"")
        output_path = tmpdir.join(""cudf-vocab-hash.txt"")
>       hash_vocab(vocab_path, output_path)

python/cudf/cudf/tests/test_hash_vocab.py:23: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
python/cudf/cudf/utils/hash_vocab_utils.py:269: in hash_vocab
    ) = _perfect_hash(keys, 10)
python/cudf/cudf/utils/hash_vocab_utils.py:129: in _perfect_hash
    internal_table, coeff_a, coeff_b = _find_hash_for_internal(b)
python/cudf/cudf/utils/hash_vocab_utils.py:102: in _find_hash_for_internal
    bins = _make_bins(hash_bin, new_length, a, b)
python/cudf/cudf/utils/hash_vocab_utils.py:60: in _make_bins
    bins[_hash_func(item, a, b, num_bins)].append(item)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

k = 233297689050786, a = 21608458564245, b = 116, size = 6

    def _hash_func(k, a, b, size):
        k = np.uint64(k)
        a = np.uint64(a)
        b = np.uint64(b)
        size = np.uint64(size)
>       return ((a * k + b) % PRIME) % size
E       RuntimeWarning: overflow encountered in ulong_scalars

python/cudf/cudf/utils/hash_vocab_utils.py:49: RuntimeWarning
------------------------------------------------------------------------------------------ Captured stdout call -------------------------------------------------------------------------------------------
Attempting to build table using 1.500000n space
Longest bin was 11
Processing bin 0 / 875 of size = 6
```

**Expected behavior**
We should not have overflows occurring. The reason for the overflow is that all the inputs to `_hash_func` are being converted to `np.uint64` (limited to 64 bits) rather than primitive Python ints (which have unlimited precision). I attempted the naive modification of just removing the conversions to `np.uint64` here (which also requires rewriting some of the call sites to do conversions since they involve indexing into numpy arrays or adding numpy ints to Python ints), but my quick conversion led to the test failing outright. I didn't check my work all that thoroughly so it's possible I made an error, but we should make sure that we understand whether the numpy integer overflow here is some property that we are depending on implicitly, if it is a bug that users could actually hit and we need to fix, or if it's just the expected behavior and the warning can be silenced.
",2022-12-16T17:48:11Z,0,0,Vyas Ramasubramani,@rapidsai,True
432,[FEA] DatetimeProperties day_name matching pandas,"**Is your feature request related to a problem? Please describe.**
converting code with `import cudf as pd`

```
>>> import cudf as pd
>>> pd.__version__
'22.12.01'
>>> df = pd.Series(pd.date_range(""1984"", freq='s', periods=6))
>>> df.to_pandas().dt.day_name()
0    Sunday
1    Sunday
2    Sunday
3    Sunday
4    Sunday
5    Sunday
dtype: object
>>> df.dt.day_name()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: 'DatetimeProperties' object has no attribute 'day_name'
>>> type(df.dt)
<class 'cudf.core.series.DatetimeProperties'>
```

**Describe the solution you'd like**
implementation of `day_name()` matching https://pandas.pydata.org/docs/reference/api/pandas.Series.dt.day_name.html

",2022-12-17T15:01:49Z,0,0,Matthew Farrellee,,False
433,[BUG] cannot pass numpy funcs to groupby().agg(),"**Describe the bug**
converting code with `import cudf as pd`

**Steps/Code to reproduce bug**
```
>>> import cudf as pd
>>> pd.__version__
'22.12.01'
>>> df = pd.DataFrame({'a': [1,2,3]*10})

>>> df.to_pandas().groupby('a').agg({'a': np.sum})
    a
a    
1  10
2  20
3  30

>>> df.groupby('a').agg({'a': np.sum})
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File "".../python3.8/contextlib.py"", line 75, in inner
    return func(*args, **kwds)
  File "".../python3.8/site-packages/cudf/core/groupby/groupby.py"", line 460, in agg
    ) = self._groupby.aggregate(columns, normalized_aggs)
  File ""groupby.pyx"", line 309, in cudf._lib.groupby.GroupBy.aggregate
  File ""groupby.pyx"", line 184, in cudf._lib.groupby.GroupBy.aggregate_internal
  File ""aggregation.pyx"", line 866, in cudf._lib.aggregation.make_groupby_aggregation
  File ""<__array_function__ internals>"", line 180, in sum
  File "".../python3.8/site-packages/numpy/core/fromnumeric.py"", line 2298, in sum
    return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,
  File "".../python3.8/site-packages/numpy/core/fromnumeric.py"", line 84, in _wrapreduction
    return reduction(axis=axis, out=out, **passkwargs)
TypeError: sum() got an unexpected keyword argument 'out'
```
",2022-12-17T15:10:18Z,0,0,Matthew Farrellee,,False
434,[BUG] pivot_table does not accept single index / columns,"**Describe the bug**
cannot use `import cudf as pd`

**Steps/Code to reproduce bug**
```
>>> import cudf as pd
>>> pd.__version__
'22.12.01'

>>> df = pd.DataFrame({""A"": [""foo"", ""foo"", ""foo"", ""foo"", ""foo"",
...                          ""bar"", ""bar"", ""bar"", ""bar""],
...                    ""B"": [""one"", ""one"", ""one"", ""two"", ""two"",
...                          ""one"", ""one"", ""two"", ""two""],
...                    ""C"": [""small"", ""large"", ""large"", ""small"",
...                          ""small"", ""large"", ""small"", ""small"",
...                          ""large""],
...                    ""D"": [1, 2, 2, 3, 3, 4, 5, 6, 7],
...                    ""E"": [2, 4, 5, 5, 6, 6, 8, 9, 9]})
>>> df
     A    B      C  D  E
0  foo  one  small  1  2
1  foo  one  large  2  4
2  foo  one  large  2  5
3  foo  two  small  3  5
4  foo  two  small  3  6
5  bar  one  large  4  6
6  bar  one  small  5  8
7  bar  two  small  6  9
8  bar  two  large  7  9


>>> df.to_pandas().pivot_table(values='D', index='A', columns='C', aggfunc='sum')
C    large  small
A                
bar     11     11
foo      4      7

>>> df.pivot_table(values='D', index='A', columns='C', aggfunc='sum')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File "".../python3.8/contextlib.py"", line 75, in inner
    return func(*args, **kwds)
  File "".../python3.8/site-packages/cudf/core/dataframe.py"", line 6749, in pivot_table
    return cudf.core.reshape.pivot_table(
  File "".../python3.8/site-packages/cudf/core/reshape.py"", line 1382, in pivot_table
    for x in keys + values:
TypeError: can only concatenate str (not ""list"") to str
```",2022-12-17T17:20:57Z,0,0,Matthew Farrellee,,False
435,[BUG] pivot_table columns type differs from pandas,"**Describe the bug**
rewriting code with `import cudf as pd`

**Steps/Code to reproduce bug**
```
>>> import cudf as pd
>>> pd.__version__
'22.12.01'
>>> df = pd.DataFrame({""A"": [""foo"", ""foo"", ""foo"", ""foo"", ""foo"",
...                          ""bar"", ""bar"", ""bar"", ""bar""],
...                    ""B"": [""one"", ""one"", ""one"", ""two"", ""two"",
...                          ""one"", ""one"", ""two"", ""two""],
...                    ""C"": [""small"", ""large"", ""large"", ""small"",
...                          ""small"", ""large"", ""small"", ""small"",
...                          ""large""],
...                    ""D"": [1, 2, 2, 3, 3, 4, 5, 6, 7],
...                    ""E"": [2, 4, 5, 5, 6, 6, 8, 9, 9]})
>>> df.pivot_table(index=['A'], columns=['B'], values='D')
B         one  two
A                 
bar  4.500000  6.5
foo  1.666667  3.0
>>> df.to_pandas().pivot_table(index=['A'], columns=['B'], values='D')
B         one  two
A                 
bar  4.500000  6.5
foo  1.666667  3.0
>>> df.to_pandas().pivot_table(index=['A'], columns=['B'], values='D').columns
Index(['one', 'two'], dtype='object', name='B')
>>> df.pivot_table(index=['A'], columns=['B'], values='D').columns
MultiIndex([('one',),
            ('two',)],
           names=['B'])
```

**Expected behavior**
an `Index` when only aggregating across one column",2022-12-17T19:27:20Z,0,0,Matthew Farrellee,,False
436,[FEA] groupby.agg() support for controlling output columns,"**Is your feature request related to a problem? Please describe.**
rewriting code with `import cudf as pd`

**Describe the solution you'd like**
```
>>> import cudf as pd
>>> pd.__version__
'22.12.01'

>>> df = pd.DataFrame({'kind': ['cat', 'dog', 'cat', 'dog'], 'height': [9.1, 6.0, 9.5, 34.0], 'weight': [7.9, 7.5, 9.9, 198.0]})
>>> df
  kind  height  weight
0  cat     9.1     7.9
1  dog     6.0     7.5
2  cat     9.5     9.9
3  dog    34.0   198.0

>>> df.to_pandas().groupby('kind').agg(min_height=('height', 'min'), max_weight=('weight', 'max'))
      min_height  max_weight
kind                        
cat          9.1         9.9
dog          6.0       198.0

>>> df.groupby('kind').agg(min_height=('height', 'min'), max_weight=('weight', 'max'))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File "".../python3.8/contextlib.py"", line 75, in inner
    return func(*args, **kwds)
TypeError: agg() got an unexpected keyword argument 'min_height'
```

references:
- https://github.com/pandas-dev/pandas/pull/26399
- https://pandas.pydata.org/pandas-docs/stable/whatsnew/v0.20.0.html#deprecate-groupby-agg-with-a-dictionary-when-renaming",2022-12-17T22:38:18Z,0,0,Matthew Farrellee,,False
437,[BUG] read_csv() got an unexpected keyword argument 'encoding',"**Is your feature request related to a problem? Please describe.**
rewriting code using `import cudf as pd`

**Describe the solution you'd like**
implementation of `encoding` parameter that matches https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html#pandas-read-csv
",2022-12-17T23:59:13Z,0,0,Matthew Farrellee,,False
438,[BUG] support index functions in cudf.DataFrame.rename,"**Is your feature request related to a problem? Please describe.**
rewriting code with `import cudf as pd`

**Describe the solution you'd like**
```
>>> import cudf as pd
>>> pd.__version__
'22.12.01'
>>> df = pd.DataFrame({'a': range(10)})
>>> df = df.rename(index=str, columns={'a': 'b'})
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File "".../python3.8/contextlib.py"", line 75, in inner
    return func(*args, **kwds)
  File "".../python3.8/site-packages/cudf/core/dataframe.py"", line 3365, in rename
    any(type(item) == str for item in index.values())
AttributeError: type object 'str' has no attribute 'values'
```",2022-12-18T12:24:38Z,0,0,Matthew Farrellee,,False
439,[FEA] support tuple construction in apply with axis=1,"**Is your feature request related to a problem? Please describe.**
rewriting code with `import cudf as pd`

**Describe the solution you'd like**
```
>>> import cudf as pd
>>> df = pd.DataFrame({'a': range(10, 20), 'b': range(110, 120)})
>>> pd.__version__
'22.12.01'

>>> df.apply(lambda row: (row[0], row[1]), axis=1)
...
numba.core.errors.NumbaNotImplementedError: UniTuple(Masked(int64) x 2) cannot be represented as a NumPy dtype
```",2022-12-18T12:39:53Z,0,0,Matthew Farrellee,,False
440,[BUG] Series.clip does not work with numpy/cupy clip,"**Describe the bug**
rewriting code with `import cudf as pd`

**Steps/Code to reproduce bug**
```
>>> import cudf as pd
>>> import cupy as np
>>> pd.__version__
'22.12.01'
>>> np.__version__
'11.4.0'

>>> df = pd.DataFrame({'a': range(10, 30)})

>>> np.clip(df.to_pandas().a, a_min=15, a_max=25)
0     15
1     15
2     15
3     15
4     15
5     15
6     16
7     17
8     18
9     19
10    20
11    21
12    22
13    23
14    24
15    25
16    25
17    25
18    25
19    25
Name: a, dtype: int64

>>> np.clip(df.a, a_min=15, a_max=25)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File "".../python3.8/site-packages/cupy/_math/misc.py"", line 172, in clip
    return a.clip(a_min, a_max, out=out)
  File "".../python3.8/contextlib.py"", line 75, in inner
    return func(*args, **kwds)
TypeError: clip() got an unexpected keyword argument 'out'
```

**Additional context**
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.clip.html
```
*args, **kwargs
    Additional keywords have no effect but might be accepted for compatibility with numpy.
```
",2022-12-18T13:19:47Z,0,0,Matthew Farrellee,,False
441,[BUG] date_range support for computed parameters,"**Is your feature request related to a problem? Please describe.**
rewriting code with `import cudf as pd`

**Describe the solution you'd like**
implementation of `cudf.date_range` that matches https://pandas.pydata.org/docs/reference/api/pandas.date_range.html

specifically the ability to compute missing parameters

```
>>> import cudf as pd
>>> pd.__version__
'22.12.01'
>>> pd.date_range(100, periods=42)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File "".../python3.8/site-packages/cudf/core/tools/datetimes.py"", line 820, in date_range
    raise ValueError(
ValueError: Of the four parameters: start, end, periods, and freq, exactly three must be specified
>>> import pandas
>>> pandas.date_range(100, periods=42)
DatetimeIndex(['1970-01-01 00:00:00.000000100',
               '1970-01-02 00:00:00.000000100',
               '1970-01-03 00:00:00.000000100',
               '1970-01-04 00:00:00.000000100',
               '1970-01-05 00:00:00.000000100',
               '1970-01-06 00:00:00.000000100',
               '1970-01-07 00:00:00.000000100',
               '1970-01-08 00:00:00.000000100',
               '1970-01-09 00:00:00.000000100',
               '1970-01-10 00:00:00.000000100',
               '1970-01-11 00:00:00.000000100',
               '1970-01-12 00:00:00.000000100',
               '1970-01-13 00:00:00.000000100',
               '1970-01-14 00:00:00.000000100',
               '1970-01-15 00:00:00.000000100',
               '1970-01-16 00:00:00.000000100',
               '1970-01-17 00:00:00.000000100',
               '1970-01-18 00:00:00.000000100',
               '1970-01-19 00:00:00.000000100',
               '1970-01-20 00:00:00.000000100',
               '1970-01-21 00:00:00.000000100',
               '1970-01-22 00:00:00.000000100',
               '1970-01-23 00:00:00.000000100',
               '1970-01-24 00:00:00.000000100',
               '1970-01-25 00:00:00.000000100',
               '1970-01-26 00:00:00.000000100',
               '1970-01-27 00:00:00.000000100',
               '1970-01-28 00:00:00.000000100',
               '1970-01-29 00:00:00.000000100',
               '1970-01-30 00:00:00.000000100',
               '1970-01-31 00:00:00.000000100',
               '1970-02-01 00:00:00.000000100',
               '1970-02-02 00:00:00.000000100',
               '1970-02-03 00:00:00.000000100',
               '1970-02-04 00:00:00.000000100',
               '1970-02-05 00:00:00.000000100',
               '1970-02-06 00:00:00.000000100',
               '1970-02-07 00:00:00.000000100',
               '1970-02-08 00:00:00.000000100',
               '1970-02-09 00:00:00.000000100',
               '1970-02-10 00:00:00.000000100',
               '1970-02-11 00:00:00.000000100'],
              dtype='datetime64[ns]', freq='D')
```

**Additional context**
gooooooaaaaaallllll!",2022-12-18T15:38:13Z,0,0,Matthew Farrellee,,False
442,[BUG] date_range support for anchored offsets,"**Is your feature request related to a problem? Please describe.**
updating code to use `import cudf as pd`

**Describe the solution you'd like**
`cudf.date_range` support matching `pandas.date_range`

see https://pandas.pydata.org/docs/reference/api/pandas.date_range.html
see https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases

```
>>> import cudf as pd
>>> pd.__version__
'22.12.01'
>>> import pandas
>>> pandas.__version__
'1.2.2'
>>> pd.date_range('2011-12-31T00:00:00.000000000', periods=52, freq='W-FRI')
DatetimeIndex(['2011-12-31', '2012-01-07', '2012-01-14', '2012-01-21',
               '2012-01-28', '2012-02-04', '2012-02-11', '2012-02-18',
               '2012-02-25', '2012-03-03', '2012-03-10', '2012-03-17',
               '2012-03-24', '2012-03-31', '2012-04-07', '2012-04-14',
               '2012-04-21', '2012-04-28', '2012-05-05', '2012-05-12',
               '2012-05-19', '2012-05-26', '2012-06-02', '2012-06-09',
               '2012-06-16', '2012-06-23', '2012-06-30', '2012-07-07',
               '2012-07-14', '2012-07-21', '2012-07-28', '2012-08-04',
               '2012-08-11', '2012-08-18', '2012-08-25', '2012-09-01',
               '2012-09-08', '2012-09-15', '2012-09-22', '2012-09-29',
               '2012-10-06', '2012-10-13', '2012-10-20', '2012-10-27',
               '2012-11-03', '2012-11-10', '2012-11-17', '2012-11-24',
               '2012-12-01', '2012-12-08', '2012-12-15', '2012-12-22'],
              dtype='datetime64[ns]')
>>> pandas.date_range('2011-12-31T00:00:00.000000000', periods=52, freq='W-FRI')
DatetimeIndex(['2012-01-06', '2012-01-13', '2012-01-20', '2012-01-27',
               '2012-02-03', '2012-02-10', '2012-02-17', '2012-02-24',
               '2012-03-02', '2012-03-09', '2012-03-16', '2012-03-23',
               '2012-03-30', '2012-04-06', '2012-04-13', '2012-04-20',
               '2012-04-27', '2012-05-04', '2012-05-11', '2012-05-18',
               '2012-05-25', '2012-06-01', '2012-06-08', '2012-06-15',
               '2012-06-22', '2012-06-29', '2012-07-06', '2012-07-13',
               '2012-07-20', '2012-07-27', '2012-08-03', '2012-08-10',
               '2012-08-17', '2012-08-24', '2012-08-31', '2012-09-07',
               '2012-09-14', '2012-09-21', '2012-09-28', '2012-10-05',
               '2012-10-12', '2012-10-19', '2012-10-26', '2012-11-02',
               '2012-11-09', '2012-11-16', '2012-11-23', '2012-11-30',
               '2012-12-07', '2012-12-14', '2012-12-21', '2012-12-28'],
              dtype='datetime64[ns]', freq='W-FRI')
```
",2022-12-18T16:14:11Z,0,0,Matthew Farrellee,,False
443,[FEA] cudf.pivot_table aggfunc accept list of function,"**Is your feature request related to a problem? Please describe.**
I wish cudf.pivot_table aggfunc could accept list of function like below
```
In [31]: df = pd.DataFrame(
    ...:     {
    ...:         ""k1"": [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],
    ...:         ""k2"": ['a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b'],
    ...:         ""c"":  ['c0', 'c0', 'c0', 'c1', 'c1', 'c0', 'c1', 'c1', 'c1', 'c1'],
    ...:         ""v"":  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],
    ...:     }
    ...: )

In [32]: df
Out[32]: 
   k1 k2   c  v
0   0  a  c0  0
1   0  b  c0  1
2   0  a  c0  2
3   0  b  c1  3
4   0  a  c1  4
5   1  b  c0  5
6   1  a  c1  6
7   1  b  c1  7
8   1  a  c1  8
9   1  b  c1  9

In [33]: pd.pivot_table(df, index=[""k1"", ""k2""], columns=['c'], values=['v'], aggfunc=['sum', 'mean'])
    ...: 
Out[33]: 
       sum       mean     
         v          v     
c       c0    c1   c0   c1
k1 k2                     
0  a   2.0   4.0  1.0  4.0
   b   1.0   3.0  1.0  3.0
1  a   NaN  14.0  NaN  7.0
   b   5.0  16.0  5.0  8.0
```

So far cudf returns this error
`ValueError: 2 columns passed, passed data had 3 columns`
",2022-12-28T06:27:09Z,0,0,Kazuki Onodera,NVIDIA,True
444,Refactor JNI `jni_writer_data_sink`,"This is a follow-up task arising from @hyperbolic2346's review of #12425. #12425 moved the implementation of `jni_writer_data_sink` from `TableJni.cpp` to its own header, with no changes to its implementation.

The implementation of `jni_writer_data_sink` could use some streamlining:
1. There is a lot of commonality in `device_write` and `host_write`. It would be good to move the common bit out to its own function.
2. `rotate_buffer()` could stand to have a better name.
3. `jni_writer_data_sink` should probably move to the `cudf::jni::io` namespace.
4. If moved to the appropriate namespace, `jni_writer_data_sink` need no longer reference `jni_` in its name.
5. `device_write_async()` might need implementing. It might simply be a matter of calling `device_write` via `std::async`. This is not currently exercised.",2022-12-30T19:48:56Z,0,0,MithunR,NVIDIA,True
445,[FEA] Support for min_periods in DataFrame correlation,"Hi. Pip installation on Google Colab with `!pip install cudf-cu11 --extra-index-url=https://pypi.ngc.nvidia.com` seems to work but results in missing functionality. For example, trying to compute column correlations in a dataframe with a `min_periods` argument specified raises `NotImplementedError: Unsupported argument 'min_periods'`. Other general functionality seems to be missing as well with various errors raised. Interestingly, everything seemed to be working yesterday. Any thoughts on what could be going on? Having a working pip installation on colab would be a game changer!",2023-01-01T12:08:14Z,0,0,,,False
446,[FEA] Either improve support for or remove type_id::EMPTY,"**Is your feature request related to a problem? Please describe.**
libcudf supports an empty type, `type_id::EMPTY`, that is analogous to [arrow's null type](https://arrow.apache.org/docs/format/Columnar.html#physical-memory-layout) used to represent a column of all null values. However, functionality for this type is only implemented in pieces and there are likely many cases where libcudf will fail if provided with such a column (#10761 is one somewhat recent example).

**Describe the solution you'd like**
We should reevaluate the usage of `EMPTY` columns in libcudf, either removing them altogether or making them work more consistently across the code base. Removal seems like the simplest path forward, but there do appear to be some parts of cuIO that do leverage `EMPTY` columns, and there's an argument to be made that for conformance with the arrow spec we should maintain this type no matter what. If we keep it, we should make it easier to test APIs with such columns to ensure that they are handled appropriately. We also may need to improve handling of these columns in the higher-level APIs backed by libcudf such as cuDF Python or the Spark plugin.

**Additional context**
It's worth noting that AFAICT a null column is trivial to optimize storage for since all that's needed is a size (both null mask and data are redundant). I don't think such columns are useful enough to spend much engineering effort on optimizations, though.",2023-01-05T00:54:29Z,0,0,Vyas Ramasubramani,@rapidsai,True
447,[BUG] libcudf::read_parquet performance worse than arrow::file_reader,"**Describe the bug**
Hi teams. I'm from HugeCTR team. I know that`libcudf::read_parquet()` is gpu-accelerated and I'm using it to load data for my application. Recently, I found out `arrow` also has implemented a file_reader which is a cpu reader. I have some performance results for both readers, and the cudf performance seems not promising. I have tested on 3 datasets, and the row_group distributition is illustrated in the left table. For arrow reader, I was parallelizing the reading of each columnchunk. The result demonstrated that for all three files, the arrow was much faster than cudf, 6x for criteo  in particular. The criteo is a prevalent dataset used in recommendation system.

![image](https://user-images.githubusercontent.com/32166257/210726184-b360cdcb-bd2d-48a8-998f-6da32fd9cb20.png)

For both readers, I used c++ api to read data. 

**Steps/Code to reproduce bug**

Since I was using the C++ api, so one has to compile following two code snippets manually,
The files linked to are actually cpp source files (Sorry for the file suffix). The compiling command is prepended in the source files. 
[read_arrow.txt](https://github.com/rapidsai/cudf/files/10350353/read_arrow.txt)
[read_cudf.txt](https://github.com/rapidsai/cudf/files/10350354/read_cudf.txt)

After compilation, running the executables:
- Running arrow reader: `./arrow_parquet <filepath> false`
- Running cudf reader: `./cudf_parquet <filepath>`

**Expected behavior**
More performant data reading.

**Environment overview**
I was using Selene cluster (DGX-A100). Docker image [dockerfile](https://github.com/NVIDIA-Merlin/Merlin/blob/v22.12.00/docker/dockerfile.merlin)
",2023-01-05T08:10:28Z,0,0,Junzhang,NVIDIA,True
448,[FEA] Cache the outputs of type utilities,"**Is your feature request related to a problem? Please describe.**
The use of type utilities in `cudf.api.types` is widespread throughout cudf. Some of this usage is problematic and should be removed -- in many places, we would be better off leveraging the columnar type hierarchy in cudf to implement additional functionality as methods of the appropriate subclass of `ColumnBase`. However, there are also many places where the usage of these utility functions is unavoidable, especially in functions like `astype` or `build_column` that need to dispatch based on an input dtype. The problem is that these methods are _slow_, especially when compared to the equivalent `isinstance` checks for primitive types. For instance, `pd.api.types.is_numeric_dtype` takes ~1.2 us as compared to ~50-80 ns for `isinstance(val, int)`. Although these absolute numbers are small, that factor of 10-20 is significant because many common cudf functions can call these functions dozens of times (consider e.g. merges or groupbys). We would benefit from reducing this overhead as much as possible.

**Describe the solution you'd like**
These functions should all wrap their logic in a cache to bypass the calls on commonly reused types. Here is a simple example:
```
In [48]: from functools import lru_cache

In [49]: import pandas as pd

In [50]: @lru_cache
    ...: def is_numeric_type_cached(dtype):
    ...:     return pd.api.types.is_numeric_dtype(dtype)
    ...: 

In [51]: def is_numeric_type(arr_or_dtype):
    ...:     if dt := getattr(arr_or_dtype, ""dtype"", None):
    ...:         arr_or_dtype = dt
    ...:     return is_numeric_type_cached(arr_or_dtype)
    ...: 

In [52]: s = pd.Series([1])

In [53]: %timeit pd.api.types.is_numeric_dtype(s)
1.88 µs ± 7.23 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)

In [54]: %timeit pd.api.types.is_numeric_dtype(s.dtype)
1.14 µs ± 7.26 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)

In [55]: %timeit is_numeric_type(s.dtype)
471 ns ± 1.2 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)

In [56]: %timeit is_numeric_type(s)
466 ns ± 1.98 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
```
Note the importance of the wrapper function to support calling these utilities with any object that exposes a `dtype` attribute such as a Series or a numpy/cupy array (we obviously don't want to cache those objects directly). We may need additional such logic in certain specific cases (perhaps categoricals or nested types). Nevertheless, as shown above using a cache could reduce the cost of the APIs of a factor of ~3 on average.

**Describe alternatives you've considered**
A more natural alternative that I considered was leveraging `functools.singledispatch`. Unfortunately, we are largely foiled in this attempt by the fact that the different possible ""dtype-like"" objects do not fit into simple class delinations, which is how dispatch is performed. In the worst case, all dtypes are specified with strings like ""int"" or ""float"", all of which end up dispatched to the same type.

**Additional context**
In theory this could be abused to result in some ridiculous caching, e.g. if a user calls these methods with some arbitrary object that isn't a valid dtype-like object and ends up being stored in the cache. To prevent the worst excesses we should limit the cache size.",2023-01-07T01:24:16Z,0,0,Vyas Ramasubramani,@rapidsai,True
449,[FEA] Improve dtype dispatch patterns in Python,"**Is your feature request related to a problem? Please describe.**
There are numerous places in cudf that require polymorphic behavior depending on the dtype of a parameter. Two of the most prominent are `ColumnBase.astype` and `build_column`, both of which essentially boil down to a switch statement based on the dtype. 

There are numerous issues with this implementation. Two of the most obvious ones are:
- Performance: See #12494 
- It violates the open-closed principle. There is no way to register new types. For `ColumnBase.astype`, this is managed by having the various `as_*_dtype` methods defined in `ColumnBase`, but there is no way to add a new one in general. The only real option would be to monkey-patch the method (same for `build_column`).
- It violates the single responsibility principle. The logic for a new data type's column leaks into the parent ColumnBase's module as well as into helper functions.

**Describe the solution you'd like**
The ideal solution here would be something like `functools.singledispatch` where we register a new override for each dtype and input types are cached to avoid needing to do the expensive `is_*_dtype` calls each time. Unfortunately, this approach is not directly viable because the caching of `singledispatch` is based on the class of the input value, and dtypes cannot be differentiated in this way. For instance, any dtype may be specified as a string.

We should implement a TypeDispatcher (name modeled after libcudf's) that can handle the required type-based single dispatch. The dispatcher could cache the determination of the dtype and run the appropriate method. Unlike `singledispatch`, the dispatch would cache values rather than types. Here is a quick example of what this could look:
```
from functools import lru_cache


class TypeDispatcher:
    def __init__(self, func):
        self._default_func = func
        self._funcs = []
        self._preds = []

    def register(self, predicate):
        # Not trying to be thread-safe here.
        self._preds.append(predicate)

        def register_typ(func):
            self._funcs.append(func)
            return func
        return register_typ

    @lru_cache
    def _get_func(self, obj):
        for pred, func in zip(self._preds, self._funcs):
            if pred(obj):
                return func
        return self._default_func

    # May want to do some functools.wraps magic to match the signature.
    def __call__(self, x, *args, **kwargs):
        return self._get_func(x)(x, *args, **kwargs)


@TypeDispatcher
def f(x):
    raise ValueError(""Unsupported type"")


@f.register(lambda x: isinstance(x, int))
def _(x):
    return x * 2


@f.register(lambda x: isinstance(x, str))
def _(x):
    return x * 3
```

The crucial benefit of this approach (even beyond the performance benefits of the cache) is the ability of external code to register new overloads. Creating a new type of column would no longer require modification of the base column, making our hierarchy much more extensible and removing one major roadblock to supporting alternate column types. Additionally, from an organizational standpoint, it makes the code much easier to follow when all the code relevant to a new column is in one place. 

**Describe alternatives you've considered**
Depending on how many different places implement this sort of dispatch, it could be beneficial to decouple the predicates from the functions so that the predicates can be reused, since by assumption they will be identical across all relevant functions. In other words, all instances of `TypeDispatcher` could share a global registry mapping cached objects to their types, but each instance could store a separate mapping from types to functions. That would allow a centralized registration of predicates and even more cache benefits. However, it would require a couple of extra things that may not be worthwhile:
1. Standardizing the way in which the types are keyed so that registration could be done identically everywhere e.g. `@f.register(""int_like"")`
2. Implementing separate APIs for registering new types and predicates to the class vs registering a new overload for a specific `TypeDispatcher` instance.

**Additional context**
For primitive predicates like the one in my example above, I would expect that the overhead of `lru_cache` and dictionary lookups would be slower than the naive if-else cascade. My assumption is that for the more complex `is_*_dtype` predicates used for dtype-dispatched functions this is not the case.

It's also worth noting that this value-based approach has pitfalls in general around what is considered equal, but those should not bite us when used for dtypes. For instance, `isinstance(1, int)` is True but so is `isinstance(1, float)`. That shouldn't cause us any particular issues though since we are working with dtypes and there are no such overlaps.

This approach may need some modification to work with class methods instead of free functions.

The [multimethod package](https://multimethod.readthedocs.io/en/latest/readme.html#overload) supports this sort of predicate-based overloading (in addition to type-based dispatch), but I think our needs are narrow enough that we are better off at least prototyping this ourselves before reaching to add another dependency.",2023-01-07T02:01:22Z,0,0,Vyas Ramasubramani,@rapidsai,True
450,[FEA] category dtype support in parquet writer,"**Is your feature request related to a problem? Please describe.**
writing code with `import cudf as pd`

**Describe the solution you'd like**
same behavior as `import pandas as pd`

```
In [1]: import cudf as pd

In [2]: pd.__version__
Out[2]: '22.12.01'

In [3]: df = pd.DataFrame({'a': ['one','two','three'] * 10})

In [4]: df.info()
<class 'cudf.core.dataframe.DataFrame'>
RangeIndex: 30 entries, 0 to 29
Data columns (total 1 columns):
 #   Column  Non-Null Count  Dtype
---  ------  --------------  -----
 0   a       30 non-null     object
dtypes: object(1)
memory usage: 234.0+ bytes

In [5]: df.a = df.astype('category')

In [6]: df.info()
<class 'cudf.core.dataframe.DataFrame'>
RangeIndex: 30 entries, 0 to 29
Data columns (total 1 columns):
 #   Column  Non-Null Count  Dtype
---  ------  --------------  -----
 0   a       30 non-null     category
dtypes: category(1)
memory usage: 57.0 bytes

In [7]: df.to_parquet('df.parquet')
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[7], line 1
----> 1 df.to_parquet('df.parquet')

File .../lib/python3.8/site-packages/cudf/core/dataframe.py:6287, in DataFrame.to_parquet(self, path, engine, compression, index, partition_cols, partition_file_name, partition_offsets, statistics, metadata_file_path, int96_timestamps, row_group_size_bytes, row_group_size_rows, max_page_size_bytes, max_page_size_rows, storage_options, return_metadata, *args, **kwargs)
   6284 """"""{docstring}""""""
   6285 from cudf.io import parquet
-> 6287 return parquet.to_parquet(
   6288     self,
   6289     path=path,
   6290     engine=engine,
   6291     compression=compression,
   6292     index=index,
   6293     partition_cols=partition_cols,
   6294     partition_file_name=partition_file_name,
   6295     partition_offsets=partition_offsets,
   6296     statistics=statistics,
   6297     metadata_file_path=metadata_file_path,
   6298     int96_timestamps=int96_timestamps,
   6299     row_group_size_bytes=row_group_size_bytes,
   6300     row_group_size_rows=row_group_size_rows,
   6301     max_page_size_bytes=max_page_size_bytes,
   6302     max_page_size_rows=max_page_size_rows,
   6303     storage_options=storage_options,
   6304     return_metadata=return_metadata,
   6305     *args,
   6306     **kwargs,
   6307 )

File .../lib/python3.8/contextlib.py:75, in ContextDecorator.__call__.<locals>.inner(*args, **kwds)
     72 @wraps(func)
     73 def inner(*args, **kwds):
     74     with self._recreate_cm():
---> 75         return func(*args, **kwds)

File .../lib/python3.8/site-packages/cudf/io/parquet.py:700, in to_parquet(df, path, engine, compression, index, partition_cols, partition_file_name, partition_offsets, statistics, metadata_file_path, int96_timestamps, row_group_size_bytes, row_group_size_rows, max_page_size_bytes, max_page_size_rows, storage_options, return_metadata, *args, **kwargs)
    698     if partition_cols is None or col not in partition_cols:
    699         if df[col].dtype.name == ""category"":
--> 700             raise ValueError(
    701                 ""'category' column dtypes are currently not ""
    702                 + ""supported by the gpu accelerated parquet writer""
    703             )
    705 if partition_cols:
    706     if metadata_file_path is not None:

ValueError: 'category' column dtypes are currently not supported by the gpu accelerated parquet writer

In [8]: df.to_pandas().to_parquet('df.parquet')

In [9]: %ls df.parquet
df.parquet
```",2023-01-07T13:39:50Z,0,0,Matthew Farrellee,,False
451,[FEA] category dtype support in parquet reader,"**Is your feature request related to a problem? Please describe.**
writing code with `import cudf as pd`

**Describe the solution you'd like**
same behavior as `import pandas as pd`
```
In [1]: import cudf as pd

In [2]: pd.__version__
Out[2]: '22.12.01'

In [3]: df = pd.DataFrame({'a': ['one','two','three'] * 10})

In [4]: df.info()
<class 'cudf.core.dataframe.DataFrame'>
RangeIndex: 30 entries, 0 to 29
Data columns (total 1 columns):
 #   Column  Non-Null Count  Dtype
---  ------  --------------  -----
 0   a       30 non-null     object
dtypes: object(1)
memory usage: 234.0+ bytes

In [5]: df.a = df.astype('category')

In [6]: df.info()
<class 'cudf.core.dataframe.DataFrame'>
RangeIndex: 30 entries, 0 to 29
Data columns (total 1 columns):
 #   Column  Non-Null Count  Dtype
---  ------  --------------  -----
 0   a       30 non-null     category
dtypes: category(1)
memory usage: 57.0 bytes

In [7]: %ls df.parquet
ls: cannot access 'df.parquet': No such file or directory

In [8]: df.to_pandas().to_parquet('df.parquet')

In [9]: %ls df.parquet
df.parquet

In [10]: pd.read_parquet('df.parquet').info()
<class 'cudf.core.dataframe.DataFrame'>
RangeIndex: 30 entries, 0 to 29
Data columns (total 1 columns):
 #   Column  Non-Null Count  Dtype
---  ------  --------------  -----
 0   a       30 non-null     object
dtypes: object(1)
memory usage: 234.0+ bytes

In [11]: import pandas

In [12]: pandas.read_parquet('df.parquet').info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 30 entries, 0 to 29
Data columns (total 1 columns):
 #   Column  Non-Null Count  Dtype   
---  ------  --------------  -----   
 0   a       30 non-null     category
dtypes: category(1)
memory usage: 290.0 bytes

In [13]: pd.DataFrame(pandas.read_parquet('df.parquet')).info()
<class 'cudf.core.dataframe.DataFrame'>
RangeIndex: 30 entries, 0 to 29
Data columns (total 1 columns):
 #   Column  Non-Null Count  Dtype
---  ------  --------------  -----
 0   a       30 non-null     category
dtypes: category(1)
memory usage: 57.0 bytes
```

the parquet reader turns the column into dtype=object
```
In [10]: pd.read_parquet('df.parquet').info()
<class 'cudf.core.dataframe.DataFrame'>
RangeIndex: 30 entries, 0 to 29
Data columns (total 1 columns):
 #   Column  Non-Null Count  Dtype
---  ------  --------------  -----
 0   a       30 non-null     object
dtypes: object(1)
memory usage: 234.0+ bytes
```",2023-01-07T13:45:35Z,0,0,Matthew Farrellee,,False
452,[QST] CPU memory spike during cudf dataframe conversion,"Hi all, I have a dataframe that is ~19K Rows, ~11.4 MB (Profiled using ```df.info(memory_usage = ""deep"")```). We are currently running into CPU out of memory issues and so profiling our memory using this sample dataset. As you can see in the screenshot attached, there is a jump in mem usage, from 840MiB -> 4148MiB, during the type conversion of ```df```. Image below shows the dataframe memory usage after conversion. 

My question is: Why is there a jump in the memory usage when converting a dataframe from pandas to cudf? Furthermore, this memory is not released after, and so increases from this point in following processing steps.

<img width=""508"" alt=""Screenshot 2023-01-09 at 6 08 11 PM"" src=""https://user-images.githubusercontent.com/58301316/211284128-f093d906-cdb5-42b9-9ea8-79d4ff237f95.png"">
<img width=""1179"" alt=""Screenshot 2023-01-10 at 11 16 09 AM"" src=""https://user-images.githubusercontent.com/58301316/211454005-c39e9bf0-393c-4bbc-83ad-9e9f23783991.png"">
",2023-01-09T10:09:07Z,0,0,Yi Kuang,,False
453,"[QST]The group by collect performance is insufficient, and the performance deteriorates with the increase of the group by column length.","**What is your question?**
The group by collect performance is insufficient, and the performance deteriorates with the increase of the group by column length.

```
>>> import cudf
>>> import time
>>>
>>> import pandas
>>> import pyarrow
>>> import numpy as np
>>>
>>> def create_table(n_rows, n_cols, n_range):
...     table = pyarrow.Table.from_pydict(
...         {f'col_{c}': np.random.randint(0, n_range, size=[n_rows]) for c in range(n_cols)})
...     return table
...
>>>
>>> def create_table_with_str(n_rows, n_cols, n_strs, n_strs_cols, n_range):
...     prefix = 'xxxx_' * ((n_strs - 10) // 5)
...     cdf = create_table(n_rows, n_cols, n_range).to_pandas()
...     for i in range(n_strs_cols):
...         cdf[f'col_{i}'] = cdf[f'col_{i}'].apply(lambda x: f'{prefix}{x:010}')
    return pyarrow.Table.from_pandas(cdf)...     return pyarrow.Table.from_pandas(cdf)
...
>>>
>>> def stat_cost(str_len):
...     tbl = create_table_with_str(2000 * 10000, 2, str_len, 1, 1500 * 10000)
...     start = time.time()
...     df = cudf.DataFrame.from_arrow(tbl)
...     print(f'from arrow cost: {time.time() - start} s, '
...           f'bandwidth: {df.shape[0] / 10000 / (time.time() - start)} WRows/s')
...     print(df)
...     start = time.time()
...     result = df.groupby(['col_0']).collect()
...     print(f'group by collect cost: {time.time() - start} s, '
...           f'bandwidth: {df.shape[0] / 10000 / (time.time() - start)} WRows/s')
...
>>>
>>> stat_cost(10)
from arrow cost: 0.09801530838012695 s, bandwidth: 20401.15471699949 WRows/s
               col_0     col_1
0         0009882104   3942519
1         0009170270   7183154
2         0000346561  14059698
3         0009672848   6882498
4         0011532285  12876681
...              ...       ...
19999995  0000388357    579814
19999996  0009951171  14008663
19999997  0002681040    318695
19999998  0003139531   5608877
19999999  0007299816  12547343

[20000000 rows x 2 columns]
group by collect cost: 1.317047119140625 s, bandwidth: 1518.522440661447 WRows/s
>>> stat_cost(20)
from arrow cost: 0.14093589782714844 s, bandwidth: 14187.992497213516 WRows/s
                         col_0     col_1
0         xxxx_xxxx_0011097676   6734961
1         xxxx_xxxx_0005386896  13758023
2         xxxx_xxxx_0012936583  12093805
3         xxxx_xxxx_0014685588    977351
4         xxxx_xxxx_0002394173   4422859
...                        ...       ...
19999995  xxxx_xxxx_0008602092   1174373
19999996  xxxx_xxxx_0006179928   9909283
19999997  xxxx_xxxx_0004578043   4414022
19999998  xxxx_xxxx_0004295524   9151066
19999999  xxxx_xxxx_0009383727   5630830

[20000000 rows x 2 columns]
group by collect cost: 3.6019299030303955 s, bandwidth: 555.254619538489 WRows/s
>>> stat_cost(30)
from arrow cost: 0.1838366985321045 s, bandwidth: 10878.289477949978 WRows/s
                                   col_0     col_1
0         xxxx_xxxx_xxxx_xxxx_0012107927  11093137
1         xxxx_xxxx_xxxx_xxxx_0008415030   6082935
2         xxxx_xxxx_xxxx_xxxx_0001637082   5181973
3         xxxx_xxxx_xxxx_xxxx_0014907884  13010547
4         xxxx_xxxx_xxxx_xxxx_0011395415   8406699
...                                  ...       ...
19999995  xxxx_xxxx_xxxx_xxxx_0013393283   9371961
19999996  xxxx_xxxx_xxxx_xxxx_0012288828   3685424
19999997  xxxx_xxxx_xxxx_xxxx_0011403282  11832112
19999998  xxxx_xxxx_xxxx_xxxx_0014808359  12467674
19999999  xxxx_xxxx_xxxx_xxxx_0007966548   3177904

[20000000 rows x 2 columns]
group by collect cost: 6.546090126037598 s, bandwidth: 305.5246419013939 WRows/s

```
![捕获](https://user-images.githubusercontent.com/43532055/211317106-522eec4e-6bc4-439f-8eda-4dd889379a24.PNG)

How to improve performance?


",2023-01-09T13:18:58Z,0,0,Liu,,False
454,[BUG] IndexError during assignment through loc[],"**Describe the bug**
rewriting code with `import cudf as pd`

**Steps/Code to reproduce bug**
```
In [1]: import cudf as pd

In [2]: pd.__version__
Out[2]: '22.12.0'

In [3]: df = pd.DataFrame(columns=['a'])

In [4]: df.loc[0] = [1]
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
Cell In [4], line 1
----> 1 df.loc[0] = [1]

File ~/.local/lib/python3.9/site-packages/cudf/core/dataframe.py:149, in _DataFrameIndexer.__setitem__(self, key, value)
    147 if not isinstance(key, tuple):
    148     key = (key, slice(None))
--> 149 return self._setitem_tuple_arg(key, value)

File ~/.local/lib/python3.9/site-packages/nvtx/nvtx.py:101, in annotate.__call__.<locals>.inner(*args, **kwargs)
     98 @wraps(func)
     99 def inner(*args, **kwargs):
    100     libnvtx_push_range(self.attributes, self.domain.handle)
--> 101     result = func(*args, **kwargs)
    102     libnvtx_pop_range(self.domain.handle)
    103     return result

File ~/.local/lib/python3.9/site-packages/cudf/core/dataframe.py:393, in _DataFrameLocIndexer._setitem_tuple_arg(self, key, value)
    386 # Otherwise, there are two situations. The key on row axis
    387 # can be a scalar or 1d. In either of the situation, the
    388 # ith element in value corresponds to the ith row in
    389 # the indexed object.
    390 # If the key is 1d, a broadcast will happen.
    391 else:
    392     for i, col in enumerate(columns_df._column_names):
--> 393         self._frame[col].loc[key[0]] = value[i]

File ~/.local/lib/python3.9/site-packages/nvtx/nvtx.py:101, in annotate.__call__.<locals>.inner(*args, **kwargs)
     98 @wraps(func)
     99 def inner(*args, **kwargs):
    100     libnvtx_push_range(self.attributes, self.domain.handle)
--> 101     result = func(*args, **kwargs)
    102     libnvtx_pop_range(self.domain.handle)
    103     return result

File ~/.local/lib/python3.9/site-packages/cudf/core/series.py:285, in _SeriesLocIndexer.__setitem__(self, key, value)
    283     value = cudf.Series(value)
    284     value = value._align_to_index(self._frame.index, how=""right"")
--> 285 self._frame.iloc[key] = value

File ~/.local/lib/python3.9/site-packages/nvtx/nvtx.py:101, in annotate.__call__.<locals>.inner(*args, **kwargs)
     98 @wraps(func)
     99 def inner(*args, **kwargs):
    100     libnvtx_push_range(self.attributes, self.domain.handle)
--> 101     result = func(*args, **kwargs)
    102     libnvtx_pop_range(self.domain.handle)
    103     return result

File ~/.local/lib/python3.9/site-packages/cudf/core/series.py:236, in _SeriesIlocIndexer.__setitem__(self, key, value)
    231     if to_dtype != self._frame._column.dtype:
    232         self._frame._column._mimic_inplace(
    233             self._frame._column.astype(to_dtype), inplace=True
    234         )
--> 236 self._frame._column[key] = value

File ~/.local/lib/python3.9/site-packages/cudf/core/column/column.py:496, in ColumnBase.__setitem__(self, key, value)
    494     if not isinstance(key, cudf.core.column.NumericalColumn):
    495         raise ValueError(f""Invalid scatter map type {key.dtype}."")
--> 496     out = self._scatter_by_column(key, value_normalized)
    498 if out:
    499     self._mimic_inplace(out, inplace=True)

File ~/.local/lib/python3.9/site-packages/cudf/core/column/column.py:580, in ColumnBase._scatter_by_column(self, key, value)
    576     return libcudf.copying.boolean_mask_scatter([value], [self], key)[
    577         0
    578     ]._with_type_metadata(self.dtype)
    579 else:
--> 580     return libcudf.copying.scatter([value], key, [self])[
    581         0
    582     ]._with_type_metadata(self.dtype)

File /usr/lib/python3.9/contextlib.py:79, in ContextDecorator.__call__.<locals>.inner(*args, **kwds)
     76 @wraps(func)
     77 def inner(*args, **kwds):
     78     with self._recreate_cm():
---> 79         return func(*args, **kwds)

File copying.pyx:265, in cudf._lib.copying.scatter()

IndexError: index out of bounds for column of size 0

In [5]: pdf = df.to_pandas()

In [6]: pdf
Out[6]: 
Empty DataFrame
Columns: [a]
Index: []

In [7]: pdf.loc[0] = [1]

In [8]: pdf
Out[8]: 
   a
0  1
```


**Expected behavior**
same behavior as `import pandas as pd`
",2023-01-09T21:16:15Z,0,0,Matthew Farrellee,,False
455,[BUG] assignment through loc[] breaks DataFrame,"**Describe the bug**
rewriting code with `import cudf as pd`

**Steps/Code to reproduce bug**
```
In [1]: import cudf as pd

In [2]: pd.__version__
Out[2]: '22.12.0'

In [3]: df = pd.DataFrame({'a':0})

In [4]: df
Out[4]: 
   a
0  0

In [5]: df.loc[1] = 2

In [6]: df
Out[6]: ---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
File ~/.local/lib/python3.9/site-packages/IPython/core/formatters.py:707, in PlainTextFormatter.__call__(self, obj)
    700 stream = StringIO()
    701 printer = pretty.RepresentationPrinter(stream, self.verbose,
    702     self.max_width, self.newline,
    703     max_seq_length=self.max_seq_length,
    704     singleton_pprinters=self.singleton_printers,
    705     type_pprinters=self.type_printers,
    706     deferred_pprinters=self.deferred_printers)
--> 707 printer.pretty(obj)
    708 printer.flush()
    709 return stream.getvalue()

File ~/.local/lib/python3.9/site-packages/IPython/lib/pretty.py:410, in RepresentationPrinter.pretty(self, obj)
    407                         return meth(obj, self, cycle)
    408                 if cls is not object \
    409                         and callable(cls.__dict__.get('__repr__')):
--> 410                     return _repr_pprint(obj, self, cycle)
    412     return _default_pprint(obj, self, cycle)
    413 finally:

File ~/.local/lib/python3.9/site-packages/IPython/lib/pretty.py:778, in _repr_pprint(obj, p, cycle)
    776 """"""A pprint that just redirects to the normal repr function.""""""
    777 # Find newlines and replace them with p.break_()
--> 778 output = repr(obj)
    779 lines = output.splitlines()
    780 with p.group():

File ~/.local/lib/python3.9/site-packages/nvtx/nvtx.py:101, in annotate.__call__.<locals>.inner(*args, **kwargs)
     98 @wraps(func)
     99 def inner(*args, **kwargs):
    100     libnvtx_push_range(self.attributes, self.domain.handle)
--> 101     result = func(*args, **kwargs)
    102     libnvtx_pop_range(self.domain.handle)
    103     return result

File ~/.local/lib/python3.9/site-packages/cudf/core/dataframe.py:1863, in DataFrame.__repr__(self)
   1860 @_cudf_nvtx_annotate
   1861 def __repr__(self):
   1862     output = self._get_renderable_dataframe()
-> 1863     return self._clean_renderable_dataframe(output)

File ~/.local/lib/python3.9/site-packages/cudf/core/dataframe.py:1741, in DataFrame._clean_renderable_dataframe(self, output)
   1738 else:
   1739     width = None
-> 1741 output = output.to_pandas().to_string(
   1742     max_rows=max_rows,
   1743     min_rows=min_rows,
   1744     max_cols=max_cols,
   1745     line_width=width,
   1746     max_colwidth=max_colwidth,
   1747     show_dimensions=show_dimensions,
   1748 )
   1750 lines = output.split(""\n"")
   1752 if lines[-1].startswith(""[""):

File ~/.local/lib/python3.9/site-packages/nvtx/nvtx.py:101, in annotate.__call__.<locals>.inner(*args, **kwargs)
     98 @wraps(func)
     99 def inner(*args, **kwargs):
    100     libnvtx_push_range(self.attributes, self.domain.handle)
--> 101     result = func(*args, **kwargs)
    102     libnvtx_pop_range(self.domain.handle)
    103     return result

File ~/.local/lib/python3.9/site-packages/cudf/core/dataframe.py:5037, in DataFrame.to_pandas(self, nullable, **kwargs)
   5034 out_index = self.index.to_pandas()
   5036 for i, col_key in enumerate(self._data):
-> 5037     out_data[i] = self._data[col_key].to_pandas(
   5038         index=out_index, nullable=nullable
   5039     )
   5041 out_df = pd.DataFrame(out_data, index=out_index)
   5042 out_df.columns = self._data.to_pandas_index()

File ~/.local/lib/python3.9/site-packages/cudf/core/column/numerical.py:732, in NumericalColumn.to_pandas(self, index, nullable, **kwargs)
    729     pd_series = self.to_arrow().to_pandas(**kwargs)
    731 if index is not None:
--> 732     pd_series.index = index
    733 return pd_series

File ~/.local/lib/python3.9/site-packages/pandas/core/generic.py:5588, in NDFrame.__setattr__(self, name, value)
   5586 try:
   5587     object.__getattribute__(self, name)
-> 5588     return object.__setattr__(self, name, value)
   5589 except AttributeError:
   5590     pass

File ~/.local/lib/python3.9/site-packages/pandas/_libs/properties.pyx:70, in pandas._libs.properties.AxisProperty.__set__()

File ~/.local/lib/python3.9/site-packages/pandas/core/series.py:572, in Series._set_axis(self, axis, labels, fastpath)
    568             pass
    570 if not fastpath:
    571     # The ensure_index call above ensures we have an Index object
--> 572     self._mgr.set_axis(axis, labels)

File ~/.local/lib/python3.9/site-packages/pandas/core/internals/managers.py:214, in BaseBlockManager.set_axis(self, axis, new_labels)
    212 def set_axis(self, axis: int, new_labels: Index) -> None:
    213     # Caller is responsible for ensuring we have an Index object.
--> 214     self._validate_set_axis(axis, new_labels)
    215     self.axes[axis] = new_labels

File ~/.local/lib/python3.9/site-packages/pandas/core/internals/base.py:69, in DataManager._validate_set_axis(self, axis, new_labels)
     66     pass
     68 elif new_len != old_len:
---> 69     raise ValueError(
     70         f""Length mismatch: Expected axis has {old_len} elements, new ""
     71         f""values have {new_len} elements""
     72     )

ValueError: Length mismatch: Expected axis has 2 elements, new values have 1 elements

In [7]: pdf = pd.DataFrame({'a':0}).to_pandas()

In [8]: pdf
Out[8]: 
   a
0  0

In [9]: pdf.loc[1] = 2

In [10]: pdf
Out[10]: 
   a
0  0
1  2

In [11]: df.loc[0]
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In [13], line 1
----> 1 df.loc[0]

File ~/.local/lib/python3.9/site-packages/cudf/core/dataframe.py:144, in _DataFrameIndexer.__getitem__(self, arg)
    142 if not isinstance(arg, tuple):
    143     arg = (arg, slice(None))
--> 144 return self._getitem_tuple_arg(arg)

File ~/.local/lib/python3.9/site-packages/nvtx/nvtx.py:101, in annotate.__call__.<locals>.inner(*args, **kwargs)
     98 @wraps(func)
     99 def inner(*args, **kwargs):
    100     libnvtx_push_range(self.attributes, self.domain.handle)
--> 101     result = func(*args, **kwargs)
    102     libnvtx_pop_range(self.domain.handle)
    103     return result

File ~/.local/lib/python3.9/site-packages/cudf/core/dataframe.py:291, in _DataFrameLocIndexer._getitem_tuple_arg(self, arg)
    286 tmp_col_name = str(uuid4())
    287 other_df = DataFrame(
    288     {tmp_col_name: column.arange(len(tmp_arg[0]))},
    289     index=as_index(tmp_arg[0]),
    290 )
--> 291 df = other_df.join(columns_df, how=""inner"")
    292 # as join is not assigning any names to index,
    293 # update it over here
    294 df.index.name = columns_df.index.name

File ~/.local/lib/python3.9/site-packages/nvtx/nvtx.py:101, in annotate.__call__.<locals>.inner(*args, **kwargs)
     98 @wraps(func)
     99 def inner(*args, **kwargs):
    100     libnvtx_push_range(self.attributes, self.domain.handle)
--> 101     result = func(*args, **kwargs)
    102     libnvtx_pop_range(self.domain.handle)
    103     return result

File ~/.local/lib/python3.9/site-packages/cudf/core/dataframe.py:4038, in DataFrame.join(self, other, on, how, lsuffix, rsuffix, sort)
   4035 if on is not None:
   4036     raise NotImplementedError(""The on parameter is not yet supported"")
-> 4038 df = self.merge(
   4039     other,
   4040     left_index=True,
   4041     right_index=True,
   4042     how=how,
   4043     suffixes=(lsuffix, rsuffix),
   4044     sort=sort,
   4045 )
   4046 df.index.name = (
   4047     None if self.index.name != other.index.name else self.index.name
   4048 )
   4049 return df

File ~/.local/lib/python3.9/site-packages/nvtx/nvtx.py:101, in annotate.__call__.<locals>.inner(*args, **kwargs)
     98 @wraps(func)
     99 def inner(*args, **kwargs):
    100     libnvtx_push_range(self.attributes, self.domain.handle)
--> 101     result = func(*args, **kwargs)
    102     libnvtx_pop_range(self.domain.handle)
    103     return result

File ~/.local/lib/python3.9/site-packages/cudf/core/dataframe.py:3987, in DataFrame.merge(self, right, on, left_on, right_on, left_index, right_index, how, sort, lsuffix, rsuffix, indicator, suffixes)
   3984 elif how in {""leftsemi"", ""leftanti""}:
   3985     merge_cls = MergeSemi
-> 3987 return merge_cls(
   3988     lhs,
   3989     rhs,
   3990     on=on,
   3991     left_on=left_on,
   3992     right_on=right_on,
   3993     left_index=left_index,
   3994     right_index=right_index,
   3995     how=how,
   3996     sort=sort,
   3997     indicator=indicator,
   3998     suffixes=suffixes,
   3999 ).perform_merge()

File ~/.local/lib/python3.9/site-packages/cudf/core/join/join.py:200, in Merge.perform_merge(self)
    189 gather_kwargs = {
    190     ""nullify"": True,
    191     ""check_bounds"": False,
    192     ""keep_index"": self._using_left_index or self._using_right_index,
    193 }
    194 left_result = (
    195     self.lhs._gather(gather_map=left_rows, **gather_kwargs)
    196     if left_rows is not None
    197     else cudf.DataFrame._from_data({})
    198 )
    199 right_result = (
--> 200     self.rhs._gather(gather_map=right_rows, **gather_kwargs)
    201     if right_rows is not None
    202     else cudf.DataFrame._from_data({})
    203 )
    205 result = cudf.DataFrame._from_data(
    206     *self._merge_results(left_result, right_result)
    207 )
    209 if self.sort:

File ~/.local/lib/python3.9/site-packages/cudf/core/indexed_frame.py:1722, in IndexedFrame._gather(self, gather_map, keep_index, nullify, check_bounds)
   1716 if not libcudf.copying._gather_map_is_valid(
   1717     gather_map, len(self), check_bounds, nullify
   1718 ):
   1719     raise IndexError(""Gather map index is out of bounds."")
   1721 return self._from_columns_like_self(
-> 1722     libcudf.copying.gather(
   1723         list(self._index._columns + self._columns)
   1724         if keep_index
   1725         else list(self._columns),
   1726         gather_map,
   1727         nullify=nullify,
   1728     ),
   1729     self._column_names,
   1730     self._index.names if keep_index else None,
   1731 )

File /usr/lib/python3.9/contextlib.py:79, in ContextDecorator.__call__.<locals>.inner(*args, **kwds)
     76 @wraps(func)
     77 def inner(*args, **kwds):
     78     with self._recreate_cm():
---> 79         return func(*args, **kwds)

File copying.pyx:178, in cudf._lib.copying.gather()

File utils.pyx:46, in cudf._lib.utils.table_view_from_columns()

RuntimeError: cuDF failure at: /project/cpp/src/table/table_view.cpp:35: Column size mismatch.
```

**Expected behavior**
same behavior as `import pandas as pd`",2023-01-09T21:22:09Z,0,0,Matthew Farrellee,,False
456,[FEA] replace(): regex parameter is not implemented yet,"**Is your feature request related to a problem? Please describe.**
writing code with `import cudf as pd`

**Describe the solution you'd like**
```
In [1]: import cudf as pd

In [2]: df = pd.DataFrame({'a': ['abc','bcd','cde']})

In [3]: df.replace('c', 'C', regex=True)
---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
Cell In [3], line 1
----> 1 df.replace('c', 'C', regex=True)

File ~/.local/lib/python3.9/site-packages/nvtx/nvtx.py:101, in annotate.__call__.<locals>.inner(*args, **kwargs)
     98 @wraps(func)
     99 def inner(*args, **kwargs):
    100     libnvtx_push_range(self.attributes, self.domain.handle)
--> 101     result = func(*args, **kwargs)
    102     libnvtx_pop_range(self.domain.handle)
    103     return result

File ~/.local/lib/python3.9/site-packages/cudf/core/indexed_frame.py:744, in IndexedFrame.replace(self, to_replace, value, inplace, limit, regex, method)
    741     raise NotImplementedError(""limit parameter is not implemented yet"")
    743 if regex:
--> 744     raise NotImplementedError(""regex parameter is not implemented yet"")
    746 if method not in (""pad"", None):
    747     raise NotImplementedError(
    748         ""method parameter is not implemented yet""
    749     )

NotImplementedError: regex parameter is not implemented yet

In [4]: df.to_pandas().replace('c', 'C', regex=True)
Out[4]: 
     a
0  abC
1  bCd
2  Cde

In [5]: pd.__version__
Out[5]: '22.12.0'
```",2023-01-09T21:30:39Z,0,0,Matthew Farrellee,,False
457,[FEA] DataFrame.query support for strings,"**Is your feature request related to a problem? Please describe.**
working with code using `import cudf as pd`

**Describe the solution you'd like**
functionality matching `import pandas as pd`
```
In [1]: import cudf as pd

In [2]: df = pd.DataFrame({'a': ['one', 'two', 'three']})

In [3]: df.query('a == ""one""')
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In [3], line 1
----> 1 df.query('a == ""one""')

File ~/.local/lib/python3.9/site-packages/cudf/core/dataframe.py:4174, in DataFrame.query(self, expr, local_dict)
   4168 callenv = {
   4169     ""locals"": callframe.f_locals,
   4170     ""globals"": callframe.f_globals,
   4171     ""local_dict"": local_dict,
   4172 }
   4173 # Run query
-> 4174 boolmask = queryutils.query_execute(self, expr, callenv)
   4175 return self._apply_boolean_mask(boolmask)

File ~/.local/lib/python3.9/site-packages/cudf/utils/queryutils.py:218, in query_execute(df, expr, callenv)
    216 # wait to check the types until we know which cols are used
    217 if any(col.dtype not in SUPPORTED_QUERY_TYPES for col in colarrays):
--> 218     raise TypeError(
    219         ""query only supports numeric, datetime, timedelta, ""
    220         ""or bool dtypes.""
    221     )
    223 colarrays = [col.data_array_view for col in colarrays]
    225 kernel = compiled[""kernel""]

TypeError: query only supports numeric, datetime, timedelta, or bool dtypes.

In [4]: pd.__version__
Out[4]: '22.12.0'

In [5]: df.to_pandas().query('a == ""one""')
Out[5]: 
     a
0  one
``` ",2023-01-09T23:25:35Z,0,0,Matthew Farrellee,,False
458,[FEA] DataFrame.query support for math ops,"**Is your feature request related to a problem? Please describe.**
working with `import cudf as pd`

**Describe the solution you'd like**
support for math ops matching `pandas`

```
In [1]: import cudf as pd

In [2]: pd.__version__
Out[2]: '22.12.0'

In [3]: df = pd.DataFrame({'a': [1**2, 2**2, 3**2]})

In [4]: df.query('sqrt(a) >= 2')
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
File ~/.local/lib/python3.9/site-packages/cudf/core/dataframe.py:7559, in extract_col(df, col)
   7558 try:
-> 7559     return df._data[col]
   7560 except KeyError:

File ~/.local/lib/python3.9/site-packages/cudf/core/column_accessor.py:155, in ColumnAccessor.__getitem__(self, key)
    154 def __getitem__(self, key: Any) -> ColumnBase:
--> 155     return self._data[key]

KeyError: 'sqrt'

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
Cell In [4], line 1
----> 1 df.query('sqrt(a) >= 2')

File ~/.local/lib/python3.9/site-packages/cudf/core/dataframe.py:4174, in DataFrame.query(self, expr, local_dict)
   4168 callenv = {
   4169     ""locals"": callframe.f_locals,
   4170     ""globals"": callframe.f_globals,
   4171     ""local_dict"": local_dict,
   4172 }
   4173 # Run query
-> 4174 boolmask = queryutils.query_execute(self, expr, callenv)
   4175 return self._apply_boolean_mask(boolmask)

File ~/.local/lib/python3.9/site-packages/cudf/utils/queryutils.py:214, in query_execute(df, expr, callenv)
    211 columns = compiled[""colnames""]
    213 # prepare col args
--> 214 colarrays = [cudf.core.dataframe.extract_col(df, col) for col in columns]
    216 # wait to check the types until we know which cols are used
    217 if any(col.dtype not in SUPPORTED_QUERY_TYPES for col in colarrays):

File ~/.local/lib/python3.9/site-packages/cudf/utils/queryutils.py:214, in <listcomp>(.0)
    211 columns = compiled[""colnames""]
    213 # prepare col args
--> 214 colarrays = [cudf.core.dataframe.extract_col(df, col) for col in columns]
    216 # wait to check the types until we know which cols are used
    217 if any(col.dtype not in SUPPORTED_QUERY_TYPES for col in colarrays):

File ~/.local/lib/python3.9/site-packages/cudf/core/dataframe.py:7567, in extract_col(df, col)
   7561 if (
   7562     col == ""index""
   7563     and col not in df.index._data
   7564     and not isinstance(df.index, MultiIndex)
   7565 ):
   7566     return df.index._data.columns[0]
-> 7567 return df.index._data[col]

File ~/.local/lib/python3.9/site-packages/cudf/core/column_accessor.py:155, in ColumnAccessor.__getitem__(self, key)
    154 def __getitem__(self, key: Any) -> ColumnBase:
--> 155     return self._data[key]

KeyError: 'sqrt'

In [5]: df.to_pandas().query('sqrt(a) >= 2')
Out[5]: 
   a
1  4
2  9
```

**Additional context**
https://github.com/pandas-dev/pandas/blob/v1.5.2/pandas/core/computation/ops.py#L39-L60",2023-01-10T00:45:18Z,0,0,Matthew Farrellee,,False
459,[BUG] read_parquet/read_orc with filters do not filter specific rows,"**Describe the bug**
When using `cudf.read_parquet` or `read_orc` with the filters argument to filter out rows based on certain predicates, the methods today just filter out reading row groups (or stripes) that can be completely eliminated based on the given condition, but does return all rows from the read row groups without applying the given filters again. This behavior can be confusing to users assuming that all the relevant data has already been filtered out and is contrary to how dask, dask-cuDF and PyArrow behave today.

Example:

Data:
```
Col Name: A
Row Group 0: 1,5,1
Row Group 1: 5,5,5
```
```python
cudf.read_parquet(""data"", filters=[('a','!=',5)])
```
Would return 1 , 5, 1 which is all elements from RG0 (RG1 gets filtered out).
Expected output would be 1,1


**Steps/Code to reproduce bug**
```python
df = cudf.DataFrame()

In [6]: df[""a""] = [1,5]*2500 + [5]*5000

In [7]: df.to_parquet(""rg_test.parquet"", row_group_size_rows=5000)

In [8]: cudf.read_parquet(""rg_test.parquet"")
[10000 rows x 1 columns]

In [9]: cudf.read_parquet(""rg_test.parquet"", filters=[(""a"", ""!="", 5)])
[5000 rows x 1 columns]
```

**Expected behavior**
The 5's from row group 0 also get filtered returning only 1's, which is inline with how pyarrow, dask/dask-cudf return return the result.

**Environment overview (please complete the following information)**
 - Environment location: bare-metal
 - Method of cuDF install: conda
   - If method of install is [Docker], provide `docker pull` & `docker run` commands used

**Environment details**
Please run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details

**Additional context**
Add any other context about the problem here.
",2023-01-10T13:15:09Z,0,0,Ayush Dattagupta,Nvidia,True
460,[FEA] Allow keep='all' for nlargest/nsmallest,"**Is your feature request related to a problem? Please describe.**
writing code with `import cudf as pd`

**Describe the solution you'd like**
```
In [1]: import cudf as pd

In [2]: pd.__version__
Out[2]: '22.12.0'

In [3]: df = pd.DataFrame({'a': [1, 2, 3] * 2, 'b': list('abcdef')})

In [4]: df.nlargest(1, 'a', keep='all')
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In [4], line 1
----> 1 df.nlargest(1, 'a', keep='all')

File ~/.local/lib/python3.9/site-packages/nvtx/nvtx.py:101, in annotate.__call__.<locals>.inner(*args, **kwargs)
     98 @wraps(func)
     99 def inner(*args, **kwargs):
    100     libnvtx_push_range(self.attributes, self.domain.handle)
--> 101     result = func(*args, **kwargs)
    102     libnvtx_pop_range(self.domain.handle)
    103     return result

File ~/.local/lib/python3.9/site-packages/cudf/core/dataframe.py:3624, in DataFrame.nlargest(self, n, columns, keep)
   3558 @_cudf_nvtx_annotate
   3559 def nlargest(self, n, columns, keep=""first""):
   3560     """"""Return the first *n* rows ordered by *columns* in descending order.
   3561 
   3562     Return the first *n* rows with the largest values in *columns*, in
   (...)
   3622     Brunei      434000    12128      BN
   3623     """"""
-> 3624     return self._n_largest_or_smallest(True, n, columns, keep)

File ~/.local/lib/python3.9/site-packages/cudf/core/indexed_frame.py:2156, in IndexedFrame._n_largest_or_smallest(self, largest, n, columns, keep)
   2154     return self._gather(indices, keep_index=True, check_bounds=False)
   2155 else:
-> 2156     raise ValueError('keep must be either ""first"", ""last""')

ValueError: keep must be either ""first"", ""last""

In [5]: df.to_pandas().nlargest(1, 'a', keep='all')
Out[5]: 
   a  b
2  3  c
5  3  f
```

**Additional context**
https://github.com/pandas-dev/pandas/pull/21650",2023-01-10T13:33:01Z,0,0,Matthew Farrellee,,False
461,[FEA] support for read_html,"**Is your feature request related to a problem? Please describe.**
working with `import cudf as pd`

**Describe the solution you'd like**
functionality matching https://pandas.pydata.org/docs/reference/api/pandas.read_html.html
```
In [1]: import cudf as pd

In [2]: pd.__version__
Out[2]: '22.12.0'

In [3]: pd.read_html(""https://docs.rapids.ai/maintainers"")
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In [3], line 1
----> 1 pd.read_html(""https://docs.rapids.ai/maintainers"")

AttributeError: module 'cudf' has no attribute 'read_html'

In [4]: import pandas

In [5]: pandas.read_html(""https://docs.rapids.ai/maintainers"")
Out[5]: 
[                                         Phase        Start          End Duration
 0          Development (cuDF/RMM/rapids-cmake)  Thu, Nov 10  Wed, Jan 18  42 days
 1                         Development (others)  Thu, Nov 17  Wed, Jan 25  43 days
 2             Burn Down(cuDF/RMM/rapids-cmake)  Thu, Jan 19  Wed, Jan 25   5 days
 3                           Burn Down (others)  Thu, Jan 26   Wed, Feb 1   5 days
 4  Code Freeze/Testing (cuDF/RMM/rapids-cmake)  Thu, Jan 26  Tue, Jan 31   4 days
 5               Code Freeze/Testing (others) 1   Thu, Feb 2   Tue, Feb 7   4 days
 6                                      Release   Wed, Feb 8   Thu, Feb 9   2 days,
                                          Phase        Start          End Duration
 0          Development (cuDF/RMM/rapids-cmake)  Thu, Jan 19  Wed, Mar 22  42 days
 1                         Development (others)  Thu, Jan 26  Wed, Mar 29  42 days
 2             Burn Down(cuDF/RMM/rapids-cmake)  Thu, Mar 23  Wed, Mar 29   5 days
 3                           Burn Down (others)  Thu, Mar 30   Wed, Apr 5   5 days
 4  Code Freeze/Testing (cuDF/RMM/rapids-cmake)  Thu, Mar 30   Tue, Apr 4   4 days
 5               Code Freeze/Testing (others) 1   Thu, Apr 6  Tue, Apr 11   4 days
 6                                      Release  Wed, Apr 12  Thu, Apr 13   2 days]
```",2023-01-10T14:58:23Z,0,0,Matthew Farrellee,,False
462,[FEA] Series.str.contains support for case,"**Is your feature request related to a problem? Please describe.**
writing code with `import cudf as pd`

**Describe the solution you'd like**
same behavior as `import pandas as pd`
```
In [1]: import cudf as pd

In [2]: pd.__version__
Out[2]: '22.12.0'

In [3]: df = pd.DataFrame({'a': ['one', 'One', 'onE']})

In [4]: df.a.str.contains('one', case=False)
---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
Cell In [4], line 1
----> 1 df.a.str.contains('one', case=False)

File ~/.local/lib/python3.9/site-packages/cudf/core/column/string.py:759, in StringMethods.contains(self, pat, case, flags, na, regex)
    650 r""""""
    651 Test if pattern or regex is contained within a string of a Series or
    652 Index.
   (...)
    756 dtype: bool
    757 """"""  # noqa W605
    758 if case is not True:
--> 759     raise NotImplementedError(""`case` parameter is not yet supported"")
    760 if na is not np.nan:
    761     raise NotImplementedError(""`na` parameter is not yet supported"")

NotImplementedError: `case` parameter is not yet supported

In [5]: df.to_pandas().a.str.contains('one', case=False)
Out[5]: 
0    True
1    True
2    True
Name: a, dtype: bool
```

**Additional context**
https://pandas.pydata.org/docs/reference/api/pandas.Series.str.contains.html",2023-01-10T15:05:46Z,0,0,Matthew Farrellee,,False
463,[FEA] DataFrame.to_markdown support,"**Is your feature request related to a problem? Please describe.**
rewriting code with `import cudf as pd`

**Describe the solution you'd like**
support for `to_markdown` matching https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_markdown.html
",2023-01-10T15:47:29Z,0,0,Matthew Farrellee,,False
464,[BUG] groupby().agg() with list() expansion: TypeError: 'type' object is not iterable,"**Describe the bug**
rewriting code with `import cudf as pd`

**Steps/Code to reproduce bug**
```
In [1]: import cudf as pd

In [2]: pd.__version__
Out[2]: '22.12.0'

In [3]: df = pd.DataFrame({
   ...:     ""col1"": [1, 2, 3, 4, 3],
   ...:     ""col2"": [""a"", ""a"", ""b"", ""b"", ""c""],
   ...:     ""col3"": [""d"", ""e"", ""f"", ""g"", ""h""]
   ...: })

In [4]: df.groupby([""col2""]).agg({""col1"": ""mean"", ""col3"": lambda x: list(x)})
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In [4], line 1
----> 1 df.groupby([""col2""]).agg({""col1"": ""mean"", ""col3"": lambda x: list(x)})

File ~/.local/lib/python3.9/site-packages/nvtx/nvtx.py:101, in annotate.__call__.<locals>.inner(*args, **kwargs)
     98 @wraps(func)
     99 def inner(*args, **kwargs):
    100     libnvtx_push_range(self.attributes, self.domain.handle)
--> 101     result = func(*args, **kwargs)
    102     libnvtx_pop_range(self.domain.handle)
    103     return result

File ~/.local/lib/python3.9/site-packages/cudf/core/groupby/groupby.py:460, in GroupBy.agg(self, func)
    451 column_names, columns, normalized_aggs = self._normalize_aggs(func)
    453 # Note: When there are no key columns, the below produces
    454 # a Float64Index, while Pandas returns an Int64Index
    455 # (GH: 6945)
    456 (
    457     result_columns,
    458     grouped_key_cols,
    459     included_aggregations,
--> 460 ) = self._groupby.aggregate(columns, normalized_aggs)
    462 result_index = self.grouping.keys._from_columns_like_self(
    463     grouped_key_cols,
    464 )
    466 multilevel = _is_multi_agg(func)

File groupby.pyx:309, in cudf._lib.groupby.GroupBy.aggregate()

File groupby.pyx:184, in cudf._lib.groupby.GroupBy.aggregate_internal()

File aggregation.pyx:866, in cudf._lib.aggregation.make_groupby_aggregation()

Cell In [4], line 1, in <lambda>(x)
----> 1 df.groupby([""col2""]).agg({""col1"": ""mean"", ""col3"": lambda x: list(x)})

TypeError: 'type' object is not iterable

In [5]: df.to_pandas().groupby([""col2""]).agg({""col1"": ""mean"", ""col3"": lambda x: list(x)})
Out[5]: 
      col1    col3
col2              
a      1.5  [d, e]
b      3.5  [f, g]
c      3.0     [h]
```
",2023-01-10T15:56:31Z,0,0,Matthew Farrellee,,False
465,"[FEA] cudf.to_datetime support locale-specific formatting (%a, %A, %b, %B, %c, %x, %X, %p)","**Is your feature request related to a problem? Please describe.**
writing code with `import cudf as pd`

**Describe the solution you'd like**
`cudf.to_datetime(..., format=...)` support matching https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html and https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes

related: https://github.com/rapidsai/cudf/issues/12419",2023-01-10T19:22:58Z,0,0,Matthew Farrellee,,False
466,[FEA] support for groupby named aggregates,"**Is your feature request related to a problem? Please describe.**
writing code with `import cudf as pd`

**Describe the solution you'd like**
`groupby().agg()` matching `pandas`

https://pandas.pydata.org/docs/user_guide/groupby.html#groupby-aggregate-named

e.g.
```
In [1]: import cudf as pd

In [2]: pd.__version__
Out[2]: '22.12.0'

In [3]: df = pd.DataFrame({""size"": [""S"", ""S"", ""M"", ""L""], ""price"": [44, 29.99, 10, 19]})

In [4]: df.groupby('size').agg({'price': 'mean'})
Out[4]: 
       price
size        
L     19.000
M     10.000
S     36.995

In [5]: df.groupby('size').agg(mean_price=('price', 'mean'))
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In [5], line 1
----> 1 df.groupby('size').agg(mean_price=('price', 'mean'))

File ~/.local/lib/python3.9/site-packages/nvtx/nvtx.py:101, in annotate.__call__.<locals>.inner(*args, **kwargs)
     98 @wraps(func)
     99 def inner(*args, **kwargs):
    100     libnvtx_push_range(self.attributes, self.domain.handle)
--> 101     result = func(*args, **kwargs)
    102     libnvtx_pop_range(self.domain.handle)
    103     return result

TypeError: agg() got an unexpected keyword argument 'mean_price'

In [6]: df.to_pandas().groupby('size').agg(mean_price=('price', 'mean'))
Out[6]: 
      mean_price
size            
L         19.000
M         10.000
S         36.995

```",2023-01-10T19:57:06Z,0,0,Matthew Farrellee,,False
467,[FEA] support month (M) frequency for date_range and resample,"**Is your feature request related to a problem? Please describe.**
working with `import cudf as pd`

**Describe the solution you'd like**
https://pandas.pydata.org/docs/reference/api/pandas.date_range.html
https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.resample.html

frequencies - https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases
",2023-01-10T23:32:38Z,0,0,Matthew Farrellee,,False
468,[BUG] cudf.pivot_table behavior does not match pandas.pivor_table - mismatched index structure,"**Describe the bug**
working with `import cudf as pd`

**Steps/Code to reproduce bug**
```
In [1]: import cudf as pd

In [2]: pd.__version__
Out[2]: '22.12.00'

In [3]: df = pd.DataFrame({""A"": [""foo"", ""foo"", ""foo"", ""foo"", ""foo"",
   ...:                          ""bar"", ""bar"", ""bar"", ""bar""],
   ...:                    ""B"": [""one"", ""one"", ""one"", ""two"", ""two"",
   ...:                          ""one"", ""one"", ""two"", ""two""],
   ...:                    ""C"": [""small"", ""large"", ""large"", ""small"",
   ...:                          ""small"", ""large"", ""small"", ""small"",
   ...:                          ""large""],
   ...:                    ""D"": [1, 2, 2, 3, 3, 4, 5, 6, 7],
   ...:                    ""E"": [2, 4, 5, 5, 6, 6, 8, 9, 9]})

In [4]: df
Out[4]: 
     A    B      C  D  E
0  foo  one  small  1  2
1  foo  one  large  2  4
2  foo  one  large  2  5
3  foo  two  small  3  5
4  foo  two  small  3  6
5  bar  one  large  4  6
6  bar  one  small  5  8
7  bar  two  small  6  9
8  bar  two  large  7  9

In [5]: pd.pivot_table(df, index=[""A""], columns=[""B""], aggfunc=""size"", fill_value=0)
Out[5]: 
      C       D       E    
B   one two one two one two
A                          
bar   2   2   2   2   2   2
foo   3   2   3   2   3   2

In [6]: import pandas

In [7]: pandas.__version__
Out[7]: '1.5.2'

In [8]: pandas.pivot_table(df.to_pandas(), index=[""A""], columns=[""B""], aggfunc=""size"", fill_value=0)
Out[8]: 
B    one  two
A            
bar    2    2
foo    3    2
```",2023-01-18T14:45:09Z,0,0,Matthew Farrellee,,False
469,[FEA] cudf.crosstab support for margins parameter,"**Is your feature request related to a problem? Please describe.**
writing code with `import cudf as pd`

**Describe the solution you'd like**
matching behavior with [`pandas.crosstab`](https://pandas.pydata.org/docs/reference/api/pandas.crosstab.html)
```
In [1]: import cudf as pd

In [2]: pd.__version__
Out[2]: '22.12.00'

In [3]: df = pd.DataFrame({""A"": [""foo"", ""foo"", ""foo"", ""foo"", ""foo"",
   ...:                          ""bar"", ""bar"", ""bar"", ""bar""],
   ...:                    ""B"": [""one"", ""one"", ""one"", ""two"", ""two"",
   ...:                          ""one"", ""one"", ""two"", ""two""],
   ...:                    ""C"": [""small"", ""large"", ""large"", ""small"",
   ...:                          ""small"", ""large"", ""small"", ""small"",
   ...:                          ""large""],
   ...:                    ""D"": [1, 2, 2, 3, 3, 4, 5, 6, 7],
   ...:                    ""E"": [2, 4, 5, 5, 6, 6, 8, 9, 9]})

In [4]: pd.crosstab(df.A, df.B, margins=True)
---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
Cell In[4], line 1
----> 1 pd.crosstab(df.A, df.B, margins=True)

File /usr/local/lib/python3.8/dist-packages/cudf/core/reshape.py:1296, in crosstab(index, columns, values, rownames, colnames, aggfunc, margins, margins_name, dropna, normalize)
   1293     df[""__dummy__""] = values
   1294     kwargs = {""aggfunc"": aggfunc}
-> 1296 table = pivot_table(
   1297     data=df,
   1298     index=rownames,
   1299     columns=colnames,
   1300     values=""__dummy__"",
   1301     margins=margins,
   1302     margins_name=margins_name,
   1303     dropna=dropna,
   1304     **kwargs,
   1305 )
   1307 return table

File /usr/local/lib/python3.8/dist-packages/cudf/core/reshape.py:1352, in pivot_table(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)
   1323 """"""
   1324 Create a spreadsheet-style pivot table as a DataFrame.
   1325 
   (...)
   1349     An Excel style pivot table.
   1350 """"""
   1351 if margins is not False:
-> 1352     raise NotImplementedError(""margins is not supported yet"")
   1354 if margins_name != ""All"":
   1355     raise NotImplementedError(""margins_name is not supported yet"")

NotImplementedError: margins is not supported yet
```",2023-01-18T14:51:08Z,0,0,Matthew Farrellee,,False
470,[FEA] support passing label to value_counts(),"**Is your feature request related to a problem? Please describe.**
working with `import cudf as pd`

**Describe the solution you'd like**
matching behavior with `pandas`
```
In [1]: import cudf as pd

In [2]: pd.__version__
Out[2]: '22.12.00'

In [3]: df = pd.DataFrame({""XYZ"": [""foo"", ""foo"", ""foo"", ""foo"", ""foo""]})

In [4]: df.value_counts(""XYZ"")
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
Cell In[4], line 1
----> 1 df.value_counts(""XYZ"")

File /usr/local/lib/python3.8/dist-packages/cudf/core/dataframe.py:7188, in DataFrame.value_counts(self, subset, normalize, sort, ascending, dropna)
   7186     diff = set(subset) - set(self._data)
   7187     if len(diff) != 0:
-> 7188         raise KeyError(f""columns {diff} do not exist"")
   7189 columns = list(self._data.names) if subset is None else subset
   7190 result = (
   7191     self.groupby(
   7192         by=columns,
   (...)
   7196     .astype(""int64"")
   7197 )

KeyError: ""columns {'Y', 'X', 'Z'} do not exist""

In [5]: df.to_pandas().value_counts(""XYZ"")
Out[5]: 
XYZ
foo    5
dtype: int64
```

**Additional context**
[`pandas.DataFrame.value_counts`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.value_counts.html) declares support for `list-like` input, but is [implemented](https://github.com/pandas-dev/pandas/blob/v1.5.2/pandas/core/frame.py#L7215) with [`pandas.DataFrame.groupby`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html), which accepts `mapping, function, label, or list of labels`",2023-01-18T15:48:29Z,0,0,Matthew Farrellee,,False
471,[FEA] DataFrame.query support for datetime,"**Is your feature request related to a problem? Please describe.**
working with `import cudf as pd`

**Describe the solution you'd like**
```
In [1]: import cudf as pd

In [2]: pd.__version__
Out[2]: '22.12.00'

In [3]: df = pd.DataFrame({'A': pd.date_range(start=""2023-01-01"", periods=8, freq=""D"")})

In [4]: df.info()
<class 'cudf.core.dataframe.DataFrame'>
RangeIndex: 8 entries, 0 to 7
Data columns (total 1 columns):
 #   Column  Non-Null Count  Dtype
---  ------  --------------  -----
 0   A       8 non-null      datetime64[ns]
dtypes: datetime64[ns](1)
memory usage: 64.0 bytes

In [5]: df.query('A >= ""2023-01-02""')
---------------------------------------------------------------------------
TypingError                               Traceback (most recent call last)
Cell In[5], line 1
----> 1 df.query('A >= ""2023-01-02""')

File /usr/local/lib/python3.8/dist-packages/cudf/core/dataframe.py:4174, in DataFrame.query(self, expr, local_dict)
   4168 callenv = {
   4169     ""locals"": callframe.f_locals,
   4170     ""globals"": callframe.f_globals,
   4171     ""local_dict"": local_dict,
   4172 }
   4173 # Run query
-> 4174 boolmask = queryutils.query_execute(self, expr, callenv)
   4175 return self._apply_boolean_mask(boolmask)

File /usr/local/lib/python3.8/dist-packages/cudf/utils/queryutils.py:248, in query_execute(df, expr, callenv)
    246 # run kernel
    247 args = [out] + colarrays + envargs
--> 248 kernel.forall(nrows)(*args)
    249 out_mask = applyutils.make_aggregate_nullmask(df, columns=columns)
    250 return out.set_mask(out_mask).fillna(False)

File /usr/local/lib/python3.8/dist-packages/numba/cuda/dispatcher.py:438, in ForAll.__call__(self, *args)
    436     specialized = self.dispatcher
    437 else:
--> 438     specialized = self.dispatcher.specialize(*args)
    439 blockdim = self._compute_thread_per_block(specialized)
    440 griddim = (self.ntasks + blockdim - 1) // blockdim

File /usr/local/lib/python3.8/dist-packages/numba/cuda/dispatcher.py:667, in CUDADispatcher.specialize(self, *args)
    664 targetoptions = self.targetoptions
    665 specialization = CUDADispatcher(self.py_func,
    666                                 targetoptions=targetoptions)
--> 667 specialization.compile(argtypes)
    668 specialization.disable_compile()
    669 specialization._specialized = True

File /usr/local/lib/python3.8/dist-packages/numba/cuda/dispatcher.py:794, in CUDADispatcher.compile(self, sig)
    791 if not self._can_compile:
    792     raise RuntimeError(""Compilation disabled"")
--> 794 kernel = _Kernel(self.py_func, argtypes, **self.targetoptions)
    795 # We call bind to force codegen, so that there is a cubin to cache
    796 kernel.bind()

File /usr/local/lib/python3.8/dist-packages/numba/core/compiler_lock.py:35, in _CompilerLock.__call__.<locals>._acquire_compile_lock(*args, **kwargs)
     32 @functools.wraps(func)
     33 def _acquire_compile_lock(*args, **kwargs):
     34     with self:
---> 35         return func(*args, **kwargs)

File /usr/local/lib/python3.8/dist-packages/numba/cuda/dispatcher.py:75, in _Kernel.__init__(self, py_func, argtypes, link, debug, lineinfo, inline, fastmath, extensions, max_registers, opt, device)
     66 self.extensions = extensions or []
     68 nvvm_options = {
     69     'debug': self.debug,
     70     'lineinfo': self.lineinfo,
     71     'fastmath': fastmath,
     72     'opt': 3 if opt else 0
     73 }
---> 75 cres = compile_cuda(self.py_func, types.void, self.argtypes,
     76                     debug=self.debug,
     77                     lineinfo=self.lineinfo,
     78                     inline=inline,
     79                     fastmath=fastmath,
     80                     nvvm_options=nvvm_options)
     81 tgt_ctx = cres.target_context
     82 code = self.py_func.__code__

File /usr/local/lib/python3.8/dist-packages/numba/core/compiler_lock.py:35, in _CompilerLock.__call__.<locals>._acquire_compile_lock(*args, **kwargs)
     32 @functools.wraps(func)
     33 def _acquire_compile_lock(*args, **kwargs):
     34     with self:
---> 35         return func(*args, **kwargs)

File /usr/local/lib/python3.8/dist-packages/numba/cuda/compiler.py:212, in compile_cuda(pyfunc, return_type, args, debug, lineinfo, inline, fastmath, nvvm_options)
    210 from numba.core.target_extension import target_override
    211 with target_override('cuda'):
--> 212     cres = compiler.compile_extra(typingctx=typingctx,
    213                                   targetctx=targetctx,
    214                                   func=pyfunc,
    215                                   args=args,
    216                                   return_type=return_type,
    217                                   flags=flags,
    218                                   locals={},
    219                                   pipeline_class=CUDACompiler)
    221 library = cres.library
    222 library.finalize()

File /usr/local/lib/python3.8/dist-packages/numba/core/compiler.py:716, in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class)
    692 """"""Compiler entry point
    693 
    694 Parameter
   (...)
    712     compiler pipeline
    713 """"""
    714 pipeline = pipeline_class(typingctx, targetctx, library,
    715                           args, return_type, flags, locals)
--> 716 return pipeline.compile_extra(func)

File /usr/local/lib/python3.8/dist-packages/numba/core/compiler.py:452, in CompilerBase.compile_extra(self, func)
    450 self.state.lifted = ()
    451 self.state.lifted_from = None
--> 452 return self._compile_bytecode()

File /usr/local/lib/python3.8/dist-packages/numba/core/compiler.py:520, in CompilerBase._compile_bytecode(self)
    516 """"""
    517 Populate and run pipeline for bytecode input
    518 """"""
    519 assert self.state.func_ir is None
--> 520 return self._compile_core()

File /usr/local/lib/python3.8/dist-packages/numba/core/compiler.py:499, in CompilerBase._compile_core(self)
    497         self.state.status.fail_reason = e
    498         if is_final_pipeline:
--> 499             raise e
    500 else:
    501     raise CompilerError(""All available pipelines exhausted"")

File /usr/local/lib/python3.8/dist-packages/numba/core/compiler.py:486, in CompilerBase._compile_core(self)
    484 res = None
    485 try:
--> 486     pm.run(self.state)
    487     if self.state.cr is not None:
    488         break

File /usr/local/lib/python3.8/dist-packages/numba/core/compiler_machinery.py:368, in PassManager.run(self, state)
    365 msg = ""Failed in %s mode pipeline (step: %s)"" % \
    366     (self.pipeline_name, pass_desc)
    367 patched_exception = self._patch_error(msg, e)
--> 368 raise patched_exception

File /usr/local/lib/python3.8/dist-packages/numba/core/compiler_machinery.py:356, in PassManager.run(self, state)
    354 pass_inst = _pass_registry.get(pss).pass_inst
    355 if isinstance(pass_inst, CompilerPass):
--> 356     self._runPass(idx, pass_inst, state)
    357 else:
    358     raise BaseException(""Legacy pass in use"")

File /usr/local/lib/python3.8/dist-packages/numba/core/compiler_lock.py:35, in _CompilerLock.__call__.<locals>._acquire_compile_lock(*args, **kwargs)
     32 @functools.wraps(func)
     33 def _acquire_compile_lock(*args, **kwargs):
     34     with self:
---> 35         return func(*args, **kwargs)

File /usr/local/lib/python3.8/dist-packages/numba/core/compiler_machinery.py:311, in PassManager._runPass(self, index, pss, internal_state)
    309     mutated |= check(pss.run_initialization, internal_state)
    310 with SimpleTimer() as pass_time:
--> 311     mutated |= check(pss.run_pass, internal_state)
    312 with SimpleTimer() as finalize_time:
    313     mutated |= check(pss.run_finalizer, internal_state)

File /usr/local/lib/python3.8/dist-packages/numba/core/compiler_machinery.py:273, in PassManager._runPass.<locals>.check(func, compiler_state)
    272 def check(func, compiler_state):
--> 273     mangled = func(compiler_state)
    274     if mangled not in (True, False):
    275         msg = (""CompilerPass implementations should return True/False. ""
    276                ""CompilerPass with name '%s' did not."")

File /usr/local/lib/python3.8/dist-packages/numba/core/typed_passes.py:105, in BaseTypeInference.run_pass(self, state)
     99 """"""
    100 Type inference and legalization
    101 """"""
    102 with fallback_context(state, 'Function ""%s"" failed type inference'
    103                       % (state.func_id.func_name,)):
    104     # Type inference
--> 105     typemap, return_type, calltypes, errs = type_inference_stage(
    106         state.typingctx,
    107         state.targetctx,
    108         state.func_ir,
    109         state.args,
    110         state.return_type,
    111         state.locals,
    112         raise_errors=self._raise_errors)
    113     state.typemap = typemap
    114     # save errors in case of partial typing

File /usr/local/lib/python3.8/dist-packages/numba/core/typed_passes.py:83, in type_inference_stage(typingctx, targetctx, interp, args, return_type, locals, raise_errors)
     81     infer.build_constraint()
     82     # return errors in case of partial typing
---> 83     errs = infer.propagate(raise_errors=raise_errors)
     84     typemap, restype, calltypes = infer.unify(raise_errors=raise_errors)
     86 # Output all Numba warnings

File /usr/local/lib/python3.8/dist-packages/numba/core/typeinfer.py:1086, in TypeInferer.propagate(self, raise_errors)
   1083 force_lit_args = [e for e in errors
   1084                   if isinstance(e, ForceLiteralArg)]
   1085 if not force_lit_args:
-> 1086     raise errors[0]
   1087 else:
   1088     raise reduce(operator.or_, force_lit_args)

TypingError: Failed in cuda mode pipeline (step: nopython frontend)
Failed in cuda mode pipeline (step: nopython frontend)
No implementation of function Function(<built-in function ge>) found for signature:
 
 >>> ge(datetime64[ns], Literal[str](2023-01-02))
 
There are 26 candidate implementations:
    - Of which 24 did not match due to:
    Overload of function 'ge': File: <numerous>: Line N/A.
      With argument(s): '(datetime64[ns], unicode_type)':
     No match.
    - Of which 2 did not match due to:
    Operator Overload in function 'ge': File: unknown: Line unknown.
      With argument(s): '(datetime64[ns], unicode_type)':
     No match for registered cases:
      * (bool, bool) -> bool
      * (int8, int8) -> bool
      * (int16, int16) -> bool
      * (int32, int32) -> bool
      * (int64, int64) -> bool
      * (uint8, uint8) -> bool
      * (uint16, uint16) -> bool
      * (uint32, uint32) -> bool
      * (uint64, uint64) -> bool
      * (float32, float32) -> bool
      * (float64, float64) -> bool

During: typing of intrinsic-call at <string> (2)

File ""<string>"", line 2:
<source missing, REPL/exec in use?>

During: resolving callee type: type(CUDADispatcher(<function queryexpr_5817a7cbdce0865b at 0x7f0618ab7f70>))
During: typing of call at <string> (6)


File ""<string>"", line 6:
<source missing, REPL/exec in use?>


In [6]: df.to_pandas().query('A >= ""2023-01-02""')
Out[6]: 
           A
1 2023-01-02
2 2023-01-03
3 2023-01-04
4 2023-01-05
5 2023-01-06
6 2023-01-07
7 2023-01-08
```",2023-01-18T17:39:46Z,0,0,Matthew Farrellee,,False
472,[FEA] Make calling to `purge_nonempty_nulls` optional in various places,"There were reported performance regressions in `make_lists_column` and `make_structs_column` recently after calling to `purge_nonempty_nulls` has been added to these factory functions. We need to sanitize (i.e., remove non-empty nulls) for the input data but both checking and removing non-empty nulls may incur some (even significant) overhead.

I propose adding a parameter to the callers of `purge_nonempty_nulls` such as:
```
superimpose_nulls(..., std::unique_ptr<column>&& input, bool sanitize_input, ....);
```

By having such parameter (`bool sanitize_input`), we can make the calls to `purge_nonempty_nulls` optional. In some places such as data IO or some custom kernel, we know for sure that all the nulls are empty thus we will not have to waste the overhead of checking non-empty nulls.",2023-01-18T17:49:01Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
473,[BUG] DataFrame.query binary-ops do not match pandas,"**Describe the bug**
using `import cudf as pd`

**Steps/Code to reproduce bug**
```
In [1]: import cudf as pd

In [2]: pd.__version__
Out[2]: '22.12.00'

In [3]: df = pd.DataFrame({""A"": [""foo"", ""foo"", ""foo"", ""foo"", ""foo"",
   ...:                          ""bar"", ""bar"", ""bar"", ""bar""],
   ...:                    ""B"": [""one"", ""one"", ""one"", ""two"", ""two"",
   ...:                          ""one"", ""one"", ""two"", ""two""],
   ...:                    ""C"": [""small"", ""large"", ""large"", ""small"",
   ...:                          ""small"", ""large"", ""small"", ""small"",
   ...:                          ""large""],
   ...:                    ""D"": [1, 2, 2, 3, 3, 4, 5, 6, 7],
   ...:                    ""E"": [2, 4, 5, 5, 6, 6, 8, 9, 9]})

In [4]: df.query(""D > 5 & E > 5"")
Out[4]: 
Empty DataFrame
Columns: [A, B, C, D, E]
Index: []

In [5]: df.to_pandas().query(""D > 5 & E > 5"")
Out[5]: 
     A    B      C  D  E
7  bar  two  small  6  9
8  bar  two  large  7  9

In [6]: df.query(""D > 5 | E > 5"")
Out[6]: 
Empty DataFrame
Columns: [A, B, C, D, E]
Index: []

In [7]: df.to_pandas().query(""D > 5 | E > 5"")
Out[7]: 
     A    B      C  D  E
4  foo  two  small  3  6
5  bar  one  large  4  6
6  bar  one  small  5  8
7  bar  two  small  6  9
8  bar  two  large  7  9
```",2023-01-18T17:50:38Z,0,0,Matthew Farrellee,,False
474,[FEA] support datetime property in DataFrame.query,"**Is your feature request related to a problem? Please describe.**
writing code with `import cudf as pd`

**Describe the solution you'd like**
```
In [1]: import cudf as pd

In [2]: pd.__version__
Out[2]: '22.12.00'

In [3]: df = pd.DataFrame({'A': pd.date_range(start=""2023-01-01"", periods=8, freq=""D"")})

In [4]: df.query(""A.dt.month == 1"")
---------------------------------------------------------------------------
TypingError                               Traceback (most recent call last)
Cell In[4], line 1
----> 1 df.query(""A.dt.month == 1"")

File /usr/local/lib/python3.8/dist-packages/cudf/core/dataframe.py:4174, in DataFrame.query(self, expr, local_dict)
   4168 callenv = {
   4169     ""locals"": callframe.f_locals,
   4170     ""globals"": callframe.f_globals,
   4171     ""local_dict"": local_dict,
   4172 }
   4173 # Run query
-> 4174 boolmask = queryutils.query_execute(self, expr, callenv)
   4175 return self._apply_boolean_mask(boolmask)

File /usr/local/lib/python3.8/dist-packages/cudf/utils/queryutils.py:248, in query_execute(df, expr, callenv)
    246 # run kernel
    247 args = [out] + colarrays + envargs
--> 248 kernel.forall(nrows)(*args)
    249 out_mask = applyutils.make_aggregate_nullmask(df, columns=columns)
    250 return out.set_mask(out_mask).fillna(False)

File /usr/local/lib/python3.8/dist-packages/numba/cuda/dispatcher.py:438, in ForAll.__call__(self, *args)
    436     specialized = self.dispatcher
    437 else:
--> 438     specialized = self.dispatcher.specialize(*args)
    439 blockdim = self._compute_thread_per_block(specialized)
    440 griddim = (self.ntasks + blockdim - 1) // blockdim

File /usr/local/lib/python3.8/dist-packages/numba/cuda/dispatcher.py:667, in CUDADispatcher.specialize(self, *args)
    664 targetoptions = self.targetoptions
    665 specialization = CUDADispatcher(self.py_func,
    666                                 targetoptions=targetoptions)
--> 667 specialization.compile(argtypes)
    668 specialization.disable_compile()
    669 specialization._specialized = True

File /usr/local/lib/python3.8/dist-packages/numba/cuda/dispatcher.py:794, in CUDADispatcher.compile(self, sig)
    791 if not self._can_compile:
    792     raise RuntimeError(""Compilation disabled"")
--> 794 kernel = _Kernel(self.py_func, argtypes, **self.targetoptions)
    795 # We call bind to force codegen, so that there is a cubin to cache
    796 kernel.bind()

File /usr/local/lib/python3.8/dist-packages/numba/core/compiler_lock.py:35, in _CompilerLock.__call__.<locals>._acquire_compile_lock(*args, **kwargs)
     32 @functools.wraps(func)
     33 def _acquire_compile_lock(*args, **kwargs):
     34     with self:
---> 35         return func(*args, **kwargs)

File /usr/local/lib/python3.8/dist-packages/numba/cuda/dispatcher.py:75, in _Kernel.__init__(self, py_func, argtypes, link, debug, lineinfo, inline, fastmath, extensions, max_registers, opt, device)
     66 self.extensions = extensions or []
     68 nvvm_options = {
     69     'debug': self.debug,
     70     'lineinfo': self.lineinfo,
     71     'fastmath': fastmath,
     72     'opt': 3 if opt else 0
     73 }
---> 75 cres = compile_cuda(self.py_func, types.void, self.argtypes,
     76                     debug=self.debug,
     77                     lineinfo=self.lineinfo,
     78                     inline=inline,
     79                     fastmath=fastmath,
     80                     nvvm_options=nvvm_options)
     81 tgt_ctx = cres.target_context
     82 code = self.py_func.__code__

File /usr/local/lib/python3.8/dist-packages/numba/core/compiler_lock.py:35, in _CompilerLock.__call__.<locals>._acquire_compile_lock(*args, **kwargs)
     32 @functools.wraps(func)
     33 def _acquire_compile_lock(*args, **kwargs):
     34     with self:
---> 35         return func(*args, **kwargs)

File /usr/local/lib/python3.8/dist-packages/numba/cuda/compiler.py:212, in compile_cuda(pyfunc, return_type, args, debug, lineinfo, inline, fastmath, nvvm_options)
    210 from numba.core.target_extension import target_override
    211 with target_override('cuda'):
--> 212     cres = compiler.compile_extra(typingctx=typingctx,
    213                                   targetctx=targetctx,
    214                                   func=pyfunc,
    215                                   args=args,
    216                                   return_type=return_type,
    217                                   flags=flags,
    218                                   locals={},
    219                                   pipeline_class=CUDACompiler)
    221 library = cres.library
    222 library.finalize()

File /usr/local/lib/python3.8/dist-packages/numba/core/compiler.py:716, in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class)
    692 """"""Compiler entry point
    693 
    694 Parameter
   (...)
    712     compiler pipeline
    713 """"""
    714 pipeline = pipeline_class(typingctx, targetctx, library,
    715                           args, return_type, flags, locals)
--> 716 return pipeline.compile_extra(func)

File /usr/local/lib/python3.8/dist-packages/numba/core/compiler.py:452, in CompilerBase.compile_extra(self, func)
    450 self.state.lifted = ()
    451 self.state.lifted_from = None
--> 452 return self._compile_bytecode()

File /usr/local/lib/python3.8/dist-packages/numba/core/compiler.py:520, in CompilerBase._compile_bytecode(self)
    516 """"""
    517 Populate and run pipeline for bytecode input
    518 """"""
    519 assert self.state.func_ir is None
--> 520 return self._compile_core()

File /usr/local/lib/python3.8/dist-packages/numba/core/compiler.py:499, in CompilerBase._compile_core(self)
    497         self.state.status.fail_reason = e
    498         if is_final_pipeline:
--> 499             raise e
    500 else:
    501     raise CompilerError(""All available pipelines exhausted"")

File /usr/local/lib/python3.8/dist-packages/numba/core/compiler.py:486, in CompilerBase._compile_core(self)
    484 res = None
    485 try:
--> 486     pm.run(self.state)
    487     if self.state.cr is not None:
    488         break

File /usr/local/lib/python3.8/dist-packages/numba/core/compiler_machinery.py:368, in PassManager.run(self, state)
    365 msg = ""Failed in %s mode pipeline (step: %s)"" % \
    366     (self.pipeline_name, pass_desc)
    367 patched_exception = self._patch_error(msg, e)
--> 368 raise patched_exception

File /usr/local/lib/python3.8/dist-packages/numba/core/compiler_machinery.py:356, in PassManager.run(self, state)
    354 pass_inst = _pass_registry.get(pss).pass_inst
    355 if isinstance(pass_inst, CompilerPass):
--> 356     self._runPass(idx, pass_inst, state)
    357 else:
    358     raise BaseException(""Legacy pass in use"")

File /usr/local/lib/python3.8/dist-packages/numba/core/compiler_lock.py:35, in _CompilerLock.__call__.<locals>._acquire_compile_lock(*args, **kwargs)
     32 @functools.wraps(func)
     33 def _acquire_compile_lock(*args, **kwargs):
     34     with self:
---> 35         return func(*args, **kwargs)

File /usr/local/lib/python3.8/dist-packages/numba/core/compiler_machinery.py:311, in PassManager._runPass(self, index, pss, internal_state)
    309     mutated |= check(pss.run_initialization, internal_state)
    310 with SimpleTimer() as pass_time:
--> 311     mutated |= check(pss.run_pass, internal_state)
    312 with SimpleTimer() as finalize_time:
    313     mutated |= check(pss.run_finalizer, internal_state)

File /usr/local/lib/python3.8/dist-packages/numba/core/compiler_machinery.py:273, in PassManager._runPass.<locals>.check(func, compiler_state)
    272 def check(func, compiler_state):
--> 273     mangled = func(compiler_state)
    274     if mangled not in (True, False):
    275         msg = (""CompilerPass implementations should return True/False. ""
    276                ""CompilerPass with name '%s' did not."")

File /usr/local/lib/python3.8/dist-packages/numba/core/typed_passes.py:105, in BaseTypeInference.run_pass(self, state)
     99 """"""
    100 Type inference and legalization
    101 """"""
    102 with fallback_context(state, 'Function ""%s"" failed type inference'
    103                       % (state.func_id.func_name,)):
    104     # Type inference
--> 105     typemap, return_type, calltypes, errs = type_inference_stage(
    106         state.typingctx,
    107         state.targetctx,
    108         state.func_ir,
    109         state.args,
    110         state.return_type,
    111         state.locals,
    112         raise_errors=self._raise_errors)
    113     state.typemap = typemap
    114     # save errors in case of partial typing

File /usr/local/lib/python3.8/dist-packages/numba/core/typed_passes.py:83, in type_inference_stage(typingctx, targetctx, interp, args, return_type, locals, raise_errors)
     81     infer.build_constraint()
     82     # return errors in case of partial typing
---> 83     errs = infer.propagate(raise_errors=raise_errors)
     84     typemap, restype, calltypes = infer.unify(raise_errors=raise_errors)
     86 # Output all Numba warnings

File /usr/local/lib/python3.8/dist-packages/numba/core/typeinfer.py:1086, in TypeInferer.propagate(self, raise_errors)
   1083 force_lit_args = [e for e in errors
   1084                   if isinstance(e, ForceLiteralArg)]
   1085 if not force_lit_args:
-> 1086     raise errors[0]
   1087 else:
   1088     raise reduce(operator.or_, force_lit_args)

TypingError: Failed in cuda mode pipeline (step: nopython frontend)
Failed in cuda mode pipeline (step: nopython frontend)
Unknown attribute 'dt' of type datetime64[ns]

File ""<string>"", line 2:
<source missing, REPL/exec in use?>

During: typing of get attribute at <string> (2)

File ""<string>"", line 2:
<source missing, REPL/exec in use?>

During: resolving callee type: type(CUDADispatcher(<function queryexpr_a1b175044f595522 at 0x7f61a8bade50>))
During: typing of call at <string> (6)


File ""<string>"", line 6:
<source missing, REPL/exec in use?>


In [5]: df.to_pandas().query(""A.dt.month == 1"")
Out[5]: 
           A
0 2023-01-01
1 2023-01-02
2 2023-01-03
3 2023-01-04
4 2023-01-05
5 2023-01-06
6 2023-01-07
7 2023-01-08
```",2023-01-18T17:54:09Z,0,0,Matthew Farrellee,,False
475,[FEA] support inplace for DataFrame.query,"**Is your feature request related to a problem? Please describe.**
using `import cudf as pd`

**Describe the solution you'd like**
```
In [1]: import cudf as pd

In [2]: pd.__version__
Out[2]: '22.12.00'

In [3]: df = pd.DataFrame({""A"": [""foo"", ""foo"", ""foo"", ""foo"", ""foo"",
   ...:                          ""bar"", ""bar"", ""bar"", ""bar""],
   ...:                    ""B"": [""one"", ""one"", ""one"", ""two"", ""two"",
   ...:                          ""one"", ""one"", ""two"", ""two""],
   ...:                    ""C"": [""small"", ""large"", ""large"", ""small"",
   ...:                          ""small"", ""large"", ""small"", ""small"",
   ...:                          ""large""],
   ...:                    ""D"": [1, 2, 2, 3, 3, 4, 5, 6, 7],
   ...:                    ""E"": [2, 4, 5, 5, 6, 6, 8, 9, 9]})

In [4]: df
Out[4]: 
     A    B      C  D  E
0  foo  one  small  1  2
1  foo  one  large  2  4
2  foo  one  large  2  5
3  foo  two  small  3  5
4  foo  two  small  3  6
5  bar  one  large  4  6
6  bar  one  small  5  8
7  bar  two  small  6  9
8  bar  two  large  7  9

In [5]: df.query(""D ** 2 > 8"")
Out[5]: 
     A    B      C  D  E
3  foo  two  small  3  5
4  foo  two  small  3  6
5  bar  one  large  4  6
6  bar  one  small  5  8
7  bar  two  small  6  9
8  bar  two  large  7  9

In [6]: df.query(""D ** 2 > 8"", inplace=True)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[6], line 1
----> 1 df.query(""D ** 2 > 8"", inplace=True)

TypeError: query() got an unexpected keyword argument 'inplace'

In [7]: pdf = df.to_pandas()

In [8]: pdf
Out[8]: 
     A    B      C  D  E
0  foo  one  small  1  2
1  foo  one  large  2  4
2  foo  one  large  2  5
3  foo  two  small  3  5
4  foo  two  small  3  6
5  bar  one  large  4  6
6  bar  one  small  5  8
7  bar  two  small  6  9
8  bar  two  large  7  9

In [9]: pdf.query(""D ** 2 > 8"", inplace=True)

In [10]: pdf
Out[10]: 
     A    B      C  D  E
3  foo  two  small  3  5
4  foo  two  small  3  6
5  bar  one  large  4  6
6  bar  one  small  5  8
7  bar  two  small  6  9
8  bar  two  large  7  9
```
",2023-01-18T17:58:38Z,0,0,Matthew Farrellee,,False
476,[FEA] Int64Index for header=None in CSV reader,"**Is your feature request related to a problem? Please describe.**
Pandas read_csv without header returns integers as column names. cudf read_csv returns integer numbers as strings.

```python
import cudf
from io import StringIO
import pandas as pd
csv=""1, 2, 3\n4, 5, 6\n7, 8, 9\n""

In [11]: cudf.read_csv(StringIO(csv), header=None).columns
Out[11]: Index(['0', '1', '2'], dtype='object')

In [12]: pd.read_csv(StringIO(csv), header=None).columns
Out[12]: Int64Index([0, 1, 2], dtype='int64')
```

Similar issue will happen in JSON reader also after ""values"" support in JSON reader PR https://github.com/rapidsai/cudf/pull/12498#discussion_r1070188657


**Describe the solution you'd like**
The libcudf csv reader does not have a method to return metadata as non-string types. Provide a way to indicate there is no header information in metadata. It's possible to fix this issue at Cython/Python layer. 

**Additional context**
PR Comment  https://github.com/rapidsai/cudf/pull/12498#discussion_r1070188657
",2023-01-19T18:08:12Z,0,0,Karthikeyan,NVIDIA,True
477,[BUG] read_csv always reads values in quotes as strings,"A CSV file containing quoted values:

```python
In [15]: !cat test.csv
""1"",""2""
""3"",""4""
```

has the data type of the quoted values inferred by Pandas:

```python
In [20]: df = pd.read_csv(""test.csv"", index_col=False, header=None)

In [21]: df
Out[21]: 
   0  1
0  1  2
1  3  4

In [22]: df.dtypes
Out[22]: 
0    int64
1    int64
dtype: object
```

but cuDF reads them as strings (""object"" data type):

```python
In [23]: df = cudf.read_csv(""test.csv"", index_col=False, header=None)

In [24]: df
Out[24]: 
   0  1
0  1  2
1  3  4

In [25]: df.dtypes
Out[25]: 
0    object
1    object
dtype: object
```",2023-01-20T16:45:24Z,0,0,Ashwin Srinath,Voltron Data,False
478,[FEA] Refactor `experimental/row_operators.cuh` and make it default,"**Is your feature request related to a problem? Please describe.**
libcudf contains two sets of row operators: [legacy row operators](https://github.com/rapidsai/cudf/blob/branch-23.02/cpp/include/cudf/table/row_operators.cuh) for simple types and [experimental row operators](https://github.com/rapidsai/cudf/blob/branch-23.02/cpp/include/cudf/table/experimental/row_operators.cuh) for complex types. When we have completed ""Part 1"" of #11844, then we can safely refactor the experimental row operators to be the default, and drop the `table::experimental` namespace

**Describe the solution you'd like**
Ultimately we will deprecate the legacy row operators and move the experimental row operators out of the experimental namespace. Please note that the new equality and lexicographic comparators will include ""fast paths"" for simple types (see #11330 and #11129), so the legacy row operators will continue to play a role.

Merge plan for completing the deprecation
- [x] Implement ""fast path"" for equality comparison (closed by #12676)
- [ ] Breakup the row_comparator.cu (#11012)
- [ ] Update all algorithms on new comparator, (complete Part 1 of #11844)
- [ ] Drop the experimental namespace and remove legacy comparator

**Describe alternatives you've considered**
n/a

**Additional context**
See #10186 and follow-on issue #11844 for more information about the nested type comparator project.",2023-01-23T17:56:35Z,0,0,Gregory Kimball,,False
479,[FEA] Support more dtypes in JIT GroupBy `apply`,"**Is your feature request related to a problem? Please describe.**
When https://github.com/rapidsai/cudf/pull/11452 lands, we'll get JIT `Groupby.apply` for a subset of UDFs and importantly, dtypes. However over the summer we only got as far as writing overloads for `float64` and `int64` dtypes in the users source data. It'd be nice if we could support more dtypes, starting at least with the rest of the numeric types.

**Describe the solution you'd like**
Extend the existing `groupby.apply`, `engine='jit'` framework to support the following additional dtypes:
- `float32`
- `int32`
- `int16`
- `int8`
- `uint64`
- `uint32`
- `uint16`
- `uint8`
- `bool`

A lot of the machinery in the original PR is fairly general and should make adding many of these easy-  however there will undoubtedly be edge cases. As such it makes for a pretty good first issue for anyone jumping into the numba extension piece of the codebase.
",2023-01-25T15:45:25Z,0,0,,NVIDIA,True
480,[FEA] Java Table concatenate should gracefully handle the single table degenerate case,"**Is your feature request related to a problem? Please describe.**
When calling Table.concatenate with a single table, it throws an IllegalArgumentException rather than simply returning a new table with the same columns as the single input table.  This forces calling code to special-case this scenario to avoid a crash.

**Describe the solution you'd like**
Table.concatenate should handle a degenerate concatenate of a single table by returning a new table containing the same columns as the original input table (i.e.: zero-copy via refcount)

**Describe alternatives you've considered**
Leave it as-is, callers must each special-case this scenario to avoid a crash.",2023-01-25T19:38:19Z,0,0,Jason Lowe,NVIDIA,True
481,[FEA] support DataFrameGroupBy.shift,"**Is your feature request related to a problem? Please describe.**
working with `import cudf as pd`

**Describe the solution you'd like**

https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.shift.html

```
In [1]: import cudf as pd

In [2]: pd.__version__
Out[2]: '22.12.0'

In [3]: df = pd.DataFrame({'salary': [30000,40000,50000,85000,75000], 'gender': list('MFMFM')})

In [9]: df.groupby('gender')['salary'].transform(lambda x: x.shift(-1))
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In [9], line 1
----> 1 df.groupby('gender')['salary'].transform(lambda x: x.shift(-1))

File ~/.local/lib/python3.9/site-packages/cudf/core/groupby/groupby.py:1070, in GroupBy.transform(self, function)
   1036 """"""Apply an aggregation, then broadcast the result to the group size.
   1037 
   1038 Parameters
   (...)
   1067 agg
   1068 """"""
   1069 try:
-> 1070     result = self.agg(function)
   1071 except TypeError as e:
   1072     raise NotImplementedError(
   1073         ""Currently, `transform()` supports only aggregations.""
   1074     ) from e

File ~/.local/lib/python3.9/site-packages/cudf/core/groupby/groupby.py:1749, in SeriesGroupBy.agg(self, func)
   1748 def agg(self, func):
-> 1749     result = super().agg(func)
   1751     # downcast the result to a Series:
   1752     if len(result._data):

File ~/.local/lib/python3.9/site-packages/nvtx/nvtx.py:101, in annotate.__call__.<locals>.inner(*args, **kwargs)
     98 @wraps(func)
     99 def inner(*args, **kwargs):
    100     libnvtx_push_range(self.attributes, self.domain.handle)
--> 101     result = func(*args, **kwargs)
    102     libnvtx_pop_range(self.domain.handle)
    103     return result

File ~/.local/lib/python3.9/site-packages/cudf/core/groupby/groupby.py:460, in GroupBy.agg(self, func)
    451 column_names, columns, normalized_aggs = self._normalize_aggs(func)
    453 # Note: When there are no key columns, the below produces
    454 # a Float64Index, while Pandas returns an Int64Index
    455 # (GH: 6945)
    456 (
    457     result_columns,
    458     grouped_key_cols,
    459     included_aggregations,
--> 460 ) = self._groupby.aggregate(columns, normalized_aggs)
    462 result_index = self.grouping.keys._from_columns_like_self(
    463     grouped_key_cols,
    464 )
    466 multilevel = _is_multi_agg(func)

File groupby.pyx:309, in cudf._lib.groupby.GroupBy.aggregate()

File groupby.pyx:184, in cudf._lib.groupby.GroupBy.aggregate_internal()

File aggregation.pyx:866, in cudf._lib.aggregation.make_groupby_aggregation()

Cell In [9], line 1, in <lambda>(x)
----> 1 df.groupby('gender')['salary'].transform(lambda x: x.shift(-1))

AttributeError: type object 'cudf._lib.aggregation.GroupbyAggregation' has no attribute 'shift'

In [10]: df.to_pandas().groupby('gender')['salary'].transform(lambda x: x.shift(-1))
Out[10]: 
0    50000.0
1    85000.0
2    75000.0
3        NaN
4        NaN
Name: salary, dtype: float64
```",2023-01-26T19:37:09Z,0,0,Matthew Farrellee,,False
482,[FEA] support PEP 249 – Python Database API Specification v2.0,"**Is your feature request related to a problem? Please describe.**
connecting a database to a DataFrame

**Describe the solution you'd like**
https://peps.python.org/pep-0249/

- [ ] https://pandas.pydata.org/docs/reference/api/pandas.read_sql.html
- [ ] https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html
- [ ] https://pandas.pydata.org/docs/reference/api/pandas.read_sql_table.html
- [ ] https://pandas.pydata.org/docs/reference/api/pandas.read_sql_query.html",2023-01-26T19:49:50Z,0,0,Matthew Farrellee,,False
483,[FEA] support numeric_only parameter for groupby.mean(),"**Is your feature request related to a problem? Please describe.**
working with `import cudf as pd`

**Describe the solution you'd like**
`pandas`  has deprecated dropping invalid columns during aggregation, e.g. `mean(numeric_only=None)`

`cudf` does not support automatic dropping and does not support `numeric_only`

```
In [1]: import cudf as pd

In [2]: pd.__version__
Out[2]: '22.12.0'

In [3]: df = pd.DataFrame({'a': ['apple', 'orange'], 'b': [['one', 'two', 'three'], ['four', 'five']]})

In [4]: df.to_pandas().explode('b').groupby('a').mean()
<ipython-input-4-b1604cfc90a4>:1: FutureWarning: Dropping invalid columns in DataFrameGroupBy.mean is deprecated. In a future version, a TypeError will be raised. Before calling .mean, select only columns which should be valid for the function.
  df.to_pandas().explode('b').groupby('a').mean()
Out[4]: 
Empty DataFrame
Columns: []
Index: [apple, orange]

In [5]: df.explode('b').groupby('a').mean()
---------------------------------------------------------------------------
DataError                                 Traceback (most recent call last)
Cell In [5], line 1
----> 1 df.explode('b').groupby('a').mean()

File ~/.local/lib/python3.9/site-packages/cudf/core/mixins/mixin_factory.py:11, in _partialmethod.<locals>.wrapper(self, *args2, **kwargs2)
     10 def wrapper(self, *args2, **kwargs2):
---> 11     return method(self, *args1, *args2, **kwargs1, **kwargs2)

File ~/.local/lib/python3.9/site-packages/cudf/core/groupby/groupby.py:532, in GroupBy._reduce(self, op, numeric_only, min_count, *args, **kwargs)
    528 if min_count != 0:
    529     raise NotImplementedError(
    530         ""min_count parameter is not implemented yet""
    531     )
--> 532 return self.agg(op)

File ~/.local/lib/python3.9/site-packages/nvtx/nvtx.py:101, in annotate.__call__.<locals>.inner(*args, **kwargs)
     98 @wraps(func)
     99 def inner(*args, **kwargs):
    100     libnvtx_push_range(self.attributes, self.domain.handle)
--> 101     result = func(*args, **kwargs)
    102     libnvtx_pop_range(self.domain.handle)
    103     return result

File ~/.local/lib/python3.9/site-packages/cudf/core/groupby/groupby.py:460, in GroupBy.agg(self, func)
    451 column_names, columns, normalized_aggs = self._normalize_aggs(func)
    453 # Note: When there are no key columns, the below produces
    454 # a Float64Index, while Pandas returns an Int64Index
    455 # (GH: 6945)
    456 (
    457     result_columns,
    458     grouped_key_cols,
    459     included_aggregations,
--> 460 ) = self._groupby.aggregate(columns, normalized_aggs)
    462 result_index = self.grouping.keys._from_columns_like_self(
    463     grouped_key_cols,
    464 )
    466 multilevel = _is_multi_agg(func)

File groupby.pyx:309, in cudf._lib.groupby.GroupBy.aggregate()

File groupby.pyx:199, in cudf._lib.groupby.GroupBy.aggregate_internal()

DataError: All requested aggregations are unsupported.

In [6]: df.explode('b').groupby('a').mean(numeric_only=True)
---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
Cell In [6], line 1
----> 1 df.explode('b').groupby('a').mean(numeric_only=True)

File ~/.local/lib/python3.9/site-packages/cudf/core/mixins/mixin_factory.py:11, in _partialmethod.<locals>.wrapper(self, *args2, **kwargs2)
     10 def wrapper(self, *args2, **kwargs2):
---> 11     return method(self, *args1, *args2, **kwargs1, **kwargs2)

File ~/.local/lib/python3.9/site-packages/cudf/core/groupby/groupby.py:525, in GroupBy._reduce(self, op, numeric_only, min_count, *args, **kwargs)
    502 """"""Compute {op} of group values.
    503 
    504 Parameters
   (...)
    522     * Not supporting: numeric_only, min_count
    523 """"""
    524 if numeric_only:
--> 525     raise NotImplementedError(
    526         ""numeric_only parameter is not implemented yet""
    527     )
    528 if min_count != 0:
    529     raise NotImplementedError(
    530         ""min_count parameter is not implemented yet""
    531     )

NotImplementedError: numeric_only parameter is not implemented yet
```",2023-01-26T20:20:28Z,0,0,Matthew Farrellee,,False
484,[FEA] support numeric_only for DataFrame.corr,"**Is your feature request related to a problem? Please describe.**
working with `import cudf as pd`

**Describe the solution you'd like**
`cudf.DataFrame.corr` matching `pandas.DataFrame.corr` behavior

1. addition of `numeric_only` parameter 
2. default `numeric_only=None` with deprecation warning and lifecycle similar to `pandas`

```
In [1]: import cudf as pd

In [2]: pd.__version__
Out[2]: '22.12.0'

In [3]: df = pd.DataFrame({'a': range(10), 'b': range(10,20), 'c': list('zyxwvutsrq')})

In [4]: df.corr()
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
Cell In [4], line 1
----> 1 df.corr()

File ~/.local/lib/python3.9/site-packages/cudf/core/dataframe.py:6490, in DataFrame.corr(self, method, min_periods)
   6470 """"""Compute the correlation matrix of a DataFrame.
   6471 
   6472 Parameters
   (...)
   6487     The requested correlation matrix.
   6488 """"""
   6489 if method == ""pearson"":
-> 6490     values = self.values
   6491 elif method == ""spearman"":
   6492     values = self.rank().values

File ~/.local/lib/python3.9/site-packages/cudf/core/frame.py:433, in Frame.values(self)
    420 @property
    421 def values(self):
    422     """"""
    423     Return a CuPy representation of the DataFrame.
    424 
   (...)
    431         The values of the DataFrame.
    432     """"""
--> 433     return self.to_cupy()

File ~/.local/lib/python3.9/site-packages/nvtx/nvtx.py:101, in annotate.__call__.<locals>.inner(*args, **kwargs)
     98 @wraps(func)
     99 def inner(*args, **kwargs):
    100     libnvtx_push_range(self.attributes, self.domain.handle)
--> 101     result = func(*args, **kwargs)
    102     libnvtx_pop_range(self.domain.handle)
    103     return result

File ~/.local/lib/python3.9/site-packages/cudf/core/frame.py:533, in Frame.to_cupy(self, dtype, copy, na_value)
    507 @_cudf_nvtx_annotate
    508 def to_cupy(
    509     self,
   (...)
    512     na_value=None,
    513 ) -> cupy.ndarray:
    514     """"""Convert the Frame to a CuPy array.
    515 
    516     Parameters
   (...)
    531     cupy.ndarray
    532     """"""
--> 533     return self._to_array(
    534         (lambda col: col.values.copy())
    535         if copy
    536         else (lambda col: col.values),
    537         cupy.empty,
    538         dtype,
    539         na_value,
    540     )

File ~/.local/lib/python3.9/site-packages/cudf/core/frame.py:498, in Frame._to_array(self, get_column_values, make_empty_matrix, dtype, na_value)
    491 matrix = make_empty_matrix(
    492     shape=(len(self), ncol), dtype=dtype, order=""F""
    493 )
    494 for i, col in enumerate(self._data.values()):
    495     # TODO: col.values may fail if there is nullable data or an
    496     # unsupported dtype. We may want to catch and provide a more
    497     # suitable error.
--> 498     matrix[:, i] = get_column_values_na(col)
    499 return matrix

File cupy/_core/core.pyx:1508, in cupy._core.core.ndarray.__setitem__()

File cupy/_core/_routines_indexing.pyx:51, in cupy._core._routines_indexing._ndarray_setitem()

File cupy/_core/_routines_indexing.pyx:997, in cupy._core._routines_indexing._scatter_op()

File cupy/_core/_kernel.pyx:1292, in cupy._core._kernel.ufunc.__call__()

File cupy/_core/_kernel.pyx:1319, in cupy._core._kernel.ufunc._get_ufunc_kernel()

File cupy/_core/_kernel.pyx:1025, in cupy._core._kernel._get_ufunc_kernel()

File cupy/_core/_kernel.pyx:66, in cupy._core._kernel._get_simple_elementwise_kernel()

File cupy/_core/_kernel.pyx:322, in cupy._core._kernel._get_kernel_params()

File cupy/_core/_kernel.pyx:298, in cupy._core._kernel._ArgInfo.get_param_c_type()

File cupy/_core/_kernel.pyx:285, in cupy._core._kernel._ArgInfo.get_c_type()

File cupy/_core/_scalar.pyx:68, in cupy._core._scalar.get_typename()

File cupy/_core/_scalar.pyx:73, in cupy._core._scalar.get_typename()

KeyError: <class 'numpy.object_'>

In [5]: df.to_pandas().corr()
<ipython-input-5-98783459b7d9>:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.
  df.to_pandas().corr()
Out[5]: 
     a    b
a  1.0  1.0
b  1.0  1.0
```

[`pandas` introduced a `numeric_only` parameter](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html)

```
In [6]: df.to_pandas().corr(numeric_only=True)
Out[6]: 
     a    b
a  1.0  1.0
b  1.0  1.0

In [7]: df.corr(numeric_only=True)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In [7], line 1
----> 1 df.corr(numeric_only=True)

TypeError: corr() got an unexpected keyword argument 'numeric_only'
```
```",2023-01-26T22:41:43Z,0,0,Matthew Farrellee,,False
485,"[DOC] Fix and/or remove cuda.h ""File Not Found"" warning in our 10 Minutes docs.","## Report incorrect documentation

**Describe the problems or issues found in the documentation**
`../../thread/thread_load.cuh(36): warning: cuda.h: [jitify] File not found` warning is in our stable and nightly docs on the site on 10mins.ipynb.  

**Location of incorrect documentation**
https://docs.rapids.ai/api/cudf/stable/user_guide/10min.html#object-creation
https://docs.rapids.ai/api/cudf/nightly/user_guide/10min.html#object-creation

Could we remove this from our docs and/or fix the underlying issue?



",2023-01-27T12:30:41Z,0,0,Taurean Dyer,,False
486,[BUG] DataFrame.dropna does not support axis='index' results in mismatch w/ pandas,"**Describe the bug**
using `import cudf as pd`

**Steps/Code to reproduce bug**
```
>>> import cudf as pd
>>> import numpy as np

>>> df = pd.DataFrame({'a': ['one', 'two', 'three', 'four', np.nan, None, 'NA'], 'b': ['this', 'that', 'the', 'other', np.nan, np.nan, 'Missing'], 'c': ['something', 'or', 'other', None, np.nan, 'hmm', 'NA'], 'd': ['00', '01', '02', '03', None, None, 'Missing']})
>>> df
       a        b          c        d
0    one     this  something       00
1    two     that         or       01
2  three      the      other       02
3   four    other       <NA>       03
4   <NA>     <NA>       <NA>     <NA>
5   <NA>     <NA>        hmm     <NA>
6     NA  Missing         NA  Missing

>>> df.dropna(axis='index')
/opt/conda/envs/rapids/lib/python3.8/site-packages/cudf/core/dataframe.py:1165: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.
  mask = pd.Series(mask)
Empty DataFrame
Columns: []
Index: [0, 1, 2, 3, 4, 5, 6]
>>> df.to_pandas().dropna(axis='index')
       a        b          c        d
0    one     this  something       00
1    two     that         or       01
2  three      the      other       02
6     NA  Missing         NA  Missing
```

**Environment overview (please complete the following information)**
 - rapidsai/rapidsai-nightly:cuda11.5-runtime-ubuntu20.04-py3.8
 - sha256:4a69370ad40f47a71404805fe2b1dd0a5cf28d7f6a5d4dd1cad419f8a3c8ce5f
 - 30 jan 2023

![Screenshot from 2023-01-30 14-12-11](https://user-images.githubusercontent.com/112653/215572351-96fdd5bc-b96a-47ed-a55b-f84e5cbd9603.png)
![Screenshot from 2023-01-30 14-12-02](https://user-images.githubusercontent.com/112653/215572362-8705fc0a-9ae8-4e94-a62f-5f99a32ff235.png)

",2023-01-30T19:13:24Z,0,0,Matthew Farrellee,,False
487,[FEA] add DataFrame.interpolate methods supported by cupyx,"**Is your feature request related to a problem? Please describe.**
working with `import cudf as pd`

**Describe the solution you'd like**
[cudf.DataFrame.interpolate](https://docs.rapids.ai/api/cudf/stable/api_docs/api/cudf.dataframe.interpolate) supports `method=""linear""`

[pandas.DataFrame.interpolate](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html) supports a number of methods, including
- [ ] time
- [ ] krogh
- [ ] piecewise_polynomial
- [ ] spline
- [ ] pchip
- [ ] akima
- [ ] cubicspline

[cupyx.scipy.interpolate](https://docs.cupy.dev/en/latest/reference/scipy_interpolate.html) provides implementations for most of these

extend the supported methods by those available in `cupyx`

",2023-01-31T00:23:42Z,0,0,Matthew Farrellee,,False
488,[FEA] support cudf.Series.at,"**Is your feature request related to a problem? Please describe.**
working with `import cudf as pd`

**Describe the solution you'd like**
functionality matching https://pandas.pydata.org/docs/reference/api/pandas.Series.at.html
![image](https://user-images.githubusercontent.com/112653/215628566-44bab64b-d4b3-4250-a97a-ce28082f9c7f.png)
",2023-01-31T00:37:27Z,0,0,Matthew Farrellee,,False
489,[FEA] aggregation_request should view rather than own its aggregations,"**Is your feature request related to a problem? Please describe.**
The `aggregation_request` struct contains a `std::vector<std::unique_ptr<groupby_aggregation>>`. This forces the request to take ownership of the `groupy_aggregation`s, which leads to cumbersome code filled with unnecessary moves (unless the aggregations are temporaries, in which case ownership transfer is not an issue).

**Describe the solution you'd like**
The request should instead contain a vector of aggregations so that it can copy the aggregations when the request is constructed. That will allow calling code to pass local objects or temporaries equally easily when building up an aggregation request.

**Describe alternatives you've considered**
In an ideal world the request would not own aggregations at all and would instead own const references to the aggregations (const so that C++ lifetime extension rules would allow passing rvalues when building up the vector). However, container types cannot directly hold references, so this isn't an option.

An alternative would be to instead store instances of `std::reference_wrapper<groupby_aggregation>`, but since those create an extra level of indirection they won't bind directly to a temporary and will therefore prohibit usage of rvalues, at which point they are no better than storing by value.

**Additional context**
Currently `aggregation_request`s are typically built up by manually inserting elements into the vector of aggregations. A constructor accepting a (host) span may be a reasonable alternative, but it's not necessary to make the other changes proposed above.
",2023-01-31T00:56:32Z,0,0,Vyas Ramasubramani,@rapidsai,True
490,[BUG] `cudf::strings::from_integers` does not appear to be `compute-sanitizer --tool initcheck` clean,"Conversion from (at least) integer to string columns (done a lot for printing on the cudf-python side of things) appears to have uninitialized device memory accesses. Related #8873.

Consider the following:
```c++
#include <algorithm>
#include <cstdint>
#include <vector>

#include <cudf/column/column.hpp>
#include <cudf/column/column_view.hpp>
#include <cudf/strings/convert/convert_integers.hpp>
#include <cudf/types.hpp>

int main(int argc, char **argv) {
  using T = int64_t;
  using size_type = cudf::size_type;
  size_type size;
  if (argc > 1) {
    size = std::stoi(argv[1]);
  } else {
    size = 1;
  }
  std::vector<T> data{size};
  std::generate(data.begin(), data.end(), []() { return 1; });
  auto column = cudf::column{cudf::data_type{cudf::type_to_id<T>()}, size,
                             rmm::device_buffer{data.data(), size * sizeof(T),
                                                cudf::get_default_stream()}};
  cudf::get_default_stream().synchronize();
  auto string_col = cudf::strings::from_integers(
      column.view(), rmm::mr::get_current_device_resource());
  cudf::get_default_stream().synchronize();
  return 0;
}
```

When run as:
```
$ compute-sanitizer --tool initcheck ./test
========= COMPUTE-SANITIZER
========= Uninitialized __global__ memory read of size 4 bytes
=========     at 0x1c0 in void cub::CUB_101702_860_NS::DeviceScanKernel<cub::CUB_101702_860_NS::DeviceScanPolicy<long>::Policy600, int *, cudf::detail::sizes_to_offsets_iterator<int *, long>, cub::CUB_101702_860_NS::ScanTileState<long, (bool)1>, thrust::plus<void>, cub::CUB_101702_860_NS::detail::InputValue<long, long *>, int>(T2, T3, T4, int, T5, T6, T7)
=========     by thread (1,0,0) in block (0,0,0)
=========     Address 0x7f26c3e00204
=========     Saved host backtrace up to driver entry point at kernel launch time
=========     Host Frame: [0x304e32]
=========                in /usr/lib/x86_64-linux-gnu/libcuda.so.1
=========     Host Frame: [0x1488c]
=========                in /home/wence/Documents/src/rapids/compose/etc/conda/cuda_11.8/envs/rapids/lib/libcudart.so.11.0
=========     Host Frame:cudaLaunchKernel [0x6c318]
=========                in /home/wence/Documents/src/rapids/compose/etc/conda/cuda_11.8/envs/rapids/lib/libcudart.so.11.0
=========     Host Frame:cudf::detail::sizes_to_offsets_iterator<int*, long> thrust::cuda_cub::detail::exclusive_scan_n_impl<thrust::detail::execute_with_allocator<rmm::mr::thrust_allocator<char>, thrust::cuda_cub::execute_on_stream_base>, int*, long, cudf::detail::sizes_to_offsets_iterator<int*, long>, long, thrust::plus<void> >(thrust::cuda_cub::execution_policy<thrust::detail::execute_with_allocator<rmm::mr::thrust_allocator<char>, thrust::cuda_cub::execute_on_stream_base> >&, int*, long, cudf::detail::sizes_to_offsets_iterator<int*, long>, long, thrust::plus<void>) [0x161bf95]
=========                in /home/wence/Documents/src/rapids/cudf/cpp/build/libcudf.so
=========     Host Frame:auto cudf::detail::sizes_to_offsets<int*, int*>(int*, int*, int*, rmm::cuda_stream_view) [0x161c612]
=========                in /home/wence/Documents/src/rapids/cudf/cpp/build/libcudf.so
=========     Host Frame:auto cudf::strings::detail::make_strings_children<cudf::strings::detail::(anonymous namespace)::from_integers_fn<long> >(cudf::strings::detail::(anonymous namespace)::from_integers_fn<long>, int, int, rmm::cuda_stream_view, rmm::mr::device_memory_resource*) [0x264fb1f]
=========                in /home/wence/Documents/src/rapids/cudf/cpp/build/libcudf.so
=========     Host Frame:std::unique_ptr<cudf::column, std::default_delete<cudf::column> > cudf::strings::detail::(anonymous namespace)::dispatch_from_integers_fn::operator()<long, (void*)0>(cudf::column_view const&, rmm::cuda_stream_view, rmm::mr::device_memory_resource*) const [0x264feb1]
=========                in /home/wence/Documents/src/rapids/cudf/cpp/build/libcudf.so
=========     Host Frame:cudf::strings::detail::from_integers(cudf::column_view const&, rmm::cuda_stream_view, rmm::mr::device_memory_resource*) [0x264a1e9]
=========                in /home/wence/Documents/src/rapids/cudf/cpp/build/libcudf.so
=========     Host Frame:cudf::strings::from_integers(cudf::column_view const&, rmm::mr::device_memory_resource*) [0x264a2c7]
=========                in /home/wence/Documents/src/rapids/cudf/cpp/build/libcudf.so
=========     Host Frame:/home/wence/Documents/src/rapids/doodles/c++/test_from_integers.cpp:26:main [0x1ae5e]
=========                in /home/wence/Documents/src/rapids/doodles/c++/./test
=========     Host Frame:__libc_start_main [0x24083]
=========                in /usr/lib/x86_64-linux-gnu/libc.so.6
=========     Host Frame: [0x13079]
=========                in /home/wence/Documents/src/rapids/doodles/c++/./test
=========
========= ERROR SUMMARY: 1 error
```

This _looks_ like an off-by-one, but my tracking through the libcudf side of things didn't spot anything, so perhaps it is a bug in thrust or CUB.

**Environment overview (please complete the following information)**
 - Environment location: [Bare-metal, Docker, Cloud(specify cloud provider)]
 - Method of cuDF install: [conda, Docker, or from source]
   - If method of install is [Docker], provide `docker pull` & `docker run` commands used

**Environment details**

<details><summary>Click here to see environment details</summary><pre>

     **git***
     commit 55ef6018ed92bfd6d3cbb67d9736c72aadaa2668 (HEAD -> branch-23.02)
     Author: Karthikeyan <6488848+karthikeyann@users.noreply.github.com>
     Date:   Sat Jan 28 07:09:29 2023 +0530

     Add JSON Writer (#12474)

     Adds JSON writer with nested support.
     It supports numeric, datetime, duration, strings,  nested types such as struct and list types.
     `orient='records'` is only supported now, with `lines=True/False`.
     Usage: `df.to_json(engine='cudf')`

     closes https://github.com/rapidsai/cudf/issues/11165

     Authors:
     - Karthikeyan (https://github.com/karthikeyann)

     Approvers:
     - Vukasin Milovanovic (https://github.com/vuule)
     - GALI PREM SAGAR (https://github.com/galipremsagar)
     - David Wendt (https://github.com/davidwendt)
     - Michael Wang (https://github.com/isVoid)
     - Robert Maynard (https://github.com/robertmaynard)

     URL: https://github.com/rapidsai/cudf/pull/12474
     **git submodules***

     ***OS Information***
     DISTRIB_ID=Ubuntu
     DISTRIB_RELEASE=20.04
     DISTRIB_CODENAME=focal
     DISTRIB_DESCRIPTION=""Ubuntu 20.04.5 LTS""
     NAME=""Ubuntu""
     VERSION=""20.04.5 LTS (Focal Fossa)""
     ID=ubuntu
     ID_LIKE=debian
     PRETTY_NAME=""Ubuntu 20.04.5 LTS""
     VERSION_ID=""20.04""
     HOME_URL=""https://www.ubuntu.com/""
     SUPPORT_URL=""https://help.ubuntu.com/""
     BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
     PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
     VERSION_CODENAME=focal
     UBUNTU_CODENAME=focal
     Linux shallot 5.15.0-58-generic NVIDIA/cub#64-Ubuntu SMP Thu Jan 5 11:43:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux

     ***GPU Information***
     Wed Feb  1 16:01:12 2023
     +-----------------------------------------------------------------------------+
     | NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
     |-------------------------------+----------------------+----------------------+
     | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
     | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
     |                               |                      |               MIG M. |
     |===============================+======================+======================|
     |   0  NVIDIA RTX A6000    On   | 00000000:17:00.0 Off |                  Off |
     | 30%   31C    P8    21W / 300W |      6MiB / 49140MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     |   1  NVIDIA RTX A6000    On   | 00000000:B3:00.0  On |                  Off |
     | 30%   43C    P3    49W / 300W |   1343MiB / 49140MiB |     13%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+

     +-----------------------------------------------------------------------------+
     | Processes:                                                                  |
     |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
     |        ID   ID                                                   Usage      |
     |=============================================================================|
     |    0   N/A  N/A      3089      G   /usr/lib/xorg/Xorg                  4MiB |
     |    1   N/A  N/A      3089      G   /usr/lib/xorg/Xorg                738MiB |
     |    1   N/A  N/A      3267      G   /usr/bin/gnome-shell              185MiB |
     |    1   N/A  N/A     44652      G   ...veSuggestionsOnlyOnDemand      154MiB |
     |    1   N/A  N/A     45338      G   /usr/bin/wezterm-gui               11MiB |
     +-----------------------------------------------------------------------------+

     ***CPU***
     Architecture:                    x86_64
     CPU op-mode(s):                  32-bit, 64-bit
     Byte Order:                      Little Endian
     Address sizes:                   46 bits physical, 48 bits virtual
     CPU(s):                          32
     On-line CPU(s) list:             0-31
     Thread(s) per core:              2
     Core(s) per socket:              16
     Socket(s):                       1
     NUMA node(s):                    1
     Vendor ID:                       GenuineIntel
     CPU family:                      6
     Model:                           85
     Model name:                      Intel(R) Xeon(R) Gold 6226R CPU @ 2.90GHz
     Stepping:                        7
     CPU MHz:                         2900.000
     CPU max MHz:                     3900.0000
     CPU min MHz:                     1200.0000
     BogoMIPS:                        5800.00
     Virtualization:                  VT-x
     L1d cache:                       512 KiB
     L1i cache:                       512 KiB
     L2 cache:                        16 MiB
     L3 cache:                        22 MiB
     NUMA node0 CPU(s):               0-31
     Vulnerability Itlb multihit:     KVM: Mitigation: VMX disabled
     Vulnerability L1tf:              Not affected
     Vulnerability Mds:               Not affected
     Vulnerability Meltdown:          Not affected
     Vulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable
     Vulnerability Retbleed:          Mitigation; Enhanced IBRS
     Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp
     Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
     Vulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence
     Vulnerability Srbds:             Not affected
     Vulnerability Tsx async abort:   Mitigation; TSX disabled
     Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke avx512_vnni md_clear flush_l1d arch_capabilities

     ***CMake***
     /home/wence/Documents/src/rapids/compose/etc/conda/cuda_11.8/envs/rapids/bin/cmake
     cmake version 3.25.2

     CMake suite maintained and supported by Kitware (kitware.com/cmake).

     ***g++***
     /usr/local/sbin/g++
     g++ (conda-forge gcc 9.5.0-19) 9.5.0
     Copyright (C) 2019 Free Software Foundation, Inc.
     This is free software; see the source for copying conditions.  There is NO
     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


     ***nvcc***
     /usr/local/sbin/nvcc
     nvcc: NVIDIA (R) Cuda compiler driver
     Copyright (c) 2005-2022 NVIDIA Corporation
     Built on Wed_Sep_21_10:33:58_PDT_2022
     Cuda compilation tools, release 11.8, V11.8.89
     Build cuda_11.8.r11.8/compiler.31833905_0

     ***Python***
     /home/wence/Documents/src/rapids/compose/etc/conda/cuda_11.8/envs/rapids/bin/python
     Python 3.8.15

     ***Environment Variables***
     PATH                            : /usr/local/sbin:/usr/local/bin:/home/wence/Documents/src/rapids/compose/etc/conda/cuda_11.8/envs/rapids/bin:/home/wence/Documents/src/rapids/compose/etc/conda/cuda_11.8/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/cuda/bin
     LD_LIBRARY_PATH                 : /home/wence/Documents/src/rapids/compose/etc/conda/cuda_11.8/envs/rapids/lib:/home/wence/Documents/src/rapids/compose/etc/conda/cuda_11.8/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/lib:/home/wence/Documents/src/rapids/rmm/build/release:/home/wence/Documents/src/rapids/cudf/cpp/build/release:/home/wence/Documents/src/rapids/raft/cpp/build/release:/home/wence/Documents/src/rapids/cuml/cpp/build/release:/home/wence/Documents/src/rapids/cugraph/cpp/build/release:/home/wence/Documents/src/rapids/cuspatial/cpp/build/release
     NUMBAPRO_NVVM                   :
     NUMBAPRO_LIBDEVICE              :
     CONDA_PREFIX                    : /home/wence/Documents/src/rapids/compose/etc/conda/cuda_11.8/envs/rapids
     PYTHON_PATH                     :

     ***conda packages***
     /home/wence/Documents/src/rapids/compose/etc/conda/cuda_11.8/bin/conda

     # packages in environment at /home/wence/Documents/src/rapids/compose/etc/conda/cuda_11.8/envs/rapids:
     #
     # Name                    Version                   Build  Channel
     _libgcc_mutex             0.1                 conda_forge    conda-forge
     _openmp_mutex             4.5                  2_kmp_llvm    conda-forge
     _sysroot_linux-64_curr_repodata_hack 3                   h5bd9786_13    conda-forge
     aiobotocore               2.4.2              pyhd8ed1ab_0    conda-forge
     aiohttp                   3.8.3            py38h0a891b7_1    conda-forge
     aioitertools              0.11.0             pyhd8ed1ab_0    conda-forge
     aiosignal                 1.3.1              pyhd8ed1ab_0    conda-forge
     alabaster                 0.7.13             pyhd8ed1ab_0    conda-forge
     anyio                     3.6.2              pyhd8ed1ab_0    conda-forge
     appdirs                   1.4.4              pyh9f0ad1d_0    conda-forge
     argon2-cffi               21.3.0             pyhd8ed1ab_0    conda-forge
     argon2-cffi-bindings      21.2.0           py38h0a891b7_3    conda-forge
     arrow-cpp                 10.0.1           ha770c72_6_cpu    conda-forge
     asttokens                 2.2.1              pyhd8ed1ab_0    conda-forge
     async-timeout             4.0.2              pyhd8ed1ab_0    conda-forge
     attrs                     22.2.0             pyh71513ae_0    conda-forge
     aws-c-auth                0.6.21               hd93a3ba_3    conda-forge
     aws-c-cal                 0.5.20               hff2c3d7_3    conda-forge
     aws-c-common              0.8.5                h166bdaf_0    conda-forge
     aws-c-compression         0.2.16               hf5f93bc_0    conda-forge
     aws-c-event-stream        0.2.18               h57874a7_0    conda-forge
     aws-c-http                0.7.0                h96ef541_0    conda-forge
     aws-c-io                  0.13.12              h57ca295_1    conda-forge
     aws-c-mqtt                0.7.13              h0b5698f_12    conda-forge
     aws-c-s3                  0.2.3                h82cbbf9_0    conda-forge
     aws-c-sdkutils            0.1.7                hf5f93bc_0    conda-forge
     aws-checksums             0.1.14               h6027aba_0    conda-forge
     aws-crt-cpp               0.18.16             hf80f573_10    conda-forge
     aws-sam-translator        1.58.1             pyhd8ed1ab_0    conda-forge
     aws-sdk-cpp               1.10.57              ha834a50_1    conda-forge
     aws-xray-sdk              2.11.0             pyhd8ed1ab_0    conda-forge
     babel                     2.11.0             pyhd8ed1ab_0    conda-forge
     backcall                  0.2.0              pyh9f0ad1d_0    conda-forge
     backports                 1.0                pyhd8ed1ab_3    conda-forge
     backports.functools_lru_cache 1.6.4              pyhd8ed1ab_0    conda-forge
     backports.zoneinfo        0.2.1            py38h0a891b7_7    conda-forge
     bcrypt                    3.2.2            py38h0a891b7_1    conda-forge
     beautifulsoup4            4.11.1             pyha770c72_0    conda-forge
     binutils                  2.39                 hdd6e379_1    conda-forge
     binutils_impl_linux-64    2.39                 he00db2b_1    conda-forge
     binutils_linux-64         2.39                h5fc0e48_11    conda-forge
     blas                      1.0                         mkl    conda-forge
     bleach                    6.0.0              pyhd8ed1ab_0    conda-forge
     blosc                     1.21.3               hafa529b_0    conda-forge
     bokeh                     2.4.3              pyhd8ed1ab_3    conda-forge
     boost-cpp                 1.78.0               h75c5d50_1    conda-forge
     boto3                     1.24.59            pyhd8ed1ab_0    conda-forge
     botocore                  1.27.59            pyhd8ed1ab_0    conda-forge
     branca                    0.6.0              pyhd8ed1ab_0    conda-forge
     breathe                   4.34.0             pyhd8ed1ab_0    conda-forge
     brotli                    1.0.9                h166bdaf_8    conda-forge
     brotli-bin                1.0.9                h166bdaf_8    conda-forge
     brotlipy                  0.7.0           py38h0a891b7_1005    conda-forge
     bzip2                     1.0.8                h7f98852_4    conda-forge
     c-ares                    1.18.1               h7f98852_0    conda-forge
     c-compiler                1.3.0                h7f98852_0    conda-forge
     ca-certificates           2022.12.7            ha878542_0    conda-forge
     cachetools                5.3.0              pyhd8ed1ab_0    conda-forge
     cairo                     1.16.0            ha61ee94_1014    conda-forge
     certifi                   2022.12.7          pyhd8ed1ab_0    conda-forge
     cffi                      1.15.1           py38h4a40e3a_3    conda-forge
     cfgv                      3.3.1              pyhd8ed1ab_0    conda-forge
     cfitsio                   4.2.0                hd9d235c_0    conda-forge
     cfn-lint                  0.24.8                   py38_0    conda-forge
     charset-normalizer        2.1.1              pyhd8ed1ab_0    conda-forge
     clang                     11.1.0               ha770c72_1    conda-forge
     clang-11                  11.1.0          default_ha53f305_1    conda-forge
     clang-tools               11.1.0          default_ha53f305_1    conda-forge
     clangxx                   11.1.0          default_ha53f305_1    conda-forge
     click                     8.1.3           unix_pyhd8ed1ab_2    conda-forge
     click-plugins             1.1.1                      py_0    conda-forge
     cligj                     0.7.2              pyhd8ed1ab_1    conda-forge
     cloudpickle               2.2.1              pyhd8ed1ab_0    conda-forge
     cmake                     3.25.2               h077f3f9_0    conda-forge
     cmake_setuptools          0.1.3                      py_0    rapidsai
     colorama                  0.4.6              pyhd8ed1ab_0    conda-forge
     comm                      0.1.2              pyhd8ed1ab_0    conda-forge
     commonmark                0.9.1                      py_0    conda-forge
     contourpy                 1.0.7            py38hfbd4bf9_0    conda-forge
     coverage                  7.1.0            py38h1de0b5d_0    conda-forge
     cryptography              39.0.0           py38h3d167d9_0    conda-forge
     cubinlinker               0.2.2            py38h7144610_0    rapidsai
     cuda-profiler-api         11.8.86                       0    nvidia
     cuda-python               11.8.1           py38h241159d_2    conda-forge
     cudatoolkit               11.8.0              h37601d7_11    conda-forge
     cupy                      11.5.0           py38h405e1b6_0    conda-forge
     curl                      7.87.0               hdc1c0ab_0    conda-forge
     cxx-compiler              1.3.0                h4bd325d_0    conda-forge
     cycler                    0.11.0             pyhd8ed1ab_0    conda-forge
     cyrus-sasl                2.1.27               h9033bb2_6    conda-forge
     cython                    0.29.33          py38h8dc9893_0    conda-forge
     cytoolz                   0.12.0           py38h0a891b7_1    conda-forge
     dask                      2023.1.0+17.g2a2b9d3c           dev_0    <develop>
     dask-core                 2023.1.1a230127 py_g185eed2c9_23    dask/label/dev
     dask-cuda                 23.2.0a0+41.g66a6a46.dirty          pypi_0    pypi
     dask-glm                  0.2.1.dev52+g1daf4c5          pypi_0    pypi
     dask-ml                   2022.5.27          pyhd8ed1ab_0    conda-forge
     dataclasses               0.8                pyhc8e2a94_3    conda-forge
     datasets                  1.18.3             pyhd8ed1ab_0    conda-forge
     debugpy                   1.6.6            py38h8dc9893_0    conda-forge
     decopatch                 1.4.10             pyhd8ed1ab_0    conda-forge
     decorator                 5.1.1              pyhd8ed1ab_0    conda-forge
     defusedxml                0.7.1              pyhd8ed1ab_0    conda-forge
     dill                      0.3.6              pyhd8ed1ab_1    conda-forge
     distlib                   0.3.6              pyhd8ed1ab_0    conda-forge
     distributed               2023.1.0+9.g0161991f.dirty           dev_0    <develop>
     distro                    1.8.0              pyhd8ed1ab_0    conda-forge
     dlpack                    0.5                  h9c3ff4c_0    conda-forge
     docker-py                 6.0.0              pyhd8ed1ab_0    conda-forge
     docutils                  0.19             py38h578d9bd_1    conda-forge
     doxygen                   1.8.20               had0d8f1_0    conda-forge
     ecdsa                     0.18.0             pyhd8ed1ab_1    conda-forge
     entrypoints               0.4                pyhd8ed1ab_0    conda-forge
     exceptiongroup            1.1.0              pyhd8ed1ab_0    conda-forge
     execnet                   1.9.0              pyhd8ed1ab_0    conda-forge
     executing                 1.2.0              pyhd8ed1ab_0    conda-forge
     expat                     2.5.0                h27087fc_0    conda-forge
     faiss-proc                1.0.0                      cuda    rapidsai
     fastavro                  1.7.1            py38h1de0b5d_0    conda-forge
     fastrlock                 0.8              py38hfa26641_3    conda-forge
     filelock                  3.9.0              pyhd8ed1ab_0    conda-forge
     fiona                     1.8.22           py38hc72d8cd_2    conda-forge
     flask                     2.1.3              pyhd8ed1ab_0    conda-forge
     flask_cors                3.0.10             pyhd3deb0d_0    conda-forge
     flit-core                 3.8.0              pyhd8ed1ab_0    conda-forge
     folium                    0.14.0             pyhd8ed1ab_0    conda-forge
     font-ttf-dejavu-sans-mono 2.37                 hab24e00_0    conda-forge
     font-ttf-inconsolata      3.000                h77eed37_0    conda-forge
     font-ttf-source-code-pro  2.038                h77eed37_0    conda-forge
     font-ttf-ubuntu           0.83                 hab24e00_0    conda-forge
     fontconfig                2.14.2               h14ed4e7_0    conda-forge
     fonts-conda-ecosystem     1                             0    conda-forge
     fonts-conda-forge         1                             0    conda-forge
     fonttools                 4.38.0           py38h0a891b7_1    conda-forge
     freetype                  2.12.1               hca18f0e_1    conda-forge
     freexl                    1.0.6                h166bdaf_1    conda-forge
     frozenlist                1.3.3            py38h0a891b7_0    conda-forge
     fsspec                    2023.1.0           pyhd8ed1ab_0    conda-forge
     future                    0.18.3             pyhd8ed1ab_0    conda-forge
     gcc                       9.5.0               h1fea6ba_11    conda-forge
     gcc_impl_linux-64         9.5.0               h99780fb_19    conda-forge
     gcc_linux-64              9.5.0               h4258300_11    conda-forge
     gcovr                     5.1                pyhd8ed1ab_0    conda-forge
     gdal                      3.5.3           py38h58634bd_15    conda-forge
     geopandas                 0.12.2             pyhd8ed1ab_0    conda-forge
     geopandas-base            0.12.2             pyha770c72_0    conda-forge
     geos                      3.11.1               h27087fc_0    conda-forge
     geotiff                   1.7.1                h7a142b4_6    conda-forge
     gettext                   0.21.1               h27087fc_0    conda-forge
     gflags                    2.2.2             he1b5a44_1004    conda-forge
     giflib                    5.2.1                h36c2ea0_2    conda-forge
     glog                      0.6.0                h6f12383_0    conda-forge
     gmock                     1.10.0               h4bd325d_7    conda-forge
     gmp                       6.2.1                h58526e2_0    conda-forge
     gmpy2                     2.1.2            py38h793c122_1    conda-forge
     graphql-core              3.2.3              pyhd8ed1ab_0    conda-forge
     greenlet                  2.0.2            py38h8dc9893_0    conda-forge
     gtest                     1.10.0               h4bd325d_7    conda-forge
     gxx                       9.5.0               h1fea6ba_11    conda-forge
     gxx_impl_linux-64         9.5.0               h99780fb_19    conda-forge
     gxx_linux-64              9.5.0               h43f449f_11    conda-forge
     hdbscan                   0.8.29           py38h26c90d9_1    conda-forge
     hdf4                      4.2.15               h9772cbc_5    conda-forge
     hdf5                      1.12.2          nompi_h4df4325_101    conda-forge
     heapdict                  1.0.1                      py_0    conda-forge
     huggingface_hub           0.12.0             pyhd8ed1ab_0    conda-forge
     hypothesis                6.65.2             pyha770c72_0    conda-forge
     icu                       70.1                 h27087fc_0    conda-forge
     identify                  2.5.17             pyhd8ed1ab_0    conda-forge
     idna                      3.4                pyhd8ed1ab_0    conda-forge
     imagesize                 1.4.1              pyhd8ed1ab_0    conda-forge
     importlib-metadata        6.0.0              pyha770c72_0    conda-forge
     importlib_metadata        6.0.0                hd8ed1ab_0    conda-forge
     iniconfig                 2.0.0              pyhd8ed1ab_0    conda-forge
     ipykernel                 6.21.0             pyh210e3f2_0    conda-forge
     ipython                   8.9.0              pyh41d4057_0    conda-forge
     ipython_genutils          0.2.0                      py_1    conda-forge
     itsdangerous              2.1.2              pyhd8ed1ab_0    conda-forge
     jedi                      0.18.2             pyhd8ed1ab_0    conda-forge
     jinja2                    3.1.2              pyhd8ed1ab_1    conda-forge
     jmespath                  1.0.1              pyhd8ed1ab_0    conda-forge
     joblib                    1.2.0              pyhd8ed1ab_0    conda-forge
     jpeg                      9e                   h166bdaf_2    conda-forge
     json-c                    0.16                 hc379101_0    conda-forge
     jsondiff                  2.0.0              pyhd8ed1ab_0    conda-forge
     jsonpatch                 1.32               pyhd8ed1ab_0    conda-forge
     jsonpointer               2.0                        py_0    conda-forge
     jsonschema                3.2.0              pyhd8ed1ab_3    conda-forge
     jupyter-cache             0.5.0              pyhd8ed1ab_0    conda-forge
     jupyter_client            8.0.1              pyhd8ed1ab_0    conda-forge
     jupyter_core              5.2.0            py38h578d9bd_0    conda-forge
     jupyter_events            0.6.3              pyhd8ed1ab_0    conda-forge
     jupyter_server            2.1.0              pyhd8ed1ab_0    conda-forge
     jupyter_server_terminals  0.4.4              pyhd8ed1ab_1    conda-forge
     jupyterlab_pygments       0.2.2              pyhd8ed1ab_0    conda-forge
     kealib                    1.5.0                ha7026e8_0    conda-forge
     kernel-headers_linux-64   3.10.0              h4a8ded7_13    conda-forge
     keyutils                  1.6.1                h166bdaf_0    conda-forge
     kiwisolver                1.4.4            py38h43d8883_1    conda-forge
     krb5                      1.20.1               h81ceb04_0    conda-forge
     lcms2                     2.14                 hfd0df8a_1    conda-forge
     ld_impl_linux-64          2.39                 hcc3a1bd_1    conda-forge
     lerc                      4.0.0                h27087fc_0    conda-forge
     libabseil                 20220623.0      cxx17_h05df665_6    conda-forge
     libaec                    1.0.6                hcb278e6_1    conda-forge
     libarrow                  10.0.1           hf9c26a6_6_cpu    conda-forge
     libblas                   3.9.0            16_linux64_mkl    conda-forge
     libbrotlicommon           1.0.9                h166bdaf_8    conda-forge
     libbrotlidec              1.0.9                h166bdaf_8    conda-forge
     libbrotlienc              1.0.9                h166bdaf_8    conda-forge
     libcblas                  3.9.0            16_linux64_mkl    conda-forge
     libclang-cpp11.1          11.1.0          default_ha53f305_1    conda-forge
     libcrc32c                 1.1.2                h9c3ff4c_0    conda-forge
     libcublas                 11.11.3.6                     0    nvidia
     libcublas-dev             11.11.3.6                     0    nvidia
     libcufft                  10.9.0.58                     0    nvidia
     libcufft-dev              10.9.0.58                     0    nvidia
     libcumlprims              23.02.00a230126 cuda11_ge28945f_11    rapidsai-nightly
     libcurand                 10.3.0.86                     0    nvidia
     libcurand-dev             10.3.0.86                     0    nvidia
     libcurl                   7.87.0               hdc1c0ab_0    conda-forge
     libcusolver               11.4.1.48                     0    nvidia
     libcusolver-dev           11.4.1.48                     0    nvidia
     libcusparse               11.7.5.86                     0    nvidia
     libcusparse-dev           11.7.5.86                     0    nvidia
     libdap4                   3.20.6               hd7c4107_2    conda-forge
     libdeflate                1.17                 h0b41bf4_0    conda-forge
     libedit                   3.1.20191231         he28a2e2_2    conda-forge
     libev                     4.33                 h516909a_1    conda-forge
     libevent                  2.1.10               h28343ad_4    conda-forge
     libfaiss                  1.7.2           cuda118h2d43ea4_4_cuda    rapidsai
     libffi                    3.4.2                h7f98852_5    conda-forge
     libgcc-devel_linux-64     9.5.0               h0a57e50_19    conda-forge
     libgcc-ng                 12.2.0              h65d4601_19    conda-forge
     libgcrypt                 1.10.1               h166bdaf_0    conda-forge
     libgdal                   3.5.3               h084b287_15    conda-forge
     libgfortran-ng            12.2.0              h69a702a_19    conda-forge
     libgfortran5              12.2.0              h337968e_19    conda-forge
     libglib                   2.74.1               h606061b_1    conda-forge
     libgomp                   12.2.0              h65d4601_19    conda-forge
     libgoogle-cloud           2.5.0                h21dfe5b_1    conda-forge
     libgpg-error              1.46                 h620e276_0    conda-forge
     libgrpc                   1.51.1               h30feacc_0    conda-forge
     libgsasl                  1.8.0                         2    conda-forge
     libhwloc                  2.8.0                h32351e8_1    conda-forge
     libiconv                  1.17                 h166bdaf_0    conda-forge
     libjpeg-turbo             2.1.4                h166bdaf_0    conda-forge
     libkml                    1.3.0             h37653c0_1015    conda-forge
     liblapack                 3.9.0            16_linux64_mkl    conda-forge
     libllvm11                 11.1.0               he0ac6c6_5    conda-forge
     libnetcdf                 4.8.1           nompi_h261ec11_106    conda-forge
     libnghttp2                1.51.0               hff17c54_0    conda-forge
     libnsl                    2.0.0                h7f98852_0    conda-forge
     libntlm                   1.4               h7f98852_1002    conda-forge
     libpng                    1.6.39               h753d276_0    conda-forge
     libpq                     15.1                 hb675445_3    conda-forge
     libprotobuf               3.21.12              h3eb15da_0    conda-forge
     librdkafka                1.7.0                hb1989a6_1    conda-forge
     librttopo                 1.1.0               ha49c73b_12    conda-forge
     libsanitizer              9.5.0               h2f262e1_19    conda-forge
     libsodium                 1.0.18               h36c2ea0_1    conda-forge
     libspatialindex           1.9.3                h9c3ff4c_4    conda-forge
     libspatialite             5.0.1               h221c8f1_23    conda-forge
     libsqlite                 3.40.0               h753d276_0    conda-forge
     libssh2                   1.10.0               hf14f497_3    conda-forge
     libstdcxx-devel_linux-64  9.5.0               h0a57e50_19    conda-forge
     libstdcxx-ng              12.2.0              h46fd767_19    conda-forge
     libthrift                 0.16.0               he500d00_2    conda-forge
     libtiff                   4.5.0                h6adf6a1_2    conda-forge
     libutf8proc               2.8.0                h166bdaf_0    conda-forge
     libuuid                   2.32.1            h7f98852_1000    conda-forge
     libuv                     1.44.2               h166bdaf_0    conda-forge
     libwebp-base              1.2.4                h166bdaf_0    conda-forge
     libxcb                    1.13              h7f98852_1004    conda-forge
     libxml2                   2.10.3               h7463322_0    conda-forge
     libxslt                   1.1.37               h873f0b0_0    conda-forge
     libzip                    1.9.2                hc929e4a_1    conda-forge
     libzlib                   1.2.13               h166bdaf_4    conda-forge
     littleutils               0.2.2                      py_0    conda-forge
     livereload                2.6.3              pyh9f0ad1d_0    conda-forge
     llvm-openmp               15.0.7               h0cdce71_0    conda-forge
     llvmlite                  0.39.1           py38h38d86a4_1    conda-forge
     locket                    1.0.0              pyhd8ed1ab_0    conda-forge
     lxml                      4.9.2            py38h215a2d7_0    conda-forge
     lz4                       4.2.0            py38hd012fdc_0    conda-forge
     lz4-c                     1.9.3                h9c3ff4c_1    conda-forge
     make                      4.3                  hd18ef5c_1    conda-forge
     makefun                   1.15.0             pyhd8ed1ab_0    conda-forge
     mapclassify               2.5.0              pyhd8ed1ab_1    conda-forge
     markdown                  3.4.1              pyhd8ed1ab_0    conda-forge
     markdown-it-py            2.1.0              pyhd8ed1ab_0    conda-forge
     markupsafe                2.1.2            py38h1de0b5d_0    conda-forge
     matplotlib-base           3.6.3            py38hd6c3c57_0    conda-forge
     matplotlib-inline         0.1.6              pyhd8ed1ab_0    conda-forge
     mdit-py-plugins           0.3.3              pyhd8ed1ab_0    conda-forge
     mdurl                     0.1.0              pyhd8ed1ab_0    conda-forge
     mimesis                   7.0.0              pyhd8ed1ab_0    conda-forge
     mistune                   2.0.4              pyhd8ed1ab_0    conda-forge
     mkl                       2022.2.1         h84fe81f_16997    conda-forge
     moto                      4.1.1              pyhd8ed1ab_0    conda-forge
     mpc                       1.3.1                hfe3b2da_0    conda-forge
     mpfr                      4.1.0                h9202a9a_1    conda-forge
     mpi                       1.0                     openmpi    conda-forge
     mpi4py                    3.1.4                    pypi_0    pypi
     msgpack-python            1.0.4            py38h43d8883_1    conda-forge
     multidict                 6.0.4            py38h1de0b5d_0    conda-forge
     multipledispatch          0.6.0                      py_0    conda-forge
     multiprocess              0.70.14          py38h0a891b7_3    conda-forge
     munch                     2.5.0                      py_0    conda-forge
     munkres                   1.1.4              pyh9f0ad1d_0    conda-forge
     myst-nb                   0.17.1             pyhd8ed1ab_0    conda-forge
     myst-parser               0.18.1             pyhd8ed1ab_0    conda-forge
     nbclassic                 0.4.8              pyhd8ed1ab_0    conda-forge
     nbclient                  0.5.13             pyhd8ed1ab_0    conda-forge
     nbconvert                 7.2.9              pyhd8ed1ab_0    conda-forge
     nbconvert-core            7.2.9              pyhd8ed1ab_0    conda-forge
     nbconvert-pandoc          7.2.9              pyhd8ed1ab_0    conda-forge
     nbformat                  5.7.3              pyhd8ed1ab_0    conda-forge
     nbsphinx                  0.8.12             pyhd8ed1ab_0    conda-forge
     nccl                      2.14.3.1             h0800d71_0    conda-forge
     ncurses                   6.3                  h27087fc_1    conda-forge
     nest-asyncio              1.5.6              pyhd8ed1ab_0    conda-forge
     networkx                  3.0                pyhd8ed1ab_0    conda-forge
     ninja                     1.11.0               h924138e_0    conda-forge
     nltk                      3.8.1              pyhd8ed1ab_0    conda-forge
     nodeenv                   1.7.0              pyhd8ed1ab_0    conda-forge
     notebook                  6.5.2              pyha770c72_1    conda-forge
     notebook-shim             0.2.2              pyhd8ed1ab_0    conda-forge
     nspr                      4.35                 h27087fc_0    conda-forge
     nss                       3.82                 he02c5a1_0    conda-forge
     numba                     0.56.4           py38h9a4aae9_0    conda-forge
     numpy                     1.23.5           py38h7042d01_0    conda-forge
     numpydoc                  1.5.0              pyhd8ed1ab_0    conda-forge
     nvcc_linux-64             11.8                ha67cc55_22    conda-forge
     nvtx                      0.2.3            py38h0a891b7_2    conda-forge
     ogb                       1.3.5              pyhd8ed1ab_0    conda-forge
     openapi-schema-validator  0.2.3              pyhd8ed1ab_0    conda-forge
     openapi-spec-validator    0.4.0              pyhd8ed1ab_1    conda-forge
     openjpeg                  2.5.0                hfec8fc6_2    conda-forge
     openmpi                   4.1.4              ha1ae619_102    conda-forge
     openssl                   3.0.7                h0b41bf4_2    conda-forge
     orc                       1.8.2                hfdbbad2_0    conda-forge
     outdated                  0.2.2              pyhd8ed1ab_0    conda-forge
     packaging                 23.0               pyhd8ed1ab_0    conda-forge
     pandas                    1.5.3            py38hdc8b05c_0    conda-forge
     pandoc                    1.19.2                        0    conda-forge
     pandocfilters             1.5.0              pyhd8ed1ab_0    conda-forge
     paramiko                  3.0.0              pyhd8ed1ab_0    conda-forge
     parquet-cpp               1.5.1                         2    conda-forge
     parso                     0.8.3              pyhd8ed1ab_0    conda-forge
     partd                     1.3.0              pyhd8ed1ab_0    conda-forge
     patsy                     0.5.3              pyhd8ed1ab_0    conda-forge
     pcre2                     10.40                hc3806b6_0    conda-forge
     pexpect                   4.8.0              pyh1a96a4e_2    conda-forge
     pickleshare               0.7.5                   py_1003    conda-forge
     pillow                    9.4.0            py38hb32c036_0    conda-forge
     pip                       23.0               pyhd8ed1ab_0    conda-forge
     pixman                    0.40.0               h36c2ea0_0    conda-forge
     platformdirs              2.6.2              pyhd8ed1ab_0    conda-forge
     pluggy                    1.0.0              pyhd8ed1ab_5    conda-forge
     pooch                     1.6.0              pyhd8ed1ab_0    conda-forge
     poppler                   23.01.0              h091648b_0    conda-forge
     poppler-data              0.4.11               hd8ed1ab_0    conda-forge
     postgresql                15.1                 h3248436_3    conda-forge
     pre-commit                3.0.2            py38h578d9bd_0    conda-forge
     proj                      9.1.1                h8ffa02c_2    conda-forge
     prometheus_client         0.16.0             pyhd8ed1ab_0    conda-forge
     prompt-toolkit            3.0.36             pyha770c72_0    conda-forge
     protobuf                  4.21.12          py38h8dc9893_0    conda-forge
     psutil                    5.9.4            py38h0a891b7_0    conda-forge
     pthread-stubs             0.4               h36c2ea0_1001    conda-forge
     ptxcompiler               0.7.0            py38h241159d_3    conda-forge
     ptyprocess                0.7.0              pyhd3deb0d_0    conda-forge
     pure_eval                 0.2.2              pyhd8ed1ab_0    conda-forge
     py                        1.11.0             pyh6c4a22f_0    conda-forge
     py-cpuinfo                9.0.0              pyhd8ed1ab_0    conda-forge
     pyarrow                   10.0.1          py38hf05218d_6_cpu    conda-forge
     pyasn1                    0.4.8                      py_0    conda-forge
     pycparser                 2.21               pyhd8ed1ab_0    conda-forge
     pydantic                  1.10.4           py38h1de0b5d_1    conda-forge
     pydata-sphinx-theme       0.12.0             pyhd8ed1ab_0    conda-forge
     pygments                  2.14.0             pyhd8ed1ab_0    conda-forge
     pynacl                    1.5.0            py38h0a891b7_2    conda-forge
     pynndescent               0.5.8              pyh1a96a4e_0    conda-forge
     pynvml                    11.4.1             pyhd8ed1ab_0    conda-forge
     pyopenssl                 23.0.0             pyhd8ed1ab_0    conda-forge
     pyorc                     0.8.0            py38h4492b77_2    conda-forge
     pyparsing                 3.0.9              pyhd8ed1ab_0    conda-forge
     pyproj                    3.4.1            py38h58d5fe2_1    conda-forge
     pyrsistent                0.19.3           py38h1de0b5d_0    conda-forge
     pysocks                   1.7.1              pyha2e5f31_6    conda-forge
     pytest                    7.2.1              pyhd8ed1ab_0    conda-forge
     pytest-benchmark          4.0.0              pyhd8ed1ab_0    conda-forge
     pytest-cases              3.6.13             pyhd8ed1ab_0    conda-forge
     pytest-cov                4.0.0              pyhd8ed1ab_0    conda-forge
     pytest-xdist              3.1.0              pyhd8ed1ab_0    conda-forge
     python                    3.8.15          he550d4f_1_cpython    conda-forge
     python-confluent-kafka    1.7.0            py38h497a2fe_2    conda-forge
     python-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge
     python-fastjsonschema     2.16.2             pyhd8ed1ab_0    conda-forge
     python-jose               3.3.0              pyh6c4a22f_1    conda-forge
     python-json-logger        2.0.4              pyhd8ed1ab_0    conda-forge
     python-louvain            0.16               pyhd8ed1ab_0    conda-forge
     python-snappy             0.6.1            py38h1ddbb56_0    conda-forge
     python-xxhash             3.2.0            py38h1de0b5d_0    conda-forge
     python_abi                3.8                      3_cp38    conda-forge
     pytorch                   1.11.0              py3.8_cpu_0    pytorch
     pytorch-mutex             1.0                         cpu    pytorch
     pytz                      2022.7.1           pyhd8ed1ab_0    conda-forge
     pywin32-on-windows        0.1.0              pyh1179c8e_3    conda-forge
     pyyaml                    6.0              py38h0a891b7_5    conda-forge
     pyzmq                     25.0.0           py38he24dcef_0    conda-forge
     re2                       2022.06.01           h27087fc_1    conda-forge
     readline                  8.1.2                h0f457ee_0    conda-forge
     recommonmark              0.7.1              pyhd8ed1ab_0    conda-forge
     regex                     2022.10.31       py38h0a891b7_0    conda-forge
     requests                  2.28.2             pyhd8ed1ab_0    conda-forge
     responses                 0.21.0             pyhd8ed1ab_0    conda-forge
     rfc3339-validator         0.1.4              pyhd8ed1ab_0    conda-forge
     rfc3986-validator         0.1.1              pyh9f0ad1d_0    conda-forge
     rhash                     1.4.3                h166bdaf_0    conda-forge
     rsa                       4.9                pyhd8ed1ab_0    conda-forge
     rtree                     1.0.1            py38h02d302b_1    conda-forge
     s2n                       1.3.31               h3358134_0    conda-forge
     s3fs                      2023.1.0           pyhd8ed1ab_0    conda-forge
     s3transfer                0.6.0              pyhd8ed1ab_0    conda-forge
     sacremoses                0.0.53             pyhd8ed1ab_0    conda-forge
     scikit-build              0.16.6             pyh56297ac_0    conda-forge
     scikit-learn              1.2.1            py38h1e1a916_0    conda-forge
     scipy                     1.10.0           py38h10c12cc_0    conda-forge
     seaborn                   0.12.2               hd8ed1ab_0    conda-forge
     seaborn-base              0.12.2             pyhd8ed1ab_0    conda-forge
     sed                       4.8                  he412f7d_0    conda-forge
     send2trash                1.8.0              pyhd8ed1ab_0    conda-forge
     setuptools                67.0.0                   pypi_0    pypi
     shapely                   2.0.1            py38hd07e089_0    conda-forge
     six                       1.16.0             pyh6c4a22f_0    conda-forge
     snappy                    1.1.9                hbd366e4_2    conda-forge
     sniffio                   1.3.0              pyhd8ed1ab_0    conda-forge
     snowballstemmer           2.2.0              pyhd8ed1ab_0    conda-forge
     sortedcontainers          2.4.0              pyhd8ed1ab_0    conda-forge
     soupsieve                 2.3.2.post1        pyhd8ed1ab_0    conda-forge
     sparse                    0.13.0             pyhd8ed1ab_0    conda-forge
     spdlog                    1.8.5                h4bd325d_1    conda-forge
     sphinx                    5.3.0              pyhd8ed1ab_0    conda-forge
     sphinx-autobuild          2021.3.14          pyhd8ed1ab_0    conda-forge
     sphinx-copybutton         0.5.0              pyhd8ed1ab_0    conda-forge
     sphinx-markdown-tables    0.0.17             pyh6c4a22f_0    conda-forge
     sphinxcontrib-applehelp   1.0.4              pyhd8ed1ab_0    conda-forge
     sphinxcontrib-devhelp     1.0.2                      py_0    conda-forge
     sphinxcontrib-htmlhelp    2.0.0              pyhd8ed1ab_0    conda-forge
     sphinxcontrib-jsmath      1.0.1                      py_0    conda-forge
     sphinxcontrib-qthelp      1.0.3                      py_0    conda-forge
     sphinxcontrib-serializinghtml 1.1.5              pyhd8ed1ab_2    conda-forge
     sphinxcontrib-websupport  1.2.4              pyhd8ed1ab_1    conda-forge
     sqlalchemy                1.4.46           py38h1de0b5d_0    conda-forge
     sqlite                    3.40.0               h4ff8645_0    conda-forge
     sshpubkeys                3.3.1              pyhd8ed1ab_0    conda-forge
     stack_data                0.6.2              pyhd8ed1ab_0    conda-forge
     statsmodels               0.13.5           py38h26c90d9_2    conda-forge
     streamz                   0.6.4              pyh6c4a22f_0    conda-forge
     sysroot_linux-64          2.17                h4a8ded7_13    conda-forge
     tabulate                  0.9.0              pyhd8ed1ab_1    conda-forge
     tbb                       2021.7.0             h924138e_1    conda-forge
     tblib                     1.7.0              pyhd8ed1ab_0    conda-forge
     terminado                 0.17.1             pyh41d4057_0    conda-forge
     threadpoolctl             3.1.0              pyh8a188c0_0    conda-forge
     tiledb                    2.13.2               hd532e3d_0    conda-forge
     tinycss2                  1.2.1              pyhd8ed1ab_0    conda-forge
     tk                        8.6.12               h27826a3_0    conda-forge
     tokenizers                0.13.1           py38h8bed557_2    conda-forge
     toml                      0.10.2             pyhd8ed1ab_0    conda-forge
     tomli                     2.0.1              pyhd8ed1ab_0    conda-forge
     toolz                     0.12.0             pyhd8ed1ab_0    conda-forge
     tornado                   6.2              py38h0a891b7_1    conda-forge
     tqdm                      4.64.1             pyhd8ed1ab_0    conda-forge
     traitlets                 5.9.0              pyhd8ed1ab_0    conda-forge
     transformers              4.24.0             pyhd8ed1ab_0    conda-forge
     treelite                  3.1.0            py38h2820b77_0    conda-forge
     treelite-runtime          3.1.0                    pypi_0    pypi
     typing-extensions         4.4.0                hd8ed1ab_0    conda-forge
     typing_extensions         4.4.0              pyha770c72_0    conda-forge
     tzcode                    2022g                h166bdaf_0    conda-forge
     tzdata                    2022g                h191b570_0    conda-forge
     ucx                       1.13.1               h538f049_1    conda-forge
     ucx-proc                  1.0.0                       gpu    rapidsai
     ucx-py                    0.30.00a230131  py38_ga21f62a_17    rapidsai-nightly
     ukkonen                   1.0.1            py38h43d8883_3    conda-forge
     umap-learn                0.5.3            py38h578d9bd_0    conda-forge
     unicodedata2              15.0.0           py38h0a891b7_0    conda-forge
     urllib3                   1.26.14            pyhd8ed1ab_0    conda-forge
     virtualenv                20.17.1          py38h578d9bd_0    conda-forge
     wcwidth                   0.2.6              pyhd8ed1ab_0    conda-forge
     webencodings              0.5.1                      py_1    conda-forge
     websocket-client          1.5.0              pyhd8ed1ab_0    conda-forge
     werkzeug                  2.1.2              pyhd8ed1ab_1    conda-forge
     wheel                     0.38.4             pyhd8ed1ab_0    conda-forge
     wrapt                     1.14.1           py38h0a891b7_1    conda-forge
     xerces-c                  3.2.4                h55805fa_1    conda-forge
     xmltodict                 0.13.0             pyhd8ed1ab_0    conda-forge
     xorg-kbproto              1.0.7             h7f98852_1002    conda-forge
     xorg-libice               1.0.10               h7f98852_0    conda-forge
     xorg-libsm                1.2.3             hd9c2040_1000    conda-forge
     xorg-libx11               1.7.2                h7f98852_0    conda-forge
     xorg-libxau               1.0.9                h7f98852_0    conda-forge
     xorg-libxdmcp             1.1.3                h7f98852_0    conda-forge
     xorg-libxext              1.3.4                h7f98852_1    conda-forge
     xorg-libxrender           0.9.10            h7f98852_1003    conda-forge
     xorg-renderproto          0.11.1            h7f98852_1002    conda-forge
     xorg-xextproto            7.3.0             h7f98852_1002    conda-forge
     xorg-xproto               7.0.31            h7f98852_1007    conda-forge
     xxhash                    0.8.1                h0b41bf4_0    conda-forge
     xyzservices               2022.9.0           pyhd8ed1ab_0    conda-forge
     xz                        5.2.6                h166bdaf_0    conda-forge
     yaml                      0.2.5                h7f98852_2    conda-forge
     yarl                      1.8.2            py38h0a891b7_0    conda-forge
     zeromq                    4.3.4                h9c3ff4c_1    conda-forge
     zict                      2.2.0              pyhd8ed1ab_0    conda-forge
     zipp                      3.12.0             pyhd8ed1ab_0    conda-forge
     zlib                      1.2.13               h166bdaf_4    conda-forge
     zstd                      1.5.2                h3eb15da_6    conda-forge

</pre></details>
",2023-02-01T16:02:50Z,0,0,Lawrence Mitchell,,False
491,[BUG] `IndexedFrame._split` is inconsistent for empty dataframes,"This came up while reviewing #12704.

Quoth the documentation:

> Split a frame with split points in ``splits``. Returns a list of Frames of length `len(splits) + 1`.

Which is true, except if the input dataframe is empty:

```python
import cudf
df = cudf.DataFrame({""a"": []})
print(df._split([0])) # => []
```

This makes writing generic code difficult, since we're expecting to get back a list of N+1 things to iterate over, but in this case we don't.

Slicing empty dataframes works fine (and reproduces semantically what you ""expect"" from slicing empty python lists):

```python
df[:0], df[0:] # => (Empty DataFrame, Empty DataFrame)
```

(Arguably slicing with an out of bounds index should raise an `IndexError`, but that ship has sailed.)

What I would like:

```python
splits = [...]

assert df._split(splits) == [df[s:e] for s, e in zip([None] + splits, splits + [None])]
```

",2023-02-07T10:42:25Z,0,0,Lawrence Mitchell,,False
492,[FEA] cudf.Series.agg support,"**Is your feature request related to a problem? Please describe.**
working with `import cudf as pd`

**Describe the solution you'd like**
`cudf.Series.agg` matching https://pandas.pydata.org/docs/reference/api/pandas.Series.agg.html

```
>>> import cudf, pandas
>>> cudf.__version__
'23.02.00a+310.g58e0fde346'
>>> pandas.__version__
'1.5.3'
>>> cudf.Series([1, 1, 42, 1984]).agg('mean')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: 'Series' object has no attribute 'agg'
>>> pandas.Series([1, 1, 42, 1984]).agg('mean')
507.0
```",2023-02-07T13:02:41Z,0,0,Matthew Farrellee,,False
493,[FEA] cudf.Series.add support for NaN,"**Is your feature request related to a problem? Please describe.**
working with `import cudf as pd`

**Describe the solution you'd like**
```
>>> import cudf, pandas
>>> cudf.__version__
'23.02.00a+310.g58e0fde346'
>>> pandas.__version__
'1.5.3'
>>> import numpy as np
>>> cudf.Series([1,1, np.NaN, 9]).add([0, 1, 1, np.NaN])
NotImplemented
>>> pandas.Series([1,1, np.NaN, 9]).add([0, 1, 1, np.NaN])
0    1.0
1    2.0
2    NaN
3    NaN
dtype: float64
```",2023-02-07T13:05:33Z,0,0,Matthew Farrellee,,False
494,[BUG] NA handling inconsistent in DataFrame/Series.replace,"**Describe the bug**
working with `import cudf as pd`

**Steps/Code to reproduce bug**
```
>>> import cudf, pandas, numpy
>>> cudf.__version__
'23.02.00a+310.g58e0fde346'
>>> pandas.__version__
'1.5.3'

>>> cudf.Series([1, 2, numpy.nan]).replace(numpy.nan, -1)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/opt/conda/envs/rapids/lib/python3.8/contextlib.py"", line 75, in inner
    return func(*args, **kwds)
  File ""/opt/conda/envs/rapids/lib/python3.8/site-packages/cudf/core/series.py"", line 2180, in replace
    return super().replace(to_replace, value, *args, **kwargs)
  File ""/opt/conda/envs/rapids/lib/python3.8/contextlib.py"", line 75, in inner
    return func(*args, **kwds)
  File ""/opt/conda/envs/rapids/lib/python3.8/site-packages/cudf/core/indexed_frame.py"", line 765, in replace
    copy_data[name] = col.find_and_replace(
  File ""/opt/conda/envs/rapids/lib/python3.8/site-packages/cudf/core/column/numerical.py"", line 483, in find_and_replace
    to_replace_col = _normalize_find_and_replace_input(
  File ""/opt/conda/envs/rapids/lib/python3.8/site-packages/cudf/core/column/numerical.py"", line 771, in _normalize_find_and_replace_input
    col_to_normalize_casted = input_column_dtype.type(
ValueError: cannot convert float NaN to integer
>>> pandas.Series([1, 2, numpy.nan]).replace(numpy.nan, -1)
0    1.0
1    2.0
2   -1.0
dtype: float64
```",2023-02-07T13:40:55Z,0,0,Matthew Farrellee,,False
495,[FEA] Ability to track `CopyOnWriteBuffer` by sharing references,"**Is your feature request related to a problem? Please describe.**
With `CopyOnWriteBuffer` we should explore if we can just pass on the `CopyOnWriteBuffer` reference to another column and still create a weak reference.
for example:
```
y = CopyOnWriteBuffer(..)
x = y # Should establish a weak reference or something of that sort
x2 = y.copy(deep=True)  # currently only this achieves establishing a weak-reference.
```

",2023-02-08T18:55:49Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
496,[FEA] Update IO benchmarks for consistency between formats,"**Is your feature request related to a problem? Please describe.**
- [x] #12678
- [x] #12675 
- [x] add device buffer data source to CSV reader benchmark ([code pointer](https://github.com/rapidsai/cudf/blob/c20c8b42215e38bee207b49dad6e28ea04ccbd8c/cpp/benchmarks/io/csv/csv_reader_input.cpp#L102))
- [x] migrate CSV writer to nvbench. Currently using gbench.
- [x] add JSON writer benchmark. This benchmark could be modeled after CSV writer.
- [x] add JSON reader benchmark with file data source ([NESTED_JSON](https://github.com/rapidsai/cudf/blob/branch-23.04/cpp/benchmarks/io/json/nested_json.cpp) only does parsing and only on device buffers). This benchmark could be modeled after `BM_csv_read_io`
- [x] ~#12700~
- [x] ~#12674 -> add compression to `BM_csv_read_io` (?)~
- [x] add benchmark coverage for parquet chunked reader. Perhaps modeled after [parquet_read_io_compression](https://github.com/rapidsai/cudf/blob/c20c8b42215e38bee207b49dad6e28ea04ccbd8c/cpp/benchmarks/io/parquet/parquet_reader_input.cpp#L143)
- [x] for reader benchmarks, verify that the roundtripped table matches the starting table
- [x] convert `compression` and `io` to string axis type. [see this discussion in nvbench](https://github.com/NVIDIA/nvbench/issues/137). the goal is to choose other values from the CLI without having to run all values in automation.
- [ ] rename `pmu`, `efs`, `trc` in ORC writer chunks to `peak_memory_usage`, `encoded_file_size`, `total_rows`, to conform with the other ORC, PQ, CSV, text benchmarks

**Additional context**
The initial set of topics came from a comparison of file read throughput across the supported formats in cuIO.
<img width=""518"" alt=""image"" src=""https://user-images.githubusercontent.com/12725111/217667722-a5024218-61fb-410d-941d-9f9eed4f4c5a.png"">
We are also preparing for a comparison of memory footprint across cuIO, especially with Zstd compression/decompression.

",2023-02-08T22:43:38Z,0,0,Gregory Kimball,,False
497,[ENH]: cudf options that map onto pandas ones should have identical names,"A followup to #12619; to enable copy-on-write in pandas one says `set_option(""mode.copy_on_write"", True)`, whereas to enable in cudf one must say `set_option(""copy_on_write"", True)`.

There might be other cases, but where cudf exposes the same configuration options as pandas, the names should match as a principle of least surprise.",2023-02-13T13:00:48Z,0,0,Lawrence Mitchell,,False
498,[FEA] Add alternate handling for all null list in  join_list_elements,"**Is your feature request related to a problem? Please describe.**
All null list is considered as empty list as special handling in `join_list_elements` API. 
But this special handling may not be required by all the users.

**Describe the solution you'd like**
Provide an option to skip this special handling.

**Describe alternatives you've considered**
Replace all nulls in child columns with `""null""` before using `join_list_elements` API.  But it incurs extra memory and compute time.

**Additional context**
https://github.com/rapidsai/cudf/pull/12767 workaround due to this special handling.",2023-02-13T20:44:13Z,0,0,Karthikeyan,NVIDIA,True
499,[FEA] Faster rolling window aggregations with large window sizes,"Rolling window aggregations are slow with large window sizes. I believe this is known behavior to many contributors but a user ran into this yesterday and I couldn't find an issue or reference documentation from an initial search.

This is more likely to occur with time-oriented window sizes (e.g., ""60min"" or ""9h"") on high-frequency data rather than fixed integer length windows (as these windows are less likely to be as long). 

The examples below illustrates this behavior. Observed GPU utilization is at 100% throughout the operations. 

```python
import cudf

df = cudf.datasets.timeseries(
    start='2000-01-01',
    end='2000-06-30',
    freq='1s',
)
pdf = df.to_pandas()
print(df.shape)
print(df.head())
(15638401, 4)
                       id     name         x         y
timestamp                                             
2000-01-01 00:00:00  1069    Edith -0.208702  0.451685
2000-01-01 00:00:01  1053   Yvonne -0.383893 -0.846287
2000-01-01 00:00:02   986    Sarah -0.718822 -0.980082
2000-01-01 00:00:03   999  Norbert -0.547608 -0.291836
2000-01-01 00:00:04   999   George -0.534662  0.300049
```

```python
windows = [""1min"", ""60min"", ""3h"", ""12h"", ""1d""]

print(""cuDF Rolling Windows"")
for w in windows:
    %time out = df.rolling(w).x.max()
    
print(""\n""*3)
print(""Pandas Rolling Windows"")
for w in windows:
    %time out = pdf.rolling(w).x.max()
cuDF Rolling Windows
CPU times: user 8.25 ms, sys: 20.2 ms, total: 28.5 ms
Wall time: 49.2 ms
CPU times: user 400 ms, sys: 8 ms, total: 408 ms
Wall time: 437 ms
CPU times: user 1.08 s, sys: 150 µs, total: 1.08 s
Wall time: 1.09 s
CPU times: user 4.3 s, sys: 7.07 ms, total: 4.31 s
Wall time: 4.36 s
CPU times: user 8.69 s, sys: 11.4 ms, total: 8.7 s
Wall time: 8.76 s



Pandas Rolling Windows
CPU times: user 516 ms, sys: 44.1 ms, total: 560 ms
Wall time: 558 ms
CPU times: user 511 ms, sys: 40 ms, total: 551 ms
Wall time: 550 ms
CPU times: user 522 ms, sys: 28.1 ms, total: 550 ms
Wall time: 549 ms
CPU times: user 494 ms, sys: 48 ms, total: 542 ms
Wall time: 541 ms
CPU times: user 499 ms, sys: 44 ms, total: 544 ms
Wall time: 542 ms
```

```python
windows = [10, 100, 1000, 10000, 100000]

print(""cuDF Rolling Windows"")
for w in windows:
    %time out = df.rolling(w).x.max()
    
print(""\n""*2)
print(""Pandas Rolling Windows"")
for w in windows:
    %time out = pdf.rolling(w).x.max()
cuDF Rolling Windows
CPU times: user 4.14 ms, sys: 7 µs, total: 4.15 ms
Wall time: 3.01 ms
CPU times: user 7.92 ms, sys: 4 ms, total: 11.9 ms
Wall time: 11.7 ms
CPU times: user 91.6 ms, sys: 3.8 ms, total: 95.4 ms
Wall time: 104 ms
CPU times: user 726 ms, sys: 58 µs, total: 726 ms
Wall time: 726 ms
CPU times: user 6.76 s, sys: 3.61 ms, total: 6.77 s
Wall time: 6.79 s



Pandas Rolling Windows
CPU times: user 430 ms, sys: 80.1 ms, total: 510 ms
Wall time: 519 ms
CPU times: user 425 ms, sys: 76.1 ms, total: 502 ms
Wall time: 500 ms
CPU times: user 435 ms, sys: 64.2 ms, total: 499 ms
Wall time: 498 ms
CPU times: user 416 ms, sys: 79.7 ms, total: 495 ms
Wall time: 494 ms
CPU times: user 420 ms, sys: 72.1 ms, total: 492 ms
Wall time: 491 ms
```
```
conda list | grep cudf
cudf                      23.02.00        cuda_11_py38_g5ad4a85b9d_0    rapidsai
cudf_kafka                23.02.00        py38_g5ad4a85b9d_0    rapidsai
dask-cudf                 23.02.00        cuda_11_py38_g5ad4a85b9d_0    rapidsai
libcudf                   23.02.00        cuda11_g5ad4a85b9d_0    rapidsai
libcudf_kafka             23.02.00          g5ad4a85b9d_0    rapidsai
```

Filing an issue to document this performance behavior If already captured elsewhere, will close as a duplicate.",2023-02-14T15:20:34Z,0,0,Nick Becker,@NVIDIA,True
500,[BUG] cudf.cut fails on example from documentation,"**Describe the bug**
last example in https://docs.rapids.ai/api/cudf/stable/api_docs/api/cudf.cut.html

**Steps/Code to reproduce bug**
```
In [1]: import cudf

In [2]: cudf.__version__
Out[2]: '23.02.00'

In [3]: import numpy as np

In [4]: s = cudf.Series(np.array([2, 4, 6, 8, 10]), index=['a', 'b', 'c', 'd', 'e'])
   ...: cudf.cut(s, 3)
Out[4]: ---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/opt/conda/envs/rapids/lib/python3.8/site-packages/IPython/core/formatters.py in __call__(self, obj)
    700                 type_pprinters=self.type_printers,
    701                 deferred_pprinters=self.deferred_printers)
--> 702             printer.pretty(obj)
    703             printer.flush()
    704             return stream.getvalue()

/opt/conda/envs/rapids/lib/python3.8/site-packages/IPython/lib/pretty.py in pretty(self, obj)
    392                         if cls is not object \
    393                                 and callable(cls.__dict__.get('__repr__')):
--> 394                             return _repr_pprint(obj, self, cycle)
    395 
    396             return _default_pprint(obj, self, cycle)

/opt/conda/envs/rapids/lib/python3.8/site-packages/IPython/lib/pretty.py in _repr_pprint(obj, p, cycle)
    698     """"""A pprint that just redirects to the normal repr function.""""""
    699     # Find newlines and replace them with p.break_()
--> 700     output = repr(obj)
    701     lines = output.splitlines()
    702     with p.group():

/opt/conda/envs/rapids/lib/python3.8/site-packages/cudf/core/series.py in __repr__(self)
   1323                 )
   1324             else:
-> 1325                 pd_series = preprocess.to_pandas()
   1326             output = pd_series.to_string(
   1327                 name=self.name,

/opt/conda/envs/rapids/lib/python3.8/contextlib.py in inner(*args, **kwds)
     73         def inner(*args, **kwds):
     74             with self._recreate_cm():
---> 75                 return func(*args, **kwds)
     76         return inner
     77 

/opt/conda/envs/rapids/lib/python3.8/site-packages/cudf/core/series.py in to_pandas(self, index, nullable, **kwargs)
   1900         if index is True:
   1901             index = self.index.to_pandas()
-> 1902         s = self._column.to_pandas(index=index, nullable=nullable)
   1903         s.name = self.name
   1904         return s

/opt/conda/envs/rapids/lib/python3.8/site-packages/cudf/core/column/categorical.py in to_pandas(self, index, **kwargs)
    913         else:
    914             categories = col.categories.dropna(drop_nan=True).to_pandas()
--> 915         data = pd.Categorical.from_codes(
    916             codes, categories=categories, ordered=col.ordered
    917         )

/opt/conda/envs/rapids/lib/python3.8/site-packages/pandas/core/arrays/categorical.py in from_codes(cls, codes, categories, ordered, dtype)
    685         Categories (2, object): ['a' < 'b']
    686         """"""
--> 687         dtype = CategoricalDtype._from_values_or_dtype(
    688             categories=categories, ordered=ordered, dtype=dtype
    689         )

/opt/conda/envs/rapids/lib/python3.8/site-packages/pandas/core/dtypes/dtypes.py in _from_values_or_dtype(cls, values, categories, ordered, dtype)
    297             # Note: This could potentially have categories=None and
    298             # ordered=None.
--> 299             dtype = CategoricalDtype(categories, ordered)
    300 
    301         return cast(CategoricalDtype, dtype)

/opt/conda/envs/rapids/lib/python3.8/site-packages/pandas/core/dtypes/dtypes.py in __init__(self, categories, ordered)
    184 
    185     def __init__(self, categories=None, ordered: Ordered = False) -> None:
--> 186         self._finalize(categories, ordered, fastpath=False)
    187 
    188     @classmethod

/opt/conda/envs/rapids/lib/python3.8/site-packages/pandas/core/dtypes/dtypes.py in _finalize(self, categories, ordered, fastpath)
    338 
    339         if categories is not None:
--> 340             categories = self.validate_categories(categories, fastpath=fastpath)
    341 
    342         self._categories = categories

/opt/conda/envs/rapids/lib/python3.8/site-packages/pandas/core/dtypes/dtypes.py in validate_categories(categories, fastpath)
    534                 raise ValueError(""Categorical categories cannot be null"")
    535 
--> 536             if not categories.is_unique:
    537                 raise ValueError(""Categorical categories must be unique"")
    538 

/opt/conda/envs/rapids/lib/python3.8/site-packages/pandas/_libs/properties.pyx in pandas._libs.properties.CachedProperty.__get__()

/opt/conda/envs/rapids/lib/python3.8/site-packages/pandas/core/indexes/base.py in is_unique(self)
   2384         Return if the index has unique values.
   2385         """"""
-> 2386         return self._engine.is_unique
   2387 
   2388     @final

/opt/conda/envs/rapids/lib/python3.8/site-packages/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.is_unique.__get__()

/opt/conda/envs/rapids/lib/python3.8/site-packages/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine._do_unique_check()

/opt/conda/envs/rapids/lib/python3.8/site-packages/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine._ensure_mapping_populated()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.map_locations()

TypeError: unhashable type: 'dict'
```",2023-02-16T15:38:22Z,0,0,Matthew Farrellee,,False
501,[BUG] cudf.cut does not accept sequence of scalars,"**Describe the bug**
working with `import cudf as pd`

**Steps/Code to reproduce bug**
```
In [1]: import cudf as pd

In [2]: pd.__version__
Out[2]: '23.02.00'

In [3]: import numpy as np

In [4]: df = pd.DataFrame({'a': range(100)})

In [5]: pd.cut(df.a, bins=np.linspace(0, 100, num=6))
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-5-1d55c0477181> in <module>
----> 1 pd.cut(df.a, bins=np.linspace(0, 100, num=6))

/opt/conda/envs/rapids/lib/python3.8/site-packages/cudf/core/cut.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates, ordered)
    168                 mn = min(x)
    169                 mx = max(x)
--> 170             bins = np.linspace(mn, mx, bins + 1, endpoint=True)
    171             adj = (mx - mn) * 0.001
    172             if right:

/opt/conda/envs/rapids/lib/python3.8/site-packages/numpy/core/overrides.py in linspace(*args, **kwargs)

/opt/conda/envs/rapids/lib/python3.8/site-packages/numpy/core/function_base.py in linspace(start, stop, num, endpoint, retstep, dtype, axis)
    118 
    119     """"""
--> 120     num = operator.index(num)
    121     if num < 0:
    122         raise ValueError(""Number of samples, %s, must be non-negative."" % num)

TypeError: only integer scalar arrays can be converted to a scalar index

In [6]: import pandas

In [7]: pandas.cut(df.to_pandas().a, bins=np.linspace(0, 100, num=6))
Out[7]: 
0               NaN
1       (0.0, 20.0]
2       (0.0, 20.0]
3       (0.0, 20.0]
4       (0.0, 20.0]
          ...      
95    (80.0, 100.0]
96    (80.0, 100.0]
97    (80.0, 100.0]
98    (80.0, 100.0]
99    (80.0, 100.0]
Name: a, Length: 100, dtype: category
Categories (5, interval[float64, right]): [(0.0, 20.0] < (20.0, 40.0] < (40.0, 60.0] <
                                           (60.0, 80.0] < (80.0, 100.0]]
```

**Additional context**
https://docs.rapids.ai/api/cudf/stable/api_docs/api/cudf.cut.html
![image](https://user-images.githubusercontent.com/112653/219417718-173071c7-55c3-4bc0-9b4f-425bd0b0ee81.png)
",2023-02-16T15:52:39Z,0,0,Matthew Farrellee,,False
502,[ENH]: Reworking of `iloc` and `loc` indexing,"## Status quo

Indexing of dataframes and series happens through six user-facing routes:

- `DataFrame.__setitem__`/`DataFrame.__getitem__`
- `DataFrame.iloc.__setitem__`/`DataFrame.iloc.__getitem__`
- `DataFrame.loc.__setitem__`/`DataFrame.loc.__getitem__`
- `Series.__setitem__`/`Series.__getitem__`
- `Series.iloc.__setitem__`/`Series.iloc.__getitem__`
- `Series.loc.__setitem__`/`Series.loc.__getitem__`

These all have slightly different semantics (to match pandas behaviour), but there is still quite a lot of (possibly unnecessary) code duplication and a number of bugs around indexing. Many of these look to be because the business logic of handling slicing/gather-by-mask/indexing is intertwined with error handling and determining exactly what to slice. There's also logic effectively repeated between the loc and iloc versions in both cases.

It would be nice if the number of different paths into indexing was reduced, perhaps it is a pipe dream to share between Series and DataFrame (since a DataFrame is not just a collection of Series), but it feels like it should be possible to share more between iloc/loc/__setgetitem__.

Related issues:

```[tasklist]
### `iloc` bugs
- [ ] #12748 
- [ ] #13013
- [ ] #13015
- [ ] #13265
- [ ] #13266 
- [ ] #13267
- [ ] #13515
- [ ] #13293
```
```[tasklist]
### Index bugs
- [ ] #12954
```

```[tasklist]
### `loc` bugs
- [ ] #7448
- [ ] #8585
- [ ] #8693
- [ ] #11298
- [ ] #11944
- [ ] #12259
- [ ] #12286
- [ ] #12504
- [ ] #12505
- [ ] #12801
- [ ] #12833
- [ ] #13014
- [ ] #13015
- [ ] #13031
- [ ] #13268
- [ ] #13269
- [ ] #13270
- [ ] #13379
- [ ] https://github.com/rapidsai/cudf/issues/13653
- [ ] https://github.com/rapidsai/cudf/issues/13658
- [ ] https://github.com/rapidsai/cudf/issues/13652
```
```[tasklist]
### Views vs. copies
- [ ] #7374
- [ ] #11085
- [ ] #11990 
```

```[tasklist]
### Other (mostly dtype-related)
- [ ] #2684
- [ ] #8184
- [ ] #11477
- [ ] #12039
- [ ] #13532
```

Your issue here.

As we can see from this classification, `loc`-based indexing is definitely the harder nut to crack. The edge-cases that provoke most of the issues are cases where the values used in the indexing are _not_ in the index.",2023-02-16T16:49:43Z,0,0,Lawrence Mitchell,,False
503,[FEA] broadcast assignment through loc[],"**Is your feature request related to a problem? Please describe.**
working with `import cudf as pd`

**Describe the solution you'd like**
```
In [1]: import cudf as pd

In [2]: pd.__version__
Out[2]: '23.02.00'

In [3]: df = pd.DataFrame({'a': range(10)})

In [4]: df.loc[df.a > 5, ['b']] = 'ok'
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
/opt/conda/envs/rapids/lib/python3.8/site-packages/cudf/core/dataframe.py in _setitem_tuple_arg(self, key, value)
    318         try:
--> 319             columns_df = self._frame._get_columns_by_label(key[1])
    320         except KeyError:

/opt/conda/envs/rapids/lib/python3.8/contextlib.py in inner(*args, **kwds)
     74             with self._recreate_cm():
---> 75                 return func(*args, **kwds)
     76         return inner

/opt/conda/envs/rapids/lib/python3.8/site-packages/cudf/core/dataframe.py in _get_columns_by_label(self, labels, downcast)
   1902         """"""
-> 1903         new_data = super()._get_columns_by_label(labels, downcast)
   1904         if downcast:

/opt/conda/envs/rapids/lib/python3.8/contextlib.py in inner(*args, **kwds)
     74             with self._recreate_cm():
---> 75                 return func(*args, **kwds)
     76         return inner

/opt/conda/envs/rapids/lib/python3.8/site-packages/cudf/core/frame.py in _get_columns_by_label(self, labels, downcast)
    417         """"""
--> 418         return self._data.select_by_label(labels)
    419 

/opt/conda/envs/rapids/lib/python3.8/site-packages/cudf/core/column_accessor.py in select_by_label(self, key)
    349         elif pd.api.types.is_list_like(key) and not isinstance(key, tuple):
--> 350             return self._select_by_label_list_like(key)
    351         else:

/opt/conda/envs/rapids/lib/python3.8/site-packages/cudf/core/column_accessor.py in _select_by_label_list_like(self, key)
    464     def _select_by_label_list_like(self, key: Any) -> ColumnAccessor:
--> 465         data = {k: self._grouped_data[k] for k in key}
    466         if self.multiindex:

/opt/conda/envs/rapids/lib/python3.8/site-packages/cudf/core/column_accessor.py in <dictcomp>(.0)
    464     def _select_by_label_list_like(self, key: Any) -> ColumnAccessor:
--> 465         data = {k: self._grouped_data[k] for k in key}
    466         if self.multiindex:

KeyError: 'b'

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-4-18a695e97af4> in <module>
----> 1 df.loc[df.a > 5, ['b']] = 'ok'

/opt/conda/envs/rapids/lib/python3.8/site-packages/cudf/core/dataframe.py in __setitem__(self, key, value)
    148         if not isinstance(key, tuple):
    149             key = (key, slice(None))
--> 150         return self._setitem_tuple_arg(key, value)
    151 
    152     @_cudf_nvtx_annotate

/opt/conda/envs/rapids/lib/python3.8/contextlib.py in inner(*args, **kwds)
     73         def inner(*args, **kwds):
     74             with self._recreate_cm():
---> 75                 return func(*args, **kwds)
     76         return inner
     77 

/opt/conda/envs/rapids/lib/python3.8/site-packages/cudf/core/dataframe.py in _setitem_tuple_arg(self, key, value)
    334             new_col = cudf.Series(value, index=idx)
    335             if not self._frame.empty:
--> 336                 new_col = new_col._align_to_index(
    337                     self._frame.index, how=""right""
    338                 )

/opt/conda/envs/rapids/lib/python3.8/site-packages/cudf/core/indexed_frame.py in _align_to_index(self, index, how, sort, allow_non_unique)
   2274         if not allow_non_unique:
   2275             if not self.index.is_unique or not index.is_unique:
-> 2276                 raise ValueError(""Cannot align indices with non-unique values"")
   2277 
   2278         lhs = cudf.DataFrame._from_data(self._data, index=self.index)

ValueError: Cannot align indices with non-unique values

In [5]: pdf = df.to_pandas()

In [6]: pdf.loc[pdf.a > 5, ['b']] = 'ok'

In [7]: pdf
Out[7]: 
   a    b
0  0  NaN
1  1  NaN
2  2  NaN
3  3  NaN
4  4  NaN
5  5  NaN
6  6   ok
7  7   ok
8  8   ok
9  9   ok
```",2023-02-17T14:16:07Z,0,0,Matthew Farrellee,,False
504,[FEA] Throw more specific exceptions in libcudf,"**Is your feature request related to a problem? Please describe.**
The discussion in #10200 demonstrated a general desire to have libcudf provide more useful diagnostics of failure modes to users by throwing more informative exceptions for different types of failures instead of always throwing `cudf::logic_error`. #12426 implemented the necessary scaffolding in cuDF Python and the libcudf JNI to allow libcudf to use any C++ exception -- including custom exceptions defined within libcudf -- and have these mapped to appropriate Java or Python exceptions. #12426 also demonstrated how such a custom exception could be defined, introducing `cudf::data_type_error`. The purpose of this issue is to serve as tracking for the task of systematically converting libcudf to throw better exceptions.

**Describe the solution you'd like**
I propose that we tackle this task on a file-by-file basis. This issue can serve as a tracker for which files have been updated as well as the definitive source of what exceptions libcudf throws under what circumstances. For each file, a developer would go through each CUDF_EXPECTS/CUDF_FAIL call and see if there is a suitable alternative to `cudf::logic_error`. If none currently exists, developers should propose a new exception and explain situations in which it should be used in a new issue. If the proposal is accepted, the table of exceptions below should be updated.

Table of exceptions:

| Exception        | Description           | Issue    | Status    |
| ------------- |:-------------:|:-------------:|:-------------:|
| std::invalid_argument      | A function parameter is provided a value that is invalid in the given context. | https://github.com/rapidsai/cudf/issues/10632 | ✅ by https://github.com/rapidsai/cudf/pull/12426 |
| cudf::data_type_error      | A function is called with a column or scalar with a `data_type` for which the function is invalid. A common example is in the default SFINAE overloads of type-dispatched functions. | https://github.com/rapidsai/cudf/issues/10632 | ✅ by https://github.com/rapidsai/cudf/pull/12426 |
| std::overflow_error | The output column corresponding to the input has size exceeds cudf limit. | https://github.com/rapidsai/cudf/issues/12925 | #13323 |
| std::out_of_range | An index argument or range includes indices that are out of bounds for the column being selected | #15315 | Some work in #15319 |

List of files to be updated (please add new lines to the task list when a file is updated):",2023-03-06T18:26:52Z,0,0,Vyas Ramasubramani,@rapidsai,True
505,[FEA][JNI] Add an API that allows us to build batches on the GPU from host columns,"We currently use the ColumnBuilder API in `HostColumnVector.java` to build batches on the spark side. The problem with this API is that as soon as an attempt is made to put host data onto the GPU, the host data is put in a try-with-resources block that closes it. If the creation of the GPU data fails, or a subsequent column in a batch fails to materialize, we cannot retry it (ColumnBuilder does not allow me to build an idempotent BatchBuilder).

This task is to likely add a BatchBuilder that allows us to control when/if the host data is freed, so we can work in retry semantics like https://github.com/NVIDIA/spark-rapids/issues/7851.",2023-03-06T23:24:50Z,0,0,Alessandro Bellina,NVIDIA,True
506,[DEP]: Remove `line_terminator` from `to_csv`,"Followup to #12896, remove the deprecated keyword argument.",2023-03-07T17:19:48Z,0,0,Lawrence Mitchell,,False
507,[FEA][JNI] Leverage cub's multi-buffer copy algorithm in JNI bindings,"We would like to use https://github.com/NVIDIA/cub/issues/297 for our spilling logic https://github.com/NVIDIA/spark-rapids/issues/7672 in the spark-rapids plugin. 

We currently contiguous_split every single buffer that _could be_ spilled in the future, because at the time of spill the last thing we want to do is double our memory usage right before copying to host. The multi-buffer work should allow us to bounce-buffer out of GPU memory at spill time, without having to contiguous_split for spillable buffers.",2023-03-07T19:08:27Z,0,0,Alessandro Bellina,NVIDIA,True
508,dask_cudf.read_csv doesn't support StringIO ,"`dask_cudf.read_csv()` is calling `cudf.read_csv()` internally.
However, dask_cudf.read_csv() doesn't support StringIO input.

Description of api mislead (it seems copied over from cudf.read_csv())
```
    path : str, path object, or file-like object
        Either a path to a file (a str, pathlib.Path, or
        py._path.local.LocalPath), URL (including http, ftp, and S3 locations),
        or any object with a read() method (such as builtin open() file
        handler function or StringIO).
```
Currently, StringIO input falls into https://github.com/rapidsai/cudf/blob/branch-23.04/python/dask_cudf/dask_cudf/io/csv.py#L99
",2023-03-08T16:55:39Z,0,0,,,False
509,[FEA] Rename `cudf::structs::detail::flattened_table` and its member function `flattened_columns()`,"The current names of these class/function are very confusing:
 * The class `flattened_table` is not any table. Instead, it is a data structure holding various types of data.
 * The function name `flattened_columns()` suggests that the return type is a list of columns, but it returns a `table_view` instead.

With these name, devs may misunderstand their purpose and make wrong assumption. The issue was raised in several places, for example: https://github.com/rapidsai/cudf/pull/12878#discussion_r1126932626, and https://github.com/rapidsai/cudf/pull/12878#discussion_r1128235975.

We should rename those class/function to better reflect their purposes.
",2023-03-08T23:20:36Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
510,[BUG] Slow performance with high cardinality category columns,"**Describe the bug**
I'm not sure if it is a bug or just limitation of cuDF architecture.
Please correct me. 
 
I have DataFrames with lot of columns and some of columns are strings with type **category**.
My data sets have about 5_000_000 rows. And category columns have more than 500_000 unique values. 
What I'm wondering is that copy of categorical columns (CPU -> GPU) takes a lot time in comparison the same data  not categorized. 
More strange is that **groupby** function on categorical columns is many times slower that on non categorical columns. (Same content)   

**Steps/Code to reproduce bug**
create 2 arrays with categorical data. There are 500_000 unique values
```
cat1 = [ 'col1_cat_' + str(i)  for i in range(1, 500_000)] 
cat2 = [ 'col2_cat_' + str(i)  for i in range(1, 500_000)]
```
initialize Panda DataFrame with 5_000_000 rows
```
rng = np.random.default_rng()
col1 = np.random.choice(a=cat1,  size=5_000_000)  
col2 = np.random.choice(a=cat2,  size=5_000_000)  

df = pd.DataFrame( {  'col1': col1,  'col2': col2,  'col3': col1,  'col4': col2  })
```

size of DataFrame is about 1.5 GB
```
>> df.memory_usage(deep=True) / 1024 / 1024
Index      0.000122
col1     342.262686
col2     342.264298
col3     342.262686
col4     342.264298
dtype: float64
```
We execute groupby on CPU.  
```
%%time
df.groupby(['col1']).count()
```
*CPU times: user 2.43 s, sys: 23.8 ms, total: 2.46 s
Wall time: 2.46 s*

Now we convert string data to category type.
```
df_cat = df[['col1', 'col2', 'col3', 'col4']].astype('category')
>>df_cat.memory_usage(deep=True) / 1024 / 1024
Index     0.000122
col1     69.423543
col2     69.423198
col3     69.423543
col4     69.423198
dtype: float64
```
DF size is about 300 MB
```
%%time
df_cat.groupby(['col1']).count()
```
*CPU times: user 50.5 ms, sys: 8.29 ms, total: 58.8 ms
Wall time: 57.7 ms*

With categorical data we are __40 times__ faster that with non categorical data. That is expected performance.
Now we copy non categorized data to the GPU
```
%%time
gdf = cudf.DataFrame.from_pandas(df)
```
*CPU times: user 511 ms, sys: 221 ms, total: 731 ms
Wall time: 730 ms*
Copy of date takes about 700ms for 1.5GB

```
%%time
gdf.groupby(['col1']).count()
```
*CPU times: user 30.6 ms, sys: 3.8 ms, total: 34.4 ms
Wall time: 33 ms*
**groupby** is about 70 times faster that on CPU (2.46s). Very good performance. IMPORTANT data is **NOT CATEGORIZED**

Now we copy categorized data to the GPU and execute **groupby**
```
%%time
gdf_cat = cudf.DataFrame.from_pandas(df_cat)
```
*CPU times: user 711 ms, sys: 136 ms, total: 847 ms
Wall time: 846 ms*
```
%%time
gdf_cat.groupby(['col1']).count()
```
*CPU times: user 134 ms, sys: 55.8 ms, total: 190 ms
Wall time: 189 ms*

Amount of data is about 300MB. cuDf needs more time to copy the data to the GPU: 846ms (730ms by non categorized 1.5 GB raw strings)
But really strange behaviour is that **groupby**  on categorized data is more than 5 times slower than with raw string data!!!
**Even groupby on CPU is faster that on GPU by categorical data 57ms --> 189ms!!!!!**

Why is performance of cuDF groupy with categorical data so poor?

**Expected behavior**
**groupby** function on GPU should work faster or at least equal than on CPU

**Environment overview (please complete the following information)**
CPU RAM: 64 GB
GPU RAM: 8GB

**Environment details**
tested on 3 different systems with 3 different NVIDIA cards

**Additional context**
In attachments you can find my Jupiter Notebook with example. 
[category.ipynb.zip](https://github.com/rapidsai/cudf/files/10936367/category.ipynb.zip)
",2023-03-09T21:53:26Z,0,0,Slava,TriasDev,False
511,[FEA] Add more unit tests for nested lists and structs after having full support for lexicographic comparator,"We have full support for nested types in lexicographic comparison (by https://github.com/rapidsai/cudf/pull/12879). Having such feature, a lot of operations now can process input with more complex nested types input. For example: groupby/reduction etc.

The PR https://github.com/rapidsai/cudf/pull/12879 only added relevant unit tests for nested types in lexicographic comparison tests. Other operations also need to update their corresponding unit tests with more nested type input.",2023-03-09T23:40:39Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
512,[BUG] Issues while loading TimeStamp columns,"```
In [2]: import cudf

In [3]: cudf.read_orc('tsrepro.orc')
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In[3], line 1
----> 1 cudf.read_orc('tsrepro.orc')

File /conda/envs/rapids-23.02/lib/python3.10/site-packages/cudf/io/orc.py:370, in read_orc(filepath_or_buffer, engine, columns, filters, stripes, skiprows, num_rows, use_index, timestamp_type, use_python_file_object, storage_options, bytes_per_thread)
    366         stripes = selected_stripes
    368 if engine == ""cudf"":
    369     return DataFrame._from_data(
--> 370         *liborc.read_orc(
    371             filepaths_or_buffers,
    372             columns,
    373             stripes,
    374             skiprows,
    375             num_rows,
    376             use_index,
    377             timestamp_type,
    378         )
    379     )
    380 else:
    382     def read_orc_stripe(orc_file, stripe, columns):

File orc.pyx:84, in cudf._lib.orc.read_orc()

File orc.pyx:121, in cudf._lib.orc.read_orc()

RuntimeError: CUDF failure at: /opt/conda/conda-bld/work/cpp/src/io/orc/timezone.cpp:138: Failed to open the timezone file.
```

[tsrepro.orc.zip](https://github.com/rapidsai/cudf/files/10946593/tsrepro.orc.zip)
",2023-03-13T02:21:39Z,0,0,Lahir Marni,,False
513,[FEA] Faster dataframe to cupy conversion when dataframe is a single allocation,"When we convert a dataframe to a cupy array, we iterate over each column (as they’re independent allocations) and assign each one to a column in an empty matrix. This means it can be slow for thousands or millions of small columns.

In a select set of circumstances, all of the columns in a DataFrame may be part of a single, contiguous allocation of memory. One scenario in which this can occur is after a call to transpose. It would be nice if, in this scenario, we didn't need to iterate over every column when converting to a cupy array.

A real-world example of when this can matter is if a user is trying to run a dot product after a calling transpose. Because of the bottleneck, we're slower than pandas by quite a bit.

```python
import cudf
import cupy as cp
import pandas as pd

nrows = 10000
ncols = 4

gdf = cudf.DataFrame(cp.random.randint(0, 1000, size=(nrows, ncols)))
pdf = gdf.to_pandas()

%time gdf.T.dot(gdf)
%time pdf.T.dot(pdf)
CPU times: user 1.52 s, sys: 3.96 ms, total: 1.53 s
Wall time: 1.53 s
CPU times: user 912 µs, sys: 41 µs, total: 953 µs
Wall time: 855 µs
```

If we were to do any special casing here, we'd want to closely evaluate any impact on performance for the more general case, as the dataframe to cupy codepath is used across the board.",2023-03-13T14:59:58Z,0,0,Nick Becker,@NVIDIA,True
514,[FEA] Investigate removing C++ tests of detail APIs,"**Is your feature request related to a problem? Please describe.**
Some C++ test files are written to test detail APIs rather than public APIs. One particular example that was brought up in https://github.com/rapidsai/cudf/pull/12888#discussion_r1132719780 is [`detail_gather_tests.cu`](https://github.com/rapidsai/cudf/blob/branch-23.04/cpp/tests/copying/detail_gather_tests.cu). Ideally we should not be testing detail APIs directly, only public APIs. In practice, we should minimize cases where we must test detail APIs directly.

**Describe the solution you'd like**
We should evaluate removing tests of detail APIs. In cases where the associated public APIs are not sufficiently tested, the detail tests should be converted to test the public APIs. In other cases where the detail tests are purely redundant they should be removed. If detail APIs are being called as part of a sequence of cudf calls in a more complex test of public APIs, those calls should be rewritten to use public APIs. The remainder should be cases where detail APIs lack an exact public analog and testing the underlying APIs is valuable. We will need to assess those tests carefully.
",2023-03-13T15:37:04Z,0,0,Vyas Ramasubramani,@rapidsai,True
515,[FEA]: Support groupby.resample,"**Is your feature request related to a problem? Please describe.**
While working on a pandas to cudf workflow comparison, I noticed that `groupby(...).resample(...)` has not been implemented in cudf yet

**Describe the solution you'd like**
```
In [26]: from datetime import datetime

In [27]: import pandas as pd

In [28]: import cudf

In [29]: data = {""group"": list(""abab""), ""values"": range(4), ""ts"": [datetime(2023,
    ...:  1, 1), datetime(2023, 1, 2), datetime(2023, 1, 3), datetime(2023, 1, 4)
    ...: ]}

In [30]: df = pd.DataFrame(data)

In [31]: df.groupby(""group"").resample(""D"", on=""ts"")[""values""].mean()
Out[31]:
group  ts
a      2023-01-01    0.0
       2023-01-02    NaN
       2023-01-03    2.0
b      2023-01-02    1.0
       2023-01-03    NaN
       2023-01-04    3.0
Name: values, dtype: float64

In [32]: cu_df = cudf.DataFrame(data)

In [33]: cu_df.groupby(""group"").resample(""D"", on=""ts"")[""values""].mean()
KeyError: 'resample'

During handling of the above exception, another exception occurred:

AttributeError: DataFrameGroupBy object has no attribute resample
```

**Describe alternatives you've considered**
Can for loop over the groups of `cu_df.groupby(""group"")` and call `resample` individually.

**Additional context**
https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.resample.html
",2023-03-13T20:47:18Z,0,0,Matthew Roeschke,@rapidsai ,True
516,[BUG]  dask_cudf.read_json seems to be freezing when given a path with large number of files,"**Describe the bug**

dask_cudf.read_json seems to be freezing when given a path with large number of files. Providing the list of files directly works

Below seems to be freezing
```python3
text_ddf = dask_cudf.read_json(f'{INPUT_PATH}/data/*',engine='cudf',lines=True)
```

Below works 
```python3
files = list(map(lambda x: os.path.join(data_path, x), os.listdir(data_path)))

text_ddf = dask_cudf.read_json(files,engine='cudf',lines=True)
```

**Additional context**
CC: @ayushdg ",2023-03-15T03:54:13Z,0,0,Vibhu Jawa,Nvidia,True
517,[BUG] `is_list_like` does not match pandas equivalent for cupy arrays,"**Describe the bug**

`is_list_like` does not match pandas equivalent for cupy array objects

https://pandas.pydata.org/docs/reference/api/pandas.api.types.is_list_like.html

**Steps/Code to reproduce bug**

```python
import pandas as pd
import numpy as np
import cudf
import cupy as cp

pd.api.types.is_list_like(np.array([1]))  # => True
pd.api.types.is_list_like(cp.array([1]))  # => True

cudf.api.types.is_list_like(np.array([1]))  # => True
cudf.api.types.is_list_like(cp.array([1]))  # => False
```

**Expected behavior**

I'd expect `is_list_like` to return True matching the equivalent pandas function when called with a cupy or numpy array containing a list of values

```python
cudf.api.types.is_list_like(cp.array([1]))  # => True
```

**Environment overview (please complete the following information)**
 - Environment location: docker
 - Method of cuDF install: pip

",2023-03-20T15:04:26Z,0,0,Oliver Holworthy,,False
518,[BUG] loc indexing with length-1 Categorical not possible,"**Describe the bug**
```python
import cudf
import pandas as pd
f = pd.Series([1, 2, 3], index=pd.CategoricalIndex([0, 1, 2]))
f.loc[pd.Categorical([1])] # => second row.
cf = cudf.from_pandas(f)
cf.loc[pd.Categorical([1])] # => KeyErrror
```

I think this is similar to #13013 in that length-one categoricals are treated as scalars in some cases.

**Expected behavior**

Probably this lookup should work.",2023-03-27T10:28:01Z,0,0,Lawrence Mitchell,,False
519,[FEA] Performance issue with the Parquet reader for very large schemas (especially when containing strings),"
For parquet files that contain very large schemas with strings (either large numbers of columns, or large numbers of nested columns) we pay a very heavy price postprocessing the string data after the core decode kernels runs.  

Essentially, the ""decode"" process for strings is just emitting a large array of pointer/size pairs that are then passed to other cudf functions to reconstruct actual columns.    The problem is that we are doing this with no batching - each output string column results in an entire cudf function call (`make_strings_column`) with multiple internal kernel calls each.  In situations with thousands of columns, this gets very expensive.

![image](https://user-images.githubusercontent.com/56695930/228312025-67ea3177-9d67-4e84-84e5-cb317c895001.png)

In the image above, the green span represents the time spent in the decode kernel and the time spent in all of the `make_strings_column` calls afterwards.  The time is totally dominated by the many many calls to `make_strings_column` (the red span).  

Ideally, we would have some kind of batched interface to `make_strings_column`  (`make_strings_columns` ?) that can do the work for the thousands of output columns coalesced into fewer kernels.  


On a related note, the area under the blue line represents a similar problem involving preprocessing the file (thousands of calls to `thrust::reduce` and `thrust::exclusive_scan_by_key`).   This has been largely addressed by this PR  https://github.com/rapidsai/cudf/pull/12931  
",2023-03-28T17:01:42Z,0,0,,,False
520,[BUG] loc-based setitem with dataframe/series as rvalue doesn't match pandas,"**Describe the bug**

When performing `loc`-based setitem, pandas aligns the indices of the rvalue to match that of the lvalue. Consequently, the indexed lvalue and rvalue don't have to have the same shape (or even match indices).

```python
import pandas as pd
s = pd.Series([1, 2, 3], index=['a', 'b', 'c'])
o1 = pd.Series([5, 6, 7], index=['a', 'b', 'c'])
o2 = pd.Series([10])
s.loc[['a', 'c']] = o1
s # => [5, 2, 7]
s.loc[['a', 'b', 'c']] = o2
s # => [Nan, Nan, Nan]
```

In contrast, cudf complains in both cases.

For the former: `ValueError: Size mismatch: cannot set value of size 3 to indexing result of size 2`

For the latter: `align_to_index` fails (because the int index cannot be merged with the string index). If `o2` has a string index, it fails because the pruned aligned series is not the right length.

**Expected behavior**

Probably this should behave as pandas. I think the case where the index types are different will probably need to be special-cased.",2023-03-29T14:38:07Z,0,0,Lawrence Mitchell,,False
521,[FEA] Unify `cudf::structs::detail::flatten_nested_columns` and `cudf::experimental::decompose_structs` to improve performance for structs comparison,"For comparing structs column, both the legacy row comparators and the new experimental row comparators rely on struct flattening procedures. Each of them have their own flattening mechanism: `cudf::structs::detail::flatten_nested_columns` and `cudf::experimental::decompose_structs`. The difference between them are:
 * `cudf::structs::detail::flatten_nested_columns` replaces the input structs column with an optional column generated by materializing the input null mask. 
 * `cudf::experimental::decompose_structs` doesn't materialize any new column. Instead, it replaces the input structs column with a modified version of it, which only has either zero or one child at the innermost level.

Although these APIs produce different output, these APIs do very similar job:
 * Both extract the input structs column into a table of children columns, which are much simpler than the input structs column to be compared on device code.
 * Both replace the input by a new column, and this new column is mainly used for checking nulls.

The issue of each from these approaches are:
 * `cudf::structs::detail::flatten_nested_columns` needs to materialize null mask of the input column into a real column.
 * `cudf::experimental::decompose_structs` still has a nested structs column in the output. Although that column only has zero or one child at the innermost level, it still causes performance degradation if its nested level is very high.

As such, we can unify the two approaches, taking the pros of both while eliminating the cons. The new flattening API should:
 * Avoid materializing new columns, and
 * Avoid output columns having more than one nested level.

This seems to be very straightforward with modifying the existing `cudf::experimental::decompose_structs` API.",2023-03-29T17:06:37Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
522,Span tests have hard-coded values,"Vukasin pointed out in code review of 12981 that the span tests are using some hard-coded magic numbers. These should be addressed.

_Originally posted by @vuule in https://github.com/rapidsai/cudf/pull/12981#discussion_r1149684948_
            ",2023-03-29T17:37:58Z,0,0,Mike Wilson,,False
523,"[QST] `dask_cudf.read_parquet` failed with ""NotImplementedError: large_string""","I am a new user of `dask`/`dask_cudf`.
I have parquet files of various sizes (11GB, 2.5GB, 1.1GB), all of which failed with `NotImplementedError: large_string`. 

My `dask.dataframe` backend is `cudf`. When the backend is `pandas`, `read.parquet` works fine.

Here's an exerpt of what my data looks like in `csv` format:

    Symbol,Date,Open,High,Low,Close,Volume
    AADR,17-Oct-2017 09:00,57.47,58.3844,57.3645,58.3844,2094
    AADR,17-Oct-2017 10:00,57.27,57.2856,57.25,57.27,627
    AADR,17-Oct-2017 11:00,56.99,56.99,56.99,56.99,100
    AADR,17-Oct-2017 12:00,56.98,57.05,56.98,57.05,200
    AADR,17-Oct-2017 13:00,57.14,57.16,57.14,57.16,700
    AADR,17-Oct-2017 14:00,57.13,57.13,57.13,57.13,100
    AADR,17-Oct-2017 15:00,57.07,57.07,57.07,57.07,200
    AAMC,17-Oct-2017 09:00,87,87,87,87,100
    AAU,17-Oct-2017 09:00,1.1,1.13,1.0832,1.121,67790
    AAU,17-Oct-2017 10:00,1.12,1.12,1.12,1.12,100
    AAU,17-Oct-2017 11:00,1.125,1.125,1.125,1.125,200
    AAU,17-Oct-2017 12:00,1.1332,1.15,1.1332,1.15,27439
    AAU,17-Oct-2017 13:00,1.15,1.15,1.13,1.13,8200
    AAU,17-Oct-2017 14:00,1.1467,1.1467,1.14,1.1467,1750
    AAU,17-Oct-2017 15:00,1.1401,1.1493,1.1401,1.1493,4100
    AAU,17-Oct-2017 16:00,1.13,1.13,1.13,1.13,100
    ABE,17-Oct-2017 09:00,14.64,14.64,14.64,14.64,200
    ABE,17-Oct-2017 10:00,14.67,14.67,14.66,14.66,1200
    ABE,17-Oct-2017 11:00,14.65,14.65,14.65,14.65,600
    ABE,17-Oct-2017 15:00,14.65,14.65,14.65,14.65,836


What I did was really simple:

    import dask.dataframe as dd
    import cudf
    import dask_cudf
    
    # Failed with large_string error
    dask_cudf.read_parquet('path/to/my.parquet')
    # Failed with large_string error
    dd.read_parquet('path/to/my.parquet')

The only large string I could think of is the timestamp string.

Is there a way around this in `cudf` as it is not implemented yet? The format is `2023-03-12 09:00:00+00:00`.



",2023-03-30T16:14:30Z,0,0,QiuxiaoMu,,False
524,[QST] convert column of datetime string to column of datetime object,"I am a new user of Dask and RapidsAI.
An exerpt of my data (in `csv` format):

    Symbol,Date,Open,High,Low,Close,Volume
    AADR,17-Oct-2017 09:00,57.47,58.3844,57.3645,58.3844,2094
    AADR,17-Oct-2017 10:00,57.27,57.2856,57.25,57.27,627
    AADR,17-Oct-2017 11:00,56.99,56.99,56.99,56.99,100
    AADR,17-Oct-2017 12:00,56.98,57.05,56.98,57.05,200
    AADR,17-Oct-2017 13:00,57.14,57.16,57.14,57.16,700
    AADR,17-Oct-2017 14:00,57.13,57.13,57.13,57.13,100
    AADR,17-Oct-2017 15:00,57.07,57.07,57.07,57.07,200
    AAMC,17-Oct-2017 09:00,87,87,87,87,100
    AAU,17-Oct-2017 09:00,1.1,1.13,1.0832,1.121,67790
    AAU,17-Oct-2017 10:00,1.12,1.12,1.12,1.12,100
    AAU,17-Oct-2017 11:00,1.125,1.125,1.125,1.125,200
    AAU,17-Oct-2017 12:00,1.1332,1.15,1.1332,1.15,27439
    AAU,17-Oct-2017 13:00,1.15,1.15,1.13,1.13,8200
    AAU,17-Oct-2017 14:00,1.1467,1.1467,1.14,1.1467,1750
    AAU,17-Oct-2017 15:00,1.1401,1.1493,1.1401,1.1493,4100
    AAU,17-Oct-2017 16:00,1.13,1.13,1.13,1.13,100
    ABE,17-Oct-2017 09:00,14.64,14.64,14.64,14.64,200
    ABE,17-Oct-2017 10:00,14.67,14.67,14.66,14.66,1200
    ABE,17-Oct-2017 11:00,14.65,14.65,14.65,14.65,600
    ABE,17-Oct-2017 15:00,14.65,14.65,14.65,14.65,836

Note `Date` column is of type string.

I have some example stock market timeseries data (i.e., DOHLCV) in csv files and I read them into a `dask_cudf` dataframe (my `dask.dataframe` backend is cudf and `read.csv` is a creation dispacther that conveniently gives me a `cudf.dataframe`). 

    import dask_cudf 
    import cudf
    from dask import dataframe as dd
    
    ddf = dd.read_csv('path/to/my/data/*.csv')
    ddf
    # output
    <dask_cudf.DataFrame | 450 tasks | 450 npartitions>
    
    
    # test csv data above can be retrieved using following statements
    # df = pd.read_clipboard(sep="","")
    # cdf = cudf.from_pandas(df)
    # ddf = dask_cudf.from_cudf(cdf, npartitions=2)

I then try to convert datetime string into real datetime object (`np.datetime64[ns]` or anything equivalent in `cudf`/`dask` world). I then failed with error.

    df[""Date""] = dd.to_datetime(df[""Date""], format=""%d-%b-%Y %H:%M"").head(5)
    df.set_index(""Date"", inplace=True) # This failed with different error, will raise in a different SO thread.
    # Following statement gives me same error.
    # cudf.to_datetime(df[""Date""], format=""%d-%b-%Y %H:%M"")

Full error log is to the end.

The error message seems to suggest that I'd need to `compute` the `dask_cudf.dataframe`, turning it into a real `cudf` object, then I
can do as I would in `pandas`:

    df[""Date""] = cudf.to_datetime(df.Date)
    df = df.set_index(df.Date)

This apparently isn't ideal and it very much is the thing that `dask` is for: we'd delay this and only calculate the ultimate number we need.

what is the `dask`/`dask_cudf` way to convert a string column to datetime column in `dask_cudf`? As far as I can see, if the backend is `pandas`, the conversion is done smoothly and rarely has problem. 

Or, is it that `cudf` or GPU world in general, is not supposed to do much with date types like `datetime`, `string` ? (e.g., ideally GPU is geared towards expensive numerical computations). 

My use case involves some filtering to do with `string` and `datetime`, therefore I need to set up the `dataframe` with proper `datetime` object.

#### Error Log

    TypeError                                 Traceback (most recent call last)
    Cell In[52], line 1
    ----> 1 dd.to_datetime(df[""Date""], format=""%d-%b-%Y %H:%M"").head(2)
    
    File ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask/dataframe/core.py:1268, in _Frame.head(self, n, npartitions, compute)
       1266 # No need to warn if we're already looking at all partitions
       1267 safe = npartitions != self.npartitions
    -> 1268 return self._head(n=n, npartitions=npartitions, compute=compute, safe=safe)
    
    File ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask/dataframe/core.py:1302, in _Frame._head(self, n, npartitions, compute, safe)
       1297 result = new_dd_object(
       1298     graph, name, self._meta, [self.divisions[0], self.divisions[npartitions]]
       1299 )
       1301 if compute:
    -> 1302     result = result.compute()
       1303 return result
    
    File ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask/base.py:314, in DaskMethodsMixin.compute(self, **kwargs)
        290 def compute(self, **kwargs):
        291     """"""Compute this dask collection
        292 
        293     This turns a lazy Dask collection into its in-memory equivalent.
       (...)
        312     dask.base.compute
        313     """"""
    --> 314     (result,) = compute(self, traverse=False, **kwargs)
        315     return result
    
    File ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask/base.py:599, in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)
        596     keys.append(x.__dask_keys__())
        597     postcomputes.append(x.__dask_postcompute__())
    --> 599 results = schedule(dsk, keys, **kwargs)
        600 return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])
    
    File ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask/threaded.py:89, in get(dsk, keys, cache, num_workers, pool, **kwargs)
         86     elif isinstance(pool, multiprocessing.pool.Pool):
         87         pool = MultiprocessingPoolExecutor(pool)
    ---> 89 results = get_async(
         90     pool.submit,
         91     pool._max_workers,
         92     dsk,
         93     keys,
         94     cache=cache,
         95     get_id=_thread_get_id,
         96     pack_exception=pack_exception,
         97     **kwargs,
         98 )
        100 # Cleanup pools associated to dead threads
        101 with pools_lock:
    
    File ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask/local.py:511, in get_async(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)
        509         _execute_task(task, data)  # Re-execute locally
        510     else:
    --> 511         raise_exception(exc, tb)
        512 res, worker_id = loads(res_info)
        513 state[""cache""][key] = res
    
    File ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask/local.py:319, in reraise(exc, tb)
        317 if exc.__traceback__ is not tb:
        318     raise exc.with_traceback(tb)
    --> 319 raise exc
    
    File ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask/local.py:224, in execute_task(key, task_info, dumps, loads, get_id, pack_exception)
        222 try:
        223     task, data = loads(task_info)
    --> 224     result = _execute_task(task, data)
        225     id = get_id()
        226     result = dumps((result, id))
    
    File ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask/core.py:119, in _execute_task(arg, cache, dsk)
        115     func, args = arg[0], arg[1:]
        116     # Note: Don't assign the subtask results to a variable. numpy detects
        117     # temporaries by their reference count and can execute certain
        118     # operations in-place.
    --> 119     return func(*(_execute_task(a, cache) for a in args))
        120 elif not ishashable(arg):
        121     return arg
    
    File ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask/optimization.py:990, in SubgraphCallable.__call__(self, *args)
        988 if not len(args) == len(self.inkeys):
        989     raise ValueError(""Expected %d args, got %d"" % (len(self.inkeys), len(args)))
    --> 990 return core.get(self.dsk, self.outkey, dict(zip(self.inkeys, args)))
    
    File ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask/core.py:149, in get(dsk, out, cache)
        147 for key in toposort(dsk):
        148     task = dsk[key]
    --> 149     result = _execute_task(task, cache)
        150     cache[key] = result
        151 result = _execute_task(out, cache)
    
    File ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask/core.py:119, in _execute_task(arg, cache, dsk)
        115     func, args = arg[0], arg[1:]
        116     # Note: Don't assign the subtask results to a variable. numpy detects
        117     # temporaries by their reference count and can execute certain
        118     # operations in-place.
    --> 119     return func(*(_execute_task(a, cache) for a in args))
        120 elif not ishashable(arg):
        121     return arg
    
    File ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask/utils.py:72, in apply(func, args, kwargs)
         41 """"""Apply a function given its positional and keyword arguments.
         42 
         43 Equivalent to ``func(*args, **kwargs)``
       (...)
         69 >>> dsk = {'task-name': task}  # adds the task to a low level Dask task graph
         70 """"""
         71 if kwargs:
    ---> 72     return func(*args, **kwargs)
         73 else:
         74     return func(*args)
    
    File ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask/dataframe/core.py:6821, in apply_and_enforce(*args, **kwargs)
       6819 func = kwargs.pop(""_func"")
       6820 meta = kwargs.pop(""_meta"")
    -> 6821 df = func(*args, **kwargs)
       6822 if is_dataframe_like(df) or is_series_like(df) or is_index_like(df):
       6823     if not len(df):
    
    File ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:1100, in to_datetime(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)
       1098         result = _convert_and_box_cache(argc, cache_array)
       1099     else:
    -> 1100         result = convert_listlike(argc, format)
       1101 else:
       1102     result = convert_listlike(np.array([arg]), format)[0]
    
    File ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:413, in _convert_listlike_datetimes(arg, format, name, tz, unit, errors, infer_datetime_format, dayfirst, yearfirst, exact)
        410         return idx
        411     raise
    --> 413 arg = ensure_object(arg)
        414 require_iso8601 = False
        416 if infer_datetime_format and format is None:
    
    File pandas/_libs/algos_common_helper.pxi:33, in pandas._libs.algos.ensure_object()
    
    File ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/cudf/core/frame.py:451, in Frame.__array__(self, dtype)
        450 def __array__(self, dtype=None):
    --> 451     raise TypeError(
        452         ""Implicit conversion to a host NumPy array via __array__ is not ""
        453         ""allowed, To explicitly construct a GPU matrix, consider using ""
        454         "".to_cupy()\nTo explicitly construct a host matrix, consider ""
        455         ""using .to_numpy().""
        456     )
    
    TypeError: Implicit conversion to a host NumPy array via __array__ is not allowed, To explicitly construct a GPU matrix, consider using .to_cupy()
    To explicitly construct a host matrix, consider using .to_numpy().



",2023-03-30T16:25:12Z,0,0,QiuxiaoMu,,False
525,[FEA] `np.shape` does not work on `cudf.DataFrame`,"Curious if any numpy function that dispatches to `__array_function__` should work on `cudf.DataFrame`

```
In [3]: import numpy as np

In [4]: np.shape(pd.DataFrame([1, 2]))
Out[4]: (2, 1)

In [6]: np.shape(cudf.DataFrame([1, 2]))
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[6], line 1
----> 1 np.shape(cudf.DataFrame([1, 2]))

File <__array_function__ internals>:180, in shape(*args, **kwargs)

TypeError: no implementation found for 'numpy.shape' on types that implement __array_function__: [<class 'cudf.core.dataframe.DataFrame'>]
```",2023-03-30T23:12:00Z,0,0,Matthew Roeschke,@rapidsai ,True
526,[FEA] Story - Improve performance with long strings,"Many [strings APIs in libcudf](https://docs.rapids.ai/api/libcudf/stable/group__strings__apis.html) use thread-per-string parallelism in their implementation. This approach works great for processing smaller strings of relatively consistent length. However, for long strings (roughly 256 bytes and above) the performance of thread-per-string algorithms begins to degrade. Some strings APIs are compatible with data-parallel algorithms and can be refactored to improve performance for long strings, while other strings APIs are difficult to refactor with data-parallel algorithms. 

Let's use this issue to track the progression: 
✅ - this API works well with long strings
🟢 - we think this API will be straightforward to refactor
🟡 - we have some ideas on how to refactor this API, and we'll need to experiment
🔴 - we think this will be very difficult to refactor!
⚪ - long string support is not a priority for this API

|Module|Function|Status|Notes|
|---|---|---|---|
| [Case](https://docs.rapids.ai/api/libcudf/nightly/group__strings__case.html) | capitalize <br> title <br> is_title <br> to_lower <br> to_upper <br> swapcase | 🟡 <br> 🟡 <br> 🟡 <br> ✅#13142 <br> ✅#13142 <br>✅#13142  | |
| [Character Types](https://docs.rapids.ai/api/libcudf/nightly/group__strings__types.html) | all_characters_of_type <br> filter_characters_of_type | ✅#13259 <br> 🔴 | |
| [Combining](https://docs.rapids.ai/api/libcudf/nightly/group__strings__combine.html) | join_strings <br> concatenate <br> join_list_elements | ✅#13283 <br> 🟡 <br> 🔴 | |
| [Searching](https://docs.rapids.ai/api/libcudf/nightly/group__strings__contains.html) | contains_re <br> matches_re <br> count_re <br> like <br> find_all | 🟡 <br> ⚪ <br> 🔴 <br> 🟢#13594 <br> 🔴 |  |
| [Converting](https://docs.rapids.ai/api/libcudf/nightly/group__strings__convert.html) | to_XXXX <br> from_XXXX  | ⚪ <br> ⚪ | these are rarely long strings|
| [Copying](https://docs.rapids.ai/api/libcudf/nightly/group__strings__copy.html) | repeat_string <br> repeat_strings | ✅ <br> ✅ | One [overload](https://docs.rapids.ai/api/libcudf/nightly/group__strings__copy.html#ga160c075327cb4fb081db19884dba294c) is an exception  |
| [Slicing](https://docs.rapids.ai/api/libcudf/nightly/group__strings__slice.html) | slice_strings | ✅#13057 | One [overload](https://docs.rapids.ai/api/libcudf/nightly/group__strings__slice.html#ga2bc738cebebcf6d1331d6e9d13d4cd28) allows for skipping characters. <br>Long string support is not a priority for <br> `step > 1 or step < 0` |
| [Finding](https://docs.rapids.ai/api/libcudf/nightly/group__strings__find.html) | find <br> rfind <br> contains <br> starts_with <br> ends_with <br> find_multiple | ✅#13226 <br> ✅#13226 <br> ✅#10739 <br> ⚪ <br> ⚪ <br> 🟢  | |
| [Modifying](https://docs.rapids.ai/api/libcudf/nightly/group__strings__modify.html) | pad <br> zfill <br> reverse <br> strip <br> translate <br> filter_characters <br> wrap | 🟡 <br> ⚪ <br> 🟡 <br> ✅ <br> 🟡 <br> 🔴 <br> 🔴 |  |
| [Replacing](https://docs.rapids.ai/api/libcudf/nightly/group__strings__replace.html) | replace  <br> replace_slice <br> replace_re  <br> replace_with_backrefs | ✅#12858 <br> 🟡 <br> 🔴 <br> 🔴 | |
| [Splitting](https://docs.rapids.ai/api/libcudf/nightly/group__strings__split.html) | partition <br> split <br> split_record <br> split_re <br> split_record_re | 🟡 <br> ✅#4922 #13680 <br> ✅#12729 <br> 🔴 <br> 🔴 | |
| other | count_characters  <br> count_bytes| ✅#12779 <br> 🟢 | | 

Libcudf also includes [NVText](https://docs.rapids.ai/api/libcudf/stable/group__nvtext__apis.html) APIs that will benefit from improvements in performance when processing long strings. Generally long string performance is even more important for our text APIs, where each row could represent a sentence, paragraph or document.

|Module|Function|Status|Notes|
|---|---|---|---|
| [NGrams](https://docs.rapids.ai/api/libcudf/stable/group__nvtext__ngrams.html) | generate_ngrams <br> generate_character_ngrams <br> ngrams_tokenize|  ⚪  <br> 🟢 <br> 🟢#13480 | these are generally not long strings |
| [Normalizing](https://docs.rapids.ai/api/libcudf/stable/group__nvtext__normalize.html) | normalize_characters <br> normalize_spaces  |  🟢 <br> 🟢#13480  |  |
| [Stemming](https://docs.rapids.ai/api/libcudf/stable/group__nvtext__stemmer.html) | is_letter <br> porter_stemmer_measure  |   🟢 <br>🟢  | |
| [Edit Distance](https://docs.rapids.ai/api/libcudf/stable/group__nvtext__edit__distance.html) | edit_distance <br> edit_distance_matrix | ⚪ <br> ⚪  | these are generally not long strings |
| [Tokenizing](https://docs.rapids.ai/api/libcudf/stable/group__nvtext__tokenize.html) | byte_pair_encoding <br> subword_tokenize <br> tokenize <br> count_tokens <br> character_tokenize <br> detokenize  | 🟡 <br>🟡 <br>🟢#13480 <br>🟢#13480 <br>🟡 <br>🟡  |  |
| [Replacing](https://docs.rapids.ai/api/libcudf/stable/group__nvtext__replace.html) | replace_tokens <br> filter_tokens | 🟢#13480 <br>🟢#13480 |  |
| [MinHashing](https://docs.rapids.ai/api/libcudf/nightly/group__nvtext__minhash.html) | minhash  | ✅#13333 |  |
",2023-04-03T17:58:26Z,0,0,Gregory Kimball,,False
527,[TASK] Remove deprecated/unnecessary rolling-window overloads,"Given that the rolling-window (and grouped rolling window) functionality is roughly stable, it would make sense to remove any unnecessary overloads that were introduced during development.

Examples:

1. `grouped_time_range_rolling_window()` was generalized to `grouped_range_rolling_window()`, which also happens to work with time ranges. The former can be removed.
2. The older `rolling_window()` and `grouped_rolling_window()` methods took `preceding`/`following` bounds as `size_type`. The `bounds` concept was generalized to a `window_bounds` class, to incorporate `UNBOUNDED PRECEDING`/`UNBOUNDED FOLLOWING`. The older methods can be deprecated in favour of the newer ones.
3. Certain window functions (E.g. `LEAD`/`LAG`) allow for default values. Instead of a separate function with the extra parameter, we should consolidate to a single function with an optional parameter.",2023-04-03T21:27:32Z,0,0,MithunR,NVIDIA,True
528,[FEA] Reference counting for memory owning objects in numba kernels,"With the introduction of string datatypes inside user defined functions as well as the merging of the groupby JIT engine, there are now certain objects allowed within UDFs that are individually backed by a device allocation. This issue tracks the introduction of a CUDA version of the reference counting mechanism that powers numba's nopython mode for the CPU target. This mechanism will provide much more robust cleanup for the objects in question than the current approach which attempts to clean everything up in post-processing.

The below tasklist is a draft for now and may change as things are merged.

```[tasklist]
### Tasks
- [ ] https://github.com/rapidsai/cudf/pull/13060
- [ ] https://github.com/rapidsai/cudf/pull/13098
- [ ] https://github.com/rapidsai/cudf/pull/13125
- [ ] https://github.com/rapidsai/cudf/pull/13172
- [ ] https://github.com/rapidsai/cudf/pull/13181
```
",2023-04-04T13:58:40Z,0,0,,NVIDIA,True
529,[BUG] Calling the numpy function `all` on a DataFrame returns a Series[bool] rather than Bool,"Calling the numpy function `all`  on a DataFrame returns a Series[bool] rather than Bool. For behavioral compatibility, we'd like to match pandas behavior.

```python
import numpy as np
import pandas as pd
import cudf

df = cudf.DataFrame({""a"":[True,False], ""b"":[True,True]})
print(np.all(df), ""\n"")
print(np.all(df.to_pandas()))
a    False
b     True
dtype: bool 

False
```",2023-04-04T20:50:38Z,0,0,Nick Becker,@NVIDIA,True
530,[FEA] Async mode for cudf.series operations,"**Is your feature request related to a problem? Please describe.**

We get wide dataframes in situations like machine learning (easily 1-5K cols) and genomics (10K+ cols), and while there is some speedup from cudf (say 2-3X), it'd be easy to get to the 10X+ level with much higher GPU utilization if we could spawn concurrent tasks for each column . Getting this all the way to the df level seems tricky, but async primitives at the column level would get us far. 

One Python-native idea is doing via `async/await`, when one cudf operation is getting scheduled, allocated, & run, we can be scheduling the next, and ideally, cudf can run them independently . It smoothed out 2-3 years ago in python + javascript as a popular native choice, and has since been  a lot more popular in pydata, e.g., langchain just rewrote to support async versions of all methods. Ex: https://trends.google.com/trends/explore?date=all&q=async%20await&hl=en . Separately, there's heightened value for pydata dashboarding scenarios like plotly, streamlit, etc as these ecosystem increasingly build for async io underneath as well.

(Another idea with precedent is a lazy mode similar to haskell or dask, discussed below as well)

**Describe the solution you'd like**

I'd like to be do something like:

```python

async def f(s: cudf.Series) -> cudf.Series:
    # async mode for core series operations lets other f() calls proceed while this runs
    s2 = await  s.stra.hex_to_int('AABBCC')
   
    # math can be clean and enable the same
    # if we're super clever, this may even unlock query plan optimizations like fusion in the future
    async with cudf.async.binop_mode:
        s3_a = s2 + 1 / 3
        s3 = await s3_a

   return s3
  
cols2 = await async.gather([  f(df[col]) for col in df ])
```

**Describe alternatives you've considered**

1. Use existing abstractions

In theory we can setup threads or multiple dask workers, but (1) both are super awkward, (2) underneath, cudf will not do concurrent jobs

2. Lazy cudf

Another thought is to create a lazy mode for cudf. This has precedent with Haskell, and in modern pydata land, more so with polars. Dask does this too, and we'd use it if that can work, but it's awkward -- I haven't used, but polars sounds to be more friendly in practice:

```python

def f(s: cudf.Series) -> cudf.Series:
    # explicitly lazy ops
    s2 = s.str_lazy.hex_to_int('AABBCC')

    # binops know they're lazy
    s3 = s2 + 1 / 3

    return s3

# force with async friendliness  
cols2 = await cudf_client.compute_async([  f(df[col]) for col in df ])
```

Underneath, cudf can reinvent async/io, dask, or whatever


**Additional context**

Slack thread: https://rapids-goai.slack.com/archives/C5E06F4DC/p1680710488795869 
",2023-04-07T17:09:44Z,0,0,,Graphistry,False
531,[FEA] Add overload for `set_null_mask` that accept result of `make_null_mask` directly,"Currently, we have the `make_null_mask` API returning both null mask and null count:
```
template <typename ValidityIterator>
std::pair<rmm::device_buffer, cudf::size_type> make_null_mask(ValidityIterator begin,
                                                              ValidityIterator end)
```

However, the `set_null_mask` function can't accept that result. As such, we have to pass the result of `make_null_mask` into `set_null_mask` by two steps:
```
auto [null_mask, null_count] =
    cudf::test::detail::make_null_mask(valid_iter, valid_iter + input.size());
  output.set_null_mask(std::move(null_mask), null_count);
```

This is a bit inconvenient. We can do that in just one step like this:
```
output.set_null_mask(cudf::test::detail::make_null_mask(valid_iter, valid_iter + input.size()));
```

In order to do that, we need to have a new overload of `set_null_mask` that accept the return type of `make_null_mask` such as:
```
void set_null_mask(std::pair<rmm::device_buffer, cudf::size_type>&&) { ... }
```",2023-04-17T18:03:05Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
532,[FEA] Improve `cudf::distinct` with cuco reduction map,"**Is your feature request related to a problem? Please describe.**
#11052 introduces the keep control option into `cudf::distinct` and makes it possible for users to perform a more efficient hash-based `drop_duplicates`. The PR uses a single hash map together with thrust algorithms to mimic the behavior of a reduction map. This whole process can be largely simplified once https://github.com/NVIDIA/cuCollections/pull/98 is ready. TODO:

- [ ] Replace `static_map` + `thrust` algos with `cuco::static_reduction_map` + `cudf::sort`
- [ ] Update Python bindings to use the hash-based algorithm
- [ ] Investigate the performance impact with various map occupancy and sort-based algo v.s. hash-based algo
- [ ] Minimize memory footprint

**Describe the solution you'd like**
Uses a  `cuco::static_reduction_map` where the key is the row index and the value is the min/max index of equivalent rows (depending on the keep option).

**Describe alternatives you've considered**
We could also take a pair of row hash value and row index as the key which performs the expensive row hash computation only once for better runtime performance. This requires more memory footprint though. To be evaluated.

**Additional context**
#11656 may not be required by the new reduction map implementation.",2023-04-17T20:43:29Z,0,0,Yunsong Wang,@NVIDIA @rapidsai,True
533,[BUG] Calling torch.as_tensor on cudf Series Fails Intermittently,"**Describe the bug**
Given a cudf Series _s_, calling `torch.as_tensor(s, device='cuda')` should properly use `__cuda_array_interface__` to create a tensor without copying the series.  This usually works, but sometimes, the following error is thrown:
```
RuntimeError: The specified pointer resides on host memory and is not registered with any CUDA device.
```

This error recently caused our CI to fail, as shown here: https://github.com/rapidsai/cugraph/actions/runs/4723055386/jobs/8381931370?pr=3455#step:7:3639
",2023-04-17T21:56:15Z,0,0,Alex Barghi,NVIDIA,True
534,[FEA] Add 64-bit size type option at build-time for libcudf,"Many libcudf users have expressed interest in using a 64-bit size type (see #3958 for reference). The `cudf::size_type` uses a `int32_t` data type that limits the number of elements in libcudf columns to `INT_MAX` (2.1 billion) elements. For string columns this imposes a ~2 GB limit, for int32 columns this imposes a ~8 GB limit, and for list columns this imposes a leaf element count <2.1 billion. Downstream libraries must partition their data to avoid these limits.

We expect that using a 64-bit size type will incur significant penalties to memory footprint and data throughput. Memory footprint will double for all offset vectors, and runtime of most functions will increase due to the larger data sizes. Kernel performance may degrade even further due to increased register count and unoptimized shared memory usage.

As GPUs increase in memory, the limit from a 32-bit `cudf::size_type` will force data partitions to become smaller fractions of device memory. Excessive data partitioning also leads to performance penalties, so libcudf should enable its community to start experimenting with a 64-bit size type. Scoping for 64-bit size types in the cuDF-python layer will be tracked in a separate issue (#TBD).

- [ ] Consult with thrust/cub experts about outstanding issues with 64-bit indexing. Some libcudf functions may depend on upstream changes in CCCL, please see [cccl/47](https://github.com/NVIDIA/cccl/issues/47), [thrust/1271](https://github.com/NVIDIA/cccl/issues/744), and [cub/212](https://github.com/NVIDIA/cub/issues/212). `copy_if`, `reduce`, `parallel_for`, `merge` and `sort` may have unresolved issues.
- [ ] Consult with thrust/cub experts about making 32-bit kernels optional. Currently the 64-bit kernels and disabled in libcudf builds. Disabling the 32-bit kernels would avoid large increases in compile time and binary size when we enable 64-bit thrust/cub kernels.
- [ ] Verify compatibility of 64-bit size type with cuco data structures (needs additional scoping)
- [ ] Audit custom kernels in libcudf for the impact of a 64-big size type. Introduce conditional logic to adjust shared memory allocations and threads per block as needed based on the size type. Identify implementation details that take a 32-bit size type for granted.
- [ ] Audit cuIO size types and their interaction with `cudf::size_type`
- [ ] Resolve compilation errors from using a 64-bit size type
- [ ] Resolve test failures from using a 64-bit size type
- [ ] Review performance impact of a 64-bit size type using libcudf microbenchmark results 
- [ ] Add a build-time option for advanced users to use a 64-bit size type instead of a 32-bit size type.
- [ ] Add a CI step to build and test the 64-bit size type option.

From this stage we will have a better sense of the impact and value of using a 64-bit size type with libcudf.
",2023-04-17T23:18:55Z,0,0,Gregory Kimball,,False
535,[FEA] Add Generic scalar device view,"**Is your feature request related to a problem? Please describe.**
The scalar in libcudf does not provide a common interface to access values.
Each type has their own scalar device view
Eg. `numeric_scalar_device_view`, `fixed_point_scalar_device_view`, `timestamp_scalar_device_view`, `duration_scalar_device_view`, `string_scalar_device_view`.


**Describe the solution you'd like**
Create a _Generic Scalar device view_ to support both fixed width types and string types. List and Struct view types are out of scope now. This scalar device view should have element accessor, so that type dispatch in device works.
This will be useful for AST, and binary ops. (and any other operations where device side access of scalar is required). This may also replace existing type specific scalar device views.

**Describe alternatives you've considered**
Fixed width types have a type erased `fixed_width_scalar_device_view_base` which is used in AST.

**Additional context**
For string scalar support for AST https://github.com/rapidsai/cudf/pull/13061, a new generic scalar is created in ast namespace for its usage.",2023-04-18T09:19:28Z,0,0,Karthikeyan,NVIDIA,True
536,[DOC] Document cuDF's spilling option,"cuDF can optionally spill buffers to host memory when running short of device memory via `cudf.set_option(""spill"", True)`. We should document this option, similar to how we document copy-on-write: https://docs.rapids.ai/api/cudf/nightly/user_guide/copy-on-write.html",2023-04-18T11:45:49Z,0,0,Ashwin Srinath,Voltron Data,False
537,[FEA][JNI] Add JNI bindings for cudf::split,"In order to test scenarios where non-contiguous split would be more appropriate than `cudf::contiguous_split` (e.g. potentially for large tables where a contiguous split would force large allocations per split), I would like to add JNI bindings to `cudf::split`.

I have this coded up, but I do need to add a test. I should have a PR today or tomorrow at the latest.

Proposing `splitAndCopy` at the `Table` level, which means we would copy the table views returned by `cudf::split` to make them owning tables.",2023-04-20T15:28:25Z,0,0,Alessandro Bellina,NVIDIA,True
538,[FEA] Support aggregations/scans on lists via groupby,"This is related to https://github.com/rapidsai/cudf/issues/10408.

It's possible to use a combination of `explode()` and `groupby()` to support common aggregations on `list` columns:

```python
In [23]: df
Out[23]:
            a
0   [1, 3, 2]
1      [1, 4]
2  [9, 0, -1]

In [24]: df.explode('a').groupby(level=0).max()
Out[24]:
   a
0  3
1  4
2  9
```

Scans can similarly be computed via an additional call to `groupby-collect()`:

```python
In [33]: df.explode('a').groupby(level=0).cummax().groupby(level=0).collect()
Out[33]:
           a
0  [1, 3, 3]
1     [1, 4]
2  [9, 9, 9]
```

When an aggregation/scan supported by groupby, like `.max()` or `.min()` is called on a list column, we could transparently use this combination of explode + groupby to support that operation. That could look something like:

```python
>>> df
            a
0   [1, 3, 2]
1      [1, 4]
2  [9, 0, -1]

>>> df['a'].list.max()
0    3
1    4
2    9
Name: a, dtype: int64

>>> df['a'].list.cummax()
0    [1, 3, 3]
1       [1, 4]
2    [9, 9, 9]
Name: a, dtype: list
```",2023-04-24T19:29:51Z,0,0,Ashwin Srinath,Voltron Data,False
539,[BUG] dask_cudf  - aggregate -  to_csv memory error ,"**Describe the bug**
A clear and concise description of what the bug is.
I am loading a large dataframe (~60M x 300) by csv via dask_cudf,  then looking to do a groupby and sum, and resave this to csv. I get an OOM error - I am using an A100-80GB gpu along with 200GB of RAM. 

All rows are numerical values, besides the groupby row left as the index. Thus, this error should be reproducible via a random dataframe. 
I noted a similar issue [@10426](https://github.com/rapidsai/cudf/issues/10426), however this error message is different, therefore I was unsure if this was the case.
Additionally, I do repeatedly get a high cpu garbage collection message, however I assume that is because of the size of the dataframe and many read/writes, correct me if that is not the case.
**Steps/Code to reproduce bug**
Follow this guide http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports to craft a minimal bug report. This helps us reproduce the issue you're having and resolve the issue more quickly.

```
import numpy as np
import pandas as pd
import cudf
import cupy
from dask_cuda import LocalCUDACluster
from dask.distributed import Client
from dask.utils import parse_bytes
import dask_cudf

cluster = LocalCUDACluster(jit_unspill=True,
                           rmm_pool_size=parse_bytes(""64 GB""),
                           n_workers = 1,
                           device_memory_limit=parse_bytes(""160 GB""),
                           local_directory='local_temp',
                           threads_per_worker=32)
client = Client(cluster)


df = dask_cudf.read_csv('../02_all_study/02_tad_80_cluster_ref.tsv',sep = '\t')
df2 = df.drop('Contig',axis=1)
res = df2.groupby('ref90_cluster').sum()
res.to_csv('04_cluster_groups_csv')
```

Output (I think the error message is repeating after nanny restarts, but I have included the entire error message for thoroughness (attached as file for size):
[dask_to_csv_error.txt](https://github.com/rapidsai/cudf/files/11327385/dask_to_csv_error.txt)


**Expected behavior**
A clear and concise description of what you expected to happen.

**Environment overview (please complete the following information)**
 - Environment location: [Bare-metal, Docker, Cloud(specify cloud provider)] RHEL server
 - Method of cuDF install: [conda, Docker, or from source] conda (mamba)
   - If method of install is [Docker], provide `docker pull` & `docker run` commands used

**Environment details**
Please run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details
<details><summary>Click here to see environment details</summary><pre>

     **git***
     Not inside a git repository

     ***OS Information***
     NAME=""Red Hat Enterprise Linux Server""
     VERSION=""7.9 (Maipo)""
     ID=""rhel""
     ID_LIKE=""fedora""
     VARIANT=""Server""
     VARIANT_ID=""server""
     VERSION_ID=""7.9""
     PRETTY_NAME=""Red Hat Enterprise Linux Server 7.9 (Maipo)""
     ANSI_COLOR=""0;31""
     CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server""
     HOME_URL=""https://www.redhat.com/""
     BUG_REPORT_URL=""https://bugzilla.redhat.com/""

     REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7""
     REDHAT_BUGZILLA_PRODUCT_VERSION=7.9
     REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux""
     REDHAT_SUPPORT_PRODUCT_VERSION=""7.9""
     Red Hat Enterprise Linux Server release 7.9 (Maipo)
     Red Hat Enterprise Linux Server release 7.9 (Maipo)
     Linux atl1-1-01-006-7-0.pace.gatech.edu 3.10.0-1160.49.1.el7.x86_64 #1 SMP Tue Nov 9 16:09:48 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux

     ***GPU Information***
     Tue Apr 25 18:48:17 2023
     +-----------------------------------------------------------------------------+
     | NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
     |-------------------------------+----------------------+----------------------+
     | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
     | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
     |                               |                      |               MIG M. |
     |===============================+======================+======================|
     |   0  NVIDIA A100 80G...  On   | 00000000:25:00.0 Off |                    0 |
     | N/A   33C    P0    61W / 300W |  72218MiB / 81920MiB |      0%      Default |
     |                               |                      |             Disabled |
     +-------------------------------+----------------------+----------------------+

     +-----------------------------------------------------------------------------+
     | Processes:                                                                  |
     |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
     |        ID   ID                                                   Usage      |
     |=============================================================================|
     |    0   N/A  N/A     26580      C   ...s/rapids-23.04/bin/python    10315MiB |
     |    0   N/A  N/A     27149      C   ...s/rapids-23.04/bin/python    61901MiB |
     +-----------------------------------------------------------------------------+

     ***CPU***
     Architecture:          x86_64
     CPU op-mode(s):        32-bit, 64-bit
     Byte Order:            Little Endian
     CPU(s):                64
     On-line CPU(s) list:   0-63
     Thread(s) per core:    1
     Core(s) per socket:    32
     Socket(s):             2
     NUMA node(s):          8
     Vendor ID:             AuthenticAMD
     CPU family:            25
     Model:                 1
     Model name:            AMD EPYC 7513 32-Core Processor
     Stepping:              1
     CPU MHz:               2600.000
     CPU max MHz:           2600.0000
     CPU min MHz:           1500.0000
     BogoMIPS:              5200.16
     Virtualization:        AMD-V
     L1d cache:             32K
     L1i cache:             32K
     L2 cache:              512K
     L3 cache:              32768K
     NUMA node0 CPU(s):     0-7
     NUMA node1 CPU(s):     8-15
     NUMA node2 CPU(s):     16-23
     NUMA node3 CPU(s):     24-31
     NUMA node4 CPU(s):     32-39
     NUMA node5 CPU(s):     40-47
     NUMA node6 CPU(s):     48-55
     NUMA node7 CPU(s):     56-63
     Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc art rep_good nopl nonstop_tsc extd_apicid aperfmperf eagerfpu pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_l2 cpb cat_l3 cdp_l3 invpcid_single hw_pstate sme retpoline_amd ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif umip pku ospke vaes vpclmulqdq overflow_recov succor smca

     ***CMake***
     /bin/cmake
     cmake version 2.8.12.2

     ***g++***
     /usr/lib64/ccache/g++
     g++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)
     Copyright (C) 2015 Free Software Foundation, Inc.
     This is free software; see the source for copying conditions.  There is NO
     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


     ***nvcc***
     /usr/local/pace-apps/manual/packages/nvhpc/Linux_x86_64/22.11/compilers/bin/nvcc
     nvcc: NVIDIA (R) Cuda compiler driver
     Copyright (c) 2005-2022 NVIDIA Corporation
     Built on Tue_May__3_18:49:52_PDT_2022
     Cuda compilation tools, release 11.7, V11.7.64
     Build cuda_11.7.r11.7/compiler.31294372_0

     ***Python***
     /storage/home/hcoda1/6/rridley3/data/dir/anaconda3/envs/rapids-23.04/bin/python
     Python 3.10.10

     ***Environment Variables***
     PATH                            : /usr/local/pace-apps/manual/packages/nvhpc/Linux_x86_64/22.11/compilers/extras/qd/bin:/usr/local/pace-apps/manual/packages/nvhpc/Linux_x86_64/22.11/comm_libs/mpi/bin:/usr/local/pace-apps/manual/packages/nvhpc/Linux_x86_64/22.11/compilers/bin:/usr/local/pace-apps/manual/packages/nvhpc/Linux_x86_64/22.11/cuda/bin:/usr/local/pace-apps/spack/packages/linux-rhel7-x86_64/gcc-4.8.5/cuda-11.7.0-7sdye3id7ahz34mzhyzzqbxowjxgxkhu/bin:/storage/home/hcoda1/6/rridley3/.cargo/bin:/storage/home/hcoda1/6/rridley3/data/dir/anaconda3/envs/rapids-23.04/bin:/storage/home/hcoda1/6/rridley3/data/dir/apps:/storage/home/hcoda1/6/rridley3/.aspera/connect/bin:/opt/pace-common/bin:/opt/slurm/current/bin:/opt/pace-system/bin:/usr/lpp/mmfs/bin:/usr/lib64/ccache:/sbin:/bin:/usr/sbin:/usr/bin:/opt/iozone/bin:/storage/home/hcoda1/6/rridley3/edirect
     LD_LIBRARY_PATH                 : /usr/local/pace-apps/manual/packages/nvhpc/Linux_x86_64/22.11/comm_libs/nvshmem/lib:/usr/local/pace-apps/manual/packages/nvhpc/Linux_x86_64/22.11/comm_libs/nccl/lib:/usr/local/pace-apps/manual/packages/nvhpc/Linux_x86_64/22.11/comm_libs/mpi/lib:/usr/local/pace-apps/manual/packages/nvhpc/Linux_x86_64/22.11/math_libs/lib64:/usr/local/pace-apps/manual/packages/nvhpc/Linux_x86_64/22.11/compilers/lib:/usr/local/pace-apps/manual/packages/nvhpc/Linux_x86_64/22.11/compilers/extras/qd/lib:/usr/local/pace-apps/manual/packages/nvhpc/Linux_x86_64/22.11/cuda/extras/CUPTI/lib64:/usr/local/pace-apps/manual/packages/nvhpc/Linux_x86_64/22.11/cuda/lib64:/usr/local/pace-apps/spack/packages/linux-rhel7-x86_64/gcc-4.8.5/cuda-11.7.0-7sdye3id7ahz34mzhyzzqbxowjxgxkhu/lib64:/opt/slurm/current/lib::
     NUMBAPRO_NVVM                   :
     NUMBAPRO_LIBDEVICE              :
     CONDA_PREFIX                    : /storage/home/hcoda1/6/rridley3/data/dir/anaconda3/envs/rapids-23.04
     PYTHON_PATH                     :

     conda not found
     ***pip packages***
     /storage/home/hcoda1/6/rridley3/data/dir/anaconda3/envs/rapids-23.04/bin/pip
     Package                       Version
     ----------------------------- -----------
     aiofiles                      22.1.0
     aiohttp                       3.8.4
     aiosignal                     1.3.1
     aiosqlite                     0.18.0
     anyio                         3.6.2
     aplus                         0.11.0
     appdirs                       1.4.4
     argon2-cffi                   21.3.0
     argon2-cffi-bindings          21.2.0
     arrow                         1.2.3
     asciitree                     0.3.3
     astropy                       5.2.2
     asttokens                     2.2.1
     async-timeout                 4.0.2
     attrs                         22.2.0
     Babel                         2.12.1
     backcall                      0.2.0
     backports.functools-lru-cache 1.6.4
     beautifulsoup4                4.12.2
     blake3                        0.2.1
     bleach                        6.0.0
     bokeh                         2.4.3
     bqplot                        0.12.39
     branca                        0.6.0
     brotlipy                      0.7.0
     cached-property               1.5.2
     cachetools                    5.3.0
     certifi                       2022.12.7
     cffi                          1.15.1
     charset-normalizer            2.1.1
     click                         8.1.3
     click-plugins                 1.1.1
     cligj                         0.7.2
     cloudpickle                   2.2.1
     colorama                      0.4.6
     colorcet                      3.0.1
     comm                          0.1.3
     confluent-kafka               1.7.0
     contourpy                     1.0.7
     cryptography                  40.0.2
     cubinlinker                   0.2.2
     cucim                         23.4.1
     cuda-python                   11.8.1
     cudf                          23.4.0
     cudf-kafka                    23.4.0
     cugraph                       23.4.0
     cuml                          23.4.0
     cupy                          11.6.0
     cusignal                      23.4.0
     cuspatial                     23.4.0
     custreamz                     23.4.0
     cuxfilter                     23.4.0
     cycler                        0.11.0
     cytoolz                       0.12.0
     dask                          2023.3.2
     dask-cuda                     23.4.0
     dask-cudf                     23.4.0
     dask-labextension             6.1.0
     datashader                    0.14.4
     datashape                     0.5.4
     debugpy                       1.6.7
     decorator                     5.1.1
     defusedxml                    0.7.1
     distributed                   2023.3.2.1
     entrypoints                   0.4
     executing                     1.2.0
     fastapi                       0.95.1
     fastavro                      1.7.3
     fasteners                     0.18
     fastjsonschema                2.16.3
     fastrlock                     0.8
     filelock                      3.12.0
     Fiona                         1.9.1
     flit_core                     3.8.0
     folium                        0.14.0
     fonttools                     4.39.3
     fqdn                          1.5.1
     frozendict                    2.3.7
     frozenlist                    1.3.3
     fsspec                        2023.4.0
     future                        0.18.3
     GDAL                          3.6.2
     geopandas                     0.12.2
     graphviz                      0.20.1
     h5py                          3.8.0
     holoviews                     1.15.4
     idna                          3.4
     imagecodecs                   2023.1.23
     imageio                       2.27.0
     importlib-metadata            6.5.0
     importlib-resources           5.12.0
     ipycytoscape                  1.3.3
     ipydatawidgets                4.3.2
     ipykernel                     6.22.0
     ipyleaflet                    0.17.2
     ipympl                        0.9.3
     ipython                       8.12.0
     ipython-genutils              0.2.0
     ipyvolume                     0.6.1
     ipyvue                        1.8.0
     ipyvuetify                    1.8.4
     ipywebrtc                     0.6.0
     ipywidgets                    8.0.6
     isoduration                   20.11.0
     jedi                          0.18.2
     Jinja2                        3.1.2
     joblib                        1.2.0
     json5                         0.9.5
     jsonpointer                   2.3
     jsonschema                    4.17.3
     jupyter_client                8.2.0
     jupyter_core                  5.3.0
     jupyter-events                0.6.3
     jupyter_server                2.5.0
     jupyter_server_fileid         0.9.0
     jupyter-server-proxy          3.2.2
     jupyter_server_terminals      0.4.4
     jupyter_server_ydoc           0.8.0
     jupyter-ydoc                  0.2.3
     jupyterlab                    3.6.3
     jupyterlab-pygments           0.2.2
     jupyterlab_server             2.22.1
     jupyterlab-widgets            3.0.7
     kiwisolver                    1.4.4
     lazy_loader                   0.2
     llvmlite                      0.39.1
     locket                        1.0.0
     lz4                           4.3.2
     mapclassify                   2.5.0
     Markdown                      3.4.3
     markdown-it-py                2.2.0
     MarkupSafe                    2.1.2
     matplotlib                    3.7.1
     matplotlib-inline             0.1.6
     mdurl                         0.1.0
     mistune                       2.0.5
     msgpack                       1.0.5
     multidict                     6.0.4
     multipledispatch              0.6.0
     munch                         2.5.0
     munkres                       1.1.4
     nbclassic                     0.5.5
     nbclient                      0.7.3
     nbconvert                     7.3.1
     nbformat                      5.8.0
     nest-asyncio                  1.5.6
     networkx                      3.1
     notebook                      6.5.4
     notebook_shim                 0.2.3
     numba                         0.56.4
     numcodecs                     0.11.0
     numexpr                       2.8.4
     numpy                         1.23.5
     nvtx                          0.2.5
     packaging                     23.1
     pandas                        1.5.3
     pandocfilters                 1.5.0
     panel                         0.14.1
     param                         1.13.0
     parso                         0.8.3
     partd                         1.4.0
     patsy                         0.5.3
     pexpect                       4.8.0
     pickleshare                   0.7.5
     Pillow                        9.4.0
     pip                           23.1
     pkgutil_resolve_name          1.3.10
     platformdirs                  3.2.0
     pooch                         1.7.0
     progressbar2                  4.2.0
     prometheus-client             0.16.0
     prompt-toolkit                3.0.38
     protobuf                      4.21.12
     psutil                        5.9.5
     ptxcompiler                   0.7.0
     ptyprocess                    0.7.0
     pure-eval                     0.2.2
     pyarrow                       10.0.1
     pycparser                     2.21
     pyct                          0.4.6
     pydantic                      1.10.7
     pydeck                        0.5.0
     pyee                          8.1.0
     pyerfa                        2.0.0.3
     Pygments                      2.15.1
     pylibcugraph                  23.4.0
     pylibraft                     23.4.0
     pynvml                        11.4.1
     pyOpenSSL                     23.1.1
     pyparsing                     3.0.9
     pyppeteer                     1.0.2
     pyproj                        3.4.0
     pyrsistent                    0.19.3
     PySocks                       1.7.1
     python-dateutil               2.8.2
     python-json-logger            2.0.7
     python-utils                  3.5.2
     pythreejs                     2.4.2
     pytz                          2023.3
     pyviz-comms                   2.2.1
     PyWavelets                    1.4.1
     PyYAML                        6.0
     pyzmq                         25.0.2
     raft-dask                     23.4.0
     requests                      2.28.2
     rfc3339-validator             0.1.4
     rfc3986-validator             0.1.1
     rich                          13.3.4
     rmm                           23.4.0
     Rtree                         1.0.1
     scikit-image                  0.20.0
     scikit-learn                  1.2.2
     scipy                         1.10.1
     seaborn                       0.12.2
     Send2Trash                    1.8.0
     setuptools                    67.6.1
     shapely                       2.0.1
     simpervisor                   0.4
     six                           1.16.0
     sniffio                       1.3.0
     sortedcontainers              2.4.0
     soupsieve                     2.3.2.post1
     spectate                      1.0.1
     stack-data                    0.6.2
     starlette                     0.26.1
     statsmodels                   0.13.5
     streamz                       0.6.4
     tables                        3.7.0
     tabulate                      0.9.0
     tblib                         1.7.0
     terminado                     0.17.1
     threadpoolctl                 3.1.0
     tifffile                      2023.4.12
     tiledb                        0.21.2
     tinycss2                      1.2.1
     tomli                         2.0.1
     toolz                         0.12.0
     tornado                       6.3
     tqdm                          4.65.0
     traitlets                     5.9.0
     traittypes                    0.2.1
     treelite                      3.2.0
     treelite-runtime              3.2.0
     typing_extensions             4.5.0
     ucx-py                        0.31.0
     unicodedata2                  15.0.0
     uri-template                  1.2.0
     urllib3                       1.26.15
     vaex-astro                    0.9.3
     vaex-core                     4.16.1
     vaex-hdf5                     0.14.1
     vaex-jupyter                  0.8.1
     vaex-ml                       0.18.1
     vaex-server                   0.8.1
     vaex-viz                      0.5.4
     wcwidth                       0.2.6
     webcolors                     1.13
     webencodings                  0.5.1
     websocket-client              1.5.1
     websockets                    10.4
     wheel                         0.40.0
     widgetsnbextension            4.0.7
     xarray                        2023.4.1
     xgboost                       1.7.5
     xyzservices                   2023.2.0
     y-py                          0.5.9
     yarl                          1.8.2
     ypy-websocket                 0.8.2
     zarr                          2.14.2
     zict                          3.0.0
     zipp                          3.15.0

</pre></details>



**Additional context**
Add any other context about the problem here.
",2023-04-25T22:55:47Z,0,0,Rodney Ridley,,False
540,[BUG] Crash running parquet reader benchmarks.,"
The PARQUET_READER_NVBENCH crashes (segfault) at exit on some machines.   It doesn't seem to happen consistently for everyone, but it tends to be reproducible once it starts happening.

To reproduce, run PARQUET_READER_NVBENCH and you should get a segfault right at the end after it has printed out all of it's results.

I've narrowed it down to something specific to the `parquet_read_io_compression` suite.  In addition, `compute-sanitizer` does not turn anything up so this seems to be something purely cpu-side.",2023-04-26T18:24:44Z,1,0,,,False
541,[BUG] libcudf conda packages are shipping dependencies in the package,"**Describe the bug**
Currently, libcudf conda packages are shipping libraries/headers from libcudf dependencies. In particular:

libcudf's conda package is 548 MB (all reported sizes are unzipped). It currently ships libraries/headers from:
- kvikio (< 1 MB)
- libcudacxx (5.2 MB)
- nvbench (4.1 MB)
- nvcomp (46 MB)

**Expected behavior**
libcudf conda packages should not ship its dependencies, but should instead list other conda packages as dependencies. This will prevent files from outside libcudf from being shipped in libcudf packages.

### Specific proposals for resolution

**kvikio (easy)**
kvikio should be easy to fix. RAPIDS already produces conda packages of kvikio and just need to use those, rather than letting it be found via CPM and repackaged in libcudf.

✔️ Resolved in: https://github.com/rapidsai/cudf/pull/13231

**libcudacxx (medium difficulty)**
`libcudacxx` is part of CCCL, along with Thrust and CUB. Historically, librmm has shipped most of the CCCL headers in `include/rapids` because they've been found by CPM during the librmm build and repackaged there. However, this is not ideal. libcudacxx is an exception to librmm shipping a RAPIDS-vendored CCCL because it was not needed by librmm -- only libcudf. I suspect we're repackaging libcudacxx in many RAPIDS packages right now. The fix here is to adopt [rapids-core-dependencies](https://github.com/rapidsai/rapids-cmake/tree/branch-23.06/conda/recipes/rapids_core_dependencies) across all of RAPIDS, which should be mostly ready for use, if I recall correctly.

**nvbench (very difficult)**
Ideally, `nvbench` should be its own conda package, but we often require patched versions. Therefore, we should consider moving nvbench headers to a special subdirectory `include/rapids` as we do for CCCL (see https://github.com/rapidsai/rapids-cmake/pull/98). We should also do a more granular review to decide if (and where) we want to be shipping files like `bin/nvbench-ctl` or `lib/objects-Release/nvbench.main/main.cu.o`.

**nvcomp (medium difficulty)**
`nvcomp` should be shipping in a separate conda package (probably on either the rapidsai or nvidia channels). Then we make it a dependency of libcudf. It is the largest component of libcudf that is being repackaged (around 10% of libcudf's size).

✔️ Resolved in: https://github.com/rapidsai/cudf/pull/13566

cc: @vuule (with whom I discussed this)",2023-04-26T19:27:02Z,0,0,Bradley Dice,@NVIDIA @rapidsai,True
542,[FEA] Improve ORC reader performance for decimal types,"Decode of decimal files is an order of magnitude slower than decode of integral types. 
The reason is the use of a single thread to find the sizes of the next batch of elements, which are then decoded using the whole block. To improve kernel performance, it needs to use multiple threads to find varint boundaries:

Pass 1: every thread runs `is_boundary_byte` (highest bit == 0) to find if it's at the last byte of a varint element.
```
A = 0 0 0 0 1 0 0 1 0 0 0 1
```
 
Pass 2: Scan A to produce B. Also gets the number of elements (3 in this case).
```
B = 0 0 0 0 1 1 1 2 2 2 2 3
    ^       ^     ^
    t0      t4    t7
```
Pass 3: Threads that are on a boundary decode the element that starts at their index and store it at col[t].
t=0 writes to [0]
t=4 writes to [1]
t=7 writes to [2]
Alternatively, step 3 can store the offsets of each element so they can be decoded in parallel.",2023-04-29T00:13:39Z,0,0,Vukasin Milovanovic,NVIDIA,True
543,"[BUG] DataFrame iloc indexing is incorrect for repeated index entries in the ""columns"" part of the key","**Describe the bug**
```python
import cudf
import pandas as pd
import numpy as np
df = pd.DataFrame(np.arange(4).reshape(2, 2))
cdf = cudf.from_pandas(df)

df.iloc[:, [0, 1, 0]]
#    0  1  0
# 0  0  1  0
# 1  2  3  2

cdf.iloc[:, [0, 1, 0]]
#    0  1
# 0  0  1
# 1  2  3
```

This is because `ColumnAccessor.select_by_index` uniquifies input index arguments.

**Expected behavior**

This should match pandas.",2023-05-02T10:33:16Z,0,0,Lawrence Mitchell,,False
544,[BUG] Series and DataFrame loc indexing does not handle `Ellipsis`,"**Describe the bug**

This is the same as #13267 really, but `loc`-based indexing goes down a different code path, so I will end up fixing it separately.

```python
import cudf
import pandas as pd
import numpy as np
df = pd.DataFrame(np.arange(4).reshape(2, 2))
cdf = cudf.from_pandas(df)

df.loc[..., 0]
# 0    0
# 1    2
# Name: 0, dtype: int64

cdf.loc[..., 0] # => TypeError
```

**Expected behavior**

Should match pandas.",2023-05-02T10:50:15Z,0,0,Lawrence Mitchell,,False
545,[BUG] DataFrame `loc` indexing is incorrect with repeated column labels.,"**Describe the bug**

This is basically #13266 but for `loc`, I will fix it separately due to different code paths.

```python
import cudf
import pandas as pd
import numpy as np
df = pd.DataFrame(np.arange(4).reshape(2, 2))
cdf = cudf.from_pandas(df)

df.loc[:, [0, 1, 0]]
#    0  1  0
# 0  0  1  0
# 1  2  3  2

cdf.loc[:, [0, 1, 0]]
#    0  1
# 0  0  1
# 1  2  3
```

This is because `ColumnAccessor.select_by_label` uniquifies input label arguments.

**Expected behavior**

This should match pandas.",2023-05-02T11:04:17Z,0,0,Lawrence Mitchell,,False
546,[BUG] dataframe construction silently drops data with duplicate column names,"**Describe the bug**

In pandas, it is allowable to construct a dataframe where the same name is used for more than one of the columns (seems unlikely you would _want_ to do this, but OK).

In cudf, in contrast, duplicate column names are _not_ allowed. However, this restriction is only checked in a few APIs (`from_pandas` complains, for example, but `DataFrame.__init__` doesn't).

This is rather nefarious because operations succeed but silently drop data.

Example:
```python
import pandas as pd
import numpy as np
import cudf

df = pd.DataFrame(np.arange(4).reshape(2, 2), columns=[""A"", ""A""])
assert df.sum().sum() == 6 # succeeds
cdf = cudf.DataFrame(np.arange(4).reshape(2, 2), columns=[""A"", ""A""]) # succeeds, but throws away first column

assert cdf.sum().sum() == 6 # fails
cdf2 = cudf.from_pandas(df) # raises ValueError
```


**Expected behavior**

Either

1. Support this use case (probably a big job)
2. Raise `ValueError` in all cases (easier, minimal sufficient requirement for xdf).",2023-05-02T15:23:13Z,1,0,Lawrence Mitchell,,False
547,[ENH] add `.fillna` method to `ListMethods`,"**Describe the bug**
.fillna does not work with `ListColumns`.

**Steps/Code to reproduce bug**
```
x = cudf.Series([0, 1, None])
```
```
x.fillna(5)
0    0
1    1
2    5
dtype: int64
```
```
y = cudf.Series([[0, 1], [2, 3], [None, None]])
y.fillna(5) 
```
```
0    [0.0, 1.0]
1    [2.0, 3.0]
2    [nan, nan]
dtype: list
```

**Expected behavior**
`2    [nan, nan]` should be `2    [5, 5]`

**Environment overview (please complete the following information)**
cudf 23.06 installed via pip

**Environment details**
Not available.

**Additional context**
Add any other context about the problem here.
",2023-05-02T17:41:32Z,0,0,H. Thomson Comer,NVIDIA,True
548,[BUG] Error when printing large CategoricalIndex,"Printing a `CategoricalIndex` with more than 200 elements results in the following error:

```
TypeError: Cannot interpret 'interval[float64, right]' as a data type
```

Reproducer

```
import cudf
import cupy

p = cudf.cut(cupy.arange(201), 3)

# this works
print(p[:200])

# this doesn't
print(p)
```

with cudf version 23.04.01 and cupy version 11.6.0",2023-05-03T20:41:26Z,0,0,Filippo Simini,Argonne National Laboratory,False
549,[FEA] Support non-broadcasting (non-scalar) assignment on list columns,"Consider

```python
import cudf

s = cudf.Series([[1], [2], [3]])
g = cudf.Series([[4], [5]])

s.iloc[:2] = g
```

I would expect this to work and produce `[[4], [5], [3]]`. Instead, we get an error:
```
ValueError: Can not set <cudf.core.column.lists.ListColumn object at 0x7fea4b0f5cc0>
[
  [
    4
  ],
  [
    5
  ]
]
dtype: list into ListColumn
```

Indeed, the only things we can `__setitem__` into list columns are `cudf.NA` and a singleton list (treated as a `cudf.Scalar`) that is broadcast to all entries.",2023-05-04T15:42:15Z,0,0,Lawrence Mitchell,,False
550,[BUG] Setitem on struct columns picks out incorrect pieces when indexing of rvalue is not from zero.,"**Describe the bug**

```python
import cudf

s = cudf.Series([{""a"": 1}, {""a"": 2}, {""a"": 3}])
g = cudf.Series([{""a"": 4}, {""a"": 5}, {""a"": 6}])

s.iloc[1:2] = g.iloc[2:3]

s.iloc[1:2]
# 1    {'a': 4}
# dtype: struct

g.iloc[2:3]
# 2    {'a': 6}
# dtype: struct

# An even more fun:

x = g.iloc[2:3].copy(deep=True)

s.iloc[1] = x.iloc[0]

s.iloc[1] # => {'a': 6} Good!

s.iloc[1] = x.iloc[:]

s.iloc[1] # => {'a': 4} What!
```

Oops.",2023-05-04T16:59:24Z,0,0,Lawrence Mitchell,,False
551,[BUG] `__repr__` for a high cardinality categorical can take a very long time,"Because it involves copying data to Pandas, just printing a high-cardinality CategoricalIndex can take a very long time:

```python
dt = pd.CategoricalIndex(range(100_000_000))
print(dt)  # almost instantaneous

dt = cudf.CategoricalIndex(range(100_000_000))
print(dt)  # can take several seconds
```

We should figure out if we can do this by copying only the minimum amount of data to Pandas (or not copying to Pandas at all).",2023-05-04T21:46:48Z,0,0,Ashwin Srinath,Voltron Data,False
552,[BUG] Handle CudfException while instantiating ColumnViews ,"**Describe the bug**
While instantiating ColumnViews in a loop. We could run into a case where somewhere in the middle of the instantiations we see a `CudfException`. Any attempt to call `close()` on successfully instantiated `ColumnView`s might throw another `CudfException`, causing us to be irrecoverable. 

```
try {
  columnViews[i] = new ColumnView();
} catch (Throwable e) { 
  for (ColumnView cv: columnViews) {
    if (cv != null) cv.close(); // this could again throw
  }
  throw t;
}
```
Refer to https://github.com/rapidsai/cudf/pull/13262/files#diff-50ba2711690aca8e4f28d7b491373a4dd76443127c8b452a77b6c1fe2388d9e3R815-R828 for the exact example of this case. 

**Expected behavior**
We should be able to recover ideally although, not sure if we can. Is this an irrecoverable state and can we just let the exception propagate upstream? Could we add the original exception as a suppressed exception in the new exception? ",2023-05-08T19:47:50Z,0,0,Raza Jafri,NVIDIA,True
553,"[BUG] Misinterpretation of Parquet List schema with single GROUP child named ""array""","This bug is to track a (possible) misinterpretation of Parquet list schemas when stored in a legacy format. This is a follow-up to https://github.com/rapidsai/cudf/pull/13277.

This is specific to rules #3 and #4 in the [Parquet `LogicalType` spec](https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#backward-compatibility-rules), which states:
```
3. If the repeated field is a group with one field and is named either array or uses the LIST-annotated group's name with _tuple appended then the repeated type is the element type and elements are required.
4. Otherwise, the repeated field's type is the element type with the repeated field's repetition.
```
Consider the following schema, from the [Parquet file attached herewith](https://github.com/rapidsai/cudf/files/11427533/pq_array.zip):
```
 <pyarrow._parquet.ParquetSchema object at 0x7fe1cc5849c0>
required group field_id=-1 spark_schema {
  required group field_id=-1 my_list (List) {
    repeated group field_id=-1 array {
      required int32 field_id=-1 item;
    }
  }
}
```

`libcudf` seems to interpret this as `List<Int32>`:
```
$ gtests/PARQUET_TEST --gtest_filter=ParquetReaderTest.Myth
...
cudf::list_view<int32_t>:
Length : 1
Offsets : 0, 2
   0, 1
```
By my reading of the spec, this should be interpreted as a `List<Struct<Int32>>`. Apache Spark seems to concur:
```scala
scala> spark.read.parquet(""pq_array.parquet"").printSchema
root
 |-- my_list: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- item: integer (nullable = true)
```",2023-05-09T05:16:14Z,0,0,MithunR,NVIDIA,True
554,[FEA] chunked_pack should allow variable sized spans in the next call,"As brought up here https://github.com/rapidsai/cudf/pull/13260#discussion_r1189086902, `chunked_pack` requires that it be configured with a single size for the user's bounce buffer, and further requires the same size of device memory made available in every call to `chunked_pack::next`. 

This is overkill in the case where we have actually very little work to be done (the source table is small). So the proposal is to allow for a `get_next_size()` api that lets us know the size of buffer that should be made available to `next`",2023-05-10T19:54:21Z,0,0,Alessandro Bellina,NVIDIA,True
555,[ENH] Indices are immutable by definition and should take advantage of that,"**Is your feature request related to a problem? Please describe.**

Indices advertise a bunch of properties (for example `is_monotonic_increasing`). These are implemented by inheriting from `Frame` or `SingleColumnFrame`. But those latter classes are _mutable_, and so their implementation of the property must recompute every time.

For an index, this is not the case, and so we lose some performance (for example, every time we do a slice `.loc` we'll run a libcudf kernel to check if the index is sorted).

**Describe the solution you'd like**

Index properties (especially where they return scalars, and therefore the memory footprint is negligible) should use `functools.cached_property` to provide their (immutable) properties.

**Describe alternatives you've considered**

None

",2023-05-16T15:34:03Z,2,0,Lawrence Mitchell,,False
556,[FEA] Expand string operator support in libcudf ASTs,"After adding string scalar support in #13061, libcudf now supports string column and string scalar operands, plus comparison operators.

There are additional operators that would be useful for string types, and this issue will track the most-requested operators.

in scope:
string contains, startswith, endswith
length, find, rfind

out of scope for now:
casting to and from string to other AST types
regex support
concatenation

(Also see background issue #8858)",2023-05-16T16:20:49Z,0,0,Gregory Kimball,,False
557,Add CUDF_FUNC_RANGE to more functions in null_mask.cu,"              Wondering if functions in this file should have `CUDF_FUNC_RANGE();` for nvtx...

_Originally posted by @bdice in https://github.com/rapidsai/cudf/pull/13345#discussion_r1195417844_
            ",2023-05-16T17:37:27Z,0,0,Vyas Ramasubramani,@rapidsai,True
558,[FEA] Try using `__grid_constant__` in libcudf.,"**Is your feature request related to a problem? Please describe.**

Some features in libcudf like the AST and nested comparators have pretty hefty kernels. The `__grid_constant__` annotation (available in CUDA 11.7 and higher) may help reduce local memory usage in these features. We build with CUDA >=11.8 so this would be supported in our builds.

https://docs.nvidia.com/cuda/cuda-c-programming-guide/#grid-constant

In short:
> If the address of a _global_ function parameter is taken, the compiler will ordinarily make a copy of the kernel parameter in thread local memory and use the address of the copy, to partially support C++ semantics, which allow each thread to modify its own local copy of function parameters. Annotating a _global_ function parameter with __grid_constant__ ensures that the compiler will not create a copy of the kernel parameter in thread local memory, but will instead use the generic address of the parameter itself. Avoiding the local copy may result in improved performance.

@jrhemstad thinks a fair amount of the register/stack usage in AST and comparator code comes from thread local copies of those structs. @bdice agrees this is worth investigating.

(Some issue text contributed by @jrhemstad.)

**Describe the solution you'd like**
Try adding `__grid_constant__` to AST kernels and nested comparators to see how the kernels' usage of local memory, registers, etc. changes.",2023-05-16T17:43:02Z,3,0,Bradley Dice,@NVIDIA @rapidsai,True
559,[FEA] Rename `filters=` argument to `row_group_filters=` in `read_parquet` and `read_orc` and provide examples that show its use,"The `filters=` argument to the `read_parquet` and `read_orc` functions is misleading in its name. Because it filters by row _group_, rather than by row, I think we should rename it to `row_group_filters=`.

Additionally, the description for this kwarg is complicated and laden with jargon. I think we should simplify and provide some examples that clarify the use of this keyword:

```
    If not None, specifies a filter predicate used to filter out row groups
    using statistics stored for each row group as Parquet metadata. Row groups
    that do not match the given filter predicate are not read. The
    predicate is expressed in disjunctive normal form (DNF) like
    `[[('x', '=', 0), ...], ...]`. DNF allows arbitrary boolean logical
    combinations of single column predicates. The innermost tuples each
    describe a single column predicate. The list of inner predicates is
    interpreted as a conjunction (AND), forming a more selective and
    multiple column predicate. Finally, the outermost list combines
    these filters as a disjunction (OR). Predicates may also be passed
    as a list of tuples. This form is interpreted as a single conjunction.
    To express OR in predicates, one must use the (preferred) notation of
    list of lists of tuples
```
",2023-05-17T19:02:46Z,0,0,Ashwin Srinath,Voltron Data,False
560,[FEA] Support self comparisons in `cudf::experimental::row::lexicographic::two_table_comparator`,"As showcased in https://github.com/rapidsai/cudf/pull/13347, there's a need for `{lhs, lhs}` and `{rhs, rhs}` comparisons in an instance of `two_table_comparator`.

This can't simply be achieved by adding more overloads because `left` and `right` terminology is [baked into the comparator](https://github.com/rapidsai/cudf/blob/76ec53fae9dad71398c115bc405317a918036e52/cpp/include/cudf/table/experimental/row_operators.cuh#L1141-L1157) when it's constructed at the host-side. In a device function, the [strongly typed indices](https://github.com/rapidsai/cudf/blob/76ec53fae9dad71398c115bc405317a918036e52/cpp/include/cudf/table/experimental/row_operators.cuh#L1012-L1039) now work with the assumption that a `comp(i, j)` that is called in a device function operates on `{lhs, rhs}` or `{rhs, lhs}`.

We need to settle on a design that lets us refactor the row operators such that the assumption of working on two different tables can be removed.

Do we strongly type `device_row_comparator::operator()` [over here](https://github.com/rapidsai/cudf/blob/76ec53fae9dad71398c115bc405317a918036e52/cpp/include/cudf/table/experimental/row_operators.cuh#L541-L542) such that we can decide which columns of which tables to pass along to the `element_comparator` over [here](https://github.com/rapidsai/cudf/blob/76ec53fae9dad71398c115bc405317a918036e52/cpp/include/cudf/table/experimental/row_operators.cuh#L568-L575)?

I see the design looking something like this:
```
class device_row_comparator {
  class element_comparator {
    operator() (size_type lhs_index, size_type rhs_index);
  };
  dispatch_element_operator(lhs_col, rhs_col, lhs_index, rhs_index);

  // these call dispatch_element_operator with the correct columns and indices
  operator() (lhs_index_type lhs_index, rhs_index_type rhs_index);
  operator() (lhs_index_type lhs_index, lhs_index_type rhs_index);
  operator() (rhs_index_type lhs_index, rhs_index_type rhs_index);
};

// the template `Comparator` here and below will be an instance of `device_row_comparator`,
// such that the strongly type indices can be passed along directly
template <typename Comparator, weak_ordering... values>
class single_table_ordering {
  operator() (size_type lhs_index, size_type rhs_index) {
    return comparator(lhs_index_type{lhs_index}, lhs_index_type{rhs_index});
};

template <typename Comparator, weak_ordering... values>
class two_table_ordering {
// same as current version of strong_index_comparator with added overloads for {lhs, lhs} and {rhs, rhs}
};

class self_comparator {
  auto less() {
    return less_comparator{single_table_ordering{device_row_comparator{...}}};
  }
};

class two_table_comparator {
  auto less() {
    return less_comparator{two_table_ordering{device_row_comparator{...}}};
  }
};
```
Note: In this example, [weak_ordering_comparator_impl](https://github.com/rapidsai/cudf/blob/76ec53fae9dad71398c115bc405317a918036e52/cpp/include/cudf/table/experimental/row_operators.cuh#L612-L626) will be removed and it's functionality will instead be baked into `single_table_ordering` and `two_table_ordering`. [less_comparator](https://github.com/rapidsai/cudf/blob/76ec53fae9dad71398c115bc405317a918036e52/cpp/include/cudf/table/experimental/row_operators.cuh#L634-L644) will then be reworked with CRTP such that:
```
template <typename Comparator>
class less_comparator : Comparator<weak_ordering::LESS>
```
",2023-05-17T19:21:59Z,1,0,Divye Gala,,False
561,[FEA] Pass dataframe as a parameter to .apply_rows,"looking the apply_rows, how can I pass in a cudf dataframe as an argument  so something like 
```
import cudf
import numpy as np

dfA = cudf.DataFrame({'A': [0, 1, 2, 3, 4],
                   'B': [5, 6, 7, 8, 9],
                   'C': ['a', 'b', 'c', 'd', 'e']})
dfA

dfB = cudf.DataFrame({'A': [0, 1, 2, 3, 4],
                   'B': [5, 6, 7, 8, 9],
                   'C': ['a', 'b', 'c', 'd', 'e']})

def fn(A, B, out1,k1):
    for i, (a, b) in enumerate(zip(A, B)):
        out1[i] = 1 # awesome work done here that uses all k1

dfB.apply_rows(fn, incols = ['A','B'], outcols = dict(out1=np.float64), kwargs=dict(k1=dfA))
```",2023-05-18T03:28:58Z,0,0,,,False
562,[BUG] `loc`-based indexing of DataFrames silently discards missing keys if at least one key is present in indexer,"**Describe the bug**

When performing `loc` indexing of a `DataFrame`, if one asks for a missing key, pandas raises a `KeyError`. In constrast,  cudf only does so if none of the requested keys are in the index. If at least one requested key is present then the subsetted data frame with that key is returned and the missing keys are silently dropped. 

Series `loc` indexing does the right thing here and raises `KeyError` if any keys are missing.

**Steps/Code to reproduce bug**

```python
import pandas as pd
import cudf

# same failure with rangeindex too.
df = pd.DataFrame({""A"": range(5)}, index=list(range(5)))
cdf = cudf.from_pandas(df)

df.loc[[0, 5]] # 5 is missing, raises KeyError

cdf.loc[[0, 5]]
#    A
# 0  0
```

**Expected behavior**

Should match pandas behaviour and raise `KeyError`.",2023-05-18T10:13:54Z,0,0,Lawrence Mitchell,,False
563,`tz_convert` sometimes returns results different from Pandas (but same as `zoneinfo`),"`tz_convert` returns a result different from Pandas for this pre-1900 example:

```python
>>> pd.Series([""1899-01-01 12:00""], dtype=""datetime64[s]"").dt.tz_localize(""Europe/Paris"").dt.tz_convert(""America/New_York"")
0   1899-01-01 06:55:00-04:56
dtype: datetime64[ns, America/New_York]

>>> cudf.Series([""1899-01-01 12:00""], dtype=""datetime64[s]"").dt.tz_localize(""Europe/Paris"").dt.tz_convert(""America/New_York"")
0   1899-01-01 06:50:39-04:56
dtype: datetime64[s, America/New_York]
```

However, our result is the same as you would get with `zoneinfo`:

```python
>>> datetime(1899, 1, 1, 12, 0, tzinfo=ZoneInfo(""Europe/Paris"")).astimezone(ZoneInfo(""America/New_York""))
datetime.datetime(1899, 1, 1, 6, 50, 39, tzinfo=zoneinfo.ZoneInfo(key='America/New_York'))
```

@mroeschke I'm curious if this aligns with your experience with the difference between Pandas (pytz) and zoneinfo?

_Originally posted by @shwina in https://github.com/rapidsai/cudf/pull/13328#discussion_r1196584991_
            ",2023-05-18T10:27:08Z,0,0,Ashwin Srinath,Voltron Data,False
564,[FEA] Rename `apply_boolean_mask` to `copy_if` and move it from `stream_compaction.hpp` into `copying.hpp`,"There are actually two FEAs in  this issue which can be addressed separately:
 * In `stream_compaction` module, we have the API `apply_boolean_mask` which copies the input to the output depending on a boolean mask array. This is currently categorized into stream compaction, but it can also be put into the `copying` module too. Since we already have `copy_if_else` API in `copying`, and also have `cudf::detail::copy_if(input, binary_pred)` API that does similar thing, we should better move this `apply_boolean_mask` into `copying` for consistency.
 * In addition, the name `apply_boolean_mask` is very misleading. Its prefix `apply_` implies that we are doing something in place. However, what it does is actually copying from the (immutable) input into a new output table. Thus, it should be better rename into another overload of `copy_if` for better consistency with the current `cudf::detail::copy_if(input, binary_pred)` API.",2023-05-22T16:08:53Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
565,[BUG] Unable to write `timedelta64[s]` type correctly with parquet writer,"**Describe the bug**
Only when we have `timedelta64[s]` dtype for a column, the parquet writer seems to be writing it as a `timedelta64[ms]` column which is leading both cudf & pyarrow parquet readers to pickup the column type incorrectly.

**Steps/Code to reproduce bug**
Follow this guide http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports to craft a minimal bug report. This helps us reproduce the issue you're having and resolve the issue more quickly.
```python
In [1]: import cudf

In [3]: df = cudf.DataFrame({""seconds"": cudf.Series([1234, 3456, 32442], dtype='timedelta64[s]')})

In [4]: df
Out[4]: 
          seconds
0 0 days 00:20:34
1 0 days 00:57:36
2 0 days 09:00:42

In [5]: df.dtypes
Out[5]: 
seconds    timedelta64[s]
dtype: object

In [6]: df.to_parquet(""a"")

In [7]: cudf.read_parquet(""a"")
Out[7]: 
          seconds
0 0 days 00:20:34
1 0 days 00:57:36
2 0 days 09:00:42

In [8]: cudf.read_parquet(""a"").dtypes
Out[8]: 
seconds    timedelta64[ms]               # Should be timedelta64[s]
dtype: object

In [9]: import pyarrow as pa

In [10]: pa.parquet.read_table(""a"")
Out[10]: 
pyarrow.Table
seconds: time32[ms] not null           # Should be time32[s]
----
seconds: [[00:20:34.000,00:57:36.000,09:00:42.000]]


# If we now try to write & read using pyarrow the dtype stays intact:

In [11]: pa_table = df.to_arrow()

In [12]: pa_table
Out[12]: 
pyarrow.Table
seconds: duration[s]
----
seconds: [[1234,3456,32442]]

In [13]: pa.parquet.write_table(pa_table, ""a"")

In [15]: pa.parquet.read_table(""a"")
Out[15]: 
pyarrow.Table
seconds: duration[s]
----
seconds: [[1234,3456,32442]]

In [17]: import pandas as pd

In [18]: pd.read_parquet(""a"")
Out[18]: 
          seconds
0 0 days 00:20:34
1 0 days 00:57:36
2 0 days 09:00:42

In [19]: pd.read_parquet(""a"").dtypes
Out[19]: 
seconds    timedelta64[ns]
dtype: object

In [21]: pa.parquet.read_metadata(""a"").schema
Out[21]: 
<pyarrow._parquet.ParquetSchema object at 0x7fc8e622b0c0>
required group field_id=-1 schema {
  optional int64 field_id=-1 seconds;
}

```

**Expected behavior**
We are writing all other `timedelta` resolutions(`ns`, `ms`, `us`) correctly. It's a problem only being seen with `s`. We should be able to round-trip this type correctly if writer can correctly write this type.

**Environment overview (please complete the following information)**
 - Environment location: [Bare-metal]
 - Method of cuDF install: [from source]


**Environment details**
Please run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details
<details><summary>Click here to see environment details</summary><pre>
     
     **git***
     commit 9b1496df64b9ae9bd7b44a30cfaa42a2f7e2db3f (HEAD -> branch-23.06)
     Author: Ashwin Srinath <3190405+shwina@users.noreply.github.com>
     Date:   Mon May 22 13:52:36 2023 -0400
     
     Fix groupby head/tail for empty dataframe (#13398)
     
     Closes #13397
     
     Authors:
     - Ashwin Srinath (https://github.com/shwina)
     
     Approvers:
     - GALI PREM SAGAR (https://github.com/galipremsagar)
     - Bradley Dice (https://github.com/bdice)
     
     URL: https://github.com/rapidsai/cudf/pull/13398
     **git submodules***
     
     ***OS Information***
     DISTRIB_ID=Ubuntu
     DISTRIB_RELEASE=18.04
     DISTRIB_CODENAME=bionic
     DISTRIB_DESCRIPTION=""Ubuntu 18.04.4 LTS""
     NAME=""Ubuntu""
     VERSION=""18.04.4 LTS (Bionic Beaver)""
     ID=ubuntu
     ID_LIKE=debian
     PRETTY_NAME=""Ubuntu 18.04.4 LTS""
     VERSION_ID=""18.04""
     HOME_URL=""https://www.ubuntu.com/""
     SUPPORT_URL=""https://help.ubuntu.com/""
     BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
     PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
     VERSION_CODENAME=bionic
     UBUNTU_CODENAME=bionic
     Linux dt07 4.15.0-76-generic #86-Ubuntu SMP Fri Jan 17 17:24:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
     
     ***GPU Information***
     Mon May 22 13:53:56 2023
     +---------------------------------------------------------------------------------------+
     | NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |
     |-----------------------------------------+----------------------+----------------------+
     | GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
     | Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
     |                                         |                      |               MIG M. |
     |=========================================+======================+======================|
     |   0  Tesla T4                        On | 00000000:3B:00.0 Off |                    0 |
     | N/A   45C    P8               10W /  70W|      2MiB / 15360MiB |      0%      Default |
     |                                         |                      |                  N/A |
     +-----------------------------------------+----------------------+----------------------+
     |   1  Tesla T4                        On | 00000000:5E:00.0 Off |                    0 |
     | N/A   34C    P8                9W /  70W|      2MiB / 15360MiB |      0%      Default |
     |                                         |                      |                  N/A |
     +-----------------------------------------+----------------------+----------------------+
     |   2  Tesla T4                        On | 00000000:AF:00.0 Off |                    0 |
     | N/A   29C    P8               10W /  70W|      2MiB / 15360MiB |      0%      Default |
     |                                         |                      |                  N/A |
     +-----------------------------------------+----------------------+----------------------+
     |   3  Tesla T4                        On | 00000000:D8:00.0 Off |                    0 |
     | N/A   29C    P8               10W /  70W|      2MiB / 15360MiB |      0%      Default |
     |                                         |                      |                  N/A |
     +-----------------------------------------+----------------------+----------------------+
     
     +---------------------------------------------------------------------------------------+
     | Processes:                                                                            |
     |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
     |        ID   ID                                                             Usage      |
     |=======================================================================================|
     |  No running processes found                                                           |
     +---------------------------------------------------------------------------------------+
     
     ***CPU***
     Architecture:        x86_64
     CPU op-mode(s):      32-bit, 64-bit
     Byte Order:          Little Endian
     CPU(s):              64
     On-line CPU(s) list: 0-63
     Thread(s) per core:  2
     Core(s) per socket:  16
     Socket(s):           2
     NUMA node(s):        2
     Vendor ID:           GenuineIntel
     CPU family:          6
     Model:               85
     Model name:          Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz
     Stepping:            4
     CPU MHz:             1412.660
     BogoMIPS:            4200.00
     Virtualization:      VT-x
     L1d cache:           32K
     L1i cache:           32K
     L2 cache:            1024K
     L3 cache:            22528K
     NUMA node0 CPU(s):   0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62
     NUMA node1 CPU(s):   1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63
     Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts pku ospke md_clear flush_l1d arch_capabilities
     
     ***CMake***
     /nvme/0/pgali/envs/cudfdev/bin/cmake
     cmake version 3.26.4
     
     CMake suite maintained and supported by Kitware (kitware.com/cmake).
     
     ***g++***
     /nvme/0/pgali/envs/cudfdev/bin/g++
     g++ (conda-forge gcc 11.3.0-19) 11.3.0
     Copyright (C) 2021 Free Software Foundation, Inc.
     This is free software; see the source for copying conditions.  There is NO
     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
     
     
     ***nvcc***
     /nvme/0/pgali/envs/cudfdev/bin/nvcc
     nvcc: NVIDIA (R) Cuda compiler driver
     Copyright (c) 2005-2022 NVIDIA Corporation
     Built on Wed_Sep_21_10:33:58_PDT_2022
     Cuda compilation tools, release 11.8, V11.8.89
     Build cuda_11.8.r11.8/compiler.31833905_0
     
     ***Python***
     /nvme/0/pgali/envs/cudfdev/bin/python
     Python 3.10.11
     
     ***Environment Variables***
     PATH                            : /nvme/0/pgali/envs/cudfdev/bin:/nvme/0/pgali/envs/cudfdev/bin:/nvme/0/pgali/.cargo/bin:/home/nfs/pgali/.vscode-server/bin/b3e4e68a0bc097f0ae7907b217c1119af9e03435/bin/remote-cli:/nvme/0/pgali/.cargo/bin:/nvme/0/pgali/anaconda3/bin:/nvme/0/pgali/anaconda3/condabin:/nvme/0/pgali/.cargo/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin
     LD_LIBRARY_PATH                 : /usr/local/cuda/lib64::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64
     NUMBAPRO_NVVM                   :
     NUMBAPRO_LIBDEVICE              :
     CONDA_PREFIX                    : /nvme/0/pgali/envs/cudfdev
     PYTHON_PATH                     :
     
     ***conda packages***
     /nvme/0/pgali/anaconda3/bin/conda
     # packages in environment at /nvme/0/pgali/envs/cudfdev:
     #
     # Name                    Version                   Build  Channel
     _libgcc_mutex             0.1                 conda_forge    conda-forge
     _openmp_mutex             4.5                       2_gnu    conda-forge
     _sysroot_linux-64_curr_repodata_hack 3                   h69a702a_13    conda-forge
     accessible-pygments       0.0.4              pyhd8ed1ab_0    conda-forge
     aiobotocore               2.5.0              pyhd8ed1ab_0    conda-forge
     aiohttp                   3.8.4           py310h1fa729e_0    conda-forge
     aioitertools              0.11.0             pyhd8ed1ab_0    conda-forge
     aiosignal                 1.3.1              pyhd8ed1ab_0    conda-forge
     alabaster                 0.7.13             pyhd8ed1ab_0    conda-forge
     anyio                     3.6.2              pyhd8ed1ab_0    conda-forge
     argon2-cffi               21.3.0             pyhd8ed1ab_0    conda-forge
     argon2-cffi-bindings      21.2.0          py310h5764c6d_3    conda-forge
     arrow-cpp                 11.0.0          ha770c72_20_cpu    conda-forge
     asttokens                 2.2.1              pyhd8ed1ab_0    conda-forge
     async-timeout             4.0.2              pyhd8ed1ab_0    conda-forge
     attrs                     23.1.0             pyh71513ae_1    conda-forge
     aws-c-auth                0.6.27               he072965_1    conda-forge
     aws-c-cal                 0.5.26               hf677bf3_1    conda-forge
     aws-c-common              0.8.19               hd590300_0    conda-forge
     aws-c-compression         0.2.16               hbad4bc6_7    conda-forge
     aws-c-event-stream        0.2.20               hb4b372c_7    conda-forge
     aws-c-http                0.7.7                h2632f9a_4    conda-forge
     aws-c-io                  0.13.21              h9fef7b8_5    conda-forge
     aws-c-mqtt                0.8.11               h2282364_1    conda-forge
     aws-c-s3                  0.3.0                hcb5a9b2_2    conda-forge
     aws-c-sdkutils            0.1.9                hbad4bc6_2    conda-forge
     aws-checksums             0.1.14               hbad4bc6_7    conda-forge
     aws-crt-cpp               0.20.1               he0fdcb3_3    conda-forge
     aws-sam-translator        1.55.0             pyhd8ed1ab_0    conda-forge
     aws-sdk-cpp               1.10.57             hb0b1f3a_12    conda-forge
     aws-xray-sdk              2.12.0             pyhd8ed1ab_0    conda-forge
     babel                     2.12.1             pyhd8ed1ab_1    conda-forge
     backcall                  0.2.0              pyh9f0ad1d_0    conda-forge
     backports                 1.0                pyhd8ed1ab_3    conda-forge
     backports.functools_lru_cache 1.6.4              pyhd8ed1ab_0    conda-forge
     backports.zoneinfo        0.2.1           py310hff52083_7    conda-forge
     bcrypt                    3.2.2           py310h5764c6d_1    conda-forge
     beautifulsoup4            4.12.2             pyha770c72_0    conda-forge
     binutils                  2.39                 hdd6e379_1    conda-forge
     binutils_impl_linux-64    2.39                 he00db2b_1    conda-forge
     binutils_linux-64         2.39                h5fc0e48_13    conda-forge
     blas                      1.0                         mkl    conda-forge
     bleach                    6.0.0              pyhd8ed1ab_0    conda-forge
     blinker                   1.6.2              pyhd8ed1ab_0    conda-forge
     bokeh                     2.4.3              pyhd8ed1ab_3    conda-forge
     boto3                     1.26.76            pyhd8ed1ab_0    conda-forge
     botocore                  1.29.76            pyhd8ed1ab_0    conda-forge
     brotlipy                  0.7.0           py310h5764c6d_1005    conda-forge
     bzip2                     1.0.8                h7f98852_4    conda-forge
     c-ares                    1.19.0               hd590300_0    conda-forge
     c-compiler                1.5.2                h0b41bf4_0    conda-forge
     ca-certificates           2023.5.7             hbcca054_0    conda-forge
     cachetools                5.3.0              pyhd8ed1ab_0    conda-forge
     certifi                   2023.5.7           pyhd8ed1ab_0    conda-forge
     cffi                      1.15.1          py310h255011f_3    conda-forge
     cfgv                      3.3.1              pyhd8ed1ab_0    conda-forge
     cfn-lint                  0.75.1             pyhd8ed1ab_0    conda-forge
     charset-normalizer        2.1.1              pyhd8ed1ab_0    conda-forge
     click                     8.1.3           unix_pyhd8ed1ab_2    conda-forge
     cloudpickle               2.2.1              pyhd8ed1ab_0    conda-forge
     cmake                     3.26.4               hcfe8598_0    conda-forge
     colorama                  0.4.6              pyhd8ed1ab_0    conda-forge
     comm                      0.1.3              pyhd8ed1ab_0    conda-forge
     commonmark                0.9.1                      py_0    conda-forge
     coverage                  7.2.5           py310h2372a71_0    conda-forge
     cryptography              40.0.2          py310h34c0648_0    conda-forge
     cubinlinker               0.2.2           py310hf09951c_0    rapidsai
     cuda-python               11.8.1          py310h01a121a_2    conda-forge
     cuda-sanitizer-api        11.8.86                       0    nvidia
     cudatoolkit               11.8.0              h37601d7_11    conda-forge
     cudf                      23.6.0                   pypi_0    pypi
     cupy                      12.0.0          py310h9216885_1    conda-forge
     cxx-compiler              1.5.2                hf52228f_0    conda-forge
     cyrus-sasl                2.1.27               h9033bb2_6    conda-forge
     cython                    0.29.34         py310heca2aa9_0    conda-forge
     cytoolz                   0.12.0          py310h5764c6d_1    conda-forge
     dask                      2023.3.2           pyhd8ed1ab_0    conda-forge
     dask-core                 2023.3.2           pyhd8ed1ab_0    conda-forge
     dask-cuda                 23.06.00a       py310_230522_gcf6e9fb_24    rapidsai-nightly
     dask-cudf                 23.6.0                   pypi_0    pypi
     dataclasses               0.8                pyhc8e2a94_3    conda-forge
     datasets                  2.12.0             pyhd8ed1ab_0    conda-forge
     debugpy                   1.6.7           py310heca2aa9_0    conda-forge
     decopatch                 1.4.10             pyhd8ed1ab_0    conda-forge
     decorator                 5.1.1              pyhd8ed1ab_0    conda-forge
     defusedxml                0.7.1              pyhd8ed1ab_0    conda-forge
     dill                      0.3.6              pyhd8ed1ab_1    conda-forge
     distlib                   0.3.6              pyhd8ed1ab_0    conda-forge
     distributed               2023.3.2.1         pyhd8ed1ab_0    conda-forge
     distro                    1.8.0              pyhd8ed1ab_0    conda-forge
     dlpack                    0.5                  h9c3ff4c_0    conda-forge
     docker-py                 6.1.0              pyhd8ed1ab_0    conda-forge
     docutils                  0.19            py310hff52083_1    conda-forge
     doxygen                   1.8.20               had0d8f1_0    conda-forge
     ecdsa                     0.18.0             pyhd8ed1ab_1    conda-forge
     entrypoints               0.4                pyhd8ed1ab_0    conda-forge
     exceptiongroup            1.1.1              pyhd8ed1ab_0    conda-forge
     execnet                   1.9.0              pyhd8ed1ab_0    conda-forge
     executing                 1.2.0              pyhd8ed1ab_0    conda-forge
     expat                     2.5.0                hcb278e6_1    conda-forge
     fastavro                  1.7.4           py310h2372a71_0    conda-forge
     fastrlock                 0.8             py310hd8f1fbe_3    conda-forge
     filelock                  3.12.0             pyhd8ed1ab_0    conda-forge
     flask                     2.3.2              pyhd8ed1ab_0    conda-forge
     flask_cors                3.0.10             pyhd3deb0d_0    conda-forge
     flit-core                 3.9.0              pyhd8ed1ab_0    conda-forge
     fmt                       9.1.0                h924138e_0    conda-forge
     freetype                  2.12.1               hca18f0e_1    conda-forge
     frozenlist                1.3.3           py310h5764c6d_0    conda-forge
     fsspec                    2023.5.0           pyh1a96a4e_0    conda-forge
     future                    0.18.3             pyhd8ed1ab_0    conda-forge
     gcc                       11.3.0              h02d0930_13    conda-forge
     gcc_impl_linux-64         11.3.0              hab1b70f_19    conda-forge
     gcc_linux-64              11.3.0              he6f903b_13    conda-forge
     gflags                    2.2.2             he1b5a44_1004    conda-forge
     glog                      0.6.0                h6f12383_0    conda-forge
     gmock                     1.13.0               ha770c72_1    conda-forge
     gmp                       6.2.1                h58526e2_0    conda-forge
     gmpy2                     2.1.2           py310h3ec546c_1    conda-forge
     graphql-core              3.2.3              pyhd8ed1ab_0    conda-forge
     greenlet                  2.0.2           py310hc6cd4ac_1    conda-forge
     gtest                     1.13.0               h00ab1b0_1    conda-forge
     gxx                       11.3.0              h02d0930_13    conda-forge
     gxx_impl_linux-64         11.3.0              hab1b70f_19    conda-forge
     gxx_linux-64              11.3.0              hc203a17_13    conda-forge
     huggingface_hub           0.14.1             pyhd8ed1ab_0    conda-forge
     hypothesis                6.75.3             pyha770c72_0    conda-forge
     identify                  2.5.24             pyhd8ed1ab_0    conda-forge
     idna                      3.4                pyhd8ed1ab_0    conda-forge
     imagesize                 1.4.1              pyhd8ed1ab_0    conda-forge
     importlib-metadata        6.6.0              pyha770c72_0    conda-forge
     importlib_metadata        6.6.0                hd8ed1ab_0    conda-forge
     iniconfig                 2.0.0              pyhd8ed1ab_0    conda-forge
     intel-openmp              2022.1.0          h9e868ea_3769
     ipykernel                 6.23.1             pyh210e3f2_0    conda-forge
     ipython                   8.13.2             pyh41d4057_0    conda-forge
     ipython_genutils          0.2.0                      py_1    conda-forge
     itsdangerous              2.1.2              pyhd8ed1ab_0    conda-forge
     jedi                      0.18.2             pyhd8ed1ab_0    conda-forge
     jinja2                    3.1.2              pyhd8ed1ab_1    conda-forge
     jmespath                  1.0.1              pyhd8ed1ab_0    conda-forge
     joblib                    1.2.0              pyhd8ed1ab_0    conda-forge
     jschema-to-python         1.2.3              pyhd8ed1ab_0    conda-forge
     jsondiff                  2.0.0              pyhd8ed1ab_0    conda-forge
     jsonpatch                 1.32               pyhd8ed1ab_0    conda-forge
     jsonpickle                2.2.0              pyhd8ed1ab_0    conda-forge
     jsonpointer               2.0                        py_0    conda-forge
     jsonschema                3.2.0              pyhd8ed1ab_3    conda-forge
     junit-xml                 1.9                pyh9f0ad1d_0    conda-forge
     jupyter-cache             0.6.1              pyhd8ed1ab_0    conda-forge
     jupyter_client            8.2.0              pyhd8ed1ab_0    conda-forge
     jupyter_core              5.3.0           py310hff52083_0    conda-forge
     jupyter_events            0.6.3              pyhd8ed1ab_0    conda-forge
     jupyter_server            2.5.0              pyhd8ed1ab_0    conda-forge
     jupyter_server_terminals  0.4.4              pyhd8ed1ab_1    conda-forge
     jupyterlab_pygments       0.2.2              pyhd8ed1ab_0    conda-forge
     kernel-headers_linux-64   3.10.0              h4a8ded7_13    conda-forge
     keyutils                  1.6.1                h166bdaf_0    conda-forge
     krb5                      1.20.1               h81ceb04_0    conda-forge
     lcms2                     2.15                 haa2dc70_1    conda-forge
     ld_impl_linux-64          2.39                 hcc3a1bd_1    conda-forge
     lerc                      4.0.0                h27087fc_0    conda-forge
     libabseil                 20230125.2      cxx17_h59595ed_2    conda-forge
     libarrow                  11.0.0          h6564b11_20_cpu    conda-forge
     libblas                   3.9.0            16_linux64_mkl    conda-forge
     libbrotlicommon           1.0.9                h166bdaf_8    conda-forge
     libbrotlidec              1.0.9                h166bdaf_8    conda-forge
     libbrotlienc              1.0.9                h166bdaf_8    conda-forge
     libcblas                  3.9.0            16_linux64_mkl    conda-forge
     libcrc32c                 1.1.2                h9c3ff4c_0    conda-forge
     libcufile                 1.4.0.31                      0    nvidia
     libcufile-dev             1.4.0.31                      0    nvidia
     libcurand                 10.3.0.86                     0    nvidia
     libcurand-dev             10.3.0.86                     0    nvidia
     libcurl                   8.1.0                h409715c_0    conda-forge
     libdeflate                1.18                 h0b41bf4_0    conda-forge
     libedit                   3.1.20191231         he28a2e2_2    conda-forge
     libev                     4.33                 h516909a_1    conda-forge
     libevent                  2.1.12               h3358134_0    conda-forge
     libexpat                  2.5.0                hcb278e6_1    conda-forge
     libffi                    3.4.2                h7f98852_5    conda-forge
     libgcc-devel_linux-64     11.3.0              h210ce93_19    conda-forge
     libgcc-ng                 12.2.0              h65d4601_19    conda-forge
     libgfortran-ng            12.2.0              h69a702a_19    conda-forge
     libgfortran5              12.2.0              h337968e_19    conda-forge
     libgomp                   12.2.0              h65d4601_19    conda-forge
     libgoogle-cloud           2.10.1               hac9eb74_1    conda-forge
     libgrpc                   1.54.2               hb20ce57_2    conda-forge
     libiconv                  1.17                 h166bdaf_0    conda-forge
     libjpeg-turbo             2.1.5.1              h0b41bf4_0    conda-forge
     libkvikio                 23.06.00a       cuda11_230522_g2fbcd33_26    rapidsai-nightly
     liblapack                 3.9.0            16_linux64_mkl    conda-forge
     libllvm11                 11.1.0               he0ac6c6_5    conda-forge
     libnghttp2                1.52.0               h61bc06f_0    conda-forge
     libnsl                    2.0.0                h7f98852_0    conda-forge
     libntlm                   1.4               h7f98852_1002    conda-forge
     libnuma                   2.0.16               h0b41bf4_1    conda-forge
     libpng                    1.6.39               h753d276_0    conda-forge
     libprotobuf               3.21.12              h3eb15da_0    conda-forge
     librdkafka                1.9.2                ha5a0de0_2    conda-forge
     librmm                    23.06.00a       cuda11_230522_gc11ea8a5_19    rapidsai-nightly
     libsanitizer              11.3.0              h239ccf8_19    conda-forge
     libsodium                 1.0.18               h36c2ea0_1    conda-forge
     libsqlite                 3.42.0               h2797004_0    conda-forge
     libssh2                   1.10.0               hf14f497_3    conda-forge
     libstdcxx-devel_linux-64  11.3.0              h210ce93_19    conda-forge
     libstdcxx-ng              12.2.0              h46fd767_19    conda-forge
     libthrift                 0.18.1               h8fd135c_1    conda-forge
     libtiff                   4.5.0                ha587672_6    conda-forge
     libutf8proc               2.8.0                h166bdaf_0    conda-forge
     libuuid                   2.38.1               h0b41bf4_0    conda-forge
     libuv                     1.44.2               h166bdaf_0    conda-forge
     libwebp-base              1.3.0                h0b41bf4_0    conda-forge
     libxcb                    1.15                 h0b41bf4_0    conda-forge
     libzlib                   1.2.13               h166bdaf_4    conda-forge
     livereload                2.6.3              pyh9f0ad1d_0    conda-forge
     llvmlite                  0.39.1          py310h58363a5_1    conda-forge
     locket                    1.0.0              pyhd8ed1ab_0    conda-forge
     lz4                       4.3.2           py310h0cfdcf0_0    conda-forge
     lz4-c                     1.9.4                hcb278e6_0    conda-forge
     makefun                   1.15.1             pyhd8ed1ab_0    conda-forge
     markdown                  3.4.3              pyhd8ed1ab_0    conda-forge
     markdown-it-py            2.2.0              pyhd8ed1ab_0    conda-forge
     markupsafe                2.1.2           py310h1fa729e_0    conda-forge
     matplotlib-inline         0.1.6              pyhd8ed1ab_0    conda-forge
     mdit-py-plugins           0.3.5              pyhd8ed1ab_0    conda-forge
     mdurl                     0.1.0              pyhd8ed1ab_0    conda-forge
     mimesis                   10.0.0             pyhd8ed1ab_0    conda-forge
     mistune                   2.0.5              pyhd8ed1ab_0    conda-forge
     mkl                       2022.1.0           hc2b9512_224
     moto                      4.1.10             pyhd8ed1ab_0    conda-forge
     mpc                       1.3.1                hfe3b2da_0    conda-forge
     mpfr                      4.2.0                hb012696_0    conda-forge
     msgpack-python            1.0.5           py310hdf3cbec_0    conda-forge
     multidict                 6.0.4           py310h1fa729e_0    conda-forge
     multiprocess              0.70.14         py310h5764c6d_3    conda-forge
     myst-nb                   0.17.2             pyhd8ed1ab_0    conda-forge
     myst-parser               0.18.1             pyhd8ed1ab_0    conda-forge
     nbclassic                 1.0.0              pyhb4ecaf3_1    conda-forge
     nbclient                  0.7.4              pyhd8ed1ab_0    conda-forge
     nbconvert                 7.2.9              pyhd8ed1ab_0    conda-forge
     nbconvert-core            7.2.9              pyhd8ed1ab_0    conda-forge
     nbconvert-pandoc          7.2.9              pyhd8ed1ab_0    conda-forge
     nbformat                  5.8.0              pyhd8ed1ab_0    conda-forge
     nbsphinx                  0.9.1              pyhd8ed1ab_0    conda-forge
     ncurses                   6.3                  h27087fc_1    conda-forge
     nest-asyncio              1.5.6              pyhd8ed1ab_0    conda-forge
     networkx                  2.8.8              pyhd8ed1ab_0    conda-forge
     ninja                     1.11.1               h924138e_0    conda-forge
     nodeenv                   1.8.0              pyhd8ed1ab_0    conda-forge
     notebook                  6.5.4              pyha770c72_0    conda-forge
     notebook-shim             0.2.3              pyhd8ed1ab_0    conda-forge
     numba                     0.56.4          py310h0e39c9b_1    conda-forge
     numpy                     1.23.5          py310h53a5b5f_0    conda-forge
     numpydoc                  1.5.0              pyhd8ed1ab_0    conda-forge
     nvcc_linux-64             11.8                h41dc85b_22    conda-forge
     nvtx                      0.2.5           py310h1fa729e_0    conda-forge
     openapi-schema-validator  0.2.3              pyhd8ed1ab_0    conda-forge
     openapi-spec-validator    0.4.0              pyhd8ed1ab_1    conda-forge
     openjpeg                  2.5.0                hfec8fc6_2    conda-forge
     openssl                   3.1.0                hd590300_3    conda-forge
     orc                       1.8.3                hfdbbad2_0    conda-forge
     packaging                 23.1               pyhd8ed1ab_0    conda-forge
     pandas                    1.5.3                    pypi_0    pypi
     pandoc                    3.1.2                h32600fe_1    conda-forge
     pandocfilters             1.5.0              pyhd8ed1ab_0    conda-forge
     paramiko                  3.1.0              pyhd8ed1ab_0    conda-forge
     parquet-cpp               1.5.1                         2    conda-forge
     parso                     0.8.3              pyhd8ed1ab_0    conda-forge
     partd                     1.4.0              pyhd8ed1ab_0    conda-forge
     pbr                       5.11.1             pyhd8ed1ab_0    conda-forge
     pexpect                   4.8.0              pyh1a96a4e_2    conda-forge
     pickleshare               0.7.5                   py_1003    conda-forge
     pillow                    9.5.0           py310h582fbeb_1    conda-forge
     pip                       23.1.2             pyhd8ed1ab_0    conda-forge
     platformdirs              3.5.1              pyhd8ed1ab_0    conda-forge
     pluggy                    1.0.0              pyhd8ed1ab_5    conda-forge
     pooch                     1.7.0              pyha770c72_3    conda-forge
     pre-commit                3.3.2              pyha770c72_0    conda-forge
     prometheus_client         0.16.0             pyhd8ed1ab_0    conda-forge
     prompt-toolkit            3.0.38             pyha770c72_0    conda-forge
     prompt_toolkit            3.0.38               hd8ed1ab_0    conda-forge
     protobuf                  4.21.12         py310heca2aa9_0    conda-forge
     psutil                    5.9.5           py310h1fa729e_0    conda-forge
     pthread-stubs             0.4               h36c2ea0_1001    conda-forge
     ptxcompiler               0.8.1           py310h01a121a_0    conda-forge
     ptyprocess                0.7.0              pyhd3deb0d_0    conda-forge
     pure_eval                 0.2.2              pyhd8ed1ab_0    conda-forge
     py-cpuinfo                9.0.0              pyhd8ed1ab_0    conda-forge
     pyarrow                   11.0.0          py310he6bfd7f_20_cpu    conda-forge
     pyasn1                    0.4.8                      py_0    conda-forge
     pycparser                 2.21               pyhd8ed1ab_0    conda-forge
     pydata-sphinx-theme       0.13.3             pyhd8ed1ab_0    conda-forge
     pygments                  2.15.1             pyhd8ed1ab_0    conda-forge
     pynacl                    1.5.0           py310h5764c6d_2    conda-forge
     pynvml                    11.4.1             pyhd8ed1ab_0    conda-forge
     pyopenssl                 23.1.1             pyhd8ed1ab_0    conda-forge
     pyorc                     0.8.0           py310hd52fb3e_4    conda-forge
     pyparsing                 3.0.9              pyhd8ed1ab_0    conda-forge
     pyrsistent                0.19.3          py310h1fa729e_0    conda-forge
     pysocks                   1.7.1              pyha2e5f31_6    conda-forge
     pytest                    7.3.1              pyhd8ed1ab_0    conda-forge
     pytest-benchmark          4.0.0              pyhd8ed1ab_0    conda-forge
     pytest-cases              3.6.14             pyhd8ed1ab_0    conda-forge
     pytest-cov                4.0.0              pyhd8ed1ab_0    conda-forge
     pytest-xdist              3.3.1              pyhd8ed1ab_0    conda-forge
     python                    3.10.11         he550d4f_0_cpython    conda-forge
     python-confluent-kafka    1.9.2           py310h5764c6d_2    conda-forge
     python-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge
     python-fastjsonschema     2.17.1             pyhd8ed1ab_0    conda-forge
     python-jose               3.3.0              pyh6c4a22f_1    conda-forge
     python-json-logger        2.0.7              pyhd8ed1ab_0    conda-forge
     python-snappy             0.6.1           py310hcee4d7c_0    conda-forge
     python-xxhash             3.2.0           py310h1fa729e_0    conda-forge
     python_abi                3.10                    3_cp310    conda-forge
     pytorch                   1.11.0             py3.10_cpu_0    pytorch
     pytorch-mutex             1.0                         cpu    pytorch
     pytz                      2023.3             pyhd8ed1ab_0    conda-forge
     pywin32-on-windows        0.1.0              pyh1179c8e_3    conda-forge
     pyyaml                    6.0             py310h5764c6d_5    conda-forge
     pyzmq                     25.0.2          py310h059b190_0    conda-forge
     re2                       2023.03.02           h8c504da_0    conda-forge
     readline                  8.2                  h8228510_1    conda-forge
     recommonmark              0.7.1              pyhd8ed1ab_0    conda-forge
     regex                     2023.5.5        py310h2372a71_0    conda-forge
     requests                  2.31.0             pyhd8ed1ab_0    conda-forge
     responses                 0.18.0             pyhd8ed1ab_0    conda-forge
     rfc3339-validator         0.1.4              pyhd8ed1ab_0    conda-forge
     rfc3986-validator         0.1.1              pyh9f0ad1d_0    conda-forge
     rhash                     1.4.3                h166bdaf_0    conda-forge
     rmm                       23.06.00a       cuda11_py310_230522_gc11ea8a5_19    rapidsai-nightly
     rsa                       4.9                pyhd8ed1ab_0    conda-forge
     s2n                       1.3.44               h06160fa_0    conda-forge
     s3fs                      2023.5.0           pyhd8ed1ab_0    conda-forge
     s3transfer                0.6.1              pyhd8ed1ab_0    conda-forge
     sacremoses                0.0.53             pyhd8ed1ab_0    conda-forge
     sarif-om                  1.0.4              pyhd8ed1ab_0    conda-forge
     scikit-build              0.17.1             pyh56297ac_0    conda-forge
     scipy                     1.10.1          py310ha4c1d20_3    conda-forge
     sed                       4.8                  he412f7d_0    conda-forge
     send2trash                1.8.2              pyh41d4057_0    conda-forge
     setuptools                67.7.2             pyhd8ed1ab_0    conda-forge
     six                       1.16.0             pyh6c4a22f_0    conda-forge
     snappy                    1.1.10               h9fff704_0    conda-forge
     sniffio                   1.3.0              pyhd8ed1ab_0    conda-forge
     snowballstemmer           2.2.0              pyhd8ed1ab_0    conda-forge
     sortedcontainers          2.4.0              pyhd8ed1ab_0    conda-forge
     soupsieve                 2.3.2.post1        pyhd8ed1ab_0    conda-forge
     spdlog                    1.11.0               h9b3ece8_1    conda-forge
     sphinx                    5.3.0              pyhd8ed1ab_0    conda-forge
     sphinx-autobuild          2021.3.14          pyhd8ed1ab_0    conda-forge
     sphinx-copybutton         0.5.2              pyhd8ed1ab_0    conda-forge
     sphinx-markdown-tables    0.0.17             pyh6c4a22f_0    conda-forge
     sphinxcontrib-applehelp   1.0.4              pyhd8ed1ab_0    conda-forge
     sphinxcontrib-devhelp     1.0.2                      py_0    conda-forge
     sphinxcontrib-htmlhelp    2.0.1              pyhd8ed1ab_0    conda-forge
     sphinxcontrib-jsmath      1.0.1                      py_0    conda-forge
     sphinxcontrib-qthelp      1.0.3                      py_0    conda-forge
     sphinxcontrib-serializinghtml 1.1.5              pyhd8ed1ab_2    conda-forge
     sphinxcontrib-websupport  1.2.4              pyhd8ed1ab_1    conda-forge
     sqlalchemy                2.0.15          py310h2372a71_0    conda-forge
     sshpubkeys                3.3.1              pyhd8ed1ab_0    conda-forge
     stack_data                0.6.2              pyhd8ed1ab_0    conda-forge
     streamz                   0.6.4              pyh6c4a22f_0    conda-forge
     sysroot_linux-64          2.17                h4a8ded7_13    conda-forge
     tabulate                  0.9.0              pyhd8ed1ab_1    conda-forge
     tblib                     1.7.0              pyhd8ed1ab_0    conda-forge
     terminado                 0.17.1             pyh41d4057_0    conda-forge
     tinycss2                  1.2.1              pyhd8ed1ab_0    conda-forge
     tk                        8.6.12               h27826a3_0    conda-forge
     tokenizers                0.13.1          py310h633acb5_2    conda-forge
     toml                      0.10.2             pyhd8ed1ab_0    conda-forge
     tomli                     2.0.1              pyhd8ed1ab_0    conda-forge
     toolz                     0.12.0             pyhd8ed1ab_0    conda-forge
     tornado                   6.3.2           py310h2372a71_0    conda-forge
     tqdm                      4.65.0             pyhd8ed1ab_1    conda-forge
     traitlets                 5.9.0              pyhd8ed1ab_0    conda-forge
     transformers              4.24.0             pyhd8ed1ab_0    conda-forge
     typing-extensions         4.5.0                hd8ed1ab_0    conda-forge
     typing_extensions         4.5.0              pyha770c72_0    conda-forge
     tzdata                    2023.3                   pypi_0    pypi
     ucx                       1.14.1               h8c404fb_0    conda-forge
     ukkonen                   1.0.1           py310hbf28c38_3    conda-forge
     urllib3                   1.26.15            pyhd8ed1ab_0    conda-forge
     virtualenv                20.23.0            pyhd8ed1ab_0    conda-forge
     wcwidth                   0.2.6              pyhd8ed1ab_0    conda-forge
     webencodings              0.5.1                      py_1    conda-forge
     websocket-client          1.5.2              pyhd8ed1ab_0    conda-forge
     werkzeug                  2.3.4              pyhd8ed1ab_0    conda-forge
     wheel                     0.40.0             pyhd8ed1ab_0    conda-forge
     wrapt                     1.15.0          py310h1fa729e_0    conda-forge
     xmltodict                 0.13.0             pyhd8ed1ab_0    conda-forge
     xorg-libxau               1.0.11               hd590300_0    conda-forge
     xorg-libxdmcp             1.1.3                h7f98852_0    conda-forge
     xxhash                    0.8.1                h0b41bf4_0    conda-forge
     xz                        5.2.6                h166bdaf_0    conda-forge
     yaml                      0.2.5                h7f98852_2    conda-forge
     yarl                      1.9.1           py310h2372a71_0    conda-forge
     zeromq                    4.3.4                h9c3ff4c_1    conda-forge
     zict                      3.0.0              pyhd8ed1ab_0    conda-forge
     zipp                      3.15.0             pyhd8ed1ab_0    conda-forge
     zlib                      1.2.13               h166bdaf_4    conda-forge
     zstd                      1.5.2                h3eb15da_6    conda-forge
     
</pre></details>


**Additional context**
Add any other context about the problem here.
",2023-05-22T20:54:48Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
566,[ENH] Avoid repeated bounds-checking in `take` when arguments are known in bounds,"As noted in #13419, there are likely places where we call `take` on a column with a gather map that is known to be in-bounds. We should therefore avoid an (unnecessary) bounds-check in these cases where we know this by passing `check_bounds=False`.

_Originally posted by @bdice in https://github.com/rapidsai/cudf/pull/13419#discussion_r1203131049_
            
```[tasklist]
### Tasks
- [ ] parquet.py `_get_groups_and_offsets`
- [ ] `_DataFrameLocIndexer._getitem_tuple_arg`
- [ ] `CategoricalColumn._get_decategorized_column`
- [ ] `ColumnBase.slice`
- [ ] `Rangindex._gather` ?
- [ ] `Groupby.agg`
- [ ] timezones.py `utc_to_local` and `local_to_utc`
- [ ] `MultiIndex.__repr__`
```
",2023-05-26T09:08:20Z,0,0,Lawrence Mitchell,,False
567,[BUG] No error detection in corrupted ORC files,"**Describe the bug**
This is related to https://github.com/rapidsai/cudf/issues/13460

After we write bad data, when we read it back in on the GPU we don't produce any errors.  We don't crash. We just happily return corrupt data.

```
scala> spark.time(spark.read.parquet(""./target/TMP_PAR"").selectExpr(""MIN(ts)"", ""MAX(ts)"").show(false))
+--------------------------+--------------------------+
|min(ts)                   |max(ts)                   |
+--------------------------+--------------------------+
|2023-05-23 08:34:20.007655|2023-05-23 08:34:20.993544|
+--------------------------+--------------------------+

Time taken: 189 ms

scala> spark.time(spark.read.orc(""./target/TMP_ORC"").selectExpr(""MIN(ts)"", ""MAX(ts)"").show(false))
+-------------------+-------------------+
|min(ts)            |max(ts)            |
+-------------------+-------------------+
|2015-01-01 00:00:00|2015-01-01 00:00:08|
+-------------------+-------------------+
```

I realize that CUDF for performance reasons does not do much in the way of checks on the input data, but for input files we really should be doing something.",2023-05-26T16:39:24Z,0,0,Robert (Bobby) Evans,Nvidia,True
568,[FEA] Support for builtin operators in `apply` functions,"Recently we started work to add the `abs` operator to `apply`, in https://github.com/rapidsai/cudf/pull/13408. Previously we added several more built-in casting operators such as `float`, `int` and `bool`, in https://github.com/rapidsai/cudf/pull/11578. We should investigate adding anything else we conceivable _can_. From the list in [python's docs](https://docs.python.org/3/library/functions.html#abs), considering the numeric and string dtypes we currently support, I expect we can add the following operators:


- `bin`
- `chr`
- `divmod` 
- `hash`
- `hex`
- `oct`
- `ord`
- `pow`
- `round`
- `str`


In addition, the following may be implemented once we have some kind of sensible list, tuple, and iterable types:

- `all`, for supported iterables
- `any`, for supported iterables
- `list`
- `range`
- `slice`
- `sum`
- `tuple`
",2023-05-26T16:45:43Z,0,0,,NVIDIA,True
569,[BUG] JSON reader has no option to return the columns only for the requested schema,"**Describe the bug**
Pandas and Spark are very different in what gets returned when reading a JSON file.  In pandas you provide essentially type hints, but it will return data for all columns in the file and only for columns in the file.

Spark has a schema that they want and when given a schema they want to read only the columns that match the schema and nothing else.

This causes two problems for Spark.  The first one is a performance issue.  If we want to read only one column out of 100, CUDF is going to materialize 100 columns and then we are going to throw away 99 of them.  It would be great if we didn't need the memory for all of the extra columns or the computation needed to create them.


~The second problem is really an odd corner case.  CUDF does not support a Table with just rows, but Spark does.  So if there is a JSON file with no columns, but rows.~
~{}~
~{}~
~We have no way to actually read that without some help.~
(see https://github.com/rapidsai/cudf/issues/5712)

It really would be nice to have a mode similar to how parquet, ORC, or CSV work where only the columns that are requested are returned and all of the requested columns are returned, even if the values are all nulls.",2023-05-30T21:45:24Z,1,0,Robert (Bobby) Evans,Nvidia,True
570,[BUG] NVTX ranges not being closed in Frame._get_columns_by_label?,"**Describe the bug**
When running the following Python script, it seems that the NVTX ranges aren't being closed. See screenshot below.

```python
import cudf
from cudf.testing._utils import assert_eq

if __name__ == ""__main__"":
    for i in range(3):
        gdf = cudf.DataFrame({""a"": range(10)})
        pdf = gdf.to_pandas()
        assert_eq(gdf, pdf)
```

Then execute `nsys profile python nvtx_bug.py`.

![image](https://github.com/rapidsai/cudf/assets/3943761/750120c7-b800-4b8d-9af7-174adc93f7f6)

I didn't see this problem with `cudf.Series`, only `cudf.DataFrame`.
",2023-06-01T13:35:57Z,0,0,Bradley Dice,@NVIDIA @rapidsai,True
571,[FEA] Test more edge cases in bitmask operations,"**Is your feature request related to a problem? Please describe.**
libcudf has multiple functions (both internal and external) for manipulating bitmasks. These functions often make use of bitwise operations and CUDA intrinsics in kernels, which may have sharp edges around when bitmasks contain uninitialized bits or in other ways. We recently uncovered one in #13479, and it was only uncovered via intermittent failures from dask_cudf (Python) tests.

**Describe the solution you'd like**
We should consider adding more tests that directly play with bitmasks, perhaps bitmasks containing invalid data in certain ways, to better understand the potential failure modes. In some cases these may be test cases that we do not need to support since libcudf generally follows a ""garbage in, garbage out"" approach, but in some cases the ""garbage in"" may be generated by libcudf itself and therefore something that we need to support.

I will update this issue with specific tests cases that may prove relevant:
- A bitmask with enough bits that we have > 1 CUDA block of data, and potentially with junk bits either at the beginning or the end.
",2023-06-01T17:09:13Z,0,0,Vyas Ramasubramani,@rapidsai,True
572,[FEA] Support V2 encodings in Parquet reader and writer,"Parquet V1 format supports three types of page encodings: PLAIN, DICTIONARY, and RLE (run-length encoded) ([reference from Spark Jira](https://issues.apache.org/jira/browse/SPARK-36879)). The newer and evolving Parquet V2 specification adds support for several [additional encodings](https://parquet.apache.org/docs/file-format/data-pages/encodings/), including DELTA_BINARY_PACKED for `INT32` and `INT64` types, DELTA_BYTE_ARRAY for `strings` logical type, and DELTA_LENGTH_BYTE_ARRAY for `strings` logical type. 

In the parquet reader and writer, libcudf should support V2 metadata as well as the three variants of DELTA encoding.

| Feature | Status | Notes | 
|---|---|---|
| Add V2 reader support |  ✅ #11778 | |
| Multi-warp decode of Dremel data streams | ✅ #13203 |  | 
| Use efficient strings column factory in decoder | ✅ #13302 |  | 
| Implement DELTA_BINARY_PACKED decoding |  ✅  #13637 | see #12948 for reference |
| Implement DELTA_BYTE_ARRAY decoding | ✅ #14101 | see #12948 for reference |
| Add V2 writer support | ✅ #13751 | |
| Implement DELTA_BINARY_PACKED encoding | ✅ #14100 | | 
| Add python bindings for V2 header and options | ✅ #14316 | |
| Implement DELTA_BYTE_ARRAY encoding | ✅ #15239 | some outdated reviews in #14938 |
| Implement DELTA_LENGTH_BYTE_ARRAY encoding and decoding for unsorted data | ✅ #14590 | |
| Add C++ API support for specifying encodings | ✅ #15081 | |
| Add cuDF-python API support for specifying encodings |  | |
| Add BYTE_STREAM_SPLIT encoding and decoding  | ✅ #15311 | see issue #15226 and [parquet reference](https://github.com/apache/parquet-format/blob/master/Encodings.md#byte-stream-split-byte_stream_split--9) |

",2023-06-02T03:38:40Z,0,0,Gregory Kimball,,False
573,[FEA] GitHub Issue Infrastructure Update,"**Describe the solution you'd like**
I'd like to (in 2 separate PRs)
~1. Replace .md issue templates with .yml forms~
   - ~With these templates I'd remove the https://github.com/rapidsai/cudf/labels/%3F%20-%20Needs%20Triage label from always being on new issues~
2.  Use a GitHub Action to automatically label and reply to issues filed by people outside of RAPIDS
    - Goal: reduce the noise of the https://github.com/rapidsai/cudf/labels/%3F%20-%20Needs%20Triage label and ensure we respond quickly to external filers

**Describe alternatives you've considered**
We could also do triage rotations with the GHA but I prefer the auto-labeling and leveraging the label to track as needed.

**Additional context**
We do this in Morpheus - references: 
- [Form template issues](https://github.com/nv-morpheus/Morpheus/issues/new/choose)
- [Auto Labeler](https://github.com/nv-morpheus/Morpheus/blob/branch-23.07/.github/workflows/label-external-issues.yml)

Looking to solicit feedback prior to making any changes. If feedback is all 🟢 then I'll PR these changes in ASAP and we can work through the specific implementations in the PR review.
",2023-06-02T18:32:10Z,0,0,Ben Jarmak,Nvidia,True
574,[FEA] Improve cudf::gather scalability as number of columns increases,"As the number of columns increases for `cudf::gather` with the same gather map, we see the number of kernels called increase proportionally and the runtime increases linearly. We are wondering if there are better ways to group or ""batch"" these calls so we perform less kernel invocations that can do more work all at once, in hopes of amortizing some of the cost with many columns or deeply nested schemas.

A very simple example is below. This creates a column of 10 `int32_t` rows and adds it to a struct N times (where `N` is between 2 and 1024):

```
#include <cudf/table/table.hpp>
#include <cudf_test/column_wrapper.hpp>

#include <rmm/mr/device/cuda_memory_resource.hpp>
#include <rmm/mr/device/device_memory_resource.hpp>
#include <rmm/mr/device/pool_memory_resource.hpp>

#include <memory>
#include <string>
#include <vector>
#include <nvtx3/nvToolsExt.h>

int main(int argc, char** argv)
{
  rmm::mr::cuda_memory_resource cuda_mr{};
  rmm::mr::pool_memory_resource mr{&cuda_mr};
  rmm::mr::set_current_device_resource(&mr);
  using col_t = cudf::test::fixed_width_column_wrapper<int32_t>;

  auto const values = std::vector<int32_t>{1,2,3,4,5,6,7,8,9,10};
  for (int num_cols = 2; num_cols <= 1024; num_cols *= 2) {
    std::vector<std::unique_ptr<cudf::column>> members(num_cols);
    for (auto i = 0; i < num_cols; ++i) {
      auto wrapper = col_t(values.begin(), values.end());
      members[i] = wrapper.release();
    }
    auto struct_col = cudf::test::structs_column_wrapper(std::move(members));
    auto gather_map = std::vector<cudf::offset_type>{1}; // gather 1 row
    std::stringstream msg;
    nvtxRangePush(msg.str().c_str()); 
    auto result = cudf::gather(
      cudf::table_view{{struct_col}}, 
      cudf::test::fixed_width_column_wrapper<int32_t>(gather_map.begin(), gather_map.end()),
      cudf::out_of_bounds_policy::NULLIFY);
    nvtxRangePop();
    std::cout << ""Result: rows: "" << result->num_rows() << "" cols: "" << result->num_columns() << std::endl;

  }
  return 0;
}
```
As the column count increases by 2x, the gather kernel takes 2x longer:

![Screenshot from 2023-06-05 10-18-52](https://github.com/rapidsai/cudf/assets/1901059/500ac308-9ab3-494c-97fa-1ffec7ac875a)

A similar argument can be made for columns that have nested things like arrays of structs (each with array members). The number of calls to underlying cub calls can increase drastically.

I am filing this issue to solicit comments/patches to see how we could improve this behavior.
",2023-06-05T15:24:45Z,0,0,Alessandro Bellina,NVIDIA,True
575,[FEA] JSON reader improvements for Spark-RAPIDS,"libcudf includes a GPU-accelerated JSON reader that uses a finite-state transducer parser combined with token-processing tree algorithms to transform character buffers into columnar data. This issue tracks the technical work leading up to the launch of libcudf's JSON reader as a default component of the Spark-RAPIDS plugin. Please also refer to the [Nested JSON reader milestone](https://github.com/rapidsai/cudf/milestone/13) and [Spark-RAPIDS JSON epic](https://github.com/NVIDIA/spark-rapids/issues/9458).


### Spark compatibility issues: Blockers
| Status | Impact for Spark | Change to libcudf |
|---|---|---|
| ✅ #13344  | #12532, Blocker: if any line has an error, libcudf throws an exception  | Rework state machine to include error states and scrub tokens from lines with error | 
| ✅ #14252 | #14227, Blocker: Incorrect parsing | Fix bug in error recovery state transitions |  
✅ #14279 | #14226, Blocker: requesting alternate error recovery behavior from #13344, where valid data before an error state are preserved  | Changes in JSON parser pushdown automaton for JSON_LINES_RECOVER option  | 
| ✅ #14936 | #14288, Blocker: libcudf does not have an efficient representation for map types in Spark |  libcudf does not support map types, and modeling the map types as structs results in poor performance due to one child column per unique key. We will return the struct data that represents map types as string and then the plugin can use [unify_json_strings](https://github.com/NVIDIA/spark-rapids-jni/blob/54ef9991f46fa873d580315212aeae345da7152a/src/main/cpp/src/map_utils.cu#L63-L112) to parse tokens |
| ✅ #14572  | #14239, Blocker: fields with mixed types raise an exception | add libcudf reader option to return mixed types as strings. Also see improvements in  #15236 and  #14939 | 
| ✅ #14545 | #10004, Blocker: Can't parse data with single quote variant of JSON when `allowSingleQuotes` is enabled in Spark | Introduce a preprocessing function to normalize single and double quotes as double quotes | 
| ✅ #15324 | #15303, escaped single quotes have their escapes dropped during quote normalization | Adjust quote normalization FST |
| 🔄 #15419 | #15390 + #15409, Blocker: race conditions found in nested JSON reader | Solve synchronization problems in nested JSON reader  |
| | #15260, Blocker: crash in mixed type support | |
| 🔄 | #15278, Blocker: allow list type to be coerced to string, also see #14239. Without this, Spark-RAPIDS will fallback when user requests a field as ""string"" | Support List types coercion to string | 
| | #15277, Blocker: we need to support multi-line JSON objects. Also see #10267 | libcudf is scoping a ""multi-object"" reader |  



### Spark compatibility issues: non-blockers

| Status  | Impact for Spark | Change to libcudf  | 
|---|---|---|
| | #15222, compatibility problems with leading zeros, ""NAN"" and escape options | None for now. This feature should live in Spark-RAPIDS as a post-processing option for now, based on the approach for `get_json_object` modeled after Spark CPU code (see https://github.com/NVIDIA/spark-rapids-jni/pull/1836). Then the plugin can set to null any entries from objects that Spark would treat as invalid. Later we could provide Spark-RAPIDS access to raw tokens that they could run through a more efficient validator.  |
| ✅ #15033 |  #14865, Strip whitespace from JSON inputs, otherwise Spark will have to add this in post-processing the coerced strings types | Create new normalization pre-processing tool for whitespace | 
| 🔄 #14996 | #13473, Performance: only process columns in the schema | Skip parsing and column creation for keys not specified in the schema |
| 🔄 #15124 | Reader option performance is unknown | #15041, add JSON reader option benchmarking | |
| | Performance: Avoid preprocessing to replace empty lines with `{}`. Also see #5712 |  libcudf provides strings column data source | 
|  | #15280 find a solution when whitespace normalization fixes a line that originally was invalid | We could move whitespace normalization after tokenization. Also we would like to address #15277 so that we can remove unquoted newline characters as well. |
| | n/a, Spark-RAPIDS doesn't use byte range reading | #15185, reduce IO overhead in JSON byte range reading |
| | n/a, Spark-RAPIDS doesn't use byte range reading | #15186, address data loss edge case for byte range reading |
|  | reduce peak memory usage | add chunking to the JSON reader | 
| | #15222, Spark-RAPIDS must return null if any field is invalid | Provide token stream to Spark-RAPIDS for validation, including checks or leading zeros, special string numbers like `NaN`, `+INF`, `-INF`, and optional limits for which characters can be escaped |


",2023-06-07T16:30:32Z,0,0,Gregory Kimball,,False
576,[FEA] Use table_view interface in scatter-by-foo API in CUDF,"Both the scatter-like and gather-like Cython interfaces to libcudf's copying API accept a list of source and target columns as a ""table"". This feature is, however, only _used_ in the front-facing API by the gather-like calls.

For scatter-like calls on a dataframe, the final result is usually achieved by creating series for each column to scatter to/from and calling the singleton scatter on the series before reconstructing. This is more wasteful than it needs to be and we should probably instead just create the list of columns we wish to scatter and scatter that instead.

For example, the low-level Cython interface to scatter accepts a list of source and target columns:

https://github.com/rapidsai/cudf/blob/8055c2db6f80286b64d36f3927510bcf2e0eec02/python/cudf/cudf/_lib/copying.pyx#L240

However, the only caller in the python codebase is `ColumnBase._scatter_by_column` https://github.com/rapidsai/cudf/blob/8055c2db6f80286b64d36f3927510bcf2e0eec02/python/cudf/cudf/core/column/column.py#L647-L682 where we only have a single column to hand and hence can only pass one column.

This is usage is a consequence of `__setitem__` on `DataFrame`s ultimately being implemented by spinning over the columns (as series) and calling setitem on each series in turn. For example, `DataFrameIlocIndexer.__setitem__` has this code (when setting columns using a dataframe as the rvalue):

https://github.com/rapidsai/cudf/blob/8055c2db6f80286b64d36f3927510bcf2e0eec02/python/cudf/cudf/core/dataframe.py#L466-L477

This calls (on line 474) `Series.__setitem__` on each which either goes to `series.iloc.__setitem__` or `series.loc.__setitem__`, and eventually devolves to `ColumnBase.__setitem__`.

(Aside, this is actually a bug, since `iloc.__setitem__` should not do index alignment which `loc` will do).

Ignoring that bug issue, this code is somewhat wasteful in a number of ways:

1. we eventually throw away the `Series` object when reconstructing the dataframe, so should just operate on columns
2. at this point in `DataFrame.iloc.__setitem__` we _already know_ everything about the key (and hence which low-level libcudf operation to dispatch to) so we should do that rather than going through the front door of `Series.__setitem__` which will endeavour to rediscover everything we already know.
3. we call into libcudf n times (once for each column), rather than once with n columns, which negates the ability to use the stream parallelism that is implemented in the `scatter` code in libcudf

To fix this, I propose that `DataFrame` and `Series` should both get a new set of methods. Here's a first pass at what these should be called/their interface. These are internal and should do no argument normalisation or bounds-checking (it being the responsibility of the caller to sanitise appropriately), this includes kind/dtype casting as appropriate of the source and target.

- `_scatter_by_indices(self, source_columns, index_column, *, keep_index=True)`
- `_scatter_by_mask(self, source_columns, mask_column, *, keep_index=True)`
- `_scatter_by_slice(self, source_columns, slice, *, keep_index=True)`

Since there is also a broadcasting `scatter_from_scalar` operation in libcudf we'll probably need to support that too. This should be done with a `value_type` tag in the interface (rather than isinstancing on the `source_columns`) since, again, the caller must have already determined this information.",2023-06-07T16:46:17Z,0,0,Lawrence Mitchell,,False
577,[BUG] `Column._scatter_by_slice` doesn't handle negative-stride slices correctly.,"**Describe the bug**

```python
In [49]: col = cudf.core.column.as_column([1, 2, 3, 4])

In [50]: col
Out[50]: 
<cudf.core.column.numerical.NumericalColumn object at 0x7fcd07e6c4c0>
[
  1,
  2,
  3,
  4
]
dtype: int64

In [51]: col[::-1] = cudf.Scalar(7)

In [52]: col
Out[52]: 
<cudf.core.column.numerical.NumericalColumn object at 0x7fcd07e6c4c0>
[
  1,
  2,
  3,
  4
]
dtype: int64
```

This eventually calls `_scatter_by_slice`, which does this:

```python
    def _scatter_by_slice(
        self,
        key: builtins.slice,
        value: Union[cudf.core.scalar.Scalar, ColumnBase],
    ) -> Optional[Self]:
        """"""If this function returns None, it's either a no-op (slice is empty),
        or the inplace replacement is already performed (fill-in-place).
        """"""
        start, stop, step = key.indices(len(self))
        if start >= stop:
            return None
        num_keys = len(range(start, stop, step))

```

But that first check is not right to determine if the slice is empty.

```python
# x[::-1]
slice(None, None, -1).indices(3)
# => (2, -1, -1)
```

**Expected behavior**

This should work.",2023-06-08T08:50:43Z,0,0,Lawrence Mitchell,,False
578,[ENH]: Reimagining cudf user-input boundaries,"tl;dr: If you squint, everything is really a compiler problem in disguise.


## Overview

The pandas API is very broad and has numerous places where ""best effort"" attempts are made to DWIM (do what I mean). One example is during dataframe indexing, where a significant effort is made to ""desugar"" the user input into a canonical form.

For example, turning `df.loc[[1, 2, 3]]` into `df.loc[[1, 2, 3], :]` (that is, a single entry on a dataframe is equivalent to taking rows and *all* the columns).

This pattern is pervasive, to take another different example, `read_parquet` accepts as the ""file"" object:

1.  a string (indicating a file to open)
2.  a Path object
3.  raw bytes
4.  Anything with a `read` method
5.  A list (maybe sequence?) of the above


## The status quo

To handle this kind of multi-modal unstructured input, the user-visible API of cudf carries out inspection and validation of the inputs before dispatching to appropriate lower-level routines. By the time we reach libcudf, all decisions must have been made.

This works, but has a number of problems:

1.  It is not always clear *whose* job it is to do the validation. Is any ""pseudo-public"" method required to validate correctness of its inputs? Or is it always the job of the definitively public API to validate all inputs before handing off. One sees this in private `Frame` methods like `_gather` which have an explicit `check_bounds` flag (which often is not set when it could be, because the only *safe* default is `True`).
2.  Validation just checks for valid inputs and then returns types unchanged (so defensive programming requires consumers of the input to assume worst-case scenarios and check things again).
3.  Consequently, validation and inspection often occur (on any given input): 
    i. More than once 
    ii. Inconsistently (generally this is not deliberate, it's just hard to keep track).


## Proposal

I propose that we take a leaf out of the type-driven design crowd's book and treat the user-facing input validation as a *parsing* rather than *validating* problem. What does this mean? Rather than just checking input in user-facing API functions and dispatching to internal functions that *receive the same type* we should tighten the types as we go, ""parsing"" the unstructured user input into more structured data as we transit through the call stack.

This has, I think, a number of advantages:

1.  It clarifies *where* in the implementation validation in dispatch takes place (any time a type changes), separating the business logic of making sense of the input from the concrete implementation for the different paths.
2.  It makes cross-calling between internal APIs that *don't* do validation safe. For example, rather than `_gather` having the signature `a -> Column -> Maybe a` for some dataframe type `a` and always having to check that the provided column is inbounds as a gather map, we would tighten the type to `a -> InBoundsFor a Column -> a`. Now the gather map comes with a ""proof"" that it is inbounds for the dataframe we are gathering, and we therefore do not have to do bounds checking[^1]. Now, we can't statically enforce this (in the sense that in the implementation, someone will be able to conjure the ""proof"" out of thin air if they really want to), but type checking will at least indicate when we don't promise an in-bounds column.
3.  By separating the business logic from the backend dispatch we keep the good work we've done in producing a pandas-like API and make it *easier* to (on the long term) slot in other backends (for example a distributed, multi-node, backend underneath the cudf front-facing API).


[^1]: In a statically typed language with rank-N types you can actually make these proofs [part of the type system](https://kataskeue.com/gdp.pdf), though we can't here in Python.

## Further reading

- A nice overview of this idea with more examples: https://lexi-lambda.github.io/blog/2019/11/05/parse-don-t-validate/
- Similar ideas as the ""typestate"" pattern: https://cliffle.com/blog/rust-typestate/
- ""Make illegal states unrepresentable"": https://buttondown.email/hillelwayne/archive/making-illegal-states-unrepresentable/",2023-06-09T17:26:44Z,0,0,Lawrence Mitchell,,False
579,"When the amount of data is not small,The parameter Num of the parallel process becomes larger, but the running time becomes longer","**Describe the bug**

**Steps/Code to reproduce bug**
```
import cudf
from multiprocessing import get_context
import time
pdf = cudf.DataFrame({
        'low':[i for i in range(1000)],
        'close':[i for i in range(1000)],
    })

def get_df(idx):
    return idx.rolling(5).mean()

if __name__ == ""__main__"":
    ctx = get_context(""spawn"")
    start = time.time()
    num =1
    with ctx.Pool(num) as pool:
        cudf.concat(pool.map(get_df, [pdf.a[i:] for i in range(100, 200)]))
    print(time.time()-start)

```

num =1 ,time 3.2659552097320557
num =2, time 7.382307291030884",2023-06-10T15:14:43Z,0,0,,,False
580,[ENH] Audit `argsort + gather/scatter` patterns for missing performance,"During review of #13419 we noted a few places where there is a pattern like:

```python
some_frame = ...
indices = some_frame.some_column.argsort()
new_frame = some_frame.take(indices)
```

As well as the unnecessary bounds-check (see #13456), this is a pattern that is captured by libcudf's [`sort_by_key`](https://docs.rapids.ai/api/libcudf/stable/group__column__sort.html#ga6db0403a43150b3bca0fbb9b2fbd68a3) and [`stable_sort_by_key`](https://docs.rapids.ai/api/libcudf/stable/group__column__sort.html#gaea04f441fe246b5a7e4f6420864024d4) functions (we would want to use the latter in pandas-compat mode).

At present, libcudf implements this as a `argsort` of the key columns followed by a gather. But that's an implementation detail (there may in the future be updates to that implementation). In the Python layer we should ""say what we mean"" and call into the appropriate libcudf API.

A cursory search shows:

```
/usr/bin/rg -U -e argsort.\*'     
'.\*take

core/_base_index.py
1305:        indices = self.argsort(ascending=ascending, na_position=na_position)
1306:        index_sorted = self.take(indices)

core/indexed_frame.py
2464:                # double-argsort to map back from sorted to unsorted positions
2465:                df = df.take(index.argsort(ascending=True).argsort())

core/column/column.py
1382:        order = order.take(left_gather_map, check_bounds=False).argsort()
1383:        codes = codes.take(order)

core/groupby/groupby.py
686:            gather_map = ordering.take(to_take).argsort()
687:            return result.take(gather_map)
```

Of these, the calls in `_base_index.py`, `_column.py`, and `groupby.py` can definitely be replaced by `sort_by_key`. Note also that none of these calls pass `check_bounds=False` to `take` so incur an unnecessary kernel launch to check in-boundsness for something that is guaranteed in bounds.

The `take(argsort().argsort())` pattern is not a `sort_by_key`, however, we can elide one of the argsorts by noticing that `take` is a gather operation and for a permutation, the dual to gather is scatter. So this should be implemented as `df.scatter(index.argsort())` instead...

These are just the cases where an argsort is _immediately_ followed by a take, probably more diligent searching would find more.
```[tasklist]
### Tasks
- [ ] `RollingGroupby.__init__`
- [ ] `Groupby._head_tail`
- [ ] `IndexedFrame.interpolate`
- [ ] `IndexedFrame.sort_index`
- [ ] `IndexedFrame._reindex`
- [ ] `IndexedFrame.sort_values`
- [ ] `IndexedFrame._n_largest_or_smallest`
- [ ] `BaseIndex.sort_values`
- [ ] `MultiIndex.__repr__`
```
",2023-06-13T10:57:12Z,0,0,Lawrence Mitchell,,False
581,[BUG] The libcudf device atomic operations cause memcheck errors on types smaller than 4 bytes,"The [`device_atomics`](https://github.com/rapidsai/cudf/blob/branch-23.08/cpp/include/cudf/detail/utilities/device_atomics.cuh) implementation in libcudf are failing memchecks when used on types smaller than 4 bytes.

https://github.com/rapidsai/cudf/blob/c929a84719baf169d05e4d0ee87a0df6e77f5b45/cpp/include/cudf/detail/utilities/device_atomics.cuh#L65-L69

This was found while investigating some memcheck errors that occurred on `cudf::reduce` where the aggregation return type is `bool` (1 byte) and an `atomicOr` was used to produce the output. Reference: https://github.com/rapidsai/cudf/pull/13574

The solution is to use the libcudaxx `cuda::atomic` and `cuda::atomic_ref` in place of these implementations and potentially remove the `device_atomics.cuh` altogether. ",2023-06-14T17:28:56Z,0,0,David Wendt,NVIDIA,True
582,[FEA] Support indicator=True in cudf.DataFrame.merge,"**Is your feature request related to a problem? Please describe.**
I wish dask_cudf dataframes would support the `indicator=True` option in the merge function

**Describe the solution you'd like**
In dask dataframe the code:
```python
import pandas as pd
import dask.dataframe as dd

dfa = pd.DataFrame({""id"":[1,2,2,4], ""a"":[""a"",""b"",""c"",""d""]})
dfb = pd.DataFrame({""id"":[2,3,3,4], ""b"":[""e"",""f"",""g"",""h""]})
ddfa = dd.from_pandas(dfa, npartitions=1)
ddfb = dd.from_pandas(dfb, npartitions=1)

ddf = ddfa.merge(ddfb, on=""id"", how=""outer"", indicator=True)
print(ddf.compute())
```
returns:
```
   id    a    b      _merge
0   1    a  NaN   left_only
1   2    b    e        both
2   2    c    e        both
3   4    d    h        both
4   3  NaN    f  right_only
5   3  NaN    g  right_only
```

but in dask_cudf:
```python
import cudf
import dask_cudf as dd

dfa = cudf.DataFrame({""id"":[1,2,2,4], ""a"":[""a"",""b"",""c"",""d""]})
dfb = cudf.DataFrame({""id"":[2,3,3,4], ""b"":[""e"",""f"",""g"",""h""]})
ddfa = dd.from_cudf(dfa, npartitions=1)
ddfb = dd.from_cudf(dfb, npartitions=1)

ddf = ddfa.merge(ddfb, on=""id"", how=""outer"", indicator=True)
print(ddf.compute())
```
this throws a `NotImplementedError`:
```
Traceback (most recent call last):                                                                                                                             
  File ""/raid/cjarrett/dask-sql/min.py"", line 13, in <module>     
    ddf = ddfa.merge(ddfb, on=""id"", how=""outer"", indicator=True)
  File ""/raid/cjarrett/mambaforge/envs/dsql-6-6/lib/python3.10/site-packages/nvtx/nvtx.py"", line 101, in inner
    result = func(*args, **kwargs)
  File ""/raid/cjarrett/mambaforge/envs/dsql-6-6/lib/python3.10/site-packages/dask_cudf/core.py"", line 121, in merge
    return super().merge(
  File ""/raid/cjarrett/mambaforge/envs/dsql-6-6/lib/python3.10/site-packages/dask/dataframe/core.py"", line 5644, in merge
    return merge(
  File ""/raid/cjarrett/mambaforge/envs/dsql-6-6/lib/python3.10/site-packages/dask/dataframe/multi.py"", line 724, in merge
    return hash_join(
  File ""/raid/cjarrett/mambaforge/envs/dsql-6-6/lib/python3.10/site-packages/dask/dataframe/multi.py"", line 398, in hash_join
    meta = _lhs_meta.merge(_rhs_meta, **kwargs)
  File ""/raid/cjarrett/mambaforge/envs/dsql-6-6/lib/python3.10/site-packages/nvtx/nvtx.py"", line 101, in inner
    result = func(*args, **kwargs)
  File ""/raid/cjarrett/mambaforge/envs/dsql-6-6/lib/python3.10/site-packages/cudf/core/dataframe.py"", line 3969, in merge
    raise NotImplementedError(
NotImplementedError: Only indicator=False is currently supported
```


",2023-06-14T21:07:36Z,0,0,,,False
583,"Let `size_in_bits` take a `bytes` argument, rather than just being a templated function to multiply `sizeof(T)` by `CHAR_BIT`.","As noted by [@harrism in #13577](https://github.com/rapidsai/cudf/pull/13577#discussion_r1230283942_).
 
> I would agree with you @bdice  if `size_in_bits` took a size in bytes argument. But as it is, you have to write `data_buffer->size() * size_in_bits<std::byte>()`, which is kinda dumb.  `size_in_bits(data_buffer->size())` would be OK.  I think `* CHAR_BIT` is OK too.

This could be solved by removing the template argument from `size_in_bits` and just implementing it as a `constexpr` function that multiplies its argument by `CHAR_BIT`. All of the usages in `static_assert`s would still work (since the call would just change from `size_in_bits<T>()` to `size_in_bits(sizeof(T))`).",2023-06-19T08:50:37Z,1,0,Lawrence Mitchell,,False
584,Deprecate and remove `cudf.isclose`.,"Currently we have a function `cudf.isclose` that is undocumented. It mirrors `cupy.isclose` but also has null support. Pandas does not have a similar `isclose` function. After discussion with @shwina, @galipremsagar, and @vyasr we decided to deprecate and remove this function.

See also: https://github.com/rapidsai/cudf/issues/5105

As part of the deprecation, we decided to add a warning message indicating how to support nulls while using `cupy.isclose`.",2023-06-20T19:31:40Z,0,0,Bradley Dice,@NVIDIA @rapidsai,True
585,[FEA] Investigate why padding is needed for input buffers to compression/decompression kernels,"After `rmm` removed memory padding (https://github.com/rapidsai/rmm/pull/1278), some places in libcudf got into trouble with out-of-bound memory access. These bugs can be fixed by simply padding the memory buffer back (https://github.com/rapidsai/cudf/pull/13586) but that is not an optimal, long-term solution. The permanent solution should be discovering why such padding is needed and whether we can avoid such padding altogether.

We should investigate further the issue to find a permanent solution.",2023-06-22T20:17:35Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
586,[FEA] Add support for timezone-aware data to `from_pandas`,"**Is your feature request related to a problem? Please describe.**
Support for timezone-aware datetimes is ongoing in https://github.com/rapidsai/cudf/issues/12813, and currently timezone-aware operations like `tz_convert` and `tz_localize` are supported.

However, when trying to load in a pandas dataframe with timezone-aware data using `from_pandas`, we still error with a message that implies that timezone-aware datetimes aren't yet supported:

```python
import cudf
import pandas as pd

df = pd.DataFrame(
    {
        ""d"": pd.date_range(
            start=""2014-08-01 09:00"", freq=""8H"", periods=6, tz=""UTC""
        ),
    }
)

cudf.from_pandas(df)
```

```python-traceback
NotImplementedError                       Traceback (most recent call last)
Cell In[1], line 12
      2 import pandas as pd
      4 df = pd.DataFrame(
      5     {
      6         ""d"": pd.date_range(
   (...)
      9     }
     10 )
---> 12 cudf.from_pandas(df)

File /datasets/charlesb/micromamba/envs/dask-sql-gpuci-py39/lib/python3.9/site-packages/nvtx/nvtx.py:101, in annotate.__call__.<locals>.inner(*args, **kwargs)
     98 @wraps(func)
     99 def inner(*args, **kwargs):
    100     libnvtx_push_range(self.attributes, self.domain.handle)
--> 101     result = func(*args, **kwargs)
    102     libnvtx_pop_range(self.domain.handle)
    103     return result

File /datasets/charlesb/micromamba/envs/dask-sql-gpuci-py39/lib/python3.9/site-packages/cudf/core/dataframe.py:7491, in from_pandas(obj, nan_as_null)
   7393 """"""
   7394 Convert certain Pandas objects into the cudf equivalent.
   7395 
   (...)
   7488 <class 'pandas.core.indexes.multi.MultiIndex'>
   7489 """"""
   7490 if isinstance(obj, pd.DataFrame):
-> 7491     return DataFrame.from_pandas(obj, nan_as_null=nan_as_null)
   7492 elif isinstance(obj, pd.Series):
   7493     return Series.from_pandas(obj, nan_as_null=nan_as_null)

File /datasets/charlesb/micromamba/envs/dask-sql-gpuci-py39/lib/python3.9/site-packages/nvtx/nvtx.py:101, in annotate.__call__.<locals>.inner(*args, **kwargs)
     98 @wraps(func)
     99 def inner(*args, **kwargs):
    100     libnvtx_push_range(self.attributes, self.domain.handle)
--> 101     result = func(*args, **kwargs)
    102     libnvtx_pop_range(self.domain.handle)
    103     return result

File /datasets/charlesb/micromamba/envs/dask-sql-gpuci-py39/lib/python3.9/site-packages/cudf/core/dataframe.py:5119, in DataFrame.from_pandas(cls, dataframe, nan_as_null)
   5115 for col_name, col_value in dataframe.items():
   5116     # necessary because multi-index can return multiple
   5117     # columns for a single key
   5118     if len(col_value.shape) == 1:
-> 5119         data[col_name] = column.as_column(
   5120             col_value.array, nan_as_null=nan_as_null
   5121         )
   5122     else:
   5123         vals = col_value.values.T

File /datasets/charlesb/micromamba/envs/dask-sql-gpuci-py39/lib/python3.9/site-packages/cudf/core/column/column.py:2327, in as_column(arbitrary, nan_as_null, dtype, length)
   2317         if cudf.get_option(
   2318             ""default_float_bitwidth""
   2319         ) and infer_dtype(arbitrary) in (
   2320             ""floating"",
   2321             ""mixed-integer-float"",
   2322         ):
   2323             pa_type = np_to_pa_dtype(
   2324                 _maybe_convert_to_default_type(""float"")
   2325             )
-> 2327     data = as_column(
   2328         pa.array(
   2329             arbitrary,
   2330             type=pa_type,
   2331             from_pandas=True
   2332             if nan_as_null is None
   2333             else nan_as_null,
   2334         ),
   2335         dtype=dtype,
   2336         nan_as_null=nan_as_null,
   2337     )
   2338 except (pa.ArrowInvalid, pa.ArrowTypeError, TypeError):
   2339     if is_categorical_dtype(dtype):

File /datasets/charlesb/micromamba/envs/dask-sql-gpuci-py39/lib/python3.9/site-packages/cudf/core/column/column.py:1974, in as_column(arbitrary, nan_as_null, dtype, length)
   1968 if isinstance(arbitrary, pa.lib.HalfFloatArray):
   1969     raise NotImplementedError(
   1970         ""Type casting from `float16` to `float32` is not ""
   1971         ""yet supported in pyarrow, see: ""
   1972         ""https://issues.apache.org/jira/browse/ARROW-3802""
   1973     )
-> 1974 col = ColumnBase.from_arrow(arbitrary)
   1976 if isinstance(arbitrary, pa.NullArray):
   1977     new_dtype = cudf.dtype(arbitrary.type.to_pandas_dtype())

File /datasets/charlesb/micromamba/envs/dask-sql-gpuci-py39/lib/python3.9/site-packages/cudf/core/column/column.py:340, in ColumnBase.from_arrow(cls, array)
    334 data = pa.table([array], [None])
    336 if (
    337     isinstance(array.type, pa.TimestampType)
    338     and array.type.tz is not None
    339 ):
--> 340     raise NotImplementedError(
    341         ""cuDF does not yet support timezone-aware datetimes""
    342     )
    343 if isinstance(array.type, pa.DictionaryType):
    344     indices_table = pa.table(
    345         {
    346             ""None"": pa.chunked_array(
   (...)
    350         }
    351     )

NotImplementedError: cuDF does not yet support timezone-aware datetimes
```

**Describe the solution you'd like**
Either adding support for timezone-aware data to `from_pandas`, or updating the error message to indicate to users workarounds they can use while support is in progress.

**Describe alternatives you've considered**
It is generally possible to move timezone-aware data from pandas to cuDF by storing the timezone information somewhere, converting the data to timezone-naive (`dt.tz_localize(None)`) and then restoring the information once it's been read in with `from_pandas`.",2023-06-23T18:45:08Z,0,0,Charles Blackmon-Luca,@rapidsai,True
587,[FEA] Add Parquet and ORC unit tests based on Apache sample files,"During the 23.06 release, we encountered several important Parquet and ORC writer issues that risked data corruption. These issues included:
* Rare failure with page size estimator (PQ writer, [Report](https://github.com/rapidsai/cudf/issues/13250), [Fix](https://github.com/rapidsai/cudf/pull/13364))
* Failure with >1GB tables (PQ writer, [Report](https://github.com/rapidsai/cudf/issues/13414), [Fix](https://github.com/rapidsai/cudf/pull/13438)) 
* Failure with 10k nulls followed by >5 valid values (ORC Writer, [Report](https://github.com/rapidsai/cudf/issues/13460), [Fix](https://github.com/rapidsai/cudf/pull/13466))

After discussion with the team we agreed on these additions to our testing suite to help prevent similar issues in the future:
* Based on test files in [parquet-testing/data](https://github.com/apache/parquet-testing/tree/b2e7cc755159196e3a068c8594f7acbaecfdaaac/data), verify that ""read"" versus ""read-write-read"" result in identical tables
* Based on test files in [orc/examples](https://github.com/apache/orc/tree/main/examples), verify that ""read"" versus ""read-write-read"" result in identical tables
* Based on test files in [parquet-testing/data](https://github.com/apache/parquet-testing/tree/b2e7cc755159196e3a068c8594f7acbaecfdaaac/data), verify that ""read"" versus ""read_with_Arrow-convert_to_cudf"" result in identical tables
* Based on test files in [orc/examples](https://github.com/apache/orc/tree/main/examples), verify that ""read"" versus ""read_with_Arrow-convert_to_cudf"" result in identical tables

Note: please also see (#12739), for reader benchmarks, verify that the roundtripped table matches the starting table
",2023-06-27T19:58:33Z,0,0,Gregory Kimball,,False
588,[ENH] benchmark gather then sort vs sort then gather in merge with `sort=True`,"**Is your feature request related to a problem? Please describe.**

When we request `sort=True` in a `cudf.merge`, the current implementation does:

1. deduce left and right join columns
2. join, producing left and right gather maps
3. gather left and right columns, and merge results
4. deduce key columns to sort by
5. argsort the key columns
6. gather the result using the argsort return value

Trivially, steps 5 and 6 can be merged into a `sort_by_key` (that's #13557). However, this order probably does more data movement than it needs to. This makes two calls to gather, and one sort-by-key, at the cost of moving the full dataframe through memory twice (once in step 3, once in step 6).

Instead, we could (if sorting) first gather only the key columns we will sort by, argsort those and then use that ordering to sort the left and right gather maps.

1. deduce left and right join columns
2. join, producing left and right gather maps
3. deduce left and right key columns to order by
4. gather left key columns with left map, right key columns with right map
5. sort-by-key the left and right gather maps with the columns from step 4
6. gather left and right columns with new gather maps and merge

This makes four calls to gather and one sort-by-key, but only moves the full dataframe through memory once (in step 6). For dataframes with many non-key columns this might well be an advantage. The latency will be a bit higher, but the total data movement will be less. For example, consider (for simplicity) a left join with one key column and 10 total columns in both left and right dataframes.

The current approach (once the left and right gather maps have been determined) gathers 20 columns in step 3, argsorts one column, then gathers 20 columns again (sort-by-key merges the sort + gather into argsort + gather at the libcudf level).

The proposed alternative would gather 1 column in step 4, sorts-by-key two columns (the two gather maps), then gathers 20 columns. So we move effectively 23 columns through memory rather than 41.",2023-06-28T11:56:18Z,0,0,Lawrence Mitchell,,False
589,[BUG] loc-based indexing fails when looking up array like with ordered categorical index,"**Describe the bug**

When a frame's index is a `CategoricalIndex` that is _ordered_, looking up an array-like list of indices fails with `TypeError: Merging on categorical variables with mismatched ordering is ambiguous`.

This occurs because to perform the merge between the index and the to-be-looked-up values (to find matches), the values are turned into a categorical column, but the default category dtype is un-ordered.

**Steps/Code to reproduce bug**
```python
import cudf
s = cudf.Series([1, 2, 3, 4, 5], index=cudf.CategoricalIndex([1, 2, 3, 4, 5], categories=[1, 2, 3, 4, 5], ordered=True))
s.loc[[1, 4]] # => KeyError
```

**Expected behavior**

```python
s.to_pandas().loc[[1, 4]]
# 1    1
# 4    4
# dtype: int64
```

I note actually that pandas doesn't care at all about ordered-ness of categoricals in lookup. Which kind of makes sense because you're just looking up values.

So probably the solution is to merge the decategorized index column with the asked-for labels.",2023-07-03T12:04:54Z,0,0,Lawrence Mitchell,,False
590,[BUG] Scalar loc-based lookup in integer categorical indices is incorrect,"**Describe the bug**

`loc`-based lookup does (I think incorrect) fallback to positional indexing rather than label-based lookup when the index is a categorical one with integer values.

**Steps/Code to reproduce bug**
```python
import cudf

s = cudf.Series([1, 2], index=cudf.CategoricalIndex([3, 4], categories=[3, 4]))
s.loc[3] # IndexError: single positional indexer is out-of-bounds
```

**Expected behavior**

```python
s.to_pandas().loc[3]
# 1
```

~Annoyingly, one can't just stop doing positional indexing fallback in all cases because if the index is (say) a string index then integer indexing _does_ fall back to positional.~ This is for `Series.__getitem__` and the behaviour is deprecated in pandas 2.",2023-07-03T12:11:57Z,0,0,Lawrence Mitchell,,False
591,[FEA] Support negative years when converting date string to a timestamp,"**Is your feature request related to a problem? Please describe.**
I wish cudf could support negative years when trying to convert string date to a timestamp

**Describe the solution you'd like**
The test below should pass 
```
  @Test
  public void testNegativeTimeString() {
    final String[] TIMES_S_STRING = {""-1970-02-01 00:00:00""};
    try (ColumnVector s_string_times = ColumnVector.fromStrings(TIMES_S_STRING);
         ColumnVector s_result = s_string_times.asTimestampSeconds(""%Y-%m-%d %H:%M:%S"");
         ColumnVector s_expected = applyAndClose(ColumnVector.fromLongs(-124331673600l), cv -> cv.asTimestampSeconds())) {
      assertColumnsAreEqual(s_expected, s_result);
    }
  }

```

**Describe alternatives you've considered**
NA",2023-07-10T20:07:19Z,0,0,Raza Jafri,NVIDIA,True
592,[FEA] Support nested fields for `filter` in `cudf.read_parquet()`,"**Is your feature request related to a problem? Please describe.**

I wish I could use the `filter` argument of `cudf.read_parquet()` on nested columns. The in-progress GeoArrow specification is considering allowing a `struct<x: double, y: double>` coordinate representation which provides out-of-the-box column statistics for the inner `x` and `y`. Linestrings, polygons, and multipolygons involve layers of `list<>` nesting that still produce column statistics for the coordinates that would be nice to use with a bounding box filter (see example below).

**Describe the solution you'd like**

It would be nice if the left-hand side of a filter expression could be a tuple instead of a string to specify a nested field. I imagine the nested field of a list is more complicated here but even a nested struct field would be helpful.

**Describe alternatives you've considered**

The current workaround I've used is to flatten the fields before writing the parquet. This is OK but looses the extension type metadata (e.g., CRS) and doesn't scale to the nested types (e.g., linestring, polygon, multipolygon).

**Additional context**

A small illustration with some test data:

```python
>>> import pyarrow as pa
>>> import pyarrow.parquet as pq
>>> 
>>> 
>>> xs = pa.array([0.0, 1.0, 2.0, 3.0])
>>> ys = pa.array([1.0, 2.0, 3.0, 4.0])
>>> xys = pa.array([
...     {""x"": 0.0, ""y"": 1.0}, 
...     {""x"": 1.0, ""y"": 2.0}, 
...     {""x"": 2.0, ""y"": 3.0}, 
...     {""x"": 3.0, ""y"": 4.0}, 
... ])
>>> table = pa.table([xs, ys, xys], names=[""x"", ""y"", ""xy""])
>>> pq.write_table(table, ""test.parquet"")
>>> 
>>> # Works!
>>> bounds = [0.5, 1.5, 2.5, 3.5]
>>> cudf.read_parquet(
...     ""test.parquet"",
...     filters=[
...         [
...             ('x', '>=', bounds[0]),
...             ('y', '>=', bounds[1]),
...             ('x', '<=', bounds[2]),
...             ('y', '<=', bounds[3])
...         ]
...     ]
... )
     x    y                    xy
0  1.0  2.0  {'x': 1.0, 'y': 2.0}
1  2.0  3.0  {'x': 2.0, 'y': 3.0}
>>> 
>>> # Doesn't work:
>>> cudf.read_parquet(
...     ""test.parquet"",
...     filters=[
...         [
...             (('xy', 'x'), '>=', bounds[0]),
...             (('xy', 'y'), '>=', bounds[1]),
...             (('xy', 'x'), '<=', bounds[2]),
...             (('xy', 'y'), '<=', bounds[3])
...         ]
...     ]
... )
.conda/lib/python3.10/site-packages/cudf/io/parquet.py:674: UserWarning: Row-wise filtering failed in read_parquet for [[(('xy', 'x'), '>=', 0.5), (('xy', 'y'), '>=', 1.5), (('xy', 'x'), '<=', 2.5), (('xy', 'y'), '<=', 3.5)]]
  warnings.warn(
     x    y                    xy
0  0.0  1.0  {'x': 0.0, 'y': 1.0}
1  1.0  2.0  {'x': 1.0, 'y': 2.0}
2  2.0  3.0  {'x': 2.0, 'y': 3.0}
3  3.0  4.0  {'x': 3.0, 'y': 4.0}
```
",2023-07-11T01:39:09Z,0,0,Dewey Dunnington,@voltrondata,False
593,[FEA] Support left-semi and left-anti joins in `cudf::hash_join`,"**Is your feature request related to a problem? Please describe.**

`cudf::hash_join` makes it possible to build the hash table once and probe it multiple times. But it only supports inner join, left join and full join. I wish `cudf::hash_join` can support left-semi and left-anti join as well.
",2023-07-16T00:38:38Z,0,0,,,False
594,Should cudf hashes decimal values the same regardless of scale?,"Currently, cudf hashes decimals the same way regardless of their scale. To me, this is a violation of ""different values hash differently"". The scale is a piece of column metadata but I think it's relevant to the hash computation because hashes are typically used as a proxy for equivalent representations. I think this could be considered a bug. My proposal would be to combine the column scale into the hash of the value.

_Originally posted by @bdice in https://github.com/rapidsai/cudf/pull/13612#discussion_r1264530164_
            ",2023-07-17T16:16:17Z,0,0,Bradley Dice,@NVIDIA @rapidsai,True
595,[FEA] Hash function refactoring,"Following up from #13681 and #13612, there are some tasks I think can be done to clean up hashing code. I am opening this issue to be a tracker for the work we've deferred from other PRs.

- Functions like `getblock32` can be reused by multiple hash functions. https://github.com/rapidsai/cudf/pull/13612/files#r1265587501
Also consider optimizing this to use `memcpy` or `if constexpr` on key type perhaps.
- Functions like `rotate_bits_left` should not use CUDA intrinsics (it doesn't affect the PTX/SASS compared to a naive shift-based implementation), thereby making them `constexpr`-friendly and possible to put into a shared `hpp` header. https://github.com/rapidsai/cudf/pull/13681#discussion_r1264527017
- The `@brief` of hashing APIs should mention the algorithm name. https://github.com/rapidsai/cudf/pull/13612/files#r1267270076",2023-07-17T16:24:38Z,0,0,Bradley Dice,@NVIDIA @rapidsai,True
596,[FEA] Benchmark Jaccard with alternative distributions,"**Is your feature request related to a problem? Please describe.**
[A clear and concise description of what the problem is. Ex. I wish I could use cuDF to do [...]](https://github.com/rapidsai/cudf/pull/13669) adds the Jaccard index feature. It also includes benchmarks using normally distributed data. However, realistic data is likely to be more geometric in nature and have longer-tailed distributions with a small number of much larger strings.

**Describe the solution you'd like**
We should add a benchmark with a geometric (or other skewed) string length distribution.",2023-07-19T22:13:06Z,0,0,Vyas Ramasubramani,@rapidsai,True
597,[BUG] `TypeError` when broadcasting `Series` to `DataFrame` with non-numerical columns,"**Describe the bug**
I am getting a `TypeError` when I try to subtract a `cudf.Series` (`s`) from a `cudf.Dataframe` (`df`) having non-numerical columns, even when `s` only contains elements that correspond to the numerical columns of `df`. Note that it is not an issue when `s` leaves out columns of `df`, unless those columns are numerical.

**Steps/Code to reproduce bug**
```python
In [1]: import cudf
In [2]: df = cudf.DataFrame({""a"": [""dog""] * 10, ""b"": range(10)})
In [3]: s = df.mean(numeric_only=True)
In [4]: df - s
```
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[4], line 1
----> 1 df - s

File /datasets/rzamora/miniconda3/envs/cudf_pandas_2/lib/python3.10/site-packages/cudf/core/mixins/mixin_factory.py:11, in _partialmethod.<locals>.wrapper(self, *args2, **kwargs2)
     10 def wrapper(self, *args2, **kwargs2):
---> 11     return method(self, *args1, *args2, **kwargs1, **kwargs2)

File /datasets/rzamora/miniconda3/envs/cudf_pandas_2/lib/python3.10/site-packages/cudf/core/indexed_frame.py:3475, in IndexedFrame._binaryop(self, other, op, fill_value, can_reindex, *args, **kwargs)
   3471 if operands is NotImplemented:
   3472     return NotImplemented
   3474 return self._from_data(
-> 3475     ColumnAccessor(type(self)._colwise_binop(operands, op)),
   3476     index=out_index,
   3477 )

File /datasets/rzamora/miniconda3/envs/cudf_pandas_2/lib/python3.10/site-packages/nvtx/nvtx.py:101, in annotate.__call__.<locals>.inner(*args, **kwargs)
     98 @wraps(func)
     99 def inner(*args, **kwargs):
    100     libnvtx_push_range(self.attributes, self.domain.handle)
--> 101     result = func(*args, **kwargs)
    102     libnvtx_pop_range(self.domain.handle)
    103     return result

File /datasets/rzamora/miniconda3/envs/cudf_pandas_2/lib/python3.10/site-packages/cudf/core/frame.py:1761, in Frame._colwise_binop(cls, operands, fn)
   1753         assert False, ""At least one operand must be a column.""
   1755 # TODO: Disable logical and binary operators between columns that
   1756 # are not numerical using the new binops mixin.
   1758 outcol = (
   1759     getattr(operator, fn)(right_column, left_column)
   1760     if reflect
-> 1761     else getattr(operator, fn)(left_column, right_column)
   1762 )
   1764 if output_mask is not None:
   1765     outcol = outcol.set_mask(output_mask)

TypeError: unsupported operand type(s) for -: 'StringColumn' and 'NoneType'
```

For `pandas`, I get the following output:

```
     a    b
0  NaN -4.5
1  NaN -3.5
2  NaN -2.5
3  NaN -1.5
4  NaN -0.5
5  NaN  0.5
6  NaN  1.5
7  NaN  2.5
8  NaN  3.5
9  NaN  4.5
```

**Expected behavior**
I expect the same behavior as pandas. That is, when broadcasting a `Series` to the columns of a `DataFrame` for a binop, missing columns in the `Series`' index should result in the corresponding column having all null values in the output. This seems to work fine in `cudf`, unless the missing column is non-numerical.",2023-07-21T22:25:52Z,0,0,Richard (Rick) Zamora,@NVIDIA,True
598,[FEA] Increase maximum characters in strings columns,"In libcudf, strings columns have child columns containing character data and offsets, and the offsets child column uses a 32-bit signed size type. This limits strings columns to containing ~2.1 billion characters. For LLM training, documents have up to 1M characters, and a median around 3K characters. Due to the size type limit, LLM training pipelines have to carefully batch the data down to a few thousand rows to stay comfortably within the size type limit. We have a general issue open to explore a 64-bit size type in libcudf (#13159). For size issues with LLM training pipelines, we should consider a targeted change to only address the size limit for strings columns.

### Requirements
* We must maintain or improve throughput for functions processing strings columns with <2.1 billion characters. This requirement prevents us from using 64-bit offsets for all strings columns. It does not prevent us from using 64-bit offsets for strings columns with >2.1 billion characters.
* We must not introduce a new data type or otherwise increase compile times significantly. This requirement prevents us from dispatching between ""strings"" types and ""large strings"" types. 

### Proposed solution
One idea that satisfies these requirements would be to represent the character data as an `int64` typed column instead of an `int8` typed column. This would allow us to store 8x more bytes of character data. To access the character bytes, we would use an offset-normalizing iterator (inspired by [""indexalator""](https://github.com/rapidsai/cudf/blob/branch-23.08/cpp/include/cudf/detail/indexalator.cuh)) to identify byte positions using an `int64` iterator output. Please note that the row count 32-bit size type would still apply to the proposed ""large strings"" columns.

We should also consider an ""unbounded"" character data allocation that is not typed, but rather a single buffer up to 2^64 bytes in size. The 64-bit offset type would be able to index into much larger allocations.

Please note that this solution will not impact the offsets for list columns. We believe that the best design to allow for more than 2.1B elements in lists will be to use 64-bit size type in libcudf as discussed in #13159.

### Creating strings columns
Strings columns factories would choose child column types at the time of column creation, based on the size of the character data. This change would impact strings column factories, as well as algorithms that use strings column utilities or generate their own offsets buffers. At column creation time, the constructor will choose between `int32` offsets with `int8` character data and `int64` offsets with `int64` character data, based on the size of the character data. Any function that calls [make_offsets_child_column](https://github.com/rapidsai/cudf/blob/9e099cef25b11821c6307bb9c231656a2bae700f/cpp/include/cudf/detail/sizes_to_offsets_iterator.cuh#L298-L302) will need to be aware of the alternate child column types for large strings.

### Accessing strings data
The offset-normalizing iterator would always return `int64` type so that strings column consumers would not need to support both `int32` and `int64` offset types. See [cudf::detail::sizes_to_offsets_iterator](https://github.com/rapidsai/cudf/pull/12180) for an example of how an iterator operating on `int32` data can output `int64` data.

### Interoperability with Arrow
The new strings column variant with `int64` offsets with `int64` character data may already be Arrow-compatible. This requires more testing and some changes to our Arrow interop utilities.

### Part 1: libcudf changes to support large strings columns

Definitions:
""strings column"": `int8` character data and `int32` offset data (2.1B characters) 
""large strings column"": `int8` character data up to 2^64 bytes and `int64` offset data (18400T characters)

| Step | PR | Notes | 
|---|---|---|
| Replace `offset_type` references with `size_type` | ✅ #13788 | offsets generated by the offset-normalizing iterator will have type `int64_t` | 
| <s> Add new data-size member to `cudf::column_view`, `cudf::mutable_column_view` and `cudf::column_device_view` </s> | ❌ #14031 | solution for character counts greater than `int32` | 
| Create an offset-normalizing iterator over character data that always outputs 64-bit offsets| ✅ #14206 <br> ✅ #14234 | First step in #14043 |
| * Add the character data buffer to the parent strings column, rather than as a child column <br> * Also refactor algorithms such as concat, contiguous split and gather which access character data <br> * Update code in cuDF-python that interact with character child columns <br> * Update code in cudf-java that interact with character child columns | ✅ #14202 | See performance blocker resolved in ✅ #14540 |
| Deprecate unneeded factories and use strings column factories consistently | ✅ | #14461, #14771, #14695, #14612, +one more | 
| Introduce an environment variable to control the threshold for converting to 64-bit indices, to enable testing on smaller strings columns | ✅ `LIBCUDF_LARGE_STRINGS_THRESHOLD` added | part of #14612 | 
| Transition strings APIs to use the offset-normalizing iterator (""offsetalator"") | ✅ | See #14611, #14700, #14744, #14745, #14757, #14783, #14824  | 
| Remove references to `strings_column_view::offsets_begin()` in libcudf since it hardcodes the return type as int32. | ✅ | See #15112 #15077  | 
| Remove references to `create_chars_child_column` in libcudf since it wraps a column around chars data. | ✅ | #15241 | 
| Change the current `make_strings_children` to return a uvector for chars instead of a column | ✅ | See #15171  | 
| Introduce an environment variable `LIBCUDF_LARGE_STRINGS_ENABLED` to let users force libcudf to throw rather than start using 64-bit offsets, to allow try-catch-repartitioning instead | ✅ |  #15195  | 
| Introduce an environment variable `LIBCUDF_LARGE_STRINGS_THRESHOLD` | ✅ |  #14612 | 
| Rework `concatenate` to produce large strings when `LIBCUDF_LARGE_STRINGS_ENABLED` and character count is above the `LIBCUDF_LARGE_STRINGS_THRESHOLD` | ✅ |  See #15195  |
| cuDF-python testing. use concat to create a large string column. We should be able to operate on this column, as long as we aren't creating a large string. Can we: (1) returns int/bool, like, contains, (2) slice (3) returns smaller strings. | 🔄 | |
| Add an `experimental` version of `make_strings_children` that generates 64-bit offsets when the total character length exceeds the threshold | ✅ |#15363 | 
| Add a large strings test fixture that stores large columns between unit tests and controls the environment variables | ✅ | #15513  | 
| Check appropriate cudf tests pass with `LIBCUDF_LARGE_STRINGS_THRESHOLD` at zero  | | |
| benchmark regressions analyzed and approved | | |
| Spark-RAPIDS tests pass | | |
| Remove `experimental` namespace. Replace `make_strings_children` with implementation with the `experimental` namespace version. | ✅ | #15702  | 
| Live session with cuDF-python expert to start producing and operating on large strings | | |
| Ensure that we can interop strings columns with 64-bit offsets to arrow as LARGE_STRING type | | Also see #15093 about `large_strings` compatibility for pandas-2.2 |

### Part 2: cuIO changes to read and write large strings columns

| Step | PR | Notes |
|---|---|---| 
| Add functionality to JSON reader to construct large string columns | | Could require building a chunked JSON reader |
| Add functionality to Parquet reader to construct large string columns | | | 
| to be continued... | | | ",2023-07-22T20:58:01Z,2,0,Gregory Kimball,,False
599,[BUG] `cudf.read_text` throws an exception when reading a host buffer,"**Describe the bug**
`cudf.read_text` throws an exception when reading a host buffer, unlike the CSV, Parquet, ORC and JSON readers. Hopefully it is straightforward to enable `cudf.read_text` to read from host buffers. 

**Steps/Code to reproduce bug**
Here is a small code snippet that shows the exception:
```
from io import BytesIO
import cudf

df = cudf.DataFrame({'a':['aaaa','bbbb']})

buf = BytesIO()
df.to_csv(buf, index=False)
df2 = cudf.read_csv(buf) # this is ok
print(df2)

buf = BytesIO()
df.to_csv(buf, index=False)
df2 = cudf.read_text(buf, delimiter='\n') # this crashes
print(df2)
```

**Expected behavior**
I expect that `read_text` would support host buffers correctly in the python layer. ""multibyte_split"" works fine with HOST_BUFFER data source in the libcudf benchmarks.

**Environment overview (please complete the following information)**
nightly docker image from 23.08, A100 DGX workstation
",2023-07-22T21:20:47Z,0,0,Gregory Kimball,,False
600,[FEA] Add `bytes_per_second` to all libcudf benchmarks,"Many libcudf benchmarks report the `bytes_per_second` processed as part of the output data. This value is useful for comparing benchmarks because it normalizes the increasing execution time from processing more data. Also, `bytes_per_second` and `real_time_s` together let us compute `bytes_processed` values which serve as a useful Y-axis.

As of the end of 23.12 development, many benchmarks still do not report `bytes_per_second` in the output data. Here is a figure summarizing the metrics reported by the benchmark suite.

![image](https://github.com/rapidsai/cudf/assets/12725111/aeb8176c-e869-4200-83aa-074aa4aaee5a)

| benchmark | status | notes | 
|---|---|---|
|`APPLY_BOOLEAN_MASK` | | see #13937 | 
| `BINARYOP` | | see #13938 |
| `COPY_IF_ELSE` | | see #13960 |
| `GROUPBY` | | see #13984 |
| `GROUPBY_NV` | | |
| `HASHING` | | see #13967, #13965|
| `JOIN` | | |
| `JOIN_NV` | | |
| `QUANTILES` | | |
| `REDUCTION` | | |
| `REPLACE` | | |
| `SEARCH` | | |
| `SEARCH_NV` | | |
| `SET_OPS_NV` | | |
| `SHIFT` | | see #13950 |
| `SORT` | | |
| `SORT_NV` | | |
| `STREAM_COMPACTION_NV` | | see #14172 |
| `TRANSPOSE` | | see #14170 |


Note: For this tracking list, cuIO benchmarks are omitted because ""encoded file size"" serves a similar purpose. ",2023-07-23T21:11:18Z,0,0,Gregory Kimball,,False
601,[FEA] Support <4 byte types in `GroupBy.apply` with `engine='jit'`,"**Is your feature request related to a problem? Please describe.**
During https://github.com/rapidsai/cudf/pull/13729 we were able to round out the 4 and 8 byte dtypes supported by the groupby JIT engine, however shorter types such as `int8` and `int16` couldn't be added due to the missing support for those types from certain `libcu++` features we rely on in the shim library. 

**Describe the solution you'd like**
We should provide block level implementations for these types in c++ that don't rely on the particular libcu++ features.

**Describe alternatives you've considered**
We could also wait for libcu++ to provide things like `atomic_ref` for shorter types. 

cc @bwyogatama @PointKernel 
",2023-07-24T14:06:17Z,0,0,,NVIDIA,True
602,Require streams in public API for groupby aggregate.,"In #13727, we added a public API accepting a stream while retaining the old overload that does not accept a stream. We are migrating to requiring streams in public APIs, so the old overload without a stream should be removed. We decided to accept this for 23.08 (during burndown) with the intent to fix in 23.10.

cc: @mythrocks @vyasr",2023-07-24T18:25:53Z,1,0,Bradley Dice,@NVIDIA @rapidsai,True
603,[FEA] Expose public stream-ordered C++ APIs,"**Is your feature request related to a problem? Please describe.**
Currently most libcudf APIs operate implicitly on the default CUDA stream. As documented in #925, we would like to expose stream-ordered APIs publicly. Prior to doing so, we needed to do a great deal of work to ensure that libcudf's internals were properly stream-ordered. Among the important tasks here were removing implicit APIs that ran on a stream (see #11968) and implementing a strategy for verifying that streams are properly forwarded from APIs all the way down to the lowest level functions (see #11943). Now that all such work that we are aware of has been completed, libcudf should start exposing streams in public APIs. 

We originally considered adding stream support via a monolithic feature branch where APIs would be modified one at a time. From various discussions, however, we decided that the overhead of maintaining a feature branch would grow too large to be worthwhile, so we have instead decided to proceed by incrementally exposing streams in libcudf APIs over time. PRs adding new APIs should always include a stream in the public API. Existing APIs will be transitioned one header file at a time, with PRs going directly into the current main branch of cudf.

The purpose of this PR is to document progress on the task of transitioning existing headers, as well as to describe the process by which new stream-ordered APIs should be added.

**Describe the solution you'd like**
When adding stream support to an API, the stream parameter should be placed just before the mr parameter with a default value of `cudf::get_default_stream()`. A new test should be added that validates that the API properly forwards the stream to all CUDA calls, which can be done by following the instructions at the bottom of [libcudf's testing developer docs](https://docs.rapids.ai/api/libcudf/nightly/md_developer_guide_testing). 

To divide up the work, we consider public headers organized by directories (all relative to `cpp/include/cudf`). Devs may assign themselves a directory within which to own all headers and the public APIs contained therein.

| Directory | Source <br> files (#) | PRs | Details |
|---|---|---|---|
| cudf/ | 28 |  hashing #12090, copying #13629, concatenate #13987, filling #13990, replace #14010, search #14034, sorting #14146, null mask #14263, unary #14342 | owner: @vyasr |
| cudf/ast/          | 1 | | |
| cudf/column/  | 4 |  factories #14354  | |
| cudf/dictionary/ | 5 | encode + search + update_keys #14115 | |
| cudf/fixed_point/ | 2 | | |
| cudf/io/ | 16 | csv #14340, json #14313, pq #14359, orc #14350  | See ""text"" request in #14383 |
| cudf/labeling/ | 1 | #14401 |  |
| cudf/lists/ | 15 | combine + contains + count #14248, extract + sort + compact + reverse + gather #14272 | |
| nvtext/ | 11 | ngram #14061, replace #14329, tokenize #14317, edit #14456 | owner: @davidwendt |
| cudf/rolling/ | 1 | | |
| cudf/scalar/ | 3 |   | See for scalar refactoring work #14354 |
| cudf/strings/ | 37 | split #13997, #14247, find #14060, case #14056, replace #14261, convert #14255, filter #14293, contains #14280, combine #14281, misc #14260 | owner: @davidwendt  | 
| cudf/structs/ | 3 | | |
| cudf/table/ | 5 | | |
| cudf/tdigest/ | 1 | | |
| cudf/utilities/ | 9 | | |
| cudf/wrappers/ | 3 | | |




**Additional context**
Before we began making public stream parameters, libcudf public APIs would be paired with detail APIs that are nearly identical except for having a required stream parameter. In some cases, though, the public API may not currently have a detail ""mirror"" function but instead call multiple detail APIs. In those cases we may need to make a case-by-case determination of whether a new detail function should also be added. Some discussion of whether we want this pattern to be present everywhere, and the reasons why we might want that, are discussed in #14276.

Public stream API readiness:
* revamp the entire test suite to use ""test streams""
* create a few multi-stream benchmarks
* create a multi-stream example",2023-07-24T22:34:38Z,0,0,Vyas Ramasubramani,@rapidsai,True
604,[BUG] [JNI] Unknown scalar type: STRUCT in Scalar.toString,"We are not able to call `toString` on a struct type `Scalar`, making working with the type hard during debug sessions, as we cannot log or debug these types:

```
scala> val emptyStructCol = ColumnVector.makeStruct(100)
scala> emptyStructCol.max()
java.lang.IllegalArgumentException: Unknown scalar type: STRUCT
  at ai.rapids.cudf.Scalar.toString(Scalar.java:879)
  at scala.runtime.ScalaRunTime$.inner$1(ScalaRunTime.scala:250)
  at scala.runtime.ScalaRunTime$.stringOf(ScalaRunTime.scala:255)
  at scala.runtime.ScalaRunTime$.replStringOf(ScalaRunTime.scala:263)
```",2023-07-25T15:29:08Z,0,0,Alessandro Bellina,NVIDIA,True
605,Make custreamz and dask-cudf packages free of cuda-version dependency.,"To-do following from #13754: we may be able to remove `cuda-version` entirely and make packages that only depend on (arch, Python version) and not CUDA version.

We would only build packages on one CUDA version entry in the matrix (per Python version, per arch), and test them across all Python/arch/CUDA combinations. I think custreamz and dask-cudf are the only candidates for reduced builds like this. I think `cudf_kafka` may not be a good candidate because it links to CUDA C++ libraries, but I'm not sure.",2023-07-25T21:40:01Z,0,0,Bradley Dice,@NVIDIA @rapidsai,True
606,[FEA] Support `COUNT_ALL` and `COUNT_VALID` as `reduce` aggregations,"This is a follow-up item that arose from https://github.com/rapidsai/cudf/pull/13727.

It turns out that `COUNT_ALL` and `COUNT_VALID` are not supported in `cudf::reduce()` as `reduce_aggregation`s.
(This is likely because their values can trivially computed from the results of `column::size()` and `column::null_count()`.)

However, this causes obfuscation in any code that attempts to dispatch reduce aggregations generically. E.g. [Here](https://github.com/rapidsai/cudf/pull/13727/files#diff-ee2fd30f5a96f800a146b5b8262244eeb73a7758834b6ff41f9b4343fe281c56R133).

It would be good to have `cudf::reduce()` support `COUNT_ALL` and `COUNT_VALID` natively, so as not to require special handling in the calling code.",2023-07-25T22:42:31Z,0,0,MithunR,NVIDIA,True
607,[BUG]The cudf to_csv interface cannot read files larger than 2GB and displays a negative size error.,"**Describe the bug**
The cudf to_csv interface cannot write files larger than 2GB and displays a negative size error.


**Steps/Code to reproduce bug**
import cudf

df = cudf.read_csv(""3G.csv"")
df.to_csv(""result.csv"")


**Expected behavior**
i hope df.to_csv() create a 3G size csv

**Environment overview (please complete the following information)**
 - Environment location: [Bare-metal, Docker, Cloud(specify cloud provider)]
 - Method of cuDF install: [conda, Docker, or from source]
   - If method of install is [Docker], provide `docker pull` & `docker run` commands used

**Environment details**
Please run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details

**Additional context**
Add any other context about the problem here.
",2023-07-31T03:21:24Z,0,0,,,False
608,[FEA] Avoid host-side processing in CSV reader,"As of 23.10, the CSV reader cannot take advantage of [kvikIO](https://github.com/rapidsai/kvikio) and [GPUDirect Storage](https://developer.nvidia.com/gpudirect-storage) due to several processing steps that require the input data to be present in the system memory. These host-side processing steps include:
1. Decompression;
2. Skipping the partial data row at the start of the byte range (if reading a byte range);
3. Skipping the [byte order mark](https://en.wikipedia.org/wiki/Byte_order_mark) (BOM) chars; 
4. Parsing the column names in the header;

Decompression (1) is most likely faster on the CPU (not verified) since all data is compressed in a single block. For CSV files with decompression we may choose to decompress on the host side (also see #5142 and #12255). (2) could readily be processed on the device with `thrust::find` and (3) could be accomplished by a single thread kernel. For (4), we can copy the header data from the device and keep the column name parsing code (also see #12582), which avoids keeping input data on the host at the cost of a small D2H copy.

High level proposal:
When reading compressed CSV files, read to host and decompress, then wrap the host buffer into a datasource and pass to `load_data_and_gather_row_offsets`. When reading uncompressed input, just forward the source to `load_data_and_gather_row_offsets`. There, use `device_read` to load chunks to device memory. Items (2) and (3) can be done here (BOM skipping is currently outside of `load_data_and_gather_row_offsets`).

This approach brings several benefits:
* maintains ""byte range"" support and avoids loading data outside of the requested byte range
* enables direct device reads for uncompressed inputs
* allows the CSV reader to use kvikIO and increases consistency between IO formats",2023-08-01T22:35:14Z,0,0,Vukasin Milovanovic,NVIDIA,True
609,[BUG] The hasNull in ORC Statistics is incorrect,"**Describe the bug**
The ORC Statistics show hasNull is true but the column values are all non-nulls.

**Steps/Code to reproduce bug**
Refer to the reproduce in [13793](https://github.com/rapidsai/cudf/issues/13793), RAPIDS Accelerator bug is:[8826](https://github.com/NVIDIA/spark-rapids/issues/8826)

```
$ orc-tool meta TestAllNulls.orc
Processing data file TestAllNulls.orc [length: 267]
Structure for TestAllNulls.orc
File Version: 0.12 with ORIGINAL by ORC Java 
Rows: 3
Compression: SNAPPY
Compression size: 262144
Calendar: Julian/Gregorian
Type: struct<c1:string,c2:double>

Stripe Statistics:
  Stripe 1:
    Column 0: count: 3 hasNull: true
    Column 1: count: 3 hasNull: true min: 1 max: 3 sum: 3
    Column 2: count: 0 hasNull: true

File Statistics:
  Column 0: count: 3 hasNull: true
  Column 1: count: 3 hasNull: true min: 1 max: 3 sum: 3
  Column 2: count: 0 hasNull: true

Stripes:
  Stripe: offset: 3 data: 19 rows: 3 tail: 71 index: 50
    Stream: column 0 section ROW_INDEX start: 3 length 7
    Stream: column 1 section ROW_INDEX start: 10 length 26
    Stream: column 2 section ROW_INDEX start: 36 length 17
    Stream: column 1 section DATA start: 53 length 6
    Stream: column 1 section LENGTH start: 59 length 5
    Stream: column 2 section PRESENT start: 64 length 5
    Stream: column 2 section DATA start: 69 length 3
    Encoding column 0: DIRECT
    Encoding column 1: DIRECT_V2
    Encoding column 2: DIRECT

File length: 267 bytes
Padding length: 0 bytes
Padding ratio: 0%
```

This static is wrong:  
Column 1: count: 3 hasNull: true min: 1 max: 3 sum: 3
This column values are 1,2,3. The hasNull should be false.

**Expected behavior**
Stripe Statistics and File Statistics report correct hasNull info.

**Additional context**
I'm checking the `count` statistics for nested types, seems also has problem. I'll report another issue after one more check.",2023-08-04T02:42:34Z,0,0,Chong Gao,,False
610,[FEA] Reduce page faults when using managed memory,"**Is your feature request related to a problem? Please describe.**
In cuDF-python and RMM, it's easy to opt into [managed memory](https://docs.rapids.ai/api/rmm/stable/api/#rmm.reinitialize) (also known as Unified Memory, UM, and Unified Virtual Memory, UVM). However, libcudf is not optimized for use with managed memory and encounters many ""[just too late](https://www.nextplatform.com/2019/01/24/unified-memory-the-final-piece-of-the-gpu-programming-puzzle/)"" page faults when the ""[oversubscription factor](https://developer.nvidia.com/blog/improving-gpu-memory-oversubscription-performance/)"" is >1.

### Hinting options and strategies
* Use hinting with [cudaMemPrefetchAsync](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1ge8dc9199943d421bc8bc7f473df12e42) before operating on a column_view. I believe this hint will eagerly migrate the data to device. Open questions include: does it require an extra sync? do kernels page fault for a while until the data fully migrates? 
* Use hinting with [cudaMemAdviseSetAccessedBy](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1ggc314a8b14091f7e02a7ad15dcb36c85750a22279bce0dc29956ad4f257084623). This hinting also does not eagerly migrate the data, and seems to be focused on preventing faults between devices on the same node. It also allows direct memory access (DMA) from the device to pinned host buffers.
* Use hinting with [cudaMemAdviseSetPreferredLocation](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1ggc314a8b14091f7e02a7ad15dcb36c857a4a2bc3c7d218dcd9a1b425b432759eb). This does not eagerly migrate the data, instead it influences the page migration system. If we set the preferred location to device, I believe this hint would prevent those allocations from being evicted and could lead to poor performance of the page migration engine.  
*  Use hinting with [cudaMemAdviseSetReadMostly](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1ggc314a8b14091f7e02a7ad15dcb36c857441d911811beda174627f403142d5ff0). For column_view data, processing in libcudf algorithms will be read-only by design. We can communicate this to UVM, but for libcudf's common access pattern - read once and then write a new allocation for the results - I don't think ""read mostly"" hinting will give us higher throughput or reduced faulting.
* Use host-pinned buffers and use direct memory access (DMA) from the device to extend working memory ([see.""zero-copy"" in this blog](https://developer.nvidia.com/blog/improving-gpu-memory-oversubscription-performance/)). To execute this strategy we hint ""preferred location"" to host and ""accessed by"" to device. Memory throughput will be lower using DMA to host than using device memory, but stalling kernels on page faults will be much slower than waiting on DMA. We still need to design how and when we would choose to leave data on the host and access by DMA (always??? except intermediate allocations).
* In addition to preventing page faults, we may also want to prevent evictions by preemptively clearing device memory. There does not appear to be a mechanism for eagerly migrating data from device to host. Perhaps preferred location hinting can also drive evictions on groups of pages instead of one page at a time.

### Implementation ideas for libcudf
* Where would this hinting be located in the repository? We could implement a RAII ""advisor"" class that takes a (non-owning) reference to a column_view and performs the appropriate hinting. The advisor class would only perform hinting for column_views created using managed memory resources. It may be difficult to add hinting to column_view because the column_view object can't tell if it's underlying data was a managed or unmanaged allocation.
* Is a way to identify from a device pointer if the associated allocation is managed or unmanaged? Perhaps [cuPointerGetAttribute()](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__UNIFIED.html) should return  CU_MEMORYTYPE_UNIFIED as [CU_POINTER_ATTRIBUTE_MEMORY_TYPE](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TYPES.html#group__CUDA__TYPES_1ggc2cce590e35080745e72633dfc6e0b600409e16293b60b383f30a9b417b2917c) for managed memory. Is there a runtime API for accessing device pointer attributes? (TBD)


### Useful reference for cudaMemAdvise
![image](https://github.com/rapidsai/cudf/assets/12725111/9ff97f3c-3e42-4664-b6b6-fcfdfc07dc90)

<br>
<br>
<br>

_Please note: Using managed memory in libcudf is in early stages of scoping. This issue will improve over time._

<br>
<br>

**Describe the solution you'd like**
I would like to add a libcudf benchmark for studying managed memory performance, and then some targeted experiments (with profiling) to observe the impact of different hinting strategies. When we have identified a promising design, we will open a more targeted issue.

**Describe alternatives you've considered**
Continue to let Dask and Spark-RAPIDS catch and retry when there are device OOM errors.

**Additional context**
Please note that with managed memory pools, the pool allocation is lazy. This is different from unmanaged memory pools where we allocate the full pool upfront, trading slightly longer startup time for much faster algorithm allocations.

Useful blog posts:
https://developer.nvidia.com/blog/unified-memory-cuda-beginners/
https://developer.nvidia.com/blog/improving-gpu-memory-oversubscription-performance/
https://developer.nvidia.com/blog/maximizing-unified-memory-performance-in-cuda/
https://developer.nvidia.com/blog/beyond-gpu-memory-limits-unified-memory-pascal/
",2023-08-04T18:50:40Z,1,0,Gregory Kimball,,False
611,[FEA] Increase reader throughput by pipelining IO and compute,"-- this is a draft, please do not comment yet -- 

The end-to-end throughput of a file reader is limited by the sequential read speed of the underlying data source. We can use ""pipelining"" to overlap processing data on the device with reading data from the data source. Pipelining works by processing the data in batches, so that the previous chunk can be processed as the next chunk is reading. Pipelined readers show higher end-to-end throughput if the overlap between reading and processing is greater than the overhead from processing smaller batches. 

In cuIO, `multibyte_split` used a pipelined design that reads text data in ~33 MB chunks (2^25 bytes) into a pinned host buffer, copies the data to device, and then generates offsets data. Here's a profile reading ""Common Crawl"" document data with `cudf.read_text` from a 410 MB file:
![image](https://github.com/rapidsai/cudf/assets/12725111/0c2c8b2d-f23b-4688-8ef7-fe3286da4a72)

Note how the `get_next_chunk` function includes the OS `read` and `Memcopy HtoD`, and how the `Memcpy HtoD` overlaps with the next OS `read`. Stream-ordered kernel launches also overlap with the next OS `read`. For each 10 ms OS `read`, there is 1.5 ms of overlapping copy/compute work and 0.2 ms of overhead between each OS `read`. 

We can applying pipelining to the Parquet reader as well. Parquet reading includes several major steps: raw IO, header decoding, decompression, and data decoding. The runtime of each step varies based on the properties of the data in the file, including the data types, encoding efficiency, and compression efficiency. Furthermore Parquet files have [internal row group and page structure](https://github.com/apache/parquet-format#file-format) that restricts how the file can be split. Here is an example profile reading the same ""Common Crawl"" data as above, but from a 240 MB Snappy-compressed Parquet file:
![image](https://github.com/rapidsai/cudf/assets/12725111/a1f6700a-ab9f-42bd-bd70-eb285a042770)

Note how 90 ms is spent in OS read on the file and ~20 ms is spent processing, with decompression taking most (11.5 ms) of the processing time. Also note the GPU utilization data during the `read_parquet` function, with zero GPU utilization during the copy followed by good good SM utilization and moderate warp utilization during the compute.

We've completed prototyping work in #12358, experimenting with several approaches for pipelining the Parquet reader. Here are some performance analysis ideas for the next time we tackle this feature:
* Curate a library of real world (not generated) data files and use that to evaluate the performance of pipelining approach
* Analyze the copying, decompression, decoding times in the curated library and track which files show the biggest benefit from pipelining
* Consider setting a floor (such as 200 MB of compressed data) before pipelining kicks in, to make sure we aren't accruing too much overhead
* Evaluate network-attached storage in addition to local NVMe data sources

As far as pipelining approaches, here are some areas to consider:
| Stream usage | Chunking pattern | Notes | 
|---|---|---|
| entire read per stream | row group | tbd |
| decompression stream and decoding stream | row group | tbd |

-- this is a draft, please do not comment yet -- ",2023-08-07T19:43:53Z,0,0,Gregory Kimball,,False
612,[BUG] JIT Groupby Apply `idxmax`/`idxmin` reductions return incorrect values when the data is all NaN,"**Describe the bug**
When performing an `idxmax` or `idxmin` reduction on a dataframe column during JIT groupby apply, pandas returns `NaN` as the index label corresponding to the answer where as we return the index of the start of the group. 

**Steps/Code to reproduce bug**
```python
import pandas as pd
import cudf
df = pd.DataFrame({
    'a': [1, 1, 1, 2, 2, 2],
    'b': [float('nan')] * 6
})

gdf = cudf.from_pandas(df, nan_as_null=False)

expect = df.groupby('a').apply(lambda x: x['b'].idxmax())
got = gdf.groupby('a').apply(lambda x: x['b'].idxmax(), engine='jit')

print(expect)
print(got)

```

```
a
1   NaN
2   NaN
dtype: float64
a
1    0
2    3
dtype: int64
```



**Expected behavior**
Ideally we'd match pandas.

**Environment overview (please complete the following information)**
Bare Metal, 23.10

**Additional context**
Originally came up [here](https://github.com/rapidsai/cudf/pull/11452#issuecomment-1403031678), and then again [here](https://github.com/rapidsai/cudf/pull/13820/files#r1284758449). This problem stems from the dtype of the answer being data dependent in pandas. In most cases, the `idx_{max,min}` functions return an `int64` if the index is of type `int64`, however this edge case of all `NaN` returns a `Nan` which is of float type. This poses a compatibility problem for the JIT engine as numba decides the types of all the variables in the input code up front, and currently an `idx_{max,min}` operation returns an `int64`. This leads to three options in my mind:

1. Return some kind of ""sensible"" int (current state). This leads to edge cases where our results differ from pandas.
2. Type `idxmax` and `idxmin` operations to return a float, e.g. cast the resulting integer to a float and return nan in the edge case, correctly. This trades a value mismatch for a dtype mismatch. 
3. Raise in the edge case. This would require some engineering, related to https://github.com/rapidsai/cudf/issues/8774.


",2023-08-08T14:55:52Z,0,0,,NVIDIA,True
613,[BUG] ORC file count statistic for nested type is wrong,"**Describe the bug**
The GPU ORC file statistics show that the count for nested type is wrong while the CPU ORC file is correct.

GPU file shows different counts for nested type:
GPU:  
```
File Statistics:
  Column 0: count: 8 hasNull: true
  Column 1: count: 1 hasNull: true
```
CPU: 
```
File Statistics:
  Column 0: count: 8 hasNull: false
  Column 1: count: 8 hasNull: false
```
The data in both files are:
```
+------------+
|    struct_s|
+------------+
|{null, null}|
|      {1, 1}|
|{null, null}|
|      {3, 3}|
|{null, null}|
|      {5, 5}|
|{null, null}|
|      {7, 7}|
+------------+
```



**Steps/Code to reproduce bug**
##### Generate GPU file
```cpp
TEST_F(OrcWriterTest, NestedColumnSelection)
{
  auto const num_rows  = 8;
  std::vector<int> child_col1_data(num_rows);
  std::vector<int> child_col2_data(num_rows);
  for (int i = 0; i < num_rows; ++i) {
    child_col1_data[i] = i;
    child_col2_data[i] = i;
  }

  auto validity = cudf::detail::make_counting_transform_iterator(0, [](auto i) { return i % 2; });
  int32_col child_col1{child_col1_data.begin(), child_col1_data.end(), validity};
  int32_col child_col2{child_col2_data.begin(), child_col2_data.end(), validity};
  struct_col s_col{child_col1, child_col2};
  cudf::table_view expected({s_col});

  cudf::io::table_input_metadata expected_metadata(expected);
  expected_metadata.column_metadata[0].set_name(""struct_s"");
  expected_metadata.column_metadata[0].child(0).set_name(""field_a"");
  expected_metadata.column_metadata[0].child(1).set_name(""field_b"");

  auto filepath = ""/tmp/test-count-for-nested-type-gpu.orc"";
  cudf::io::orc_writer_options out_opts =
    cudf::io::orc_writer_options::builder(cudf::io::sink_info{filepath}, expected)
      .metadata(std::move(expected_metadata));
  cudf::io::write_orc(out_opts);
}
```

Read the GPU file
SPARK_HOME/bin/pyspark

spark.read.orc(""/tmp/test-count-for-nested-type-gpu.orc"").show()
+------------+
|    struct_s|
+------------+
|{null, null}|
|      {1, 1}|
|{null, null}|
|      {3, 3}|
|{null, null}|
|      {5, 5}|
|{null, null}|
|      {7, 7}|
+------------+

##### Generate CPU file
SPARK_HOME/bin/pyspark

```python
from pyspark.sql.types import *
schema = StructType([StructField(""struct_s"",
    StructType([
        StructField(""field_a"", IntegerType()),
        StructField(""field_b"", IntegerType()),
]))])

def get_value(i):
  if i % 2 == 0:
    return None
  else:
    return i

data = [
    ({ 'field_a': get_value(i), 'field_b': get_value(i) }, ) for i in range(0, 8)
]
df = spark.createDataFrame(
        SparkContext.getOrCreate().parallelize(data, numSlices=1),
        schema)

path = '/tmp/test-count-for-nested-type-cpu.orc'
df.coalesce(1).write.mode(""overwrite"").orc(path)
spark.read.orc(path).show()
```

```
+------------+
|    struct_s|
+------------+
|{null, null}|
|      {1, 1}|
|{null, null}|
|      {3, 3}|
|{null, null}|
|      {5, 5}|
|{null, null}|
|      {7, 7}|
+------------+
```

##### print count statistic for GPU file
```
$ orc-tool meta test-count-for-nested-type-gpu.orc
Processing data file test-count-for-nested-type-gpu.orc [length: 360]
Structure for test-count-for-nested-type-gpu.orc
File Version: 0.12 with ORIGINAL by ORC Java 
Rows: 8
Compression: SNAPPY
Compression size: 262144
Calendar: Julian/Gregorian
Type: struct<struct_s:struct<field_a:int,field_b:int>>

Stripe Statistics:
  Stripe 1:
    Column 0: count: 8 hasNull: true
    Column 1: count: 1 hasNull: true
    Column 2: count: 4 hasNull: true min: 1 max: 7 sum: 16
    Column 3: count: 4 hasNull: true min: 1 max: 7 sum: 16

File Statistics:
  Column 0: count: 8 hasNull: true
  Column 1: count: 1 hasNull: true
  Column 2: count: 4 hasNull: true min: 1 max: 7 sum: 16
  Column 3: count: 4 hasNull: true min: 1 max: 7 sum: 16

Stripes:
  Stripe: offset: 3 data: 24 rows: 8 tail: 92 index: 70
    Stream: column 0 section ROW_INDEX start: 3 length 7
    Stream: column 1 section ROW_INDEX start: 10 length 11
    Stream: column 2 section ROW_INDEX start: 21 length 26
    Stream: column 3 section ROW_INDEX start: 47 length 26
    Stream: column 2 section PRESENT start: 73 length 5
    Stream: column 2 section DATA start: 78 length 7
    Stream: column 3 section PRESENT start: 85 length 5
    Stream: column 3 section DATA start: 90 length 7
    Encoding column 0: DIRECT
    Encoding column 1: DIRECT
    Encoding column 2: DIRECT_V2
    Encoding column 3: DIRECT_V2

File length: 360 bytes
Padding length: 0 bytes
Padding ratio: 0%
```

##### print count statistic for CPU file
```
$ orc-tool meta /tmp/test-count-for-nested-type-cpu.orc 
Processing data file file:/tmp/test-count-for-nested-type-cpu.orc/part-00000-6b490836-0c65-4355-9d0e-fbaff96aec33-c000.snappy.orc [length: 388]
Structure for file:/tmp/test-count-for-nested-type-cpu.orc/part-00000-6b490836-0c65-4355-9d0e-fbaff96aec33-c000.snappy.orc
File Version: 0.12 with ORC_14 by ORC Java 1.7.4
Rows: 8
Compression: SNAPPY
Compression size: 262144
Calendar: Julian/Gregorian
Type: struct<struct_s:struct<field_a:int,field_b:int>>

Stripe Statistics:
  Stripe 1:
    Column 0: count: 8 hasNull: false
    Column 1: count: 8 hasNull: false
    Column 2: count: 4 hasNull: true bytesOnDisk: 12 min: 1 max: 7 sum: 16
    Column 3: count: 4 hasNull: true bytesOnDisk: 12 min: 1 max: 7 sum: 16

File Statistics:
  Column 0: count: 8 hasNull: false
  Column 1: count: 8 hasNull: false
  Column 2: count: 4 hasNull: true bytesOnDisk: 12 min: 1 max: 7 sum: 16
  Column 3: count: 4 hasNull: true bytesOnDisk: 12 min: 1 max: 7 sum: 16

Stripes:
  Stripe: offset: 3 data: 24 rows: 8 tail: 71 index: 76
    Stream: column 0 section ROW_INDEX start: 3 length 11
    Stream: column 1 section ROW_INDEX start: 14 length 11
    Stream: column 2 section ROW_INDEX start: 25 length 27
    Stream: column 3 section ROW_INDEX start: 52 length 27
    Stream: column 2 section PRESENT start: 79 length 5
    Stream: column 2 section DATA start: 84 length 7
    Stream: column 3 section PRESENT start: 91 length 5
    Stream: column 3 section DATA start: 96 length 7
    Encoding column 0: DIRECT
    Encoding column 1: DIRECT
    Encoding column 2: DIRECT_V2
    Encoding column 3: DIRECT_V2

File length: 388 bytes
Padding length: 0 bytes
Padding ratio: 0%

User Metadata:
  org.apache.spark.version=3.3.0
```



**Expected behavior**
The all statistics should be correct, including the `hasNull`, refer to this [issue](https://github.com/rapidsai/cudf/issues/13817)

**Environment details**
Environment details
cuDF 23.08 branch
Spark 3.3.0
orc-core-1.7.4.jar

**Additional context**
",2023-08-09T05:57:28Z,0,0,Chong Gao,,False
614,[FEA] Support floating dtypes for `corr` aggregations in JIT GroupBy Apply,"**Is your feature request related to a problem? Please describe.**
Currently only [integer dtypes](https://github.com/rapidsai/cudf/blob/branch-23.10/python/cudf/cudf/core/udf/groupby_typing.py#L257-L258) are supported when applying a correlation aggregation between columns in JIT GroupBy apply. This is mainly lacking due to requiring special treatment to match pandas treatment for special values. A note in the [pandas docs](https://pandas.pydata.org/docs/reference/api/pandas.Series.corr.html) says:

```
Compute correlation with other Series, excluding missing values.
```

In general, JIT groupby apply does not support nulls, but it seems that nans are covered under this umbrella as well. By ""excluding"" missing values, pandas means that the final correlation coefficient is the same as one which would have been computed if all pairs of datapoints where either value is nan is excluded from the inner sum of the algorithm. This can be seen as follows:

```python
>>> sr1 = pd.Series([1.0, 2.0, float('nan'), 4.0, float('nan'), 6.0])
>>> sr2 = pd.Series([1.1, 1.9, 3.0, float('nan'), float('nan'), 6.7])
>>> mask = ~(sr1.isna() | sr2.isna())
>>> sr1.corr(sr2)
0.9983374884595826
>>> sr1[mask].corr(sr2[mask])
0.9983374884595826
```



**Describe the solution you'd like**
I'd like this to work and return the same value as pandas. This requires some additional engineering inside of our shim library to support this kind of element deletion on the c++ side. Currently `corr` leverages various other block level functions such as covariance to compute the pearson correlation. We'd have to figure out if we can adapt those functions to support this case or if we'd have to do something more involved for `corr` specifically. ",2023-08-09T13:42:41Z,0,0,,NVIDIA,True
615,[BUG]: cudf.read_csv(comment=#) still including commented line,"**Describe the bug**
cudf.read_csv(comment=#) still includes the commented line

**Steps/Code to reproduce bug**
```
In [58]: import cudf

In [59]: cudf.__version__
Out[59]: '23.10.00'

In [60]: from io import StringIO

In [61]: data = ""\na,b,c\n  \n# commented line\n1,2,3\n\n4,5,6""

In [62]: cudf.read_csv(StringIO(data), comment=""#"")
Out[62]: 
   a                 b     c
0  0  # commented line  <NA>
1  1                 2     3
2  4                 5     6
```

**Expected behavior**
```
In [63]: import pandas

In [64]: pandas.read_csv(StringIO(data), comment=""#"")
Out[64]: 
   a  b  c
0  1  2  3
1  4  5  6
```

**Environment overview (please complete the following information)**
 - Environment location: Bare-metal
 - Method of cuDF install: conda
   - If method of install is [Docker], provide `docker pull` & `docker run` commands used

**Environment details**
Please run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details

**Additional context**
Add any other context about the problem here.
",2023-08-11T19:25:09Z,0,0,Matthew Roeschke,@rapidsai ,True
616,[BUG] Loc indexing with of DatetimeIndex with non ISO date-like string slice returns emtpy DataFrame,"**Describe the bug**
Loc indexing with of DatetimeIndex with non ISO date-like string slice returns emtpy DataFrame

**Steps/Code to reproduce bug**
```
In [60]: import cudf

In [61]: df = cudf.DataFrame(range(3), index=cudf.date_range(""2020-01-01"", freq=""D"", periods=3))

In [63]: df.loc[""2020-01-01"":""2020-01-02""]  # OK
Out[63]: 
            0
2020-01-01  0
2020-01-02  1

In [66]: df.loc[""20200101"":""20200102""]
Out[66]: 
Empty DataFrame
Columns: [0]
Index: []
```

**Expected behavior**
(I don't like this behavior in pandas but), it seems like pandas performs more inference on this slice label and infers it as date-like and sliceable on the index

```
In [68]: df = pandas.DataFrame(range(3), index=pandas.date_range(""2020-01-01"", freq=""D"", periods=3))

In [69]: df.loc[""20200101"":""20200102""]
Out[69]: 
            0
2020-01-01  0
2020-01-02  1
```

**Environment overview (please complete the following information)**
 - Environment location: Bare-metal
 - Method of cuDF install: conda
   - If method of install is [Docker], provide `docker pull` & `docker run` commands used

**Environment details**
Please run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details

**Additional context**
Add any other context about the problem here.
",2023-08-11T21:21:57Z,0,0,Matthew Roeschke,@rapidsai ,True
617,[BUG] Sliced DataFrame with MultiIndex removes unused level values,"**Describe the bug**
Sliced DataFrame with MultiIndex removes unused level values unlike pandas

**Steps/Code to reproduce bug**
```
In [1]: import cudf

In [2]: arrays = [
   ...: 
   ...:     [""bar"", ""bar"", ""baz"", ""baz"", ""foo"", ""foo"", ""qux"", ""qux""],
   ...: 
   ...:     [""one"", ""two"", ""one"", ""two"", ""one"", ""two"", ""one"", ""two""],
   ...: 
   ...: ]
   ...: 
   ...: 
   ...: 
   ...: tuples = list(zip(*arrays))

In [4]: index = cudf.MultiIndex.from_tuples(tuples, names=[""first"", ""second""])

In [6]: import numpy as np

In [7]: df = cudf.DataFrame(np.random.randn(3, 8), index=[""A"", ""B"", ""C""], columns=index)

In [8]: df
Out[8]: 
first        bar                 baz                 foo                 qux          
second       one       two       one       two       one       two       one       two
A       0.248351 -0.162527  2.140643  1.358278 -0.435850 -1.279155  0.932680  2.611601
B      -1.321047 -0.403212  0.018679  0.404523  0.015960  1.242899  0.642264 -2.499115
C      -0.930653  1.753720  0.247863  0.580408  0.227763 -0.565367 -0.868276  1.056060

In [9]: df[[""foo"",""qux""]].columns.levels
Out[9]: FrozenList([['foo', 'qux'], ['one', 'two']])

In [10]: df.to_pandas()[[""foo"",""qux""]].columns.levels
Out[10]: FrozenList([['bar', 'baz', 'foo', 'qux'], ['one', 'two']])
```

**Expected behavior**
I think in pandas this is an optimization similar to Categorical to keep all level values just in case of recomputations (as noted by a `remove_unused_levels` API for `MultiIndex`). cudf might not need this but it doesn't seem to match pandas behavior

**Environment overview (please complete the following information)**
 - Environment location: Bare-metal
 - Method of cuDF install: conda
   - If method of install is [Docker], provide `docker pull` & `docker run` commands used

**Environment details**
Please run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details

**Additional context**
Add any other context about the problem here.
",2023-08-12T00:24:10Z,0,0,Matthew Roeschke,@rapidsai ,True
618,[BUG] LocIndexing a MultiIndex with a lists does not select all matching level,"**Describe the bug**
LocIndexing a MultiIndex with a lists does not select all matching level

**Steps/Code to reproduce bug**
```
In [19]: import cudf

In [20]: cudf.__version__
Out[20]: '23.10.00'

In [21]: s = cudf.Series(
    ...: 
    ...:     [1, 2, 3, 4, 5, 6],
    ...: 
    ...:     index=cudf.MultiIndex.from_product([[""A"", ""B""], [""c"", ""d"", ""e""]]),
    ...: 
    ...: )

In [22]: s
Out[22]: 
A  c    1
   d    2
   e    3
B  c    4
   d    5
   e    6
dtype: int64

In [23]: s.loc[([""A"", ""B""], [""c"", ""d""])]
Out[23]: 1

In [24]: s.to_pandas().loc[([""A"", ""B""], [""c"", ""d""])]
Out[24]: 
A  c    1
   d    2
B  c    4
   d    5
dtype: int64
```
**Expected behavior**
When specifying a tuple of lists, the lists select the matching labels in each level. It appears only the first matching label in each level was selected

**Environment overview (please complete the following information)**
 - Environment location: Bare-metal
 - Method of cuDF install: conda
   - If method of install is [Docker], provide `docker pull` & `docker run` commands used

**Environment details**
Please run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details

**Additional context**
Add any other context about the problem here.
",2023-08-12T00:38:21Z,0,0,Matthew Roeschke,@rapidsai ,True
619,[DOC] Document all pandas-compatibility mode behaviors,"## Report incorrect documentation
N/A

**Location of incorrect documentation**
In recent releases, there are new behaviors that have been enabled in `pandas-compatibility` mode. Not all of those behaviors are documented, hence this is a doc request to list all those behaviors in the public user-facing docs.

",2023-08-14T17:56:44Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
620,[FEA] `cudf.read_text` does not use the GDS-enabled datasource,"libcudf's `multibyte_split` has multiple ways to specify input data source when reading from a file:
1. `auto const source = cudf::io::text::make_source_from_file(temp_file_name); // no GDS`

2. `auto const datasource = cudf::io::datasource::create(temp_file_name);`
`auto const source = cudf::io::text::make_source(*datasource); // supports GDS`

`cudf.read_text` uses the first option to create the `data_chunk_source` object, so it does not benefit from potential direct reads with GDS.

`cudf.read_text` should create a `cudf::io::datasource` object to improve performance on GDS-enabled systems.",2023-08-14T22:47:03Z,0,0,Vukasin Milovanovic,NVIDIA,True
621,[FEA] Improve ORC reader filtering and performance,"### Background

libcudf includes readers and writers for two popular binary formats for columnar data: [Apache Parquet](https://en.wikipedia.org/wiki/Apache_Parquet) and [Apache ORC](https://en.wikipedia.org/wiki/Apache_ORC). These formats were originally introduced in 2013, and both have open source specifications ([ORC](https://orc.apache.org/specification/ORCv1/), [PQ](https://github.com/apache/parquet-format)) and reference implementations ([ORC](https://github.com/apache/orc), [PQ](https://github.com/apache/parquet-mr)) maintained by Apache. ORC also serves as the foundation for [Meta’s variant DWRF and their new format ""Alpha""](https://www.cidrdb.org/cidr2023/papers/p77-chattopadhyay.pdf).

Both formats have hierarchical data layouts, support encoding and compression, include fully-featured type systems, and find widespread use in database systems and data warehousing. Please refer to [this paper](https://arxiv.org/pdf/2304.05028.pdf) by Zeng et al for a detailed comparison of the concepts, features and performance of Parquet and ORC binary formats. Please note that Parquet files are composed of “row groups” (~128 MB) and “pages” (~1 MB), and ORC files are composed of “stripes” (~70 MB) and “row groups” (10K rows). 

Some of the differences include:
* finer granularity in data buffers by default in ORC (better for filtered IO and targeted lookups)
* finer granularity in bloom filters in ORC (supported at ""row group"" level in ORC, but not at the ""page"" level in Parquet)
* Dremel-encoding for list types in Parquet (faster decoding for >8 levels of nesting)
* support for [ACID transaction tables](https://orc.apache.org/docs/acid.html) in ORC datasets (enabling data updates without full re-write)
* In Parquet the data ""page"" is also the unit of encoding and compression, whereas in ORC each encoding ""stream"" and ""compression chunk"" often includes multiple ""row groups"".

### Expanding functionality of the ORC reader

The libcudf Parquet reader has gained functionality in key areas, including the chunked reader (release 22.12) to control how much of a table is materialized, and AST-based filtering (release 23.08) to avoid reading row groups that aren’t needed. Filtered IO (including bloom filters) is even more important to ORC users thanks to the fine granularity of ORC row groups (10k rows per row group). We should align our Parquet and ORC reader designs and separate shared utilities from format-specific details wherever possible.

| Topic | Status | Notes
|---|---|---|
| Add AST-based stripe filtering to the ORC reader | | #13348 added AST-based row group filtering to the Parquet reader. For this topic, we should accept an AST filter parameter, use it to determine matches stripes, read only those strips, and then post-filter the rows in the resulting table. We already have a `read_raw_orc_statistics` function to support these steps. We may refactor some of the AST + min/max stats tools to `utilities`. Also see issue #12512 | 
| Add chunked reader for ORC | | See #12228 about this topic from Spark-RAPIDS. Chunked readers are useful because they allow for partial materialization of tables from their binary representation. #11867 added chunking for Parquet decoding, which means the compressed row groups were fully read and decompressed and then decoded up to a requested size in bytes. (tbd) is extending chunking to include Parquet decompression as well. Chunking helps libcudf applications avoid two limits: the [size_type](https://github.com/rapidsai/cudf/issues/13159)  limit on row count and the GPU working memory limit for each worker |  
| Support [bloom filters](https://en.wikipedia.org/wiki/Bloom_filter) in ORC reader | | See #4410. Due to ORC’s common usage for data lookup and filtered IO, supporting bloom filters in reads is especially important for ORC. This feature would allow the caller to specify equality conditions and check against ORC bloom filters.  | 
| Support index roundtripping in ORC | | See #8708, a request from cuDF-python to preserve the index when writing+reading a file | 

### Performance optimizations for binary format reading

| Topic | Status | Notes
|---|---|---|
| Optimize ORC reader performance for list data | ✅ #13708 | We observed poor performance with singlely-nested lists and high row counts |
| Optimize ORC reader performance for decimal data | | See #13251, we need a parallel algorithm to replace the single-thread decoding of the variable-width encoded representation |
| Evaluate multi-kernel decoding in ORC | | See #13622 for experiments with multiple decode kernels, and #13302 for an example of a specialized strings decode kernel | 
| Experiment with pipelining ORC reads | | See #13828 for information about reader pipelining |",2023-08-15T04:13:18Z,0,0,Gregory Kimball,,False
622,Row group selection support in Parquet chunked reader,"              The chunked reader and row-group specification should work together fine (though I don't think we have a test specifically for it).  The only input to the chunked reader is ""number of bytes in a chunk"" so it doesn't care at all if the row groups being inspected in the first place are the entire file or part of it.

_Originally posted by @nvdbaranec in https://github.com/rapidsai/cudf/pull/13348#discussion_r1275340682_
            
Chunked parquet reader ignores row group selection silently. Chunked parquet reader could support row group selection, and also predicate pushdown using row group filtering.

Enable user row group selection for chunked parquet reader. 
As a follow up, add predicate pushdown support using row group filtering to chunked reader too.",2023-08-18T02:40:11Z,0,0,Karthikeyan,NVIDIA,True
623,[FEA] Modernize CSV reader and expand reader options,"### Background

The CSV reader in cuDF/libcudf is a common IO interface for ingesting raw data, and is frequently the first IO interface that new users test when getting started with RAPIDS. There have been many improvements to the CSV reader over the years, but much of the implementation has remained the same from its introduction in #3213 and rework in #5024. We see several opportunities to address the [CSV reader continuous improvement](https://github.com/rapidsai/cudf/milestone/12) milestone, and this story associates open issues with particular functions and kernels in the CSV reading process.

### Step 1: Decompression and preprocessing
The CSV reader begins with host-side processing in `select_data_and_row_offsets`. With the exception of decompression, we would like to migrate this processing to be done device side and refactor this function to use a kvikIO data source. Note, this refactor could also include adding support for the `header` parameter and `byte_range` at the same time ([code pointer](https://github.com/rapidsai/cudf/blob/b798a70d608cbbe2c7f372a8c21354455ba56f74/cpp/src/io/csv/reader_impl.cu#L442)).

The initial processing interacts with several issues:
* #13797 is small story issue about this topic
* #4999 batch the full read as small chunks
* #5142 
* #11728 describes how the initial byte range parsing to find the first row assumes the byte_range starts in an unquoted state. If a user provides a byte_range that starts in a quoted field, then the reader will fail! The solution described in this issue interacts the next step ""identify row offsets"". 
* #12255 needs investigation
* #12582 return empty `metadata.schema_info` when column names are autogenerated

### Step 2: Identify row offsets (delimiters)
The next step is identifying record delimiters and computing row offsets in `load_data_and_gather_row_offsets` (invoked by `select_data_and_row_offsets`). This algorithm operates in three main steps: `gather_row_offsets` called with empty data, `select_row_context`, and `gather_row_offsets` called with row context data. The row context state machine is difficult to refactor because it uses a custom data representation that stores several logical values within a single 32-bit or 64-bit physical type ([code pointer](https://github.com/rapidsai/cudf/blob/b798a70d608cbbe2c7f372a8c21354455ba56f74/cpp/src/io/csv/csv_gpu.hpp#L64)). The row context tracks whether the content is in a comment block or in a quoted block. 
* `gather_row_offsets` runs a [4-state](https://github.com/rapidsai/cudf/blob/b798a70d608cbbe2c7f372a8c21354455ba56f74/cpp/src/io/csv/csv_gpu.hpp#L40) ""row context"" state machine over 16 KB blocks of characters and returns the number of un-quoted, un-commented record delimiters from the block given each possible initial state
* `select_row_context` is invoked in a host-side loop over `row_ctr` data for each 16 KB block, starting from a `state 0` initial context. 
* `gather_row_offsets` is called in a second pass with a valid `all_row_offsets` data parameter.

Major design topics:
* We should consider a larger refactor of the ""identify row offsets"" code based on using a new FST instance ([code pointer](https://github.com/rapidsai/cudf/tree/branch-23.10/cpp/src/io/fst)). Using an FST instance would easily allow us to add additional states beyond the existing 4-state machine. Please refer to the [ParPaRaw paper](https://arxiv.org/pdf/1905.13415.pdf) from Elias Stehle et al for more information about parallel algorithms for CSV parsing.
* To unblock Spark-RAPIDS usage of the CSV, we may also choose to support user-provided `all_row_offsets` parameter to the read function or as a reader option. This would allow Spark to bypass the first `gather_row_offsets` pass and `select_row_context` in `load_data_and_gather_row_offsets`. When calling `read_csv` on a strings column, Spark already has the row offsets. 
* Also note that refactoring the interface to provide row offsets is relevant to #11728, where we would want to provide pre-computed offsets. For this issue we might prefer a new detail API rather than new parameters in the public API - more design work is needed.

The row offsets algorithm interacts with several open issues:
* #6572 complex preprocessing or changes to the row context state machine. 
* (Spark blocker) #11984 Pandas and Spark don't have the same escaping conventions, and the row offset state machine doesn't have an escaped state. Needs confirmation - does this impact the row offsets step?
* (Spark blocker) #11948 to handle misplaced quotes. The issue shows a file getting truncated so fields with misplaced quotes seem to compromise the row offset data. #2398 suggests a workaround
* #6305 another quoting/escaping issue 
* #13856 commented lines should not emit row offsets
* Issue n/a: Add unit tests for `gather_row_offsets` kernel

### Step 3: Determine column types
The next step is determining the data types for each column that does not map to a user-provided data type. The function `determine_column_types` completes this work by collecting the user-provided data types, and then calling `infer_column_types` to handle the unspecified data types. `infer_column_types` invokes the `detect_column_types`->`data_type_detection` kernel to collect statistics about the data in each field, and then use the conventions of the pandas CSV reader to select a column type. 
* We should consider refactoring the ""determine"", ""infer"", and ""detect"" function names to improve clarity
* (good first issue) #14066 update thread indexing
* #5080 performance improvements for type inference
* (Spark blocker) #11984 `seek_field_end` supports escape characters within data fields. perhaps field traversal is already Spark-compatible
* (Spark blocker) #11948 misplaced quotes could fail with `seek_field_end`
* #6313 pandas doesn't infer as `float` if there are any nulls
* #9987 would change `seek_field_end`, maybe not much else
* Issue n/a: Add unit tests for `seek_field_end` kernel

### Step 4: Decode data and populate device buffers
The final step, `decode_data`, does another pass over the data to decode values according to the determined columns types. The kernel is `decode_row_column_data`->`convert_csv_to_cudf`
* (Spark blocker) #13892 trim white space . Probably a modest change to `trim_whitespaces_quotes`. Related to #6659
* (Spark blocker) #12145 add option to decode `""""` as empty strings or `null`. Probably an additional parsing option.
* (Spark blocker) #11984 `convert_csv_to_cudf` also uses `seek_field_end` which nominally supports escape characters
* (Spark blocker) #11948 misplaced quotes could fail with `seek_field_end`
* #4001 support additional `nanValue` options
* #10599 float parsing consistency, this is probably a `wontfix`



",2023-08-18T19:27:48Z,0,0,Gregory Kimball,,False
624,[BUG] `skipna` is non-functional for groupby scan operations,"**Describe the bug**
Additional parameters such as `skipna` support seems to be missing from groupby scan operations.

**Steps/Code to reproduce bug**
```python
In [1]: import cudf

In [2]: df = cudf.DataFrame({""a"": 1, ""b"": [1, cudf.NA, 2]})
df
In [3]: df
Out[3]: 
   a     b
0  1     1
1  1  <NA>
2  1     2

In [4]: df.groupby(""a"").cumsum(skipna=False)
Out[4]: 
      b
0     1
1  <NA>
2     3

In [5]: df.groupby(""a"").cumsum(skipna=True)
Out[5]: 
      b
0     1
1  <NA>
2     3

In [6]: df.to_pandas().groupby(""a"").cumsum(skipna=False)
Out[6]: 
     b
0  1.0
1  NaN
2  NaN

In [7]: df.to_pandas().groupby(""a"").cumsum(skipna=True)
Out[7]: 
     b
0  1.0
1  NaN
2  3.0

```

",2023-08-23T02:41:17Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
625,[FEA] Allow mode.pandas_compatible to make nunique treat nan as NA,"**Is your feature request related to a problem? Please describe.**
In pandas, `nunique` ignores NA including nan. There is no way to currently mimic this behavior in cudf

```
In [58]: import pandas

In [59]: import cudf

In [60]: import numpy as np

In [61]: cudf.Series([0.1, np.nan], nan_as_null=False).nunique()
Out[61]: 2

In [62]: pandas.Series([0.1, np.nan]).nunique()
Out[62]: 1
```

**Describe the solution you'd like**
It might be nice if `mode.pandas_compatible` would treat nan as NA and exclude `np.nan` from the `nunique`

**Describe alternatives you've considered**
Adding a `nan_as_null`, but I am not sure about thoughts of expanding the API

**Additional context**

",2023-08-29T00:45:46Z,0,0,Matthew Roeschke,@rapidsai ,True
626,[FEA] Add variable bit-width keys and improved key order for Parquet dict pages,"Original title:
""[FEA] research enabling BIT_PACKED encoding for columns in parquet writer""

We have a user report of larger sizes for parquet encoded files via the GPU as opposed to Spark CPU. With their sample data, I can get a 30% increase in the GPU file size vs the CPU. I have been able to produce a single row group and the same number of files, so I am down to column encodings. The types of columns are all INT64 nullable columns.

It looks like one of the differences between the two files is that in cuDF columns are not using the BIT_PACKED encoding:

CPU (`ENC:RLE,BIT_PACKED,PLAIN_DICTIONARY`):
```
col1:         INT64 SNAPPY DO:4 FPO:508942 SZ:856794/1169887/1.37 VC:822216 ENC:RLE,BIT_PACKED,PLAIN_DICTIONARY ST:[min: 1237, max: 1234559, num_nulls: 0]
col2:         INT64 SNAPPY DO:856798 FPO:1365736 SZ:856794/1169887/1.37 VC:822216 ENC:RLE,BIT_PACKED,PLAIN_DICTIONARY ST:[min: 1237, max: 1234559, num_nulls: 0]
```

GPU (`ENC:PLAIN_DICTIONARY,RLE`):
```
col1:         INT64 SNAPPY DO:4 FPO:509620 SZ:924686/1234683/1.34 VC:822216 ENC:PLAIN_DICTIONARY,RLE ST:[min: 1237, max: 1234559, num_nulls: 0]
col2:         INT64 SNAPPY DO:924690 FPO:1434330 SZ:924742/1234683/1.34 VC:822216 ENC:PLAIN_DICTIONARY,RLE ST:[min: 1237, max: 1234559, num_nulls: 0]
```

Discussing with @nvdbaranec, he suggested that BIT_PACKED could be enough of a reason for the difference. I have generated two files with my own mock data (just sequences of longs) and encoded it with the CPU and the GPU. I have placed two of the generated file in this zip file:

[bit_packed_example.zip](https://github.com/rapidsai/cudf/files/12467035/bit_packed_example.zip)

I would appreciate any comments. If you want me to try a small change in cuDF and rebuild/retest, I am happy to do so.

",2023-08-29T16:32:33Z,0,0,Alessandro Bellina,NVIDIA,True
627,"[FEA] Support `""raise""` option for `ambiguous` and `nonexistent` in tz_localize","**Is your feature request related to a problem? Please describe.**
pandas currently defaults to raising `pytz` exceptions (possibly zoneinfo exceptions when the pytz changes as the default) when localizing an ambiguous or nonexistent time. It would be useful to have this functionality in cudf as well.

**Describe the solution you'd like**
Supporting a `ambiguous=""raise""` and `nonexistent=""raise""` option

**Describe alternatives you've considered**
None

**Additional context**
https://pandas.pydata.org/docs/reference/api/pandas.Series.dt.tz_localize.html
",2023-08-29T20:37:12Z,0,0,Matthew Roeschke,@rapidsai ,True
628,[BUG] DataFrame.join(Series) does not maintain order of Series values in mode.pandas_compatible,"**Describe the bug**
DataFrame.join(Series) does not maintain order of Series values

**Steps/Code to reproduce bug**
```python
In [12]: import cudf

In [13]: df = cudf.DataFrame([1, 2])

In [16]: ser = cudf.Series(list(""abcdef""), index=[0, 0, 0, 1, 1, 1], name=""var1"")

In [17]: df.join(ser)
Out[17]: 
   0 var1
0  1    a
0  1    c
0  1    b
1  2    d
1  2    e
1  2    f

In [18]: df.to_pandas().join(ser.to_pandas())
Out[18]: 
   0 var1
0  1    a
0  1    b
0  1    c
1  2    d
1  2    e
1  2    f
```

**Expected behavior**
I would expect the order of the joined values to be maintained.

**Environment overview (please complete the following information)**
 - Environment location: Bare-metal
 - Method of cuDF install: conda
   - If method of install is [Docker], provide `docker pull` & `docker run` commands used

**Environment details**
Please run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details

**Additional context**
Add any other context about the problem here.
",2023-08-29T22:33:30Z,0,0,Matthew Roeschke,@rapidsai ,True
629,[BUG] cudf.from_pandas unnecessarily sorts incoming MultiIndex levels,"**Describe the bug**
cudf.from_pandas unnecessarily sorts incoming MultiIndex levels

**Steps/Code to reproduce bug**
```python
In [1]: import cudf

In [2]: mi = cudf.MultiIndex.from_tuples([(""a"", ""b""), (""c"", ""d"")])

In [4]: import pandas

In [5]: mi = pandas.MultiIndex.from_tuples([(""e"", ""f""), (""c"", ""d"")])

In [6]: new_mi = mi.insert(0, (""a"", ""b""))

In [7]: new_mi.levels[0]
Out[7]: Index(['c', 'e', 'a'], dtype='object')

In [8]: cudf.from_pandas(new_mi).levels[0]
Out[8]: StringIndex(['a' 'e' 'c'], dtype='object', name=0)
```

**Expected behavior**
Not sure if there's a specific reason why cudf needs to sort the levels or if there are any subsequent behaviors dependent on sorting, but it would be great if level order was preserved. This could be gated behind `mode.pandas_compatible`

**Environment overview (please complete the following information)**
 - Environment location: Bare-metal
 - Method of cuDF install: conda
   - If method of install is [Docker], provide `docker pull` & `docker run` commands used

**Environment details**
Please run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details

**Additional context**
Add any other context about the problem here.
",2023-08-30T18:29:01Z,0,0,Matthew Roeschke,@rapidsai ,True
630,[BUG] Cannot assign DataFrame.column.name,"**Describe the bug**
DataFrame.pop removes column name

**Steps/Code to reproduce bug**
```python
In [1]: import cudf

In [2]: df = cudf.DataFrame({""a"": [1., 2.]})

In [3]: df.index.name = ""baz""

In [4]: df
Out[4]: 
       a
baz     
0    1.0
1    2.0

In [5]: df[""foo""] = ""bar""

In [6]: df
Out[6]: 
       a  foo
baz          
0    1.0  bar
1    2.0  bar

In [7]: df.pop(""foo"")
Out[7]: 
baz
0    bar
1    bar
Name: foo, dtype: object

In [8]: df.columns.name # should be ""baz""

In [9]: df.columns
Out[9]: Index(['a'], dtype='object')
```

**Expected behavior**
```python
In [8]: df.columns.name
""baz""
```

**Environment overview (please complete the following information)**
 - Environment location: Bare-metal
 - Method of cuDF install: conda
   - If method of install is [Docker], provide `docker pull` & `docker run` commands used

**Environment details**
Please run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details

**Additional context**
Add any other context about the problem here.
",2023-08-30T22:48:25Z,0,0,Matthew Roeschke,@rapidsai ,True
631,[QST] Dask-cudf/Xgboost out of memory error,"I'm trying to train an xgboost model on a machine with 8xA100 GPUs with 80GB memory each but I'm getting an out of memory error:
`MemoryError('std::bad_alloc: out_of_memory: CUDA error at: .../include/rmm/mr/device/cuda_memory_resource.hpp')`. The error is slightly different if I use `rmm_pool_size` parameter but it is still a memory error `""MemoryError('std::bad_alloc: out_of_memory: RMM failure at:../include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')`

I'm using a `LocalCUDACluster` to distribute the workload amongst the 8 GPUs. I can tell by looking at the dask dashboard, that the data is mostly loading into a single GPU and all of the other GPUs are sitting empty and idle. 

I read the data using `dask_cudf.read_parquet(file_name, blocksize=int(2e4))` and it is a dataframe of size `(20459297, 213)`. Though I would like to try it with much larger datasets.  The training completes successfully with a smaller dataframe of size `(16304159, 213)` and fewer workers but it still mostly uses a single GPU. 

Edit: Here's a screenshot of the dashboard when the model is successfully training - note this is with only 2 GPUs and the smaller dataframe noted above

<img width=""1672"" alt=""Screenshot"" src=""https://github.com/rapidsai/cudf/assets/18453604/98706b71-19e3-4266-9c9c-23472994675c"">

The GPUs are running CUDA 12.0 and driver  525.60.13
Here are the versions of some relevant packages

```
xgboost                   1.7.4           rapidsai_py310h1395376_6    rapidsai
rapids                    23.08.00        cuda12_py310_230809_g2a5b6f0_0    rapidsai
python                    3.10.12         hd12c33a_0_cpython    conda-forge
rapids-xgboost            23.08.00        cuda12_py310_230809_g2a5b6f0_0    rapidsai
cuda-version              12.0                 hffde075_2    conda-forge
cudf                      23.08.00        cuda12_py310_230809_g8150d38e08_0    rapidsai
dask                      2023.7.1           pyhd8ed1ab_0    conda-forge
dask-core                 2023.7.1           pyhd8ed1ab_0    conda-forge
dask-cuda                 23.08.00        py310_230809_gefbd6ca_0    rapidsai
dask-cudf                 23.08.00        cuda12_py310_230809_g8150d38e08_0    rapidsai
```

Any help on the memory issue would be much appreciated

",2023-08-31T08:34:47Z,0,0,,,False
632,[BUG] `CompactProtocolFieldWriter` does not write empty value string in key-value pair,"During debugging a Parquet issue, I found that the Parquet metadata is not correctly written by cudf. In particular, in a round trip test, a pair of metadata `{""key_str"", """"}` is written then read as `{""key_str"", null}` in Spark. This is due to the `CompactProtocolFieldWriter` does not write empty value string:
```
size_t CompactProtocolWriter::write(KeyValue const& k)
{
  CompactProtocolFieldWriter c(*this);
  c.field_string(1, k.key);
  if (not k.value.empty()) { c.field_string(2, k.value); }                 <================ here
  return c.value();
}
```

We should write all value strings even for empty value.

However, I'm not sure if this is the expected behavior of compact protocol?",2023-08-31T18:25:18Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
633,[FEA] Define `__hash__` on CategoricalDtype,"**Is your feature request related to a problem? Please describe.**
Would be nice if `hash(CategoricalDtype(...))` was supported

```
In [9]: import cudf

In [10]: import pandas

In [11]: hash(cudf.CategoricalDtype(list(""abc"")))
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[11], line 1
----> 1 hash(cudf.CategoricalDtype(list(""abc"")))

TypeError: unhashable type: 'CategoricalDtype'

In [12]: hash(pandas.CategoricalDtype(list(""abc"")))
Out[12]: 1532899084736511412
```

**Describe the solution you'd like**
Define `__hash__` on `CategoricalDtype`

**Describe alternatives you've considered**
Custom hash based on order and categories

**Additional context**
Add any other context, code examples, or references to existing implementations about the feature request here.
",2023-08-31T21:45:05Z,0,0,Matthew Roeschke,@rapidsai ,True
634,[FEA] provide external libraries a way of getting a `DeviceBuffer` pointer that can become spillable again,"**Is your feature request related to a problem? Please describe.**

When running in a multi-gpu setting, message passing with ucx-py takes a `DeviceBuffer` and obtains the device memory pointer through the `__cuda_array_interface__`. This, correctly, marks the buffer as unspillable. 

It would be nice if there were a way to expose a pointer that is marked as unspillable until the external library drops the reference (kind of like `acquire_spill_lock`). `ucx-py` could then use it, and scope the pointer use to the lifetime of the message request (once the request is completed, the pointer can be dropped and is available for spilling again).

**Describe the solution you'd like**

If we were to hand back an object that had a `weakref.finalize(obj, unmark_spillable)` callback, when it was dropped, we could let the buffer be spillable again.

**Describe alternatives you've considered**

Making ucx-py aware of cudf and using `acquire_spill_lock`.

cc @madsbk / @vyasr / @galipremsagar ",2023-09-01T16:35:34Z,0,0,Lawrence Mitchell,,False
635,[BUG] Overflow in to_datetime date parsing returns incorrect result,"**Describe the bug**
Overflow in to_datetime date parsing returns incorrect result

**Steps/Code to reproduce bug**
```python
In [2]: from cudf import *

In [5]: to_datetime([99991231], format=""%Y%m%d"")
Out[5]: DatetimeIndex(['1816-03-29 05:56:08.066277376'], dtype='datetime64[ns]')

In [6]: to_datetime([""99991231""], format=""%Y%m%d"")
Out[6]: DatetimeIndex(['1816-03-29 05:56:08.066277376'], dtype='datetime64[ns]')

In [7]: to_datetime([""99991231""])
Out[7]: DatetimeIndex(['1816-03-29 05:56:08.066277376'], dtype='datetime64[ns]')
```

**Expected behavior**
If the date cannot be represented, an `Exception` should be raised that the representation overflows

**Environment overview (please complete the following information)**
 - Environment location: Bare-metal
 - Method of cuDF install: conda
   - If method of install is [Docker], provide `docker pull` & `docker run` commands used

**Environment details**
Please run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details

**Additional context**
Add any other context about the problem here.
",2023-09-06T01:00:53Z,0,0,Matthew Roeschke,@rapidsai ,True
636,Use grid stride in CSV reader kernels,"Currently, the CSV reader parses data using a thread per row, and a separate thread is used for each row, regardless of the file size.
Using a grid stride loop would allow kernels to launch with preset number of blocks even with large input.

This applies both to the parser and the data inference kernels.",2023-09-08T01:30:04Z,0,0,Vukasin Milovanovic,NVIDIA,True
637,[FEA] Better scaling for simple regular expressions on long strings,"**Is your feature request related to a problem? Please describe.**
In Spark we have had multiple customers that try to process really long strings with simple regular expressions. The reality is that in most cases they don't need a regular expression but the API/expression Spark exposes is for a regular expression so they use it.  An example of this is string split, where what is being split on is a regular expression. They will split on things like a comma `,` or try to parse JSON like formatted strings by splitting on `}` or `}}` sequences. But they do this on very large strings. Strings that are over 120KiB in size.  When this happens we see really bad performance on the GPU. Even worse than single threaded performance on the CPU to process the same data.  Here is an example where we are essentially doing an `explode(split(column_name, ""}}""))` on 500 rows. It is 500 rows, because the length of the strings involved end up making that about 1 row group in parquet, so this is the data that a single Spark task sees.

String Length | GPU Median Time | CPU Median Time | Hacked GPU Median Time
-- | -- | -- | --
10 | 316 | 249 | 395
100 | 321 | 298 | 371
1,000 | 333 | 753 | 377
10,000 | 1,352 | 5,401 | 407
20,000 | 4,914 | 10,630 | 459
40,000 | 15,810 | 21,261 | 564
80,000 | 66,409 | 43,487 | 773
100,000 | 111,781 | 54,989 | 902
120,000 | 134,409 | 66,066 | 1,035
140,000 | 212,333 | 76,900 | 1,232

![chart(1)](https://github.com/rapidsai/cudf/assets/3441321/70bc2bb0-6eea-4c26-95df-33b397a0c567)

In this the `Hacked GPU Median Time` is when I hacked the Spark plugin to ignore the regular expression and instead use the non-regular expression CUDF API to split the string.

**Describe the solution you'd like**
In the Rapids Plugin we have put in place a number of optimizations where we will parse the regular expression and if possible transpile it to a string that we can do a non regular expression split on. We think it is worth pushing this type of optimization into CUDF itself and not just for splits.  It would really be nice if CUDF could spend time to look for alternative ways to execute a regular expression, especially for really long string, that don't need a single thread per string to work properly.

Examples include 

  * Seeing if a regular expression can be transpiled to static string and using an alternative string operation instead. This could apply to split, matches, etc.
  * When just checking if a regular expression matches a string we could match things like `FOO.*` and convert it into a starts with operation instead. This could also apply to contains or ends with. 

I would like it in CUDF because I think it would benefit everyone, not just the Spark plugin, but also I think the RAPIDS team could do a better job in many cases of finding these optimizations than we are doing.

**Describe alternatives you've considered**
Update our own regular expression checker code to start doing more of these optimizations.",2023-09-12T14:22:02Z,0,0,Robert (Bobby) Evans,Nvidia,True
638,[BUG] Malformed fixed length byte array Parquet file loads corrupted data instead of error,"**Describe the bug**
Using libcudf to load a Parquet file that is malformed ""succeeds"" by producing a table with some corrupted rows rather than returning an error as expected.  Spark 3.5, parquet-mr 1.13.1, and pyarrow 13 all produce unexpected EOF errors when trying to load the same file.

**Steps/Code to reproduce bug**
Load https://github.com/apache/parquet-testing/blob/master/data/fixed_length_byte_array.parquet using libcudf.  Note that it will produce a table with 1000 rows with no nulls, and some of the rows have a list of bytes longer than 4 entries.  According to the [docs for the file](https://github.com/apache/parquet-testing/blob/master/data/fixed_length_byte_array.md), the data is supposed to be a single column with a fixed-length byte array of size 4, yet some rows load with more than four bytes, some with no bytes.

**Expected behavior**
libcudf should return an error when trying to load the file rather than producing corrupted rows.
",2023-09-13T15:40:58Z,0,0,Jason Lowe,NVIDIA,True
639,[FEA] Parallelize gpuInitStringDescriptors when Parquet input type is FIXED_LEN_BYTE_ARRAY,"As part of the preprocessing of PLAIN encoded string data in the parquet reader, a pass through the page data is performed to either gather string sizes, or initialize `{ptr, length}` tuples for use by the decoder. For variable width string data, this pass must be performed by a single thread. But in the case of fixed width data, all threads in the warp should be able to participate.

https://github.com/rapidsai/cudf/blob/1bfeee7575e137bc75741cb2caf015e55ecab2cd/cpp/src/io/parquet/page_decode.cuh#L409
",2023-09-14T22:18:22Z,0,0,Ed Seidl,@LLNL,False
640,[BUG] Interleaving cuDF operations with Tensorflow results in `CUDA_ERROR_INVALID_VALUE`,"**Describe the bug**
When executing cuDF code interleaved in certain tensorflow code paths, a device to host copy will result in a `CUDA_ERROR_INVALID_VALUE` error.

Full stack trace
```
tests/test_tensorflow.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/nfs/wangm/mambaforge/envs/xdf-integration/lib/python3.10/site-packages/nvtx/nvtx.py:101: in inner
    result = func(*args, **kwargs)
/home/nfs/wangm/mambaforge/envs/xdf-integration/lib/python3.10/site-packages/cudf/core/dataframe.py:5055: in to_pandas
    out_data[i] = self._data[col_key].to_pandas(
/home/nfs/wangm/mambaforge/envs/xdf-integration/lib/python3.10/site-packages/cudf/core/column/numerical.py:685: in to_pandas
    pd_series = pd.Series(self.values_host, copy=False)
/home/nfs/wangm/mambaforge/envs/xdf-integration/lib/python3.10/site-packages/cudf/core/column/column.py:234: in values_host
    return self.data_array_view(mode=""read"").copy_to_host()
/home/nfs/wangm/mambaforge/envs/xdf-integration/lib/python3.10/site-packages/cudf/core/column/column.py:156: in data_array_view
    return cuda.as_cuda_array(obj).view(self.dtype)
/home/nfs/wangm/mambaforge/envs/xdf-integration/lib/python3.10/site-packages/numba/cuda/api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
/home/nfs/wangm/mambaforge/envs/xdf-integration/lib/python3.10/site-packages/numba/cuda/cudadrv/devices.py:232: in _require_cuda_context
    return fn(*args, **kws)
/home/nfs/wangm/mambaforge/envs/xdf-integration/lib/python3.10/site-packages/numba/cuda/api.py:47: in from_cuda_array_interface
    devptr = driver.get_devptr_for_active_ctx(desc['data'][0])
/home/nfs/wangm/mambaforge/envs/xdf-integration/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py:2964: in get_devptr_for_active_ctx
    driver.cuPointerGetAttribute(byref(devptr), attr, ptr)
/home/nfs/wangm/mambaforge/envs/xdf-integration/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py:327: in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <numba.cuda.cudadrv.driver.Driver object at 0x7ff5f744e7a0>, fname = 'cuPointerGetAttribute', retcode = 1

    def _check_ctypes_error(self, fname, retcode):
        if retcode != enums.CUDA_SUCCESS:
            errname = ERROR_MAP.get(retcode, ""UNKNOWN_CUDA_ERROR"")
            msg = ""Call to %s results in %s"" % (fname, errname)
            _logger.error(msg)
            if retcode == enums.CUDA_ERROR_NOT_INITIALIZED:
                self._detect_fork()
>           raise CudaAPIError(retcode, msg)
E           numba.cuda.cudadrv.driver.CudaAPIError: [1] Call to cuPointerGetAttribute results in CUDA_ERROR_INVALID_VALUE

/home/nfs/wangm/mambaforge/envs/xdf-integration/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py:395: CudaAPIError
```

**Steps/Code to reproduce bug**
Min repro:
```python
import cudf
import tensorflow as tf

df = cudf.DataFrame({""a"": [1, 2, 3], ""b"": [4, 5, 6]})

with tf.device(""/GPU:1""):
    # Perform some operations on the GPU with tf
    inp = tf.keras.Input(shape=(), name=""inp"", dtype=tf.int64)
    b = inp[:, tf.newaxis]

# Copy data to host
pdf = df.to_pandas()
```

**Expected behavior**
Since keras layer only deal with constructing computation graph on the tensorflow side and shouldn't have interaction with cudf. cuDF operation shouldn't fail.

**Environment overview (please complete the following information)**
 - Environment location: [Bare-metal]
 - Method of cuDF install: [conda]]

**Environment details**
N/A

**Additional context**
N/A
",2023-09-18T10:41:46Z,0,0,Michael Wang,Nvidia Rapids,True
641,[FEA] Disallow sets from data/index/column constructors?,"**Is your feature request related to a problem? Please describe.**
pandas has disallowed sets from data/index/column constructors since they are unordered and construction assumes order of elements. Should cudf do the same?

```python
In [1]: import cudf

In [3]: cudf.Series({1, 2, 3})
Out[3]: 
0    1
1    2
2    3
dtype: int64

In [4]: cudf.Series({""a"": {1, 2, 3}})
Out[4]: 
a    [1, 2, 3]
dtype: list

In [5]: cudf.DataFrame({""a"": {1, 2, 3}})
Out[5]: 
   a
0  1
1  2
2  3

In [6]: cudf.DataFrame(range(2), index={1, 2})
Out[6]: 
   0
1  0
2  1

In [8]: cudf.DataFrame(range(2), columns={1})
Out[8]: 
   1
0  0
1  1
```

**Describe the solution you'd like**
All the examples raise a ValueError

**Describe alternatives you've considered**
Convert to list

**Additional context**
Add any other context, code examples, or references to existing implementations about the feature request here.
",2023-09-19T20:33:10Z,0,0,Matthew Roeschke,@rapidsai ,True
642,[FEA] Optionally support titlecase for capitalize,"**Is your feature request related to a problem? Please describe.**
Spark has a method calling initcap. We implemented this using strings::capitalize, but recently ran into some problems because the first letter it uses is not an uppercase letter, it is a title case letter.

https://unicode.org/faq/casemap_charprop.html#4

Most of the time they are the same, but there are a few cases where they are not and ß is one of them. I would love an option for capitalize that uses title case instead of upper case. Or if we could get a separate initcap function that uses title case would also be great.",2023-09-20T19:31:15Z,0,0,Robert (Bobby) Evans,Nvidia,True
643,[FEA] Ability to round-trip all pandas columns dtypes,"**Is your feature request related to a problem? Please describe.**
With the current `Column` design and `to_pandas` API implementation it is only possible to convert a cudf series to numpy dtype or pandas nullable dtypes. However, pandas also support arrow-backed dtypes. 

```python
In [1]: import pandas as pd

In [2]: np_series = pd.Series([1, 2, 3], dtype='int64')

In [3]: pd_series = pd.Series([1, 2, 3], dtype=pd.Int64Dtype())

In [4]: import pyarrow as pa

In [5]: arrow_series = pd.Series([1, 2, 3], dtype=pd.ArrowDtype(pa.int64()))

In [6]: np_series
Out[6]: 
0    1
1    2
2    3
dtype: int64

In [7]: pd_series
Out[7]: 
0    1
1    2
2    3
dtype: Int64

In [8]: arrow_series
Out[8]: 
0   1
1   2
2   3
dtype: int64[pyarrow]

In [9]: import cudf

In [10]: cudf.from_pandas(np_series).to_pandas()
Out[10]: 
0    1
1    2
2    3
dtype: int64

In [11]: cudf.from_pandas(pd_series).to_pandas()
Out[11]: 
0    1
1    2
2    3
dtype: int64

In [12]: cudf.from_pandas(arrow_series).to_pandas()
Out[12]: 
0    1
1    2
2    3
dtype: int64
```

**Describe the solution you'd like**
I would like cudf to have the ability to round-trip the data type of pandas successfully.


",2023-09-21T05:55:10Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
644,[BUG] casting from `float32` to `Decimal64Dtype` is resulting in incorrect values,"**Describe the bug**
When there is a type-cast from `float32` to `Decimal64Dtype`, the decimal values yield incorrect results. This issue was surfaced when testing `arrow-13`. `arrow-12` has the same bug and it was fixed in `arrow-13`, we have similar issue in our libcudf too.

**Steps/Code to reproduce bug**
```python
IIn [20]: import pyarrow as pa # This is pyarrow-13, not 12

In [21]: import cudf

In [22]: data = [14.12302, 97938.2, None, 0.0, -8.302014, None, 94.31304, -112.2314, 0.3333333]

In [23]: s = cudf.Series(data, dtype='float32')

In [24]: s
Out[24]: 
0     14.12302017
1     97938.20313
2            <NA>
3             0.0
4    -8.302014351
5            <NA>
6     94.31304169
7    -112.2313995
8     0.333333313
dtype: float32

In [25]: parray = pa.array(data, type=pa.float32())

In [26]: parray
Out[26]: 
<pyarrow.lib.FloatArray object at 0x7ff00e3bf700>
[
  14.12302,
  97938.2,
  null,
  0,
  -8.302014,
  null,
  94.31304,
  -112.2314,
  0.3333333
]

In [27]: decimal_dtype = cudf.Decimal64Dtype(11, 4)

In [28]: s.astype(decimal_dtype)
Out[28]: 
0       14.1230
1    97938.2016      # Incorrect, it should be 97938.2031
2          None
3        0.0000
4       -8.3020
5          None
6       94.3130
7     -112.2314
8        0.3333
dtype: decimal64

In [29]: parray.cast(decimal_dtype.to_arrow())
Out[29]: 
<pyarrow.lib.Decimal128Array object at 0x7ff00e1bebc0>
[
  14.1230,
  97938.2031,
  null,
  0.0000,
  -8.3020,
  null,
  94.3130,
  -112.2314,
  0.3333
]
```

**Expected behavior**
```python
In [28]: s.astype(decimal_dtype)
Out[28]: 
0       14.1230
1    97938.2031      # Incorrect, it should be 97938.2031
2          None
3        0.0000
4       -8.3020
5          None
6       94.3130
7     -112.2314
8        0.3333
dtype: decimal64
```

**Environment overview (please complete the following information)**
 - Environment location: [Bare-metal]
 - Method of cuDF install: [from source]


**Environment details**
Please run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details
<details><summary>Click here to see environment details</summary><pre>
     
     **git***
     commit 4b2a8e2ccdeff36ff58d28d80c8742a474e1d69a (HEAD -> arrow_13)
     Author: galipremsagar <sagarprem75@gmail.com>
     Date:   Thu Sep 21 23:13:31 2023 -0700
     
     Upgrade to arrow 13
     **git submodules***
     
     ***OS Information***
     DISTRIB_ID=Ubuntu
     DISTRIB_RELEASE=18.04
     DISTRIB_CODENAME=bionic
     DISTRIB_DESCRIPTION=""Ubuntu 18.04.4 LTS""
     NAME=""Ubuntu""
     VERSION=""18.04.4 LTS (Bionic Beaver)""
     ID=ubuntu
     ID_LIKE=debian
     PRETTY_NAME=""Ubuntu 18.04.4 LTS""
     VERSION_ID=""18.04""
     HOME_URL=""https://www.ubuntu.com/""
     SUPPORT_URL=""https://help.ubuntu.com/""
     BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
     PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
     VERSION_CODENAME=bionic
     UBUNTU_CODENAME=bionic
     Linux dt07 4.15.0-76-generic #86-Ubuntu SMP Fri Jan 17 17:24:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
     
     ***GPU Information***
     Fri Sep 22 03:31:47 2023
     +---------------------------------------------------------------------------------------+
     | NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |
     |-----------------------------------------+----------------------+----------------------+
     | GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
     | Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
     |                                         |                      |               MIG M. |
     |=========================================+======================+======================|
     |   0  Tesla T4                        On | 00000000:3B:00.0 Off |                    0 |
     | N/A   46C    P0               27W /  70W|   1348MiB / 15360MiB |      0%      Default |
     |                                         |                      |                  N/A |
     +-----------------------------------------+----------------------+----------------------+
     |   1  Tesla T4                        On | 00000000:5E:00.0 Off |                    0 |
     | N/A   34C    P8                9W /  70W|      4MiB / 15360MiB |      0%      Default |
     |                                         |                      |                  N/A |
     +-----------------------------------------+----------------------+----------------------+
     |   2  Tesla T4                        On | 00000000:AF:00.0 Off |                    0 |
     | N/A   29C    P8               10W /  70W|      4MiB / 15360MiB |      0%      Default |
     |                                         |                      |                  N/A |
     +-----------------------------------------+----------------------+----------------------+
     |   3  Tesla T4                        On | 00000000:D8:00.0 Off |                    0 |
     | N/A   28C    P8               10W /  70W|      4MiB / 15360MiB |      0%      Default |
     |                                         |                      |                  N/A |
     +-----------------------------------------+----------------------+----------------------+
     
     +---------------------------------------------------------------------------------------+
     | Processes:                                                                            |
     |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
     |        ID   ID                                                             Usage      |
     |=======================================================================================|
     +---------------------------------------------------------------------------------------+
     
     ***CPU***
     Architecture:        x86_64
     CPU op-mode(s):      32-bit, 64-bit
     Byte Order:          Little Endian
     CPU(s):              64
     On-line CPU(s) list: 0-63
     Thread(s) per core:  2
     Core(s) per socket:  16
     Socket(s):           2
     NUMA node(s):        2
     Vendor ID:           GenuineIntel
     CPU family:          6
     Model:               85
     Model name:          Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz
     Stepping:            4
     CPU MHz:             1194.971
     BogoMIPS:            4200.00
     Virtualization:      VT-x
     L1d cache:           32K
     L1i cache:           32K
     L2 cache:            1024K
     L3 cache:            22528K
     NUMA node0 CPU(s):   0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62
     NUMA node1 CPU(s):   1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63
     Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts pku ospke md_clear flush_l1d arch_capabilities
     
     ***CMake***
     /nvme/0/pgali/envs/cudfdev/bin/cmake
     cmake version 3.27.6
     
     CMake suite maintained and supported by Kitware (kitware.com/cmake).
     
     ***g++***
     /nvme/0/pgali/envs/cudfdev/bin/g++
     g++ (conda-forge gcc 11.4.0-2) 11.4.0
     Copyright (C) 2021 Free Software Foundation, Inc.
     This is free software; see the source for copying conditions.  There is NO
     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
     
     
     ***nvcc***
     /nvme/0/pgali/envs/cudfdev/bin/nvcc
     nvcc: NVIDIA (R) Cuda compiler driver
     Copyright (c) 2005-2022 NVIDIA Corporation
     Built on Wed_Sep_21_10:33:58_PDT_2022
     Cuda compilation tools, release 11.8, V11.8.89
     Build cuda_11.8.r11.8/compiler.31833905_0
     
     ***Python***
     /nvme/0/pgali/envs/cudfdev/bin/python
     Python 3.10.12
     
     ***Environment Variables***
     PATH                            : /nvme/0/pgali/envs/cudfdev/bin:/nvme/0/pgali/envs/cudfdev/bin:/nvme/0/pgali/anaconda3/bin:/nvme/0/pgali/envs/cudfdev/bin:/nvme/0/pgali/.cargo/bin:/home/nfs/pgali/.vscode-server/bin/abd2f3db4bdb28f9e95536dfa84d8479f1eb312d/bin/remote-cli:/nvme/0/pgali/.cargo/bin:/nvme/0/pgali/anaconda3/bin:/nvme/0/pgali/anaconda3/condabin:/nvme/0/pgali/.cargo/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin:/usr/local/cuda/bin
     LD_LIBRARY_PATH                 : /usr/local/cuda/lib64:/usr/local/cuda/lib64::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64
     NUMBAPRO_NVVM                   :
     NUMBAPRO_LIBDEVICE              :
     CONDA_PREFIX                    : /nvme/0/pgali/envs/cudfdev
     PYTHON_PATH                     :
     
     ***conda packages***
     /nvme/0/pgali/anaconda3/bin/conda
     # packages in environment at /nvme/0/pgali/envs/cudfdev:
     #
     # Name                    Version                   Build  Channel
     _libgcc_mutex             0.1                 conda_forge    conda-forge
     _openmp_mutex             4.5                       2_gnu    conda-forge
     _sysroot_linux-64_curr_repodata_hack 3                   h69a702a_13    conda-forge
     accessible-pygments       0.0.4              pyhd8ed1ab_0    conda-forge
     aiobotocore               2.5.4              pyhd8ed1ab_0    conda-forge
     aiohttp                   3.8.5           py310h2372a71_0    conda-forge
     aioitertools              0.11.0             pyhd8ed1ab_0    conda-forge
     aiosignal                 1.3.1              pyhd8ed1ab_0    conda-forge
     alabaster                 0.7.13             pyhd8ed1ab_0    conda-forge
     anyio                     4.0.0              pyhd8ed1ab_0    conda-forge
     argon2-cffi               23.1.0             pyhd8ed1ab_0    conda-forge
     argon2-cffi-bindings      21.2.0          py310h5764c6d_3    conda-forge
     arrow                     1.2.3              pyhd8ed1ab_0    conda-forge
     asttokens                 2.4.0              pyhd8ed1ab_0    conda-forge
     async-lru                 2.0.4              pyhd8ed1ab_0    conda-forge
     async-timeout             4.0.3              pyhd8ed1ab_0    conda-forge
     attrs                     23.1.0             pyh71513ae_1    conda-forge
     aws-c-auth                0.7.3                he2921ad_3    conda-forge
     aws-c-cal                 0.6.2                hc309b26_1    conda-forge
     aws-c-common              0.9.0                hd590300_0    conda-forge
     aws-c-compression         0.2.17               h4d4d85c_2    conda-forge
     aws-c-event-stream        0.3.2                h2e3709c_0    conda-forge
     aws-c-http                0.7.12               hc865f51_1    conda-forge
     aws-c-io                  0.13.32              h1a03231_3    conda-forge
     aws-c-mqtt                0.9.6                h3a0376c_0    conda-forge
     aws-c-s3                  0.3.17               h1678ad6_0    conda-forge
     aws-c-sdkutils            0.1.12               h4d4d85c_1    conda-forge
     aws-checksums             0.1.17               h4d4d85c_1    conda-forge
     aws-crt-cpp               0.23.1               hf7d0843_2    conda-forge
     aws-sam-translator        1.75.0             pyhd8ed1ab_0    conda-forge
     aws-sdk-cpp               1.11.156             he6c2984_2    conda-forge
     aws-xray-sdk              2.12.0             pyhd8ed1ab_0    conda-forge
     babel                     2.12.1             pyhd8ed1ab_1    conda-forge
     backcall                  0.2.0              pyh9f0ad1d_0    conda-forge
     backports                 1.0                pyhd8ed1ab_3    conda-forge
     backports.functools_lru_cache 1.6.5              pyhd8ed1ab_0    conda-forge
     backports.zoneinfo        0.2.1           py310hff52083_7    conda-forge
     bcrypt                    4.0.1           py310hcb5633a_1    conda-forge
     beautifulsoup4            4.12.2             pyha770c72_0    conda-forge
     benchmark                 1.8.0                h59595ed_0    conda-forge
     binutils                  2.40                 hdd6e379_0    conda-forge
     binutils_impl_linux-64    2.40                 hf600244_0    conda-forge
     binutils_linux-64         2.40                 hbdbef99_2    conda-forge
     blas                      1.0                         mkl    conda-forge
     bleach                    6.0.0              pyhd8ed1ab_0    conda-forge
     blinker                   1.6.2              pyhd8ed1ab_0    conda-forge
     bokeh                     3.2.2              pyhd8ed1ab_0    conda-forge
     boto3                     1.28.17            pyhd8ed1ab_0    conda-forge
     botocore                  1.31.17            pyhd8ed1ab_3    conda-forge
     brotlipy                  0.7.0           py310h5764c6d_1005    conda-forge
     bzip2                     1.0.8                h7f98852_4    conda-forge
     c-ares                    1.19.1               hd590300_0    conda-forge
     c-compiler                1.5.2                h0b41bf4_0    conda-forge
     ca-certificates           2023.7.22            hbcca054_0    conda-forge
     cached-property           1.5.2                hd8ed1ab_1    conda-forge
     cached_property           1.5.2              pyha770c72_1    conda-forge
     cachetools                5.3.1              pyhd8ed1ab_0    conda-forge
     certifi                   2023.7.22          pyhd8ed1ab_0    conda-forge
     cffi                      1.15.1          py310h2fee648_5    conda-forge
     cfgv                      3.3.1              pyhd8ed1ab_0    conda-forge
     cfn-lint                  0.80.2             pyhd8ed1ab_0    conda-forge
     charset-normalizer        3.2.0              pyhd8ed1ab_0    conda-forge
     click                     8.1.7           unix_pyh707e725_0    conda-forge
     cloudpickle               2.2.1              pyhd8ed1ab_0    conda-forge
     cmake                     3.27.6               hcfe8598_0    conda-forge
     colorama                  0.4.6              pyhd8ed1ab_0    conda-forge
     comm                      0.1.4              pyhd8ed1ab_0    conda-forge
     commonmark                0.9.1                      py_0    conda-forge
     contourpy                 1.1.1           py310hd41b1e2_0    conda-forge
     coverage                  7.3.1           py310h2372a71_1    conda-forge
     cryptography              41.0.4          py310h75e40e8_0    conda-forge
     cubinlinker               0.3.0           py310hfdf336d_0    rapidsai
     cuda-nvtx                 11.8.86                       0    nvidia
     cuda-python               11.8.2          py310h01a121a_0    conda-forge
     cuda-sanitizer-api        11.8.86                       0    nvidia
     cuda-version              11.8                 h70ddcb2_2    conda-forge
     cudatoolkit               11.8.0              h4ba93d1_12    conda-forge
     cudf                      23.10.0                  pypi_0    pypi
     cupy                      12.2.0          py310hbb1d8f0_1    conda-forge
     cxx-compiler              1.5.2                hf52228f_0    conda-forge
     cyrus-sasl                2.1.27               h54b06d7_7    conda-forge
     cython                    3.0.2           py310hc6cd4ac_2    conda-forge
     cytoolz                   0.12.2          py310h2372a71_0    conda-forge
     dask                      2023.9.2           pyhd8ed1ab_0    conda-forge
     dask-core                 2023.9.3a230920 py_gda256320e_1    dask/label/dev
     dask-cudf                 23.10.0                  pypi_0    pypi
     dataclasses               0.8                pyhc8e2a94_3    conda-forge
     datasets                  2.14.4             pyhd8ed1ab_0    conda-forge
     debugpy                   1.8.0           py310hc6cd4ac_0    conda-forge
     decopatch                 1.4.10             pyhd8ed1ab_0    conda-forge
     decorator                 5.1.1              pyhd8ed1ab_0    conda-forge
     defusedxml                0.7.1              pyhd8ed1ab_0    conda-forge
     dill                      0.3.7              pyhd8ed1ab_0    conda-forge
     distlib                   0.3.7              pyhd8ed1ab_0    conda-forge
     distributed               2023.9.2           pyhd8ed1ab_0    conda-forge
     distro                    1.8.0              pyhd8ed1ab_0    conda-forge
     dlpack                    0.5                  h9c3ff4c_0    conda-forge
     docker-py                 6.1.3              pyhd8ed1ab_0    conda-forge
     docutils                  0.19            py310hff52083_1    conda-forge
     doxygen                   1.9.1                hb166930_1    conda-forge
     ecdsa                     0.18.0             pyhd8ed1ab_1    conda-forge
     entrypoints               0.4                pyhd8ed1ab_0    conda-forge
     exceptiongroup            1.1.3              pyhd8ed1ab_0    conda-forge
     execnet                   2.0.2              pyhd8ed1ab_0    conda-forge
     executing                 1.2.0              pyhd8ed1ab_0    conda-forge
     fastavro                  1.8.3           py310h2372a71_0    conda-forge
     fastrlock                 0.8.2           py310hc6cd4ac_0    conda-forge
     filelock                  3.12.4             pyhd8ed1ab_0    conda-forge
     flask                     2.3.3              pyhd8ed1ab_0    conda-forge
     flask_cors                3.0.10             pyhd3deb0d_0    conda-forge
     fmt                       9.1.0                h924138e_0    conda-forge
     fqdn                      1.5.1              pyhd8ed1ab_0    conda-forge
     freetype                  2.12.1               h267a509_2    conda-forge
     frozenlist                1.4.0           py310h2372a71_0    conda-forge
     fsspec                    2023.9.1           pyh1a96a4e_0    conda-forge
     future                    0.18.3             pyhd8ed1ab_0    conda-forge
     gcc                       11.4.0               h7baecda_2    conda-forge
     gcc_impl_linux-64         11.4.0               h7aa1c59_2    conda-forge
     gcc_linux-64              11.4.0               hfd045f2_2    conda-forge
     gflags                    2.2.2             he1b5a44_1004    conda-forge
     glog                      0.6.0                h6f12383_0    conda-forge
     gmock                     1.14.0               ha770c72_1    conda-forge
     gmp                       6.2.1                h58526e2_0    conda-forge
     gmpy2                     2.1.2           py310h3ec546c_1    conda-forge
     graphql-core              3.2.3              pyhd8ed1ab_0    conda-forge
     greenlet                  2.0.2           py310hc6cd4ac_1    conda-forge
     gtest                     1.14.0               h00ab1b0_1    conda-forge
     gxx                       11.4.0               h7baecda_2    conda-forge
     gxx_impl_linux-64         11.4.0               h7aa1c59_2    conda-forge
     gxx_linux-64              11.4.0               hfc1ae95_2    conda-forge
     huggingface_hub           0.17.2             pyhd8ed1ab_0    conda-forge
     hypothesis                6.86.2             pyha770c72_0    conda-forge
     identify                  2.5.29             pyhd8ed1ab_0    conda-forge
     idna                      3.4                pyhd8ed1ab_0    conda-forge
     imagesize                 1.4.1              pyhd8ed1ab_0    conda-forge
     importlib-metadata        6.8.0              pyha770c72_0    conda-forge
     importlib_metadata        6.8.0                hd8ed1ab_0    conda-forge
     importlib_resources       5.13.0             pyhd8ed1ab_0    conda-forge
     iniconfig                 2.0.0              pyhd8ed1ab_0    conda-forge
     intel-openmp              2022.1.0          h9e868ea_3769
     ipykernel                 6.25.2             pyh2140261_0    conda-forge
     ipython                   8.15.0             pyh0d859eb_0    conda-forge
     isoduration               20.11.0            pyhd8ed1ab_0    conda-forge
     itsdangerous              2.1.2              pyhd8ed1ab_0    conda-forge
     jedi                      0.19.0             pyhd8ed1ab_0    conda-forge
     jinja2                    3.1.2              pyhd8ed1ab_1    conda-forge
     jmespath                  1.0.1              pyhd8ed1ab_0    conda-forge
     joblib                    1.3.2              pyhd8ed1ab_0    conda-forge
     jschema-to-python         1.2.3              pyhd8ed1ab_0    conda-forge
     json5                     0.9.14             pyhd8ed1ab_0    conda-forge
     jsondiff                  2.0.0              pyhd8ed1ab_0    conda-forge
     jsonpatch                 1.32               pyhd8ed1ab_0    conda-forge
     jsonpickle                2.2.0              pyhd8ed1ab_0    conda-forge
     jsonpointer               2.4             py310hff52083_2    conda-forge
     jsonschema                4.19.1             pyhd8ed1ab_0    conda-forge
     jsonschema-spec           0.2.4              pyhd8ed1ab_0    conda-forge
     jsonschema-specifications 2023.7.1           pyhd8ed1ab_0    conda-forge
     jsonschema-with-format-nongpl 4.19.1             pyhd8ed1ab_0    conda-forge
     junit-xml                 1.9                pyh9f0ad1d_0    conda-forge
     jupyter-cache             0.6.1              pyhd8ed1ab_0    conda-forge
     jupyter-lsp               2.2.0              pyhd8ed1ab_0    conda-forge
     jupyter_client            8.3.1              pyhd8ed1ab_0    conda-forge
     jupyter_core              5.3.1           py310hff52083_0    conda-forge
     jupyter_events            0.7.0              pyhd8ed1ab_2    conda-forge
     jupyter_server            2.7.3              pyhd8ed1ab_0    conda-forge
     jupyter_server_terminals  0.4.4              pyhd8ed1ab_1    conda-forge
     jupyterlab                4.0.6              pyhd8ed1ab_0    conda-forge
     jupyterlab_pygments       0.2.2              pyhd8ed1ab_0    conda-forge
     jupyterlab_server         2.25.0             pyhd8ed1ab_0    conda-forge
     kernel-headers_linux-64   3.10.0              h4a8ded7_13    conda-forge
     keyutils                  1.6.1                h166bdaf_0    conda-forge
     krb5                      1.21.2               h659d440_0    conda-forge
     lazy-object-proxy         1.9.0           py310h1fa729e_0    conda-forge
     lcms2                     2.15                 h7f713cb_2    conda-forge
     ld_impl_linux-64          2.40                 h41732ed_0    conda-forge
     lerc                      4.0.0                h27087fc_0    conda-forge
     libabseil                 20230802.1      cxx17_h59595ed_0    conda-forge
     libarrow                  13.0.0           h1935d02_4_cpu    conda-forge
     libblas                   3.9.0            16_linux64_mkl    conda-forge
     libbrotlicommon           1.1.0                hd590300_0    conda-forge
     libbrotlidec              1.1.0                hd590300_0    conda-forge
     libbrotlienc              1.1.0                hd590300_0    conda-forge
     libcblas                  3.9.0            16_linux64_mkl    conda-forge
     libcrc32c                 1.1.2                h9c3ff4c_0    conda-forge
     libcufile                 1.4.0.31                      0    nvidia
     libcufile-dev             1.4.0.31                      0    nvidia
     libcurand                 10.3.0.86                     0    nvidia
     libcurand-dev             10.3.0.86                     0    nvidia
     libcurl                   8.3.0                hca28451_0    conda-forge
     libdeflate                1.19                 hd590300_0    conda-forge
     libedit                   3.1.20191231         he28a2e2_2    conda-forge
     libev                     4.33                 h516909a_1    conda-forge
     libevent                  2.1.12               hf998b51_1    conda-forge
     libexpat                  2.5.0                hcb278e6_1    conda-forge
     libffi                    3.4.2                h7f98852_5    conda-forge
     libgcc-devel_linux-64     11.4.0               h922705a_2    conda-forge
     libgcc-ng                 13.2.0               h807b86a_2    conda-forge
     libgfortran-ng            13.2.0               h69a702a_2    conda-forge
     libgfortran5              13.2.0               ha4646dd_2    conda-forge
     libgomp                   13.2.0               h807b86a_2    conda-forge
     libgoogle-cloud           2.12.0               h8d7e28b_2    conda-forge
     libgrpc                   1.57.0               ha4d0f93_1    conda-forge
     libiconv                  1.17                 h166bdaf_0    conda-forge
     libjpeg-turbo             2.1.5.1              hd590300_1    conda-forge
     libkvikio                 23.10.00a       cuda11_230922_g8db93d0_23    rapidsai-nightly
     liblapack                 3.9.0            16_linux64_mkl    conda-forge
     libllvm14                 14.0.6               hcd5def8_4    conda-forge
     libnghttp2                1.52.0               h61bc06f_0    conda-forge
     libnsl                    2.0.0                h7f98852_0    conda-forge
     libntlm                   1.4               h7f98852_1002    conda-forge
     libnuma                   2.0.16               h0b41bf4_1    conda-forge
     libpng                    1.6.39               h753d276_0    conda-forge
     libprotobuf               4.23.4               hf27288f_6    conda-forge
     librdkafka                1.9.2                ha5a0de0_2    conda-forge
     librmm                    23.10.00a       cuda11_230922_gcd76d414_24    rapidsai-nightly
     libsanitizer              11.4.0               h4dcbe23_2    conda-forge
     libsodium                 1.0.18               h36c2ea0_1    conda-forge
     libsqlite                 3.43.0               h2797004_0    conda-forge
     libssh2                   1.11.0               h0841786_0    conda-forge
     libstdcxx-devel_linux-64  11.4.0               h922705a_2    conda-forge
     libstdcxx-ng              13.2.0               h7e041cc_2    conda-forge
     libthrift                 0.19.0               h8fd135c_0    conda-forge
     libtiff                   4.6.0                h29866fb_1    conda-forge
     libutf8proc               2.8.0                h166bdaf_0    conda-forge
     libuuid                   2.38.1               h0b41bf4_0    conda-forge
     libuv                     1.46.0               hd590300_0    conda-forge
     libwebp-base              1.3.2                hd590300_0    conda-forge
     libxcb                    1.15                 h0b41bf4_0    conda-forge
     libzlib                   1.2.13               hd590300_5    conda-forge
     livereload                2.6.3              pyh9f0ad1d_0    conda-forge
     llvmlite                  0.40.1          py310h1b8f574_0    conda-forge
     locket                    1.0.0              pyhd8ed1ab_0    conda-forge
     lz4                       4.3.2           py310h0cfdcf0_0    conda-forge
     lz4-c                     1.9.4                hcb278e6_0    conda-forge
     make                      4.3                  hd18ef5c_1    conda-forge
     makefun                   1.15.1             pyhd8ed1ab_0    conda-forge
     markdown                  3.4.4              pyhd8ed1ab_0    conda-forge
     markdown-it-py            2.2.0              pyhd8ed1ab_0    conda-forge
     markupsafe                2.1.3           py310h2372a71_1    conda-forge
     matplotlib-inline         0.1.6              pyhd8ed1ab_0    conda-forge
     mdit-py-plugins           0.4.0              pyhd8ed1ab_0    conda-forge
     mdurl                     0.1.0              pyhd8ed1ab_0    conda-forge
     mimesis                   11.1.0             pyhd8ed1ab_0    conda-forge
     mistune                   3.0.1              pyhd8ed1ab_0    conda-forge
     mkl                       2022.1.0           hc2b9512_224
     moto                      4.2.3              pyhd8ed1ab_0    conda-forge
     mpc                       1.3.1                hfe3b2da_0    conda-forge
     mpfr                      4.2.0                hb012696_0    conda-forge
     mpmath                    1.3.0              pyhd8ed1ab_0    conda-forge
     msgpack-python            1.0.5           py310hdf3cbec_0    conda-forge
     multidict                 6.0.4           py310h1fa729e_0    conda-forge
     multiprocess              0.70.15         py310h2372a71_0    conda-forge
     myst-nb                   0.17.2             pyhd8ed1ab_0    conda-forge
     myst-parser               0.18.1             pyhd8ed1ab_0    conda-forge
     nbclient                  0.7.4              pyhd8ed1ab_0    conda-forge
     nbconvert                 7.8.0              pyhd8ed1ab_0    conda-forge
     nbconvert-core            7.8.0              pyhd8ed1ab_0    conda-forge
     nbconvert-pandoc          7.8.0              pyhd8ed1ab_0    conda-forge
     nbformat                  5.9.2              pyhd8ed1ab_0    conda-forge
     nbsphinx                  0.9.3              pyhd8ed1ab_0    conda-forge
     ncurses                   6.4                  hcb278e6_0    conda-forge
     nest-asyncio              1.5.6              pyhd8ed1ab_0    conda-forge
     networkx                  3.1                pyhd8ed1ab_0    conda-forge
     ninja                     1.11.1               h924138e_0    conda-forge
     nodeenv                   1.8.0              pyhd8ed1ab_0    conda-forge
     notebook                  7.0.4              pyhd8ed1ab_0    conda-forge
     notebook-shim             0.2.3              pyhd8ed1ab_0    conda-forge
     numba                     0.57.1          py310h0f6aa51_0    conda-forge
     numpy                     1.24.4          py310ha4c1d20_0    conda-forge
     numpydoc                  1.5.0              pyhd8ed1ab_0    conda-forge
     nvcc_linux-64             11.8                h667003e_22    conda-forge
     nvcomp                    2.6.1                h0800d71_2    conda-forge
     nvtx                      0.2.8           py310h2372a71_0    conda-forge
     openapi-schema-validator  0.6.1              pyhd8ed1ab_0    conda-forge
     openapi-spec-validator    0.6.0              pyhd8ed1ab_0    conda-forge
     openjpeg                  2.5.0                h488ebb8_3    conda-forge
     openssl                   3.1.3                hd590300_0    conda-forge
     orc                       1.9.0                h52d3b3c_2    conda-forge
     overrides                 7.4.0              pyhd8ed1ab_0    conda-forge
     packaging                 23.1               pyhd8ed1ab_0    conda-forge
     pandas                    1.5.3           py310h9b08913_1    conda-forge
     pandoc                    3.1.3                h32600fe_0    conda-forge
     pandocfilters             1.5.0              pyhd8ed1ab_0    conda-forge
     paramiko                  3.3.1              pyhd8ed1ab_0    conda-forge
     parso                     0.8.3              pyhd8ed1ab_0    conda-forge
     partd                     1.4.0              pyhd8ed1ab_1    conda-forge
     pathable                  0.4.3              pyhd8ed1ab_0    conda-forge
     pbr                       5.11.1             pyhd8ed1ab_0    conda-forge
     pexpect                   4.8.0              pyh1a96a4e_2    conda-forge
     pickleshare               0.7.5                   py_1003    conda-forge
     pillow                    10.0.1          py310h29da1c1_1    conda-forge
     pip                       23.2.1             pyhd8ed1ab_0    conda-forge
     pkgutil-resolve-name      1.3.10             pyhd8ed1ab_1    conda-forge
     platformdirs              3.10.0             pyhd8ed1ab_0    conda-forge
     pluggy                    1.3.0              pyhd8ed1ab_0    conda-forge
     pre-commit                3.4.0              pyha770c72_1    conda-forge
     prometheus_client         0.17.1             pyhd8ed1ab_0    conda-forge
     prompt-toolkit            3.0.39             pyha770c72_0    conda-forge
     prompt_toolkit            3.0.39               hd8ed1ab_0    conda-forge
     protobuf                  4.23.4          py310h620c231_2    conda-forge
     psutil                    5.9.5           py310h2372a71_1    conda-forge
     pthread-stubs             0.4               h36c2ea0_1001    conda-forge
     ptxcompiler               0.8.1           py310h01a121a_0    conda-forge
     ptyprocess                0.7.0              pyhd3deb0d_0    conda-forge
     pure_eval                 0.2.2              pyhd8ed1ab_0    conda-forge
     py-cpuinfo                9.0.0              pyhd8ed1ab_0    conda-forge
     pyarrow                   13.0.0          py310hf9e7431_4_cpu    conda-forge
     pyasn1                    0.5.0              pyhd8ed1ab_0    conda-forge
     pycparser                 2.21               pyhd8ed1ab_0    conda-forge
     pydantic                  1.10.12         py310h2372a71_1    conda-forge
     pydata-sphinx-theme       0.14.1             pyhd8ed1ab_0    conda-forge
     pygments                  2.16.1             pyhd8ed1ab_0    conda-forge
     pynacl                    1.5.0           py310h5764c6d_2    conda-forge
     pyopenssl                 23.2.0             pyhd8ed1ab_1    conda-forge
     pyparsing                 3.1.1              pyhd8ed1ab_0    conda-forge
     pysocks                   1.7.1              pyha2e5f31_6    conda-forge
     pytest                    7.4.2              pyhd8ed1ab_0    conda-forge
     pytest-benchmark          4.0.0              pyhd8ed1ab_0    conda-forge
     pytest-cases              3.6.14             pyhd8ed1ab_0    conda-forge
     pytest-cov                4.1.0              pyhd8ed1ab_0    conda-forge
     pytest-xdist              3.3.1              pyhd8ed1ab_0    conda-forge
     python                    3.10.12         hd12c33a_0_cpython    conda-forge
     python-confluent-kafka    1.9.2           py310h5764c6d_2    conda-forge
     python-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge
     python-fastjsonschema     2.18.0             pyhd8ed1ab_0    conda-forge
     python-jose               3.3.0              pyh6c4a22f_1    conda-forge
     python-json-logger        2.0.7              pyhd8ed1ab_0    conda-forge
     python-snappy             0.6.1           py310hcee4d7c_0    conda-forge
     python-xxhash             3.3.0           py310h2372a71_0    conda-forge
     python_abi                3.10                    4_cp310    conda-forge
     pytorch                   1.11.0             py3.10_cpu_0    pytorch
     pytorch-mutex             1.0                         cpu    pytorch
     pytz                      2023.3.post1       pyhd8ed1ab_0    conda-forge
     pywin32-on-windows        0.1.0              pyh1179c8e_3    conda-forge
     pyyaml                    6.0.1           py310h2372a71_1    conda-forge
     pyzmq                     25.1.1          py310h5bbb5d0_0    conda-forge
     rdma-core                 28.9                 h59595ed_1    conda-forge
     re2                       2023.03.02           h8c504da_0    conda-forge
     readline                  8.2                  h8228510_1    conda-forge
     recommonmark              0.7.1              pyhd8ed1ab_0    conda-forge
     referencing               0.30.2             pyhd8ed1ab_0    conda-forge
     regex                     2023.8.8        py310h2372a71_0    conda-forge
     requests                  2.31.0             pyhd8ed1ab_0    conda-forge
     responses                 0.23.1             pyhd8ed1ab_0    conda-forge
     rfc3339-validator         0.1.4              pyhd8ed1ab_0    conda-forge
     rfc3986-validator         0.1.1              pyh9f0ad1d_0    conda-forge
     rhash                     1.4.4                hd590300_0    conda-forge
     rmm                       23.10.00a       cuda11_py310_230922_gcd76d414_24    rapidsai-nightly
     rpds-py                   0.10.3          py310hcb5633a_0    conda-forge
     rsa                       4.9                pyhd8ed1ab_0    conda-forge
     s2n                       1.3.51               h06160fa_0    conda-forge
     s3fs                      2023.9.1           pyhd8ed1ab_0    conda-forge
     s3transfer                0.6.2              pyhd8ed1ab_0    conda-forge
     sacremoses                0.0.53             pyhd8ed1ab_0    conda-forge
     sarif-om                  1.0.4              pyhd8ed1ab_0    conda-forge
     scikit-build              0.17.6             pyh4af843d_0    conda-forge
     scipy                     1.11.2          py310hb13e2d6_1    conda-forge
     sed                       4.8                  he412f7d_0    conda-forge
     send2trash                1.8.2              pyh41d4057_0    conda-forge
     setuptools                68.2.2             pyhd8ed1ab_0    conda-forge
     six                       1.16.0             pyh6c4a22f_0    conda-forge
     snappy                    1.1.10               h9fff704_0    conda-forge
     sniffio                   1.3.0              pyhd8ed1ab_0    conda-forge
     snowballstemmer           2.2.0              pyhd8ed1ab_0    conda-forge
     sortedcontainers          2.4.0              pyhd8ed1ab_0    conda-forge
     soupsieve                 2.5                pyhd8ed1ab_1    conda-forge
     spdlog                    1.11.0               h9b3ece8_1    conda-forge
     sphinx                    5.3.0              pyhd8ed1ab_0    conda-forge
     sphinx-autobuild          2021.3.14          pyhd8ed1ab_0    conda-forge
     sphinx-copybutton         0.5.2              pyhd8ed1ab_0    conda-forge
     sphinx-markdown-tables    0.0.17             pyh6c4a22f_0    conda-forge
     sphinxcontrib-applehelp   1.0.7              pyhd8ed1ab_0    conda-forge
     sphinxcontrib-devhelp     1.0.5              pyhd8ed1ab_0    conda-forge
     sphinxcontrib-htmlhelp    2.0.4              pyhd8ed1ab_0    conda-forge
     sphinxcontrib-jsmath      1.0.1              pyhd8ed1ab_0    conda-forge
     sphinxcontrib-qthelp      1.0.6              pyhd8ed1ab_0    conda-forge
     sphinxcontrib-serializinghtml 1.1.9              pyhd8ed1ab_0    conda-forge
     sphinxcontrib-websupport  1.2.6              pyhd8ed1ab_0    conda-forge
     sqlalchemy                2.0.21          py310h2372a71_0    conda-forge
     sshpubkeys                3.3.1              pyhd8ed1ab_0    conda-forge
     stack_data                0.6.2              pyhd8ed1ab_0    conda-forge
     streamz                   0.6.4              pyh6c4a22f_0    conda-forge
     sympy                     1.12            pypyh9d50eac_103    conda-forge
     sysroot_linux-64          2.17                h4a8ded7_13    conda-forge
     tabulate                  0.9.0              pyhd8ed1ab_1    conda-forge
     tblib                     2.0.0              pyhd8ed1ab_0    conda-forge
     terminado                 0.17.1             pyh41d4057_0    conda-forge
     tinycss2                  1.2.1              pyhd8ed1ab_0    conda-forge
     tk                        8.6.12               h27826a3_0    conda-forge
     tokenizers                0.13.1          py310h633acb5_2    conda-forge
     toml                      0.10.2             pyhd8ed1ab_0    conda-forge
     tomli                     2.0.1              pyhd8ed1ab_0    conda-forge
     toolz                     0.12.0             pyhd8ed1ab_0    conda-forge
     tornado                   6.3.3           py310h2372a71_1    conda-forge
     tqdm                      4.66.1             pyhd8ed1ab_0    conda-forge
     traitlets                 5.10.0             pyhd8ed1ab_0    conda-forge
     transformers              4.24.0             pyhd8ed1ab_0    conda-forge
     types-pyyaml              6.0.12.11          pyhd8ed1ab_0    conda-forge
     typing-extensions         4.8.0                hd8ed1ab_0    conda-forge
     typing_extensions         4.8.0              pyha770c72_0    conda-forge
     typing_utils              0.1.0              pyhd8ed1ab_0    conda-forge
     tzdata                    2023c                h71feb2d_0    conda-forge
     ucx                       1.14.1               h64cca9d_5    conda-forge
     ukkonen                   1.0.1           py310hbf28c38_3    conda-forge
     uri-template              1.3.0              pyhd8ed1ab_0    conda-forge
     urllib3                   1.26.15            pyhd8ed1ab_0    conda-forge
     virtualenv                20.24.4            pyhd8ed1ab_0    conda-forge
     wcwidth                   0.2.6              pyhd8ed1ab_0    conda-forge
     webcolors                 1.13               pyhd8ed1ab_0    conda-forge
     webencodings              0.5.1              pyhd8ed1ab_2    conda-forge
     websocket-client          1.6.3              pyhd8ed1ab_0    conda-forge
     werkzeug                  2.3.7              pyhd8ed1ab_0    conda-forge
     wheel                     0.41.2             pyhd8ed1ab_0    conda-forge
     wrapt                     1.15.0          py310h1fa729e_0    conda-forge
     xmltodict                 0.13.0             pyhd8ed1ab_0    conda-forge
     xorg-libxau               1.0.11               hd590300_0    conda-forge
     xorg-libxdmcp             1.1.3                h7f98852_0    conda-forge
     xxhash                    0.8.2                hd590300_0    conda-forge
     xyzservices               2023.7.0           pyhd8ed1ab_0    conda-forge
     xz                        5.2.6                h166bdaf_0    conda-forge
     yaml                      0.2.5                h7f98852_2    conda-forge
     yarl                      1.9.2           py310h2372a71_0    conda-forge
     zeromq                    4.3.4                h9c3ff4c_1    conda-forge
     zict                      3.0.0              pyhd8ed1ab_0    conda-forge
     zipp                      3.17.0             pyhd8ed1ab_0    conda-forge
     zlib                      1.2.13               hd590300_5    conda-forge
     zstd                      1.5.5                hfc55251_0    conda-forge
     
</pre></details>

**Additional context**
Add any other context about the problem here.
",2023-09-22T10:32:57Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
645,[BUG] Sanitizer reports misaligned error when doing reduction on short type values in cuda12 ENV,"**Describe the bug**
Sanitizer reports misaligned error when doing reduction on short type values in cuda12 ENV


**Steps/Code to reproduce bug**
Code:
```cpp
#include <cudf/types.hpp>
#include <cudf/aggregation.hpp>
#include <cudf/reduction.hpp>
#include <cudf_test/base_fixture.hpp>
#include <cudf_test/column_wrapper.hpp>

template <typename T, typename SourceElementT = T>
using column_wrapper =
  typename std::conditional<std::is_same_v<T, cudf::string_view>,
                            cudf::test::strings_column_wrapper,
                            cudf::test::fixed_width_column_wrapper<T, SourceElementT>>::type;
using int16_col   = column_wrapper<int16_t>;

struct MyReductionTest : public cudf::test::BaseFixture {};
TEST_F(MyReductionTest, AlignmentIssue)
{
  std::vector<int16_t> v({1, 2, 3});
  int16_col col(v.begin(), v.end());
  
  auto const output_dtype                 = cudf::data_type{cudf::type_id::INT16};
  auto min_agg = cudf::make_min_aggregation();
  std::unique_ptr<cudf::scalar> reduction1 = cudf::reduce(col, *dynamic_cast<cudf::reduce_aggregation *>(&(*min_agg)), output_dtype);

  auto const output_dtype2                 = cudf::data_type{cudf::type_id::BOOL8};
  auto any_agg = cudf::make_any_aggregation();
  std::unique_ptr<cudf::scalar> reduction2 = cudf::reduce(col, *dynamic_cast<cudf::reduce_aggregation *>(&(*any_agg)), output_dtype2);
}

```

Compile and Run with sanitizer:
```
compute-sanitizer --tool memcheck \
    --launch-timeout 600 \
    --error-exitcode -2 \
    --log-file ""./sanitizer_for_pid_%p.log"" \
    ./my-exe
```

Print sanitizer log:
```
head sanitizer_for_pid_42.log 
========= COMPUTE-SANITIZER
========= Invalid __shared__ read of size 16 bytes
=========     at 0x38c0 in void cub::CUB_101702_600_700_750_800_860_900_NS::DeviceReduceSingleTileKernel<cub::CUB_101702_600_700_750_800_860_900_NS::DeviceReducePolicy<short, short, int, cudf::DeviceMin>::Policy600, thrust::transform_iterator<thrust::identity<short>, thrust::transform_iterator<cudf::detail::value_accessor<short>, thrust::counting_iterator<int, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::use_default, thrust::use_default>, thrust::use_default, thrust::use_default>, short *, int, cudf::DeviceMin, short>(T2, T3, T4, T5, T6)
=========     by thread (0,0,0) in block (0,0,0)
=========     Address 0x8 is misaligned
=========     Saved host backtrace up to driver entry point at kernel launch time
=========     Host Frame: [0x2d18f2]
=========                in /usr/lib64/libcuda.so.1
=========     Host Frame:__cudart1049 [0xd9bd3b]
=========                in /home/chongg/code/spark-rapids-jni/target/cmake-build/gtests/./my-exe
```

The main errors are:
```
Invalid __shared__ read of size 16 bytes
Address 0x8 is misaligned
```

Others:
```
There are 2 reductions in the code.
If another reduction follows a min reduction, then errors occur.
```

**Expected behavior**
Fix Sanitizer error.

**Environment overview (please complete the following information)**
 - Environment location: Docker
 - Method of cuDF install: from source

**Environment details**
Docker image:  urm.nvidia.com/sw-spark-docker/plugin-jni:centos7-cuda12.0.1-blossom
CUDA 12, for more details, refer to https://github.com/NVIDIA/spark-rapids-jni/issues/1349

**Additional context**
Refer to https://github.com/NVIDIA/spark-rapids-jni/issues/1349",2023-09-26T09:42:13Z,0,0,Chong Gao,,False
646,[BUG] Reduction operations return `nan` instead of `NA` ,"**Describe the bug**
For non-float columns, we will need to return `NA` for results where needed instead of `nan`. 

**Steps/Code to reproduce bug**
```python
In [32]: import pandas as pd

In [33]: s = pd.Series([1, 2, 3, 4, 5, None], dtype='Int64')

In [34]: s.sum(min_count=6)
Out[34]: <NA>

In [35]: import cudf

In [36]: gs = cudf.from_pandas(s)

In [37]: gs
Out[37]: 
0       1
1       2
2       3
3       4
4       5
5    <NA>
dtype: int64

In [38]: gs.sum(min_count=6)
Out[38]: nan
```

**Expected behavior**
```python
In [38]: gs.sum(min_count=6)
Out[38]: NA
```

**Environment overview (please complete the following information)**
 - Environment location: [Bare-metal]
 - Method of cuDF install: [from source]

",2023-09-27T18:13:46Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
647,[BUG] Potential for use-after-free on libcudf/cudf interface boundaries,"**Describe the bug**

This comes out of [a review of #14133](https://github.com/rapidsai/cudf/pull/14133#discussion_r1338480398) which introduces a new `Scalar` type on the python side in pylibcudf, but I think that the issues are pervasive.

## Background

`libcudf` uses RMM for all memory allocations, this happens through a `memory_resource` object. Allocating `libcudf` functions all take an explicit `memory_resource` argument (as a raw pointer) that is defaulted to `rmm::get_current_device_resource()` (the resource set by `rmm::set_per_device_resource`). RMM `device_buffer`s hold a raw pointer of their allocating memory resource (needed for deallocation), and it is the user's (in this case `libcudf`'s) responsibility to keep that `memory_resource` alive until the `device_buffer` has been dropped:

This is fine:
```c++
memory_resource mr = ...;
{
    device_buffer buf{..., &mr};
    ... // do things with buf
   ~buf(); // called here, fine.
}
```

This is not (this is not a real example, since I think the `device_buffer` constructors don't allow exactly this, but bear with me):
```c++
device_buffer buf;
{
   memory_resource mr = ...;
   buf = {..., &mr};
   ... // do things with buf
   ~mr(); // called here, boom;
}
buf no longer valid
```

### How does `get_current_device_resource` work?

`set_per_device_resource` stores a raw pointer to a memory resource in a `std::unordered_map` and `get_current_device_resource` just looks up in the map. Again, the user is responsible for keeping the `mr` alive.

### How does this work in cudf (python) land?

The Cython wrappers in RMM expose memory resources, and `set/get_current_device_resource`. `set_per_device_memory_resource` sets the value in both the C++ level `std::unordered_map` _and_ a Python level dict (https://github.com/rapidsai/rmm/blob/5f07014db51535902f8cdb515596af162d1ae6ca/python/rmm/_lib/memory_resource.pyx#L1041-L1049). `get_current_device_resource` looks up in the Python dict (https://github.com/rapidsai/rmm/blob/5f07014db51535902f8cdb515596af162d1ae6ca/python/rmm/_lib/memory_resource.pyx#L1013-1026). `DeviceBuffer`s keep the ""current"" memory resource alive by storing a reference: https://github.com/rapidsai/rmm/blob/5f07014db51535902f8cdb515596af162d1ae6ca/python/rmm/_lib/device_buffer.pyx#L93-L93, but when taking ownership of a C++-level device buffer https://github.com/rapidsai/rmm/blob/5f07014db51535902f8cdb515596af162d1ae6ca/python/rmm/_lib/device_buffer.pyx#L161-L171, we don't use the mr stored in the C++ struct.

The Python level dict is ""necessary"" due to the usual expected way in which people will use things from python. That is they will expect that: `set_per_device_resource(device, CudaMemoryResource())` keeps the created memory resource alive. 

If a C++ library calls `set_per_device_resource` at some point, C++-land (`libcudf`) and Python-land (cudf) can end up disagreeing about what the current memory resource is (because Python-level `get_current_device_resource` doesn't look at the C++-level map).

## So what's the problem?

If `libcudf` ever allocates memory that it hands off to cudf with a memory resource that is _not_ managed by cudf, we have the potential for use-after-free, because cudf has no way of taking (or sharing) ownership of that memory resource from the RMM buffer.

AFAICT, cudf never explicitly passes a memory resource into `libcudf`, and so the allocation behaviour is always relying on C++ and Python agreeing about what `get_current_device_resource` returns, _and_ that cudf was the creator of that resource. Effectively the pattern is:

```python
# In python
...
1. mr = rmm.get_current_device_resource()
# No mr passed here, so C++-level get_current_device_resource() is used
2. new_column = libcudf.some_algorithm(existing_data)
# take ownership of the data, fingers crossed that new_column.data.mr is mr
3. cudf_column = Column.from_unique_ptr(new_column, mr)
```

If _either_ someone in C++ has set a different per-device resource _or_ the current thread is pre-empted between lines 1 and 2 (and the current device resource is changed), then line 3 will take ""ownership"" of the data, but not be keeping the correct memory resource for the data alive.

## How can we fix this?

If the Cython layer in cudf always explicitly passes a memory resource to `libcudf` algorithms, this problem goes away. We can then always guarantee that the memory resource we have on the Python side is the one used to allocate the data in libcudf so we can keep it alive.

Alternately, if the memory_resource pointers in RMM were smart pointers it might be possible to keep things alive that way. Right now we can't make Python and C++ always agree on what the current default resource is (because on the C++ side RMM doesn't have a smart pointer stored, because it doesn't take ownership).",2023-09-28T17:29:46Z,0,0,Lawrence Mitchell,,False
648,[FEA] Remove mr from test fixture once we require CUDA 12,"**Is your feature request related to a problem? Please describe.**
The base fixture used by our gtests holds an mr for using in tests. However, this mr is just cudf's own default mr, so this mr doesn't serve much purpose.

**Describe the solution you'd like**
We should remove this mr from the fixture.

**Additional context**
The removal was attempted in https://github.com/rapidsai/cudf/pull/14075, but it uncovered what appears to be a compiler bug that only manifested on arm machines using CUDA 11. We were unable to determine exactly what caused the compiler error. Since the issue is specific to CUDA 11, the change can be made once we drop support for CUDA 11 and CUDA 12 becomes the minimum required version.
",2023-09-28T22:18:44Z,0,0,Vyas Ramasubramani,@rapidsai,True
649,[FEA] Add option to read JSON field as unparsed string,"**Is your feature request related to a problem? Please describe.**

When reading JSON in Spark, if a field has mixed types,  Spark will infer the type as String to avoid data loss due to the uncertainty of the actual data type.

For example, given this input file, Spark will read column `bar` as a numeric type and column `foo` as a string type.

```
$ cat test.json
{ ""foo"": [1,2,3], ""bar"": 123 }
{ ""foo"": { ""a"": 1 }, ""bar"": 456 }
```

Here is the Spark code that demonstrates this:

```
scala> val df = spark.read.json(""test.json"")
df: org.apache.spark.sql.DataFrame = [bar: bigint, foo: string]                 

scala> df.show
+---+-------+
|bar|    foo|
+---+-------+
|123|[1,2,3]|
|456|{""a"":1}|
+---+-------+
```

Currently, Spark RAPIDS fails for this example because cuDF does not support mixed types in a column:

```
Caused by: ai.rapids.cudf.CudfException: CUDF failure at: /home/jenkins/agent/workspace/jenkins-spark-rapids-jni_nightly-pre_release-181-cuda11/thirdparty/cudf/cpp/src/io/json/json_column.cu:577: A mix of lists and structs within the same column is not supported
  at ai.rapids.cudf.Table.readJSON(Native Method)
```

**Describe the solution you'd like**
I would like the ability to specify to read certain columns as unparsed strings.

**Describe alternatives you've considered**
I am also exploring some workarounds in the Spark RAPIDS plugin.

**Additional context**

",2023-09-29T21:37:44Z,0,0,Andy Grove,@Apple,False
650,[FEA] Support mixed datetime string formats in date parsing,"**Is your feature request related to a problem? Please describe.**
Date-parsing functionality, e.g. `to_datetime`, does not always correctly handle when string arguments in an iterable have different formats. Currently a format is inferred from the first argument and is applied to all other arguments

```python
In [2]: cudf.to_datetime([""2020-01-01"", ""2020-01-01 10:11:12""])
Out[2]: DatetimeIndex(['2020-01-01', '2020-01-01'], dtype='datetime64[ns]')  # not OK

In [3]: cudf.to_datetime([""2020-01-01 10:11:12"", ""2020-01-01""])
Out[3]: DatetimeIndex(['2020-01-01 10:11:12', '2020-01-01 00:00:00'], dtype='datetime64[ns]')  # technically OK
```

**Describe the solution you'd like**
Ideally each string should be independently parsed to account for mixed formats. pandas 2.0 supports a `format=""mixed""` argument for the user to explicit call-out that the data has mixed format to trigger the slower, string-by-string parsing mode

**Describe alternatives you've considered**
Calling `to_datetime` in a loop over each format.

**Additional context**
Add any other context, code examples, or references to existing implementations about the feature request here.
",2023-10-02T17:31:07Z,0,0,Matthew Roeschke,@rapidsai ,True
651,Consolidate and optimize integer power implementations in libcudf,"Recently, some issues were identified where `std::pow` was being used with fixed-point values (#14210, #14233, #14242). There are many places in libcudf where we require an ""integer power"" operator, because `std::pow` uses floating-point values and thus gives erroneous results when handling high-precision integral values like `decimal128`. [This thread](https://github.com/rapidsai/cudf/pull/14233#discussion_r1340671836) covers a few key changes that are needed in libcudf:
- Consolidating existing ""integer power"" operators:
  - https://github.com/rapidsai/cudf/blob/7825790eac838e7a852d9b2429c64e78710cee28/cpp/include/cudf/fixed_point/fixed_point.hpp#L93
  - https://github.com/rapidsai/cudf/blob/7825790eac838e7a852d9b2429c64e78710cee28/cpp/src/binaryop/compiled/operation.cuh#L251
  - Changes from #14233
  - Changes from #14242
- Using a lookup table where possible (#9346), and exponentiation-by-squaring if outside the lookup table's bounds

Related: https://github.com/rapidsai/cudf/issues/10178",2023-10-03T06:21:35Z,0,0,Bradley Dice,@NVIDIA @rapidsai,True
652,Add developer documentation of why we need the no_gc_clear decorator for Scalars,"**thought (non-blocking):** ‏Perhaps we should (in a followup) document this issue more discoverably by adding some text to the dev docs.

_Originally posted by @wence- in https://github.com/rapidsai/cudf/pull/14133#discussion_r1338617532_
            ",2023-10-04T20:38:31Z,0,0,Vyas Ramasubramani,@rapidsai,True
653,[BUG] String columns written with `fastparquet` seem to be read incorrectly via CUDF's Parquet reader,"**Description**

This was uncovered in [Spark tests](https://github.com/NVIDIA/spark-rapids/pull/9366) that compare Parquet read/write compatibility with [`fastparquet`](https://fastparquet.readthedocs.io/en/latest/index.html).

The last row of a String column written with `fastparquet` seems to be interpreted by CUDF as having more null characters at the end than expected.

**Repro**

I'll spare the Scala/Spark details in this bug. [Here](https://github.com/NVIDIA/spark-rapids/files/12812419/fastparquet_string.zip) is a zipped Parquet file that seems to be read differently in CUDF.

From https://github.com/NVIDIA/spark-rapids/issues/9387:
```
GPU COLUMN LENGTH - NC: 0 DATA: DeviceMemoryBufferView{address=0x30a003400, length=20, id=-1} VAL: DeviceMemoryBufferView{address=0x30a001e00, length=64, id=-1}
COLUMN LENGTH - STRING
0 ""all"" 616c6c
1 ""the"" 746865
2 ""leaves"" 6c65617665730000000000000000
```

It would be good to check with the CUDF native Parquet reader, and compare against the results from  `parquet-mr`.",2023-10-05T23:20:44Z,0,0,MithunR,NVIDIA,True
654,[FEA] Allow ColumnAccessor to track MultiIndex levels that are Categorical or RangeIndex,"**Is your feature request related to a problem? Please describe.**
Currently, `DataFrame.columns` that is a `MultiIndex` will not correctly return levels that are `Categorical` or wrapped in a `RangeIndex` like pandas. This is because dtype inference in done by pandas but cannot infer the above points:

https://github.com/rapidsai/cudf/blob/5039d043a08e7ea7e5656bab60a6fced4dfa2f1d/python/cudf/cudf/core/column_accessor.py#L265 

**Describe the solution you'd like**
`ColumnAccessor` will probably need to track whether each level was created by a `Index` or `CategoricalIndex` or `RangeIndex` in order for `DataFrame.columns` to not lose this information.

**Describe alternatives you've considered**
Not sure...

**Additional context**
Add any other context, code examples, or references to existing implementations about the feature request here.
",2023-10-10T21:05:28Z,0,0,Matthew Roeschke,@rapidsai ,True
655,[BUG] read_json fails when reading multiple files but works with the individual files,"**Describe the bug**
For certain datasets, `cudf.read_json(lines=True, engine=""cudf"")` fails when combining multiple files, while reading them individually works as expected.

**Steps/Code to reproduce bug**
```
df = cudf.read_json(files, engine=""cudf_legacy"", lines=True)
```

**Expected behavior**
No error.

**Environment overview (please complete the following information)**
 - Environment location: [Bare-metal, Docker, Cloud(specify cloud provider)]
 - Method of cuDF install: [conda, Docker, or from source]
   - 22.10 nightly from oct 05

**Environment details**
Please run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details

**Additional context**
Add any other context about the problem here.
",2023-10-10T21:33:33Z,0,0,Ayush Dattagupta,Nvidia,True
656,[FEA] Allow IntervalDtype to support `closed=None`,"**Is your feature request related to a problem? Please describe.**
Currently pandas supports and defaults to `closed=None` in `IntervalDtype`. This allows for the dtype string `""interval""` to just work and avoids having to sync `IntervalIndex(closed=, dtype=IntervalDtype(closed=))` if the default isn't wanted

**Describe the solution you'd like**
Support `closed=None` in `IntervalDtype`

**Describe alternatives you've considered**
N/A

**Additional context**
Add any other context, code examples, or references to existing implementations about the feature request here.
",2023-10-12T00:00:38Z,0,0,Matthew Roeschke,@rapidsai ,True
657,[FEA] Switch cudf.Subwordtokenizer to use the vocab file directly instead of hash_vocab. ,"**Is your feature request related to a problem? Please describe.**

We currently rely on the hashed vocab file using `cudf.utils.hash_vocab_utils.hash_vocab`, we should move to using the `vocab file` directly. 

This will be similar to the API we added here: https://github.com/rapidsai/cudf/pull/13930

**Describe the solution you'd like**

```python3
cudf_tokenizer = cudf.core.subword_tokenizer.SubwordTokenizer(vocab_file=xyz.txt)
```
Instead of earlier:

```python3
from cudf.utils.hash_vocab_utils import hash_vocab
hash_vocab('bert-base-cased-vocab.txt', 'voc_hash.txt')
cudf_tokenizer = SubwordTokenizer('voc_hash.txt')
```

**Additional context**
This should help the switch from hugging face like tokenizer to be easier. 

CC: @davidwendt 


@MarcRomeijn, @cwharris, @markmotrin - Given your experience with the cuDF tokenizer, we'd value any feedback or suggestions for enhancing the `Subword tokenizer` API and features you would like.  ",2023-10-17T20:57:42Z,0,0,Vibhu Jawa,Nvidia,True
658,[FEA] Pinned memory pools for parquet decode,"We are investigating using pinned memory pool at the cuDF layer and replacing `cudaFreeHost` calls in `pinned_host_vector` due to traces we have seen that indicate synchronization or a ""lining up"" of kernels during parquet decode. Here's query88 from NDS at 3TB on our performance cluster running with an A100. In the nsys trace (pardon the amount of streams), we can see parquet nvcomp and decode kernels working on the first three quarters of the trace:

![Screenshot from 2023-10-23 13-08-15](https://github.com/rapidsai/cudf/assets/1901059/b64915ab-c934-4d95-905e-851bcf689a66)

The bottom trace is cuDF without changes. The top trace is a modified cuDF where we replaced calls to [cudaMallocHost](https://github.com/rapidsai/cudf/blob/branch-23.12/cpp/include/cudf/detail/utilities/pinned_host_vector.hpp#L157) and [cudaFreeHost](https://github.com/rapidsai/cudf/blob/branch-23.12/cpp/include/cudf/detail/utilities/pinned_host_vector.hpp#L174) with `allocate` and `deallocate` against a modified RMM `pool_memory_resource` that isn't stream aware and  has a single free list.

When we run with the modified cuDF, our NDS benchmark shows a 5% improvement at 3TB and a 6% improvement between old cuDF and new cuDF if we allow all 16 spark threads to submit work concurrently. In other words, we believe the `cudaFreeHost` calls specifically are preventing parquet heavy jobs from using more of the GPU due to synchronization.

The proposal here is to allow a pinned memory pool to be passed to parquet primarily, but there are probably other formats and areas in cuDF that might benefit from this. 

Note that another experiment we wanted to attempt was to remove pinned memory alltogether, which cuDF already has a flag for `LIBCUDF_IO_PREFER_PAGEABLE_TMP_MEMORY=1`, but we ran into issues for parquet only (https://github.com/rapidsai/cudf/issues/14311). Before I found this flag, I had tried replacing `cudaMallocHost` and `cudaFreeHost` with `malloc` and `free` and I ran into the same issue, so I think the parquet code is dependent on some sort of synchronization in the CUDA host pinned memory allocator.",2023-10-23T18:16:42Z,0,0,Alessandro Bellina,NVIDIA,True
659,[BUG] AST Limitation: Unable to Handle Single String Literal Expressions,"**Describe the bug**

The AST evaluator currently encounters a limitation where it is unable to handle an expression consisting of just one string literal, this is not a problem with other data types. While this scenario may be considered an edge case, it would be nice to address it for consistency with how other libraries, such as Pandas, handle similar situations.  

```
df.eval("" 'alex' "")
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/alexander/envs/cudf_dev/lib/python3.10/site-packages/nvtx/nvtx.py"", line 101, in inner
    result = func(*args, **kwargs)
  File ""/home/alexander/envs/cudf_dev/lib/python3.10/site-packages/cudf/core/dataframe.py"", line 7065, in eval
    None: libcudf.transform.compute_column(
  File ""/home/alexander/envs/cudf_dev/lib/python3.10/contextlib.py"", line 79, in inner
    return func(*args, **kwds)
  File ""transform.pyx"", line 196, in cudf._lib.transform.compute_column
RuntimeError: CUDF failure at: /opt/mambaforge/conda-bld/libcudf-ext_1692118605400/work/cpp/src/column/column_factories.cpp:161: Invalid, non-fixed-width type.

```
**Steps/Code to reproduce bug**
```
import cudf
df = cudf.DataFrame({'A': [], 'B': []})
df.eval("" 'alex' "")
```

`RuntimeError: CUDF failure at: /opt/mambaforge/conda-bld/libcudf-ext_1692118605400/work/cpp/src/column/column_factories.cpp:161: Invalid, non-fixed-width type.`

**Expected behavior**

For a similar  example but with other data types. 

```
import cudf
df = cudf.DataFrame({'A': [], 'B': []})
df.eval("" 123 "")
```

**Environment overview (please complete the following information)**
 - Environment location: conda
 - Method of cuDF install: conda
 -  branch-23.10, origin/branch-23.10
 
",2023-10-24T20:46:43Z,0,0,Alexander Ocsa,@voltrondata ,False
660,[FEA][JNI] Throw specific exception from `Table.readJSON` instead of `AssertionError`,"**Is your feature request related to a problem? Please describe.**
Per the discussion at https://github.com/NVIDIA/spark-rapids/pull/9304#discussion_r1372291170, the Spark plugin currently has to parse the error message from an `AssertionError`, which is an anti-pattern. This happens when the plugin calls `Table.readJSON` where the input is not JSON format.

This exception is thrown in the constructor for `Table`:

```java
  public Table(long[] cudfColumns) {
    assert cudfColumns != null && cudfColumns.length > 0 : ""CudfColumns can't be null or empty"";
```

**Describe the solution you'd like**
We should throw a specific exception instead.

**Describe alternatives you've considered**
None

**Additional context**
None
",2023-10-25T21:10:05Z,0,0,Andy Grove,@Apple,False
661,[FEA] Stream-preserving constructors for scalar class and its derived classes,"**Is your feature request related to a problem? Please describe.**
When the class constructor is implicitly called (when the object is passed-by-value, for instance), the constructor is called with the default stream, and not the CUDA stream passed by the user on which the algorithm is running. 
`string_scalar(string_scalar const& other,
                rmm::cuda_stream_view stream        = cudf::get_default_stream(),
                rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());`

**Describe the solution you'd like**
Consider requiring stream to be passed to constructor by making it a non-default argument.
`string_scalar(string_scalar const& other,
                rmm::cuda_stream_view stream ,
                rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());
`

**Describe alternatives you've considered**
None

**Additional context**
None
",2023-10-25T23:44:00Z,0,0,Shruti Shivakumar,NVIDIA,True
662,[FEA] center implementation for rolling window,"Hi, 

I am currently doing some feature engineering on a timeseries. My index is a datetime and I want to apply a rolling window. 
FYI the pandas code is running. 
`df['feature'] = df['feature'].rolling(f'{120}s', center=True, min_periods=1).sum()`

which leads to the following error: 

`NotImplementedError: center is not implemented for offset-based windows`

basically, I want to change to cudf for runtime optimization. As I have a datetime as an index, rolling operation with centring is much appreciated due to the fact of handling missing values, just shifting by timestamp is not trivial as discussed [here.](https://github.com/pandas-dev/pandas/issues/20012) 

I would appreciate if you could take a look. (Maybe it occurs due to some changes in the API interface)
",2023-10-26T16:19:44Z,0,0,Marc,,False
663,[BUG] Inconsistent Results Between AST and Pandas Evaluators for Expressions with Nullable Columns,"## Describe the bug

The AST evaluator and pandas evaluator are yielding divergent outcomes when evaluating expressions that include nullable columns.
 
Steps/Code to reproduce bug

```
import cudf
import pandas as pd
from io import StringIO

# Your CSV data as a string
csv_data = """"""\
Brand#32,MED PKG,,,754.84,4428,3537.78
Brand#53,MED BOX,,,3.98,1646,23333.44
Brand#41,WRAP BAG,1,8,763.34,3547,14687.36
Brand#43,SM BOX,2,46,15.45,6523,72572.36
Brand#42,WRAP PKG,3,31,778.19,3849,33608.96
Brand#23,LG JAR,4,43,101.93,4592,50407.61
""""""
dtype_schema = {
    'p_brand': 'string',   
    'p_container': 'string', 
    'l_linenumber': 'float64',
    'l_quantity': 'float64',  
    'ps_supplycost': 'float64',   
    'ps_availqty': 'float64',  
    'l_extendedprice': 'float64'
}

df = pd.read_csv(StringIO(csv_data), header=None, delimiter=',', dtype=dtype_schema, names=[name for name in dtype_schema])

df.eval(""(p_brand == 'Brand#23' or p_container == 'MED BOX') or (l_quantity < ps_supplycost)"").value_counts()
gdf = cudf.from_pandas(df)

gdf.eval(""(p_brand == 'Brand#23' or p_container == 'MED BOX') or (l_quantity < ps_supplycost)"").value_counts()
``` 

## Expected behavior

The cuDF AST evaluator is expected to yield results consistent with those produced by the pandas evaluator.

See: 
```
>>> df.eval(""(p_brand == 'Brand#23' or p_container == 'MED BOX') or (l_quantity < ps_supplycost)"").value_counts()
True     4
False     2
dtype: int64

>>> gdf = cudf.from_pandas(df)
>>> gdf.eval(""(p_brand == 'Brand#23' or p_container == 'MED BOX') or (l_quantity < ps_supplycost)"").value_counts()
True     3
False     1
dtype: int32

>>> df[ ((df['p_brand'] == 'Brand#23') | (df['p_container'] == 'MED BOX')) | (df['l_quantity'] < df['ps_supplycost']) ]
    p_brand p_container  l_linenumber  l_quantity  ps_supplycost  ps_availqty  l_extendedprice
1  Brand#53     MED BOX           NaN         NaN           3.98       1646.0         23333.44
2  Brand#41    WRAP BAG           1.0         8.0         763.34       3547.0         14687.36
4  Brand#42    WRAP PKG           3.0        31.0         778.19       3849.0         33608.96
5  Brand#23      LG JAR           4.0        43.0         101.93       4592.0         50407.61
>>> gdf[ ((gdf['p_brand'] == 'Brand#23') | (gdf['p_container'] == 'MED BOX')) | (gdf['l_quantity'] < gdf['ps_supplycost']) ]
    p_brand p_container  l_linenumber  l_quantity  ps_supplycost  ps_availqty  l_extendedprice
2  Brand#41    WRAP BAG           1.0         8.0         763.34       3547.0         14687.36
4  Brand#42    WRAP PKG           3.0        31.0         778.19       3849.0         33608.96
5  Brand#23      LG JAR           4.0        43.0         101.93       4592.0         50407.61
>>>
```


Environment overview  

Environment location: conda
Method of cuDF install: conda
branch-23.10, origin/branch-23.10",2023-11-04T00:47:47Z,0,0,Alexander Ocsa,@voltrondata ,False
664,[FEA] Support ``axis=0`` argument in ``clip``,"**Is your feature request related to a problem? Please describe.**
cuDF [does not currently support the axis argument in clip](https://github.com/rapidsai/cudf/blame/16051a718509c218010e6912d6f8e0fca7a7aa24/python/cudf/cudf/core/indexed_frame.py#L947). When trying to use `axis=0` to align a `Series` of lower/upper values, the following error message is provided:

```
NotImplementedError: `axis is not yet supported in clip`
```

**Additional context**
I have no reason to believe that `axis=0` is a high priority. However, I'm raising this issue for tracking purposes.
Feel free to close if there is already a duplicate issue for (a quick search came up empty).
",2023-11-07T17:42:51Z,0,0,Richard (Rick) Zamora,@NVIDIA,True
665,[FEA] Respect `set_output_as_binary` in ORC writer,"**Is your feature request related to a problem? Please describe.**
The cuDF ORC writer does not follow the `set_output_as_binary` option added for Parquet to write a string column with binary type.

**Describe the solution you'd like**
To respect the `set_output_as_binary` option added in #6816 for the ORC writer as well.

**Describe alternatives you've considered**
We could copy the table to host and use the Arrow ORC writer or similar instead, but this would mean more copying.

**Additional context**
Quick demo using libcudf 23.10: https://github.com/lidavidm/cudf-orc-binary-feature-request",2023-11-08T14:58:57Z,0,0,David Li,,False
666,[FEA] Allow capturing cudf specific exceptions in `cudf.pandas` by using ExceptionGroup in Python 3.11,"**Is your feature request related to a problem? Please describe.**
Currently when using `cudf.pandas`,  there's no way to capture or act on the cudf exception thrown whether the operation succeeds or fails on during the pandas path.

```python
In [1]: %load_ext cudf.pandas

In [2]: import pandas as pd

In [3]: try:
   ...:    pd.to_datetime(""2020-01-01"", utc=True)
   ...: except NotImplementedError:
   ...:     print(""hello"")

In [4]: import cudf

In [5]: cudf.to_datetime(""2020-01-01"", utc=True)
NotImplementedError: utc is not yet implemented
```

**Describe the solution you'd like**

If `_fast_slow_function_call` was structured like:

```python
In [8]: def f():
   ...:     try:
   ...:         raise NotImplementedError(""fast doesn't work"")
   ...:     except Exception as err_fast:
   ...:         try:
   ...:             raise ValueError(""Slow doesn't work"")
   ...:         except Exception as err_slow:
   ...:             raise ExceptionGroup(""Fast and slow did't work"", [err_fast, err_slow])
   ...: 

In [9]: f()
Traceback (most recent call last):
  File ""<ipython-input-8-f0c78d1a1137>"", line 6, in f
    raise ValueError(""Slow doesn't work"")
ValueError: Slow doesn't work

During handling of the above exception, another exception occurred:

  + Exception Group Traceback (most recent call last):
  |   File ""/opt/miniconda3/envs/pandas-dev/lib/python3.11/site-packages/IPython/core/interactiveshell.py"", line 3548, in run_code
  |     exec(code_obj, self.user_global_ns, self.user_ns)
  |   File ""<ipython-input-9-c43e34e6d405>"", line 1, in <module>
  |     f()
  |   File ""<ipython-input-8-f0c78d1a1137>"", line 8, in f
  |     raise ExceptionGroup(""Fast and slow did't work"", [err_fast, err_slow])
  | ExceptionGroup: Fast and slow did't work (2 sub-exceptions)
  +-+---------------- 1 ----------------
    | Traceback (most recent call last):
    |   File ""<ipython-input-8-f0c78d1a1137>"", line 3, in f
    |     raise NotImplementedError(""fast doesn't work"")
    | NotImplementedError: fast doesn't work
    +---------------- 2 ----------------
    | Traceback (most recent call last):
    |   File ""<ipython-input-8-f0c78d1a1137>"", line 6, in f
    |     raise ValueError(""Slow doesn't work"")
    | ValueError: Slow doesn't work
    +------------------------------------

In [10]: try:
    ...:     f()
    ...: except* NotImplementedError:
    ...:     print(""Try doing something else"")
    ...: 
Try doing something else
  + Exception Group Traceback (most recent call last):
  |   File ""/opt/miniconda3/envs/pandas-dev/lib/python3.11/site-packages/IPython/core/interactiveshell.py"", line 3548, in run_code
  |     exec(code_obj, self.user_global_ns, self.user_ns)
  |   File ""<ipython-input-10-e0e980fb6b7c>"", line 2, in <module>
  |     f()
  |   File ""<ipython-input-8-f0c78d1a1137>"", line 8, in f
  |     raise ExceptionGroup(""Fast and slow did't work"", [err_fast, err_slow])
  | ExceptionGroup: Fast and slow did't work (1 sub-exception)
  +-+---------------- 1 ----------------
    | Traceback (most recent call last):
    |   File ""<ipython-input-8-f0c78d1a1137>"", line 3, in f
    |     raise NotImplementedError(""fast doesn't work"")
    | NotImplementedError: fast doesn't work
    | 
    | During handling of the above exception, another exception occurred:
    | 
    | Traceback (most recent call last):
    |   File ""<ipython-input-8-f0c78d1a1137>"", line 6, in f
    |     raise ValueError(""Slow doesn't work"")
    | ValueError: Slow doesn't work
    +------------------------------------
```

**Additional context**

Admittedly I don't have a definitive use case where capturing the cudf exception is necessary, and using ExceptionGroup might break ""drop in replacement"" potential of `cudf.pandas`, but noting that this is a limitation in the current design  
",2023-11-08T23:12:38Z,0,0,Matthew Roeschke,@rapidsai ,True
667,[FEA] need multibyte_split support stream ,"**Is your feature request related to a problem? Please describe.**
want to use cuda multi stream (pool)  read a big file.

**Describe the solution you'd like**
just add a new `multibyte_split` method with  ` rmm::cuda_stream_view stream` param  to public use.

head: https://github.com/rapidsai/cudf/blob/branch-23.12/cpp/include/cudf/io/text/multibyte_split.hpp
```
std::unique_ptr<cudf::column> multibyte_split(
  data_chunk_source const& source,
  std::string const& delimiter,
  parse_options options               = {},
  rmm::cuda_stream_view stream = cudf::get_default_stream(),
  rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());
```

src: https://github.com/rapidsai/cudf/blob/branch-23.12/cpp/src/io/text/multibyte_split.cu
```
std::unique_ptr<cudf::column> multibyte_split(cudf::io::text::data_chunk_source const& source,
                                              std::string const& delimiter,
                                              parse_options options,
                                              rmm::cuda_stream_view stream,
                                              rmm::mr::device_memory_resource* mr)
{
  auto result = detail::multibyte_split(
    source, delimiter, options.byte_range, options.strip_delimiters, stream, mr);

  return result;
}
```

**Describe alternatives you've considered**

**Additional context**
",2023-11-09T13:50:28Z,0,0,weedge,,False
668,[QST] Respecting pandas options that affect default behaviors,"**What is your question?**
How should pandas global options interact with cudf.pandas? These can change the behavior/result of pandas operations _without_ changing the call being made.

For example, pandas has a `use_inf_as_na` option that makes inf behave like nulls. Should the pandas-path be forced if cudf.pandas detects a global option that isn't supported?

```python
# with %load_ext cudf.pandas

In [3]: ser = pd.Series([np.inf, np.nan])

In [4]: ser
Out[4]:
0     Inf
1    <NA>
dtype: float64

# Incorrect
In [5]: ser.dropna()
Out[5]:
0    inf
dtype: float64

In [6]: pd.options.mode.use_inf_as_na = True

In [7]: ser
Out[7]:
0     Inf
1    <NA>
dtype: float64

In [8]: ser.dropna()
Out[8]:
0    inf
dtype: float64

# if we try setting this option in a non-cudf supported way we get a different (incorrect) answer
In [10]: with pd.option_context(""use_inf_as_na"", True):
    ...:     print(ser.dropna())
    ...:
0   NaN
dtype: float64
```

The pandas result is:

```
>>> import pandas as pd; import numpy as np
>>> ser = pd.Series([np.inf, np.nan])
>>> pd.options.mode.use_inf_as_na = True
>>> ser.dropna()
Series([], dtype: float64)
```",2023-11-14T13:57:47Z,0,0,Ashwin Srinath,Voltron Data,False
669,Report results of running pandas unit tests in CI and fail the job when necessary,"Every PR runs a job called `pandas-tests` that run the Pandas unit tests using `cudf.pandas`. Currently, there are two issues with this job:

- the results of those tests aren't reported in the job summary or anywhere convenient to look at
- the job always passes, regardless of how many tests were passed or failed

Ideally, we should post the results as a job summary and fail the job if the number of tests passed relative to the development branch falls below a threshold (say, 0.1%). ",2023-11-14T14:24:16Z,0,0,Ashwin Srinath,Voltron Data,False
670,[QST] How how to expose __dict__ when `cudf.pandas` is enabled?,"
Some of the pandas tests spelunk the `__dict__` of a module to determine what to collect. E.g. `tests/dtypes/test_generic.py` does:
```
from pandas.core.dtypes import generic as gt
...
    @pytest.mark.parametrize(""abctype"", [e for e in gt.__dict__ if e.startswith(""ABC"")])
    def test_abc_coverage(self, abctype):
```

with `cudf.pandas` enabled:
```
from pandas.core.dtypes import generic as gt
gt.__dict__.keys() # does _not_ contain ABCCategorical
```
",2023-11-14T14:35:41Z,0,0,Ashwin Srinath,Voltron Data,False
671,Groupby hash aggregations use sort-based implementation if nested-type columns are used as values,"We should be able to use nested-type columns as values and still be able to invoke a hash-based groupby, as hash-based is generally faster so we do not want to be silently using sort-based. https://github.com/rapidsai/cudf/blob/abc0d41d1d9033d581948ae19384e0aa0f33da77/cpp/src/groupby/hash/groupby.cu#L654-L656

Reference thread: https://github.com/rapidsai/cudf/pull/13795#discussion_r1373454172",2023-11-14T19:44:30Z,1,0,Divye Gala,,False
672,`read_json` does not compile if using `std::string_view` instead of `std::string`,"Reference thread: https://github.com/rapidsai/cudf/pull/13795#discussion_r1373437533

Error:
```/home/nfs/dgala/cudf/cpp/examples/nested_types/deduplication.cpp: In function 'cudf::io::table_with_metadata read_json(std::string_view)':
/home/nfs/dgala/cudf/cpp/examples/nested_types/deduplication.cpp:66:52: error: no matching function for call to 'cudf::io::source_info::source_info(std::string_view&)'
   66 |   auto source_info = cudf::io::source_info(filepath);```",2023-11-14T19:47:51Z,0,0,Divye Gala,,False
673,[FEA] The C++ tests for parquet don't test row group selection very well.,"
There's only a very basic row group selection test in the C++ gtests.  It would probably be useful to have a more thorough set of tests.",2023-11-15T21:29:37Z,0,0,,,False
674,[BUG] Index accessors wrongly in `dir` for Index subclasses in `cudf.pandas`,"**Describe the bug**
Index accessors wrongly in `dir` for Index subclasses in `cudf.pandas`.

I think this is because the proxy Index subclasses objects have `bases=(Index,)` which has the accessors as additional attributes.

**Steps/Code to reproduce bug**
```python
In [1]: %load_ext cudf.pandas

In [2]: import pandas as pd

In [3]: ""str"" in dir(pd.RangeIndex(2))
Out[3]: True
```
**Expected behavior**
```python
In [1]: import pandas as pd

In [2]: ""str"" in dir(pd.RangeIndex(2))
Out[2]: False
```

**Environment overview (please complete the following information)**
 - Environment location: Bare-metal
 - Method of cuDF install: from source

**Environment details**
Please run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details

**Additional context**
Add any other context about the problem here.
",2023-11-15T21:58:07Z,0,0,Matthew Roeschke,@rapidsai ,True
675,[BUG] Can't initialize woodwork on a cudf.pandas dataframe (featuretools integration),"Currently, we can't initialize [woodwork](https://github.com/alteryx/woodwork) on a cudf.pandas dataframe. This is a core step in getting things working with [featuretools](https://github.com/alteryx/featuretools) (woodwork provides the underlying type system and metadata management).

During the `init` call, we end up with a raw cuDF dataframe somewhere. It's not immediately clear why from a quick debug. Woodwork is storing a weak reference to the dataframe ([here,](https://github.com/alteryx/woodwork/blob/ff80e32362afb8067a4b5225a21834c762170db7/woodwork/table_accessor.py#L393-L395) and [here](https://github.com/alteryx/woodwork/blob/ff80e32362afb8067a4b5225a21834c762170db7/woodwork/table_accessor.py#L46)) but cudf.pandas correctly operates with weakref (in general). Perhaps it's something about the way woodwork is [injecting itself into the pandas object during import](https://github.com/alteryx/woodwork/blob/ff80e32362afb8067a4b5225a21834c762170db7/woodwork/__init__.py#L27-L36)


```
mamba create -n featuretools -c rapidsai-nightly -c conda-forge -c nvidia cudf=23.10 python=3.10 cuda-version=11.8 featuretools jupyterlab
```

```python
%load_ext cudf.pandas
import pandas as pd
import woodwork as ww

df = pd.read_csv(""https://oss.alteryx.com/datasets/online-retail-logs-2018-08-28.csv"")
df.ww.init(name='retail')
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Input In [2], in <cell line: 5>()
      2 import woodwork as ww
      4 df = pd.read_csv(""https://oss.alteryx.com/datasets/online-retail-logs-2018-08-28.csv"")
----> 5 df.ww.init(name='retail')

File ~/miniconda3/envs/featuretools/lib/python3.10/site-packages/woodwork/table_accessor.py:101, in WoodworkTableAccessor.init(self, **kwargs)
     49 def init(self, **kwargs):
     50     """"""Initializes Woodwork typing information for a DataFrame with a partial schema.
     51 
     52     Logical type priority:
   (...)
     99             Any errors resulting from skipping validation with invalid inputs may not be easily understood.
    100     """"""
--> 101     self.init_with_partial_schema(**kwargs)

File ~/miniconda3/envs/featuretools/lib/python3.10/site-packages/woodwork/table_accessor.py:237, in WoodworkTableAccessor.init_with_partial_schema(self, schema, index, time_index, logical_types, ignore_columns, already_sorted, name, semantic_tags, table_metadata, column_metadata, use_standard_tags, column_descriptions, column_origins, null_invalid_values, validate, **kwargs)
    234         existing_use_standard_tags[col_name] = col_schema.use_standard_tags
    236 # overwrite schema parameters with specified kwargs
--> 237 logical_types = _infer_missing_logical_types(
    238     self._dataframe,
    239     logical_types,
    240     existing_logical_types,
    241     ignore_columns,
    242     null_invalid_values=null_invalid_values,
    243 )
    244 column_descriptions = {
    245     **existing_col_descriptions,
    246     **(column_descriptions or {}),
    247 }
    248 column_metadata = {**existing_col_metadata, **(column_metadata or {})}

File ~/miniconda3/envs/featuretools/lib/python3.10/site-packages/woodwork/table_accessor.py:1851, in _infer_missing_logical_types(dataframe, force_logical_types, existing_logical_types, ignore_columns, null_invalid_values)
   1849 elif name is None and len(dataframe.columns) == 1:
   1850     series = dataframe.iloc[:, 0]
-> 1851 parsed_logical_types[name] = _get_column_logical_type(
   1852     series,
   1853     logical_type,
   1854     name,
   1855 )
   1856 updated_series = parsed_logical_types[name].transform(
   1857     series,
   1858     null_invalid_values=null_invalid_values,
   1859 )
   1860 if updated_series is not series:
   1861     # NotImplementedError thrown by dask when attempting to re-initialize
   1862     # data after being assigned a numeric column name

File ~/miniconda3/envs/featuretools/lib/python3.10/site-packages/woodwork/utils.py:419, in _get_column_logical_type(series, logical_type, name)
    417     return _parse_logical_type(logical_type, name)
    418 else:
--> 419     return ww.type_system.infer_logical_type(series)

File ~/miniconda3/envs/featuretools/lib/python3.10/site-packages/woodwork/type_sys/type_system.py:378, in TypeSystem.infer_logical_type(self, series)
    376     series = series.to_pandas()
    377 else:
--> 378     raise ValueError(
    379         f""Unsupported series type `{type(series)}`"",
    380     )  # pragma: no cover
    382 # For dask or spark collections, unknown type special case comes
    383 # *after* head calls to avoid evaluating a potentially large
    384 # dataset
    385 if series.count() == 0:

ValueError: Unsupported series type `<class 'cudf.core.series.Series'>`
```

```
conda list | grep ""cudf\|featuretools\|pandas""
# packages in environment at /home/nicholasb/miniconda3/envs/featuretools:
cudf                      23.12.0a703              pypi_0    pypi
featuretools              1.28.0             pyhd8ed1ab_0    conda-forge
libcudf                   23.12.00a703    cuda11_231116_g53127de4d9_703    rapidsai-nightly
pandas                    1.5.3                    pypi_0    pypi
```",2023-11-16T19:31:56Z,0,0,Nick Becker,@NVIDIA,True
676,[DOC] The C++ `clang-format` style does not have copyright information,I just noticed that the `.clang-format` style config doesn't have any copyright information (originally reported by @jlowe). And our CI system doesn't detect such issue for files started by a dot (`.`).,2023-11-21T19:42:46Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
677,[FEA] Parquet reader:  replace skip_rows / num_rows with start_row / end_row,"
Our external interface to the parquet reader allows the user to specify `skip_rows` / `num_rows` parameters when calling it.  Internally, we use the same values.  But it is a very unwieldy way to think about things.  I think it would be easier to immediately convert those values to `start_row` and `end_row` and use that everywhere.  It's a nontrivial amount of work to do this without causing bugs but I think the code would be more natural (in the std::algorithms / iterator sense of the word).",2023-11-21T20:35:55Z,0,0,,,False
678,[FEA] Check dtype requirements on multiindex codes,"**Is your feature request related to a problem? Please describe.**

(Seen as part of a review of #14470).

Multiindex codes and levels are effectively a categorical encoding of the columns of the multiindex entries. The codes are used to index the levels. As such, they should probably have type equivalent to `cudf::size_type`. Currently, however, they are a int64. This is a larger memory footprint than necessary. Moreover, it (in some constructor circumstances) necessitates more copies than necessary.

**Describe the solution you'd like**

Use correct dtype. Since the public `codes` and `levels` properties wrap the results in pandas `FrozenList` objects to mimic the pandas API, it may be possible to just store the codes/levels pairs as `CategoricalColumn`s internally, rather than the current structure.

**Describe alternatives you've considered**

n/a

**Additional context**

n/a",2023-11-22T14:23:32Z,0,0,Lawrence Mitchell,,False
679,[ENH] Audit cudf APIs for use of inappropriate algorithms,"**Is your feature request related to a problem? Please describe.**

Historically (I think) certain features were available in libcudf before others. For example, while hash joins appeared quite early on `cudf::contains` was only factored out of the semi join infrastructure in #11100.

As a result, there are a number of places in cudf where an API was implemented using a sub-optimal approach (be that in terms of memory footprint or performance) just because it was needed in the Python API.

For example in #14478, we replace a sub-optimal (in both memory _and_ performance) inner join, with a call to `cudf::contains` now that it is available.

**Describe the solution you'd like**

We should go through and check for other instances of this historical anti-pattern and either:

- replace with calls to appropriate (existing) libcudf primitives
- gather feature requests for new libcudf primitives based on the usage we observe.

**Describe alternatives you've considered**

n/a

**Additional context**

It is probable that candidates can be found by looking calls to `merge` in the cudf codebase. As well as argsort/scatter/gather patterns (that's #13557).
```[tasklist]
### Tasks
- [ ] https://github.com/rapidsai/cudf/issues/13557
- [ ] https://github.com/rapidsai/cudf/pull/14478
- [ ] https://github.com/rapidsai/cudf/issues/14480
- [ ] https://github.com/rapidsai/cudf/issues/14485
- [ ] https://github.com/rapidsai/cudf/issues/14486
- [ ] https://github.com/rapidsai/cudf/issues/13630
- [ ] https://github.com/rapidsai/cudf/issues/13565
- [ ] https://github.com/rapidsai/cudf/issues/13456
- [ ] https://github.com/rapidsai/cudf/issues/14487
```
",2023-11-22T18:58:02Z,2,0,Lawrence Mitchell,,False
680,[FEA] Remove special-case implementation of `MultiIndex.isin`,"**Is your feature request related to a problem? Please describe.**

`isin` is implemented ""by-hand"" for `MultiIndex` by going via the frame representation and then calling `merge`. This means that any updates to correctness/performance of `DataFrame.isin` must be hand-ported to the `MultiIndex` case.

**Describe the solution you'd like**

`MultiIndex.isin` should do necessary pre-/post-processing and call `DataFrame.isin` rather than re-implementing the core algorithm.

**Describe alternatives you've considered**

n/a

",2023-11-22T19:05:26Z,0,0,Lawrence Mitchell,,False
681,[FEA] Check why we need `__iter__` special overrides for `cudf.pandas`,"**Describe the FEA**
We currently have __iter__ overrides for all proxy objects like: https://github.com/rapidsai/cudf/blob/branch-24.02/python/cudf/cudf/pandas/_wrappers/pandas.py#L159, This is a feature request to get to the bottom of it and see if we can completely eliminate this special handling.

",2023-11-22T19:22:43Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
682,[BUG] Global variables not accessible/modifiable when falling back to slow path,"**Describe the bug**
```python
In [5]: %load_ext cudf.pandas

In [6]: import cudf as pd

In [7]: pd.Series(range(2)).apply(udf)
.
.
ValueError: user defined function compilation failed.

In [8]: lst
Out[8]: []

In [9]: import pandas

In [10]: pandas.Series(range(2)).apply(udf)
Out[10]: 
0    0
1    1
dtype: int64

In [11]: lst
Out[11]: []

In [12]: 
```

",2023-11-22T19:38:17Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
683,[FEA] Implement column-wise hashes,"**Is your feature request related to a problem? Please describe.**

cudf columns are mutable and therefore do not (or should not) implement `__hash__` (in the same way that numpy arrays do not do so).

_However_, there are circumstances under which we would nonetheless like to be able to compute a hash of a column:

1. When the wrapping object is actually an immutable one (for example `Index` objects) and so `__hash__` is safe;
2. When tokenizing keys for dask task graphs (see https://github.com/rapidsai/cudf/pull/13695), where the objects may be mutable, but the required semantics are ""two objects that compare equal should hash the same"". This enables dask to perform some amount of optimisation on the task graph for repeated execution and task merging.

**Describe the solution you'd like**

I would like to be able to hash a column with a libcudf call and receive a single $k$-bit hash. The first point above does not need to worry excessively about collisions, and python hash values are 64bit ints, so a 64-bit murmur- or xx-hash is likely sufficient. For dask, collisions are more problematic, so a 128-bit md5 would be better (this is what dask uses for pandas dataframes).

**Describe alternatives you've considered**

Compute row-wise hashes of columns (on dataframes) to produce a single column of hashes and then copy to host to hash there.",2023-11-23T11:09:38Z,0,0,Lawrence Mitchell,,False
684,[PERF/ENH] `Series.map` sorts a larger dataset than it needs to,"`Series.map` which substitutes values in `self` that match some key with its corresponding value does:
```
            lhs = cudf.DataFrame({""x"": self, ""orig_order"": arange(len(self))})
            rhs = cudf.DataFrame(
                {
                    ""x"": arg.keys(),
                    ""s"": arg.values(),
                    ""bool"": full(len(arg), True, dtype=self.dtype),
                }
            )
            res = lhs.merge(rhs, on=""x"", how=""left"").sort_values(
                by=""orig_order""
            )
            result = res[""s""]
            result.name = self.name
            result.index = self.index
```

So the order is the same as the input.

This has two pessimisations:

1. In pandas-compat mode (since #14428) this merge doesn't need sorting
2. Since we only return `s`, we can get away with `sort_by_key` of `res[""s""]` rather than sorting a multi-column dataframe",2023-11-23T15:12:00Z,0,0,Lawrence Mitchell,,False
685,[PERF/ENH] `Index.intersection` does more hashing work than necessary,"Index intersection performs an inner merge of the unique values of the left and right indices (the unique is done so that indices with repeated values don't blow up the memory footprint). This does a full hash of both indices, then the merge (hashing again). Finally, if requested, the result is sorted.

This could be replaced, I think with positive performance effect by either:

- `leftsemi` join + `drop_duplicates`
- `libcudf.search.contains` + `apply_boolean_mask` + `drop_duplicates`

One would have to think through the consequences of either of these wrt any ordering guarantees we might want when `sort=False` (possibly gated behind pandas-compat mode).

This applies _mutatis mutandis_ to `MultiIndex.intersection` too.",2023-11-23T17:08:58Z,0,0,Lawrence Mitchell,,False
686,[BUG] `Index.union` does not match pandas for indexes with duplicate entries,"**Describe the bug**

Pandas treats, for `Index.union` only, indexes with duplicate entries as [multisets](https://en.wikipedia.org/wiki/Multiset#Basic_properties_and_operations), for which the union operation produces multiplicities that are the max of the left and right multiplicities. I've asked for clarification of this here https://github.com/pandas-dev/pandas/issues/56137, since it is _only_ `union` that uses the multiset definitions.

cudf, in contrast, performs the union as an outer join. Which produces multiplicities that are the product of the multiplicities of the left and right indexes (with identity for missing values of 1). This matches the pandas behaviour in the case where all entries have multiplicity greater than one in exactly one of the left or right indexes. However, if an entry has a multiplicity larger than one in both left and right indexes, we get the wrong answer.

**Steps/Code to reproduce bug**

```
import cudf
import pandas as pd

left = pd.Index([1, 1])
right = pd.Index([1, 2, 1, 1])
print(left.union(right))

cleft = cudf.from_pandas(left)
cright = cudf.from_pandas(right)

print(cleft.union(cright))
# Int64Index([1, 1, 1, 2], dtype='int64')
# Int64Index([1, 1, 1, 1, 1, 1, 2], dtype='int64')
```

**Expected behavior**

Match pandas.

This can be done with `value_counts`/`merge`/`repeat` in some combo, but there's probably a slightly smarter way.

**Notes**

Also applies to `MultiIndex`.",2023-11-23T17:23:39Z,0,0,Lawrence Mitchell,,False
687,[FEA] Remove inconsistencies in cython wrappers when  handling order/null-precedence,"Where libcudf search/sort functions accept a `table_view` as input, one must specify the order (ascending or descending) and null precedence (beginning or end) as a `std::vector` of length equal to the number of columns in the `table_view` (or else an empty such vector to used libcudf's defaults).

In the cudf cython wrapping of these functions we are, in contrast, inconsistent in the way we handle these arguments. Some functions accept a list for both order and null precedence (e.g. `libcudf.sort.sort`); some accept a list for order but only a single value for null precedence (e.g. `libcudf.sort.order_by`); some accept a list for neither order nor null precedence (e.g. `libcudf.search.search_sorted`).

This should be cleaned up and all such functions should uniformly accept a list for both order and null precedence. Higher-level python functions that operate on single columns and call the table interfaces should be responsible for any argument munging.
",2023-11-24T14:53:40Z,0,0,Lawrence Mitchell,,False
688,[BUG] codecov doesn't include tests run in `cudf_pandas_tests/` when generating report,"The codecov report generated in every PR doesn't consider the tests in `cudf_pandas_tests/` when generating its coverage report.  To fix this, we should update our invocation here:

https://github.com/rapidsai/cudf/blob/branch-24.02/ci/cudf_pandas_scripts/run_tests.sh#L52

to match the ones we use when running other python tests:

https://github.com/rapidsai/cudf/blob/branch-24.02/ci/test_python_cudf.sh#L17

(in particular, all the `--cov*` arguments)



",2023-11-27T17:31:24Z,0,0,Ashwin Srinath,Voltron Data,False
689,[FEA] Enable cuDF spilling in cudf.pandas by default,"It's fairly common for workflows to involve processing 1-2 large tables (e.g., financial transaction data, website clicks, or sensor readings) and 1-2 smaller tables (e.g., customer, stock, store, or sensor IDs and attributes).

We'd like to enable people using pandas for these workloads today to seamlessly accelerate their code, but these kinds of workflows are at risk of running out of memory due to often requiring multiple dataframes to be active at once.

For example, loading the following CSV file containing ~12.5 GB of data (19 GB on disk) requires a peak of about 36 GB of GPU memory. If a user's workflow will have already created nearly any cuDF objects, they'll be at high risk of running out of memory on a 40 GB A100.

```python
import cudf
import pandas as pd
N = 260000000
K = 5

dtypes = {f""x{i}"": float for i in range(K)}
dtypes[""id""] = int

df = cudf.datasets.randomdata(
    nrows=N,
    dtypes=dtypes,
    seed=12,
)
print(df.memory_usage().sum() / 1e9)
df.to_csv(""df1.csv"", chunksize=1e7)
12.48
```

```python
import cudf
df = cudf.read_csv(""df1.csv"")
# Baseline GPU memory: 417.00 MB. Peak GPU memory: 35835.00 MB
```

As cudf.pandas already uses the CPU when a workflow would otherwise throw an error in cuDF, we should explore enabling cuDF spilling to potentially prevent the out-of-memory errors arising from operations that would have succeeded if not for other cuDF objects in memory.",2023-11-27T17:53:38Z,0,0,Nick Becker,@NVIDIA,True
690,[FEA] cudf.pandas profiler should track and report the time spend transferring data between the CPU/GPU,"The cudf.pandas profiler provides insight into which operations ran on the GPU and which ran on the CPU (and how many times), but it doesn't record the number of CPU/GPU transfers (and how long they took). The profiler should track and report the time spend transferring data between the CPU/GPU.

This is useful information for both profiling workflows, and seeing where we might be doing something wrong.",2023-11-27T18:12:14Z,1,0,Nick Becker,@NVIDIA,True
691,[QST] cudf.pandas prefer using CPU over GPU in some cases,"Hi,
I'm trying to move from a basic pandas to cudf.pandas and I faced with the issue. It's not clear how cudf decides to use **CPU** or **GPU** in calculations.
Here is the example when I have a dataframe with around 280kk rows and 9 columns.
The steps:
1) I perform `.groupby.sum() `for the original df. I takes too much time and the profiler show that all calculations were on **CPU** not GPU.
2) I cut df like `[:100000000]` so that there are 100kk rows left.
3)  I perform `.groupby.sum() `for the modified df and... it takes 0.1 sec and the profiler says **GPU** was using for that.

So, here is some question.
- what's the reason that 100kk df is being calculated on GPU and 280kk df on CPU? Hard to belive that the size is the reason.
- If not the size then what's the criteria for that?

Thanks in advance.
p.s. I also tried `.sort_values()` and there were the same.

```
COM_ORDER_LINE.shape
(284125143, 9)
```
```
COM_ORDER_LINE.head()

CODE | ORDER_CODE | VERSION_CODE | ID_WARE | QTY_ORDERED | CATALOG_PRICE | PRICE | TO_PAY | DISCOUNT_TOTAL
10000006215177 | 10000006215175 | 10000006215176 | 1.787585e+11 | 1 | 3799.0 | 2659.0 | 2659.0 | 1140.0
10000006215189 | 10000006215187 | 10000006215188 | 1.736505e+11 | 1 | 9999.0 | 6999.0 | 6999.0 | 3000.0
10000006215364 | 10000006215362 | 10000006215363 | 1.736709e+11 | 1 | 1399.0 | 980.0 | 980.0 | 419.0
```
```
%%cudf.pandas.profile
df=COM_ORDER_LINE.groupby(['ID_WARE'])['PRICE'].sum()
```


```
Total time elapsed: 31.764 seconds                                    
                                          0 GPU function calls in 0.000 seconds                                   
                                          3 CPU function calls in 23.186 seconds                                  
                                                                                                                  
                                                          Stats                                                   
                                                                                                                  
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓
┃ Function                     ┃ GPU ncalls ┃ GPU cumtime ┃ GPU percall ┃ CPU ncalls ┃ CPU cumtime ┃ CPU percall ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩
│ DataFrame.groupby            │ 0          │ 0.000       │ 0.000       │ 1          │ 2.929       │ 2.929       │
│ DataFrameGroupBy.__getitem__ │ 0          │ 0.000       │ 0.000       │ 1          │ 2.915       │ 2.915       │
│ SeriesGroupBy.sum            │ 0          │ 0.000       │ 0.000       │ 1          │ 17.341      │ 17.341      │
└──────────────────────────────┴────────────┴─────────────┴─────────────┴────────────┴─────────────┴─────────────┘

Not all pandas operations ran on the GPU. The following functions required CPU fallback:

- DataFrame.groupby
- DataFrameGroupBy.__getitem__
- SeriesGroupBy.sum
```

```
COM_ORDER_LINE_100KK = COM_ORDER_LINE[:100000000]
COM_ORDER_LINE_100KK.shape
(100000000, 9)
```

```
%%cudf.pandas.profile
df=COM_ORDER_LINE_100KK.groupby(['ID_WARE'])['PRICE'].sum()
```

```
Total time elapsed: 0.109 seconds                                     
                                          3 GPU function calls in 0.082 seconds                                   
                                          0 CPU function calls in 0.000 seconds                                   
                                                                                                                  
                                                          Stats                                                   
                                                                                                                  
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓
┃ Function                     ┃ GPU ncalls ┃ GPU cumtime ┃ GPU percall ┃ CPU ncalls ┃ CPU cumtime ┃ CPU percall ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩
│ DataFrame.groupby            │ 1          │ 0.000       │ 0.000       │ 0          │ 0.000       │ 0.000       │
│ DataFrameGroupBy.__getitem__ │ 1          │ 0.001       │ 0.001       │ 0          │ 0.000       │ 0.000       │
│ SeriesGroupBy.sum            │ 1          │ 0.081       │ 0.081       │ 0          │ 0.000       │ 0.000       │
└──────────────────────────────┴────────────┴─────────────┴─────────────┴────────────┴─────────────┴─────────────┘
```

",2023-11-27T18:14:30Z,0,0,Andrey Komrakov,Sportmaster Ltd.,False
692,[DOC] Document cuDF options,"## Report needed documentation

**Report needed documentation**
Currently options are simply documented by embedding the output of `cudf.describe_option` directly in our docs: https://docs.rapids.ai/api/cudf/stable/user_guide/api_docs/options/#api-options. This approach is not very informative.

**Describe the documentation you'd like**
We should instead document individual options separately. [The corresponding pandas page](https://pandas.pydata.org/docs/user_guide/options.html) does not document all options in detail, but it does include [extended information on the frequently used options](https://pandas.pydata.org/docs/user_guide/options.html#frequently-used-options). We should mimic that approach and focus on documenting the most important options.
",2023-11-27T18:15:41Z,0,0,Vyas Ramasubramani,@rapidsai,True
693,[FEA] Enable tracing of proxied objects in cudf.pandas,"**Is your feature request related to a problem? Please describe.**
Due to the wrapping behavior in cudf.pandas it can be very difficult to debug issues, and even more challenging to identify performance bottlenecks that arise from seemingly trivial functions triggering highly nontrivial behavior under the hood due to wrapped function calls.

**Describe the solution you'd like**
It would be very helpful if the proxy objects supported tracing of proxied function calls that show when and how often arguments are being transferred from slow to fast libs and vice versa. This trace would give visibility into the true stack trace rather than the obfuscated trace produced by inspecting the proxied call stack. ",2023-11-27T18:47:15Z,0,0,Vyas Ramasubramani,@rapidsai,True
694,Operations between cudf.Series and pd.NaT succeed when they shouldn't,"**Describe the bug**
Operations between pd.NaT and non-timestamp types succeed, whereas 

**Steps/Code to reproduce bug**
```
In [3]: cudf.Series([1, 2, 3]) > pd.Timestamp(""nat"")
Out[3]:
0    <NA>
1    <NA>
2    <NA>
dtype: bool
```

**Expected behavior**
This comparison should fail like it does with pandas:
```
In [6]: pd.Series([1, 2, 3]) > pd.Timestamp(""nat"")
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[6], line 1
----> 1 pd.Series([1, 2, 3]) > pd.Timestamp(""nat"")
...
TypeError: '>' not supported between instances of 'numpy.ndarray' and 'NaTType'
```",2023-11-27T19:18:02Z,0,0,Vyas Ramasubramani,@rapidsai,True
695,[BUG] `np.<ufunc>()` returns `None` when passed a `cudf.MultiIndex`,"**Describe the bug**
`np.exp` silently returns `None` for `cudf.MultiIndex`

**Steps/Code to reproduce bug**
```
In [1]: import numpy as np
In [2]: import cudf
In [3]: np.exp(cudf.MultiIndex.from_tuples([(1, 2), (3, 4)]))  # returns None
```

**Expected behavior**
This code should raise an error, which is what happens with a `pandas.MultiIndex`
```
In [4]: pandas as pd
In [5]: np.exp(pd.MultiIndex.from_tuples([(1, 2), (3, 4)]))
...
TypeError: loop of ufunc does not support argument 0 of type tuple which has no callable exp method
```",2023-11-27T19:26:59Z,0,0,Vyas Ramasubramani,@rapidsai,True
696,[FEA] Add GZIP compression support to parquet writer,"**Is your feature request related to a problem? Please describe.**
The parquet format in Apache Spark supports many compression codecs ([link](https://spark.apache.org/docs/2.4.3/sql-data-sources-parquet.html#configuration)), including: none, uncompressed, snappy, gzip, lzo, brotli, lz4, zstd. 

cuDF has both internal implementation and an nvCOMP integration to provide compression and decompression codecs.  For the parquet format, GZIP compression is [DEFLATE](https://developer.nvidia.com/blog/accelerating-load-times-for-directx-games-and-apps-with-gdeflate-for-directstorage/) plus a header. nvCOMP does not support the deflate version with this header, so the reader still uses the internal gzip decompression implementation. We don't have internal gzip compression implementation. To support GZIP in the PQ writer we would need to use nvCOMP GDEFLATE codec + write the header on our own.

**Describe the solution you'd like**
Add support for GZIP compressioning to the cuDF parquet writer by adding a header writing implementation and using nvCOMP deflate.

**Describe alternatives you've considered**
n/a

**Additional context**
Also see Spark-RAPIDS request here: https://github.com/NVIDIA/spark-rapids/issues/9718
",2023-11-28T05:27:35Z,0,0,Gregory Kimball,,False
697,[FEA] Expose `negative_index_policy` from `cudf::detail::gather` in public `cudf::gather` API,"**Is your feature request related to a problem? Please describe.**

The libcudf detail API for `gather` allows one to separately specify how negative indices are handled (whether or not a negative index `i` should map to `i + length(column)`) and how out of bounds indices are handled (whether they should be nullified or not checked for).

In contrast, the public API only allows specifying the policy for out-of-bounds indices explicitly, whether or not negative indices are wrapped is a function of the signedness of the input map column: if the map is an unsigned type, then ""negative"" indices are not allowed, if the type is signed then wrapping occurs.

In cudf, there are cases where we ingest data where pandas specifies that the marker for ""missing"" data is `-1`. For example `MultiIndex` construction can take `levels` and `codes` where `codes` indexes levels and `-1` means ""produce a null"". Right now we use `cudf::gather` to produce the indexed levels, but must first pre-process the `codes` column to replace `-1` with an actually out of bounds size type. This requires an extra pass over the input (and copy to set values).

Whenever we perform a join, we obtain gather maps from libcudf which store signed entries (`cudf::size_type`). The entries are guaranteed to either be positive and in-bounds or the sentinel value `std::numeric_limits<size_type>::min()` indicating that an output row should be nullified. Despite this knowledge, since the column we're using for the gather map is a signed type, we have no way of performing the gather without paying the cost of wrapping negative indices, which we _know_ will be a no-op.

**Describe the solution you'd like**
I'd like to be able to specify the treatment of negative indices independently of the signedness of the gather map when calling `cudf::gather`.

**Describe alternatives you've considered**

For data ingest, we could just build the user input as an unsigned type. However, that has the disadvantage that they then don't see what they might ""expect"" if inspecting the result that cudf delivers.

For the join case, I don't think there is a way without copying the gather map (since it is UB to `reinterpret_cast` between pointers of different types).",2023-11-28T13:09:37Z,0,0,Lawrence Mitchell,,False
698,[FEA] Allow control of mask state in return value of `cudf::contains`,"**Is your feature request related to a problem? Please describe.**

`cudf::contains`, specifically the column overload, searches for a bunch of needles in a haystack. If any of the needles are null, the return value has nulls in the same location. The detail API has finer-grained control over whether nulls should compare equal (so that if the haystack contains a null and one of needles is null, the output column has a `true` value in that slot).

It would be nice to be able to control whether the bitmask is copied from the needles to the result or not. In Python cudf, we use `contains` to implement `Series.isin` where the semantics are that nulls are just treated as any other value. So if the needles contain a null, the result is true or false depending on whether the haystack also has a null. Right now, we call contains, and then must perform some post-processing to obtain a result that internally has already been computed.

**Describe the solution you'd like**

A flag specifying whether `contains` masks its output.

**Describe alternatives you've considered**

Above-board, I do this in Python with:

```
if needles.null_count > 0:
    result.fillna(haystack.null_count > 0)
```
This is an extra allocation + kernel launch.

The cheap way of doing it is to obtain the result, and then drop the mask on the floor. This happens to work due to the way `cudf::contains` is implemented. However, I would rather not do this because libcudf explicitly does not guarantee that the masked out entries of a column contain valid data, so I am relying on an implementation detail which could change.",2023-11-28T17:13:09Z,0,0,Lawrence Mitchell,,False
699,[BUG] AssertionError when `cudf.pandas` is used with `ydata_profiling` ,"I am getting an AssertionError in the ydata test note book: https://github.com/ydataai/ydata-profiling/blob/develop/tests/notebooks/meteorites.ipynb

```python
from pathlib import Path

import numpy as np
import pandas as pd
import requests
from IPython.display import display
from IPython.utils.capture import capture_output

import ydata_profiling
from ydata_profiling.utils.cache import cache_file

file_name = cache_file(
    ""meteorites.csv"",
    ""https://data.nasa.gov/api/views/gh4g-9sfh/rows.csv?accessType=DOWNLOAD"",
)

df = pd.read_csv(file_name)

# Note: Pandas does not support dates before 1880, so we ignore these for this analysis
df[""year""] = pd.to_datetime(df[""year""], errors=""coerce"")

# Example: Constant variable
df[""source""] = ""NASA""

# Example: Boolean variable
df[""boolean""] = np.random.choice([True, False], df.shape[0])

# Example: Mixed with base types
df[""mixed""] = np.random.choice([1, ""A""], df.shape[0])

# Example: Highly correlated variables
df[""reclat_city""] = df[""reclat""] + np.random.normal(scale=5, size=(len(df)))

# Example: Duplicate observations
duplicates_to_add = pd.DataFrame(df.iloc[0:10])
duplicates_to_add[""name""] = duplicates_to_add[""name""] + "" copy""

df = pd.concat([df, duplicates_to_add], ignore_index=True)

# Inline report without saving
with capture_output() as out:
    pr = df.profile_report(
        sort=None,
        html={""style"": {""full_width"": True}},
        progress_bar=False,
        minimal=True,
    )
    display(pr)

assert len(out.outputs) == 2
assert out.outputs[0].data[""text/plain""] == ""<IPython.core.display.HTML object>""
assert out.outputs[1].data[""text/plain""] == """"
```

```
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
Cell In[3], line 11
      3     pr = df.profile_report(
      4         sort=None,
      5         html={""style"": {""full_width"": True}},
      6         progress_bar=False,
      7         minimal=True,
      8     )
      9     display(pr)
---> 11 assert len(out.outputs) == 2
     12 assert out.outputs[0].data[""text/plain""] == ""<IPython.core.display.HTML object>""
     13 assert out.outputs[1].data[""text/plain""] == """"

AssertionError: 
```",2023-11-28T18:10:50Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
700,[BUG] Treatment of logical and bitwise binops in `DataFrame.eval` does not match pandas,"**Describe the bug**

In pandas, `eval` treats `a op b` as always meaning the bitwise version `op in {and, or, &, |, ^}`. In cudf, due to the way we parse the expression without type information (and the dispatching scheme to the AST interpreter in libcudf), `and` and `or` mean ""logical"" and `&`, `|`, and `^` mean ""bitwise"". There's a final wrinkle that (like spark) for bools only, masked values are treated as `False`.

This can cause differences in the result between calling `eval` with pandas and with cudf. Although the docstring mentions these differences, when using `cudf.pandas`, we don't see the cudf docstring (only the pandas one).

**Expected behavior**

Eventually, we should match pandas. I think this should be done by running a type inference pass on the user-provided expression and rewriting to an appropriate combination of bitwise and logical operations. This would have the nice side-effect of also allowing mixed-type operands in `eval` expressions by cudf upcasting before passing off to libcudf.

In the short term, we should probably raise a `NotImplementedError` when running in pandas-compat mode if the expression contains logical/bitwise binops.",2023-11-28T18:15:07Z,0,0,Lawrence Mitchell,,False
701,[BUG] `python -i -m cudf.pandas foo.py` does not leave interpreter with variables from `foo.py` in scope,"**Describe the bug**
I think this is because the runpy commands we're using return the scripts globals dictionary and we need to use this to update the outer globals dict.


Note that the [runpy documentation](https://docs.python.org/3/library/runpy.html) says:

> [...], any functions and classes defined by the executed code are not guaranteed to work correctly after a [runpy](https://docs.python.org/3/library/runpy.html#module-runpy) function has returned. If that limitation is not acceptable for a given use case, [importlib](https://docs.python.org/3/library/importlib.html#module-importlib) is likely to be a more suitable choice than this module.


**Steps/Code to reproduce bug**
```python
(cudfdev) /cudf$ cat foo.py 
VALUE = 1

(cudfdev) /cudf$ python -i foo.py
>>> VALUE
1
>>> exit()
(cudfdev) /cudf$ python -i -m cudf.pandas foo.py
>>> VALUE
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
NameError: name 'VALUE' is not defined
>>> exit()
```

**Expected behavior**
```python
(cudfdev) /cudf$ python -i -m cudf.pandas foo.py
>>> VALUE
1
```

**Environment overview (please complete the following information)**
 - Environment location: [Bare-metal]
 - Method of cuDF install: [from source]
",2023-11-28T19:37:03Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
702,[FEA] Test known differences between cudf and pandas ,"
## Problem

We'd like for cudf to match pandas exactly -- but sometimes we return a result that has a documented (known) difference from pandas.

Some of these differences are documented here:
- https://docs.rapids.ai/api/cudf/stable/user_guide/pandascompat/
- https://docs.rapids.ai/api/cudf/stable/user_guide/pandas-comparison/

One particular difference I'd like to call out is join ordering, which is described in https://github.com/rapidsai/cudf/issues/14001.

## Proposed Action

Review the list of documented cudf-pandas differences and ensure that we have explicit tests for these cases, independent from the pandas integration tests. We want to make sure these differences don't result in ""bad behavior"" from `cudf.pandas` like silently returning results that differ from pandas.",2023-11-28T19:42:11Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
703,[BUG] IPython magic reset does not always sufficiently clear pandas objects to enable using cuDF via `cudf.pandas`,"When we've already imported cuDF, the Ipython magic `%reset -f --aggressive` is no longer enough to clear the environment/sys modules of all pandas objects so that we can `%load_ext cudf.pandas` and use cuDF under the hood,

```python

In [1]: import cudf

In [2]: %reset -f --aggressive
culling sys module...

In [3]: %load_ext cudf.pandas
/nvme/0/pgali/envs/cudfdev/lib/python3.10/site-packages/numba/__init__.py:34: UserWarning: The NumPy module was reloaded (imported a second time). This can in some cases result in small but subtle issues and is discouraged.
  import numpy as np
---------------------------------------------------------------------------
ArrowKeyError                             Traceback (most recent call last)
Cell In[3], line 1
----> 1 get_ipython().run_line_magic('load_ext', 'cudf.pandas')

File /nvme/0/pgali/envs/cudfdev/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2456, in InteractiveShell.run_line_magic(self, magic_name, line, _stack_depth)
   2454     kwargs['local_ns'] = self.get_local_scope(stack_depth)
   2455 with self.builtin_trap:
-> 2456     result = fn(*args, **kwargs)
   2458 # The code below prevents the output from being displayed
   2459 # when using magics with decorator @output_can_be_silenced
   2460 # when the last Python token in the expression is a ';'.
   2461 if getattr(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, False):

File /nvme/0/pgali/envs/cudfdev/lib/python3.10/site-packages/IPython/core/magics/extension.py:33, in ExtensionMagics.load_ext(self, module_str)
     31 if not module_str:
     32     raise UsageError('Missing module name.')
---> 33 res = self.shell.extension_manager.load_extension(module_str)
     35 if res == 'already loaded':
     36     print(""The %s extension is already loaded. To reload it, use:"" % module_str)

File /nvme/0/pgali/envs/cudfdev/lib/python3.10/site-packages/IPython/core/extensions.py:76, in ExtensionManager.load_extension(self, module_str)
     69 """"""Load an IPython extension by its module name.
     70 
     71 Returns the string ""already loaded"" if the extension is already loaded,
     72 ""no load function"" if the module doesn't have a load_ipython_extension
     73 function, or None if it succeeded.
     74 """"""
     75 try:
---> 76     return self._load_extension(module_str)
     77 except ModuleNotFoundError:
     78     if module_str in BUILTINS_EXTS:

File /nvme/0/pgali/envs/cudfdev/lib/python3.10/site-packages/IPython/core/extensions.py:91, in ExtensionManager._load_extension(self, module_str)
     89 with self.shell.builtin_trap:
     90     if module_str not in sys.modules:
---> 91         mod = import_module(module_str)
     92     mod = sys.modules[module_str]
     93     if self._call_load_ipython_extension(mod):

File /nvme/0/pgali/envs/cudfdev/lib/python3.10/importlib/__init__.py:126, in import_module(name, package)
    124             break
    125         level += 1
--> 126 return _bootstrap._gcd_import(name[level:], package, level)

File <frozen importlib._bootstrap>:1050, in _gcd_import(name, package, level)

File <frozen importlib._bootstrap>:1027, in _find_and_load(name, import_)

File <frozen importlib._bootstrap>:992, in _find_and_load_unlocked(name, import_)

File <frozen importlib._bootstrap>:241, in _call_with_frames_removed(f, *args, **kwds)

File <frozen importlib._bootstrap>:1050, in _gcd_import(name, package, level)

File <frozen importlib._bootstrap>:1027, in _find_and_load(name, import_)

File <frozen importlib._bootstrap>:1006, in _find_and_load_unlocked(name, import_)

File <frozen importlib._bootstrap>:688, in _load_unlocked(spec)

File <frozen importlib._bootstrap_external>:883, in exec_module(self, module)

File <frozen importlib._bootstrap>:241, in _call_with_frames_removed(f, *args, **kwds)

File /nvme/0/pgali/envs/cudfdev/lib/python3.10/site-packages/cudf/__init__.py:19
     16 from rmm.allocators.cupy import rmm_cupy_allocator
     17 from rmm.allocators.numba import RMMNumbaManager
---> 19 from cudf import api, core, datasets, testing
     20 from cudf._version import __git_commit__, __version__
     21 from cudf.api.extensions import (
     22     register_dataframe_accessor,
     23     register_index_accessor,
     24     register_series_accessor,
     25 )

File /nvme/0/pgali/envs/cudfdev/lib/python3.10/site-packages/cudf/api/__init__.py:3
      1 # Copyright (c) 2021, NVIDIA CORPORATION.
----> 3 from cudf.api import extensions, types
      5 __all__ = [""extensions"", ""types""]

File /nvme/0/pgali/envs/cudfdev/lib/python3.10/site-packages/cudf/api/types.py:18
     15 from pandas.api import types as pd_types
     17 import cudf
---> 18 from cudf.core.dtypes import (  # noqa: F401
     19     _BaseDtype,
     20     dtype,
     21     is_categorical_dtype,
     22     is_decimal32_dtype,
     23     is_decimal64_dtype,
     24     is_decimal128_dtype,
     25     is_decimal_dtype,
     26     is_interval_dtype,
     27     is_list_dtype,
     28     is_struct_dtype,
     29 )
     32 def is_numeric_dtype(obj):
     33     """"""Check whether the provided array or dtype is of a numeric dtype.
     34 
     35     Parameters
   (...)
     43         Whether or not the array or dtype is of a numeric dtype.
     44     """"""

File /nvme/0/pgali/envs/cudfdev/lib/python3.10/site-packages/cudf/core/dtypes.py:28
     25 from cudf.utils.docutils import doc_apply
     27 if PANDAS_GE_150:
---> 28     from pandas.core.arrays.arrow.extension_types import ArrowIntervalType
     29 else:
     30     from pandas.core.arrays._arrow_utils import ArrowIntervalType

File /nvme/0/pgali/envs/cudfdev/lib/python3.10/site-packages/pandas/core/arrays/arrow/extension_types.py:49
     47 # register the type with a dummy instance
     48 _period_type = ArrowPeriodType(""D"")
---> 49 pyarrow.register_extension_type(_period_type)
     52 class ArrowIntervalType(pyarrow.ExtensionType):
     53     def __init__(self, subtype, closed: IntervalClosedType) -> None:
     54         # attributes need to be set first before calling
     55         # super init (as that calls serialize)

File /nvme/0/pgali/envs/cudfdev/lib/python3.10/site-packages/pyarrow/types.pxi:1836, in pyarrow.lib.register_extension_type()

File /nvme/0/pgali/envs/cudfdev/lib/python3.10/site-packages/pyarrow/error.pxi:91, in pyarrow.lib.check_status()

ArrowKeyError: A type extension with name pandas.period already defined
```

Instead, we need to explicitly restart the kernel to use `cudf.pandas` , which we can do via in IPython via `get_ipython().kernel.do_shutdown(restart=True)`.

This isn't a big deal, as we shouldn't rely on this kind of `reset` based behavior anyway. But, I believe this is a change in behavior and we may want to look into it down the road.",2023-11-28T19:49:04Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
704,[BUG] `pyarrow.table` does not accept `cudf.pandas.DataFrame`,"### Background

Most of pyarrow is implemented in Cython. They have a lazy-loaded pandas-shim which they use to interoperate with pandas. This is implemented as the `_PandasAPIShim` `cdef` class. There is a singleton shim object that is accessible as `pyarrow.lib._pandas_api` from python (and as both `pyarrow.lib._pandas_api` and `pyarrow.lib.pandas_api` from cython):
```
In [1]: import pyarrow
In [2]: pyarrow.lib.pandas_api
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[2], line 1
----> 1 pyarrow.lib.pandas_api

AttributeError: module 'pyarrow.lib' has no attribute 'pandas_api'

In [3]: pyarrow.lib._pandas_api
Out[3]: <pyarrow.lib._PandasAPIShim at 0x7f3838413c10>
```

So at import time we make this API shim, and then it lazily initialises itself on first use.

This object saves a number of things:

1. The observed pandas module it exported (by doing `import pandas; self.pandas = pandas`)
2. Various type constructors (e.g. `self.data_frame = pandas.DataFrame`)

The first is relatively unproblematic. What we would like is for that module to be our intercepted wrapped module, which we can arrange with a little bit of rejigging of imports in cudf. It is the memoisation of the type constructors that is the problematic thing.

### cudf.pandas wrapping scheme
Recall that the way our wrapping scheme works is that we deliver wrapped _modules_ and decide at `__getattr__` time whether any attribute lookups deliver real or wrapped attributes. So:

```
%load_ext cudf.pandas

import pandas as pd # pd is _always_ a wrapped module

DataFrame = pd.DataFrame # this is context-dependant either a real or wrapped constructor
```

This works well, as long as someone doesn't memoise an attribute lookup. If they do, we only get to make the decision about what type of attribute to deliver once:

```
In [1]: %load_ext cudf.pandas

In [3]: from cudf.pandas.module_finder import disable_transparent_mode_if_enabled

In [4]: with disable_transparent_mode_if_enabled():
   ...:     from pandas import DataFrame
   ...: 

In [5]: DataFrame
Out[5]: pandas.core.frame.DataFrame

In [6]: from pandas import DataFrame

In [7]: DataFrame
Out[7]: cudf.pandas._wrappers.pandas.DataFrame
```

Unfortunately, pyarrow's pandas shim does exactly this. And we can't make the right decision, because sometimes (when used inside cudf) we need to deliver real objects, other times (when the user is using pyarrow) we need to deliver wrapped ones.

I said it would be a miracle if @shwina's approach worked, and it _kind of_ does, but unfortunately it's not quite miraculous enough. Here's what's going on:

`pa.table` uses `pa.lib._pandas_api.is_data_frame` to determine if the passed object is a pandas dataframe. The leading underscore here is crucial! This is a python object that we can replace. Similarly, `pa.Table.from_pandas` calls out to some Python code that uses `pa.lib._pandas_api` (which we can control).

However, the `to_pandas` method on the resulting object calls `pyarrow.lib.pandas_api.data_frame` note _no_ leading underscore. This is a Cython level module attribute that we _can't_ replace from Python.

So, we have this:

```
%load_ext cudf.pandas

import pyarrow as pa
pa.lib._pandas_api = pa.lib._PandasAPIShim()
pa.lib.pandas_api = pa.lib._pandas_api # this doesn't do anything at the Cython level

import pandas as pd
df = pd.DataFrame({""a"": [1, 2, 3]})
tab = pa.table(df) # works
df2 = tab.to_pandas()

print(type(df))
# <class 'cudf.pandas._wrappers.pandas.DataFrame'>
print(type(df2))
# <class 'pandas.core.frame.DataFrame'>
```

### What if we initialise the Cython level object with wrapped attributes?

This seems like it might work, we have to be a bit careful about how we're importing pyarrow in cudf, but we can make this work so that `pa.lib.pandas_api` is a shim wrapper that sees our wrapped attributes.

_But_ the memoisation breaks things, because inside cudf we use `to_arrow().to_pandas()` in various places to produce an honest-to-goodness pandas object, but this will now produce a wrapped object (and wrapping things in a `disable_transparent_mode_if_enabled()` context manager won't help because we _already_ took the decision about what constructor to deliver).

### Options

1. Convince the arrow folks that the memoisation of attribute lookup in their pandas shim is not really a performance win, and that it would be convenient if they just used `self.pandas.DataFrame`. This would allow us to (context-dependently) provide a real or wrapped object as appropriate. We would likely do this after releasing cudf-PAM since then the motivation is clear.
2. Rather than letting module attribute lookup be context dependent, always deliver wrapped types such that the constructor context-dependently decides whether or not to deliver a real or wrapped type.
3. Something else?



**Steps/Code to reproduce bug**
```python
In [1]: %load_ext cudf.pandas

In [2]: import pandas as pd

In [3]: df = pd.DataFrame({'a': [1, 2, 3]})

In [4]: import pyarrow as pa

In [5]: pa.table(df)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[5], line 1
----> 1 pa.table(df)

File /nvme/0/pgali/envs/cudfdev/lib/python3.10/site-packages/pyarrow/table.pxi:5165, in pyarrow.lib.table()

TypeError: Expected pandas DataFrame, python dictionary or list of arrays
```

",2023-11-28T19:55:10Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
705,"[BUG] `df.index.name = ""indexer""` does not work as expected under `cudf.pandas`","**Describe the bug**
When setting the `name` attribute for `df.index`, the name of the index is supposed to update. When following the reproducer below, it doesn't work as expected.

**Steps/Code to reproduce bug**
```python
import pandas as pd

df1 = pd.DataFrame(columns=[""x""], dtype=""int64"")
df2 = df1.dtypes
df3 = df2.reset_index()

df3.index.name = ""indexer""
assert df3.index.name == ""indexer""
```

**Expected behavior**
No exception should be thrown from above.

**Environment overview (please complete the following information)**
Bare-metal, conda

",2023-11-29T01:19:57Z,0,0,Michael Wang,Nvidia Rapids,True
706,[BUG] FLOAT32 rounding more inaccurate than necessary,"**Describe the bug**
When rounding single-precision floats to a specified number of decimal places, the result can be slightly inaccurate due to the intermediate computations being forced into FLOAT32 as well.  round.cu has rounding functors for non-fixed-point types, but all of the intermediate results are in the same type as the input rather than the highest precision type, double.  This means more error is introduced during the rounding computation than is necessary.

**Steps/Code to reproduce bug**
The following code demonstrates the problem:
```c++
#include <cudf/round.hpp>
#include <cudf_test/column_utilities.hpp>
#include <cudf_test/column_wrapper.hpp>
#include <cudf_test/debug_utilities.hpp>

int main(int argc, char** argv) {
  auto const input =
    cudf::test::fixed_width_column_wrapper<float>{6.121944898040965e-05f};
  cudf::test::print(input);
  auto const result = cudf::round(input, 10, cudf::rounding_method::HALF_UP);
  cudf::test::print(*result);
  return 0;
}
```

Rounding the value to the tenth decimal place should round _down_ to approximately 6.12194e-05 but instead the value is rounded _up_ to approximately 6.12195e-05 as shown in the output when running the program:
```
6.1219449e-05
6.12194999e-05
```

**Expected behavior**
FLOAT32 rounding should use FLOAT64 for intermediate results during computation to try to avoid injecting errors beyond what is necessary when dealing with floating point numbers.  When I manually performed the computations on this example input value for round.cu's half_up_positive logic but using double instead of float for the intermediate values, the answer came out rounded down as expected.

It seems that the functors for floating point rounding in round.cu should _not_ be using whatever the input type is but rather `double` explicitly to avoid unnecessary additional error during the computation.",2023-11-29T21:46:28Z,0,0,Jason Lowe,NVIDIA,True
707,[FEA] Proxy ndarrays don't pass an `instancecheck()` for `np.ndarray`,"There's a lot of code out there that does something like:

```python
if not isinstance(x, np.ndarray):
    raise TypeError(""Not a numpy array"")
```

This is a problem for `cudf.pandas`, because proxy ndarray types, such as those returned by `pd.Series.values` do not pass the `isinstance()` check.

Ideally, more projects would avoid doing a hard `isinstance()` check and instead use something like EAFP:

```python
np.asarray(x)
```

..but that is not the world we live in today. Too many third-party libraries that people want to use with `cudf.pandas` use the pattern above, so it's on us to solve the problem right now.",2023-11-30T17:23:07Z,0,0,Ashwin Srinath,Voltron Data,False
708,"[BUG] Parquet column selection by name with schemas including   list<struct<X, Y>> does not work.","If you have a schema that contains a list-of-struct, selecting a subset of the inner columns doesn't work.  Example

`list<struct<int, float>>`
If the schema for this column was

```
A           (list)
   B        (struct)
       C    (int)
       D    (float)
```
Attempting to select ""A.B.C"" would not work.  I believe this is being caused by some schema preprocessing that we are doing that is injecting fake schema elements to ease schema interpretation.  Essentially we see a schema that looks like this:

```
A            (list)
  list       (the fake element
     B       (struct)
        C    (int)
        D    (float)
```
So ""A.B.C"" doesn't actually exist, only ""A.list.B.C"" and the code returns 0 columns.",2023-11-30T21:30:21Z,0,0,,,False
709,[BUG] `MultiIndex.equals` does not match pandas for numerical indexes with unequal dtypes,"**Describe the bug**

```python
import cudf

left = cudf.MultiIndex.from_tuples([(1,)])
right = cudf.MultiIndex.from_tuples([(1.0,)])

print(left.equals(right)) # => False

print(left.to_pandas().equals(right.to_pandas())) # => True
```

**Expected behavior**

This should match pandas. Note that `Index.equals` does do dtype casting.",2023-12-05T15:58:40Z,0,0,Lawrence Mitchell,,False
710,[BUG] merge join key matching is too eager to cast strings,"**Describe the bug**

```
import cudf

left = cudf.DataFrame({""key"": [1, 2, 3]})
right = cudf.DataFrame({""key"": [""1"", ""4"", ""5""]})

# Casts both key columns to float64 and merges
got = left.merge(right, on=""key"", how=""outer"")

# raises ValueError (merging on int + string column)
expect = left.to_pandas().merge(right.to_pandas(), on=""key"", how=""outer"")
```

**Expected behavior**

This should match pandas. A consequence, since merge is also used for Index setops, `union`, `intersection`, and `difference` are buggy.",2023-12-05T17:34:17Z,0,0,Lawrence Mitchell,,False
711,[FEA] Add Avro reader benchmarks to the cuIO benchmarking suite,"**Is your feature request related to a problem? Please describe.**
We have reader benchmarks for CSV, JSON, Parquet and ORC in the cuIO nvbench benchmarking suite. We should add benchmarking for the Avro reader. 

The cuIO benchmarks are located here:
https://github.com/rapidsai/cudf/tree/branch-24.02/cpp/benchmarks/io

Unfortunately, we don't have an Avro writer implementation in libcudf, so the naive approach of modeling benchmarks after [json_reader_input.cpp](https://github.com/rapidsai/cudf/blob/branch-24.02/cpp/benchmarks/io/json/json_reader_input.cpp) will not work. 

**Describe the solution you'd like**
Our options would be:
* add an MVP Avro writer to libcudf
* add/use a dependency to write Avro files 
* maintain a repository of large Avro files (>100 MB) for benchmarking purposes

**Describe alternatives you've considered**
Continue without automated benchmarks for the Avro reader

**Additional context**
The libcudf Avro reader does not support nested types so the benchmarks should start by only covering primitive types.
",2023-12-05T19:34:27Z,0,0,Gregory Kimball,,False
712,[FEA] Support canonical arrow extension types: FixedShapeTensorType and VariableShapeTensorType,"**Is your feature request related to a problem? Please describe.**
Feeding data from the CPU to the GPU is a bottleneck especially for computer vision. I'd like to store satellite images as parquet with georeferencing information in some columns and a column with Arrow's new FixedShapeTensorType extension array and then load it with cudf and get zero copy benefits when passing the tensor to the GPU and pytorch.

However it looks like cudf can't interpret this type.
```
import pyarrow as pa

tensor_type = pa.fixed_shape_tensor(pa.int32(), (2, 2))
arr = [[1, 2, 3, 4], [10, 20, 30, 40], [100, 200, 300, 400]]
storage = pa.array(arr, pa.list_(pa.int32(), 4))
tensor_array = pa.ExtensionArray.from_storage(tensor_type, storage)

data = [
     pa.array([1, 2, 3]),
     pa.array(['foo', 'bar', None]),
     tensor_array,
]
my_schema = pa.schema([('f0', pa.int8()),
                        ('f1', pa.string()),
                        ('tensors_int', tensor_type)])
table = pa.Table.from_arrays(data, schema=my_schema)

table.cast(table.schema)
```

```
import cudf

cudf.DataFrame.from_arrow(table)
```

```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In[2], line 3
      1 import cudf
----> 3 cudf.DataFrame.from_arrow(table)

File ~/miniforge3/envs/rapids-23.10/lib/python3.10/site-packages/nvtx/nvtx.py:115, in annotate.__call__.<locals>.inner(*args, **kwargs)
    112 @wraps(func)
    113 def inner(*args, **kwargs):
    114     libnvtx_push_range(self.attributes, self.domain.handle)
--> 115     result = func(*args, **kwargs)
    116     libnvtx_pop_range(self.domain.handle)
    117     return result

File ~/miniforge3/envs/rapids-23.10/lib/python3.10/site-packages/cudf/core/dataframe.py:5322, in DataFrame.from_arrow(cls, table)
   5319         for col_meta in table.schema.pandas_metadata[""column_indexes""]:
   5320             col_index_names.append(col_meta[""name""])
-> 5322 out = super().from_arrow(table)
   5323 if col_index_names is not None:
   5324     out._data._level_names = col_index_names

File ~/miniforge3/envs/rapids-23.10/lib/python3.10/site-packages/nvtx/nvtx.py:115, in annotate.__call__.<locals>.inner(*args, **kwargs)
    112 @wraps(func)
    113 def inner(*args, **kwargs):
    114     libnvtx_push_range(self.attributes, self.domain.handle)
--> 115     result = func(*args, **kwargs)
    116     libnvtx_pop_range(self.domain.handle)
    117     return result

File ~/miniforge3/envs/rapids-23.10/lib/python3.10/site-packages/cudf/core/frame.py:1053, in Frame.from_arrow(cls, data)
   1036     cudf_category_frame = {
   1037         name: build_categorical_column(
   1038             cudf_dictionaries_columns[name],
   (...)
   1046         )
   1047     }
   1049 # Handle non-dict arrays
   1050 cudf_non_category_frame = {
   1051     name: col
   1052     for name, col in zip(
-> 1053         data.column_names, libcudf.interop.from_arrow(data)
   1054     )
   1055 }
   1057 result = {**cudf_non_category_frame, **cudf_category_frame}
   1059 # There are some special cases that need to be handled
   1060 # based on metadata.

File ~/miniforge3/envs/rapids-23.10/lib/python3.10/contextlib.py:79, in ContextDecorator.__call__.<locals>.inner(*args, **kwds)
     76 @wraps(func)
     77 def inner(*args, **kwds):
     78     with self._recreate_cm():
---> 79         return func(*args, **kwds)

File interop.pyx:199, in cudf._lib.interop.from_arrow()

RuntimeError: CUDF failure at:/opt/conda/conda-bld/work/cpp/src/interop/from_arrow.cu:87: Unsupported type_id conversion to cudf
```


**Describe the solution you'd like**
I want to be able to load parquet files with cudf that contain a column with this tensor type and then easily hand it off from cudf to pytorch.

**Describe alternatives you've considered**
There are other formats like zarr for N-D arrays and direct loading to gpu, but I don't think zero copy between cpu and gpu is supported https://xarray.dev/blog/xarray-kvikio

Or I can continue loading parquet files with references to cloud optimized geotiff files, which is a lot slower.

**Additional context**
docs for the type are here: https://arrow.apache.org/docs/python/generated/pyarrow.FixedShapeTensorArray.html
and others are looking at implementing it for dataloading https://github.com/huggingface/datasets/issues/5272
there's also a variable shape equal dimension number type which would be very useful for efficiently loading satellite imagery time series where the time length can vary a lot depending on the sample, or the height and width can vary a lot depending on the sensor resolution: https://arrow.apache.org/docs/format/CanonicalExtensions.html#variable-shape-tensor
",2023-12-06T21:04:04Z,1,0,Ryan Avery,@wherobots,False
713,[FEA] Tighten up promotion when merging with non-equal key column dtypes,"**Is your feature request related to a problem? Please describe.**

To date, cudf has attempted to match pandas semantics when matching join keys in a merge. libcudf does not perform merges between mismatching table dtypes. Consequently, the first step of a merge in cudf is to determine a ""common"" dtype for each pair of columns used as keys in the merge.

The pandas rules are mostly (though not completely since there is some under the table work that happens in the join algorithm) encoded in https://github.com/pandas-dev/pandas/blob/f7c73a5f1aaf0724598e60c0cc5732604ec842a8/pandas/core/reshape/merge.py#L1340

There are a few problems when trying to match these in cudf:

- not all column types in pandas can be represented in cudf (we do not have an `object` column for example)
- it is difficult to unambiguously determine the type promotion rules since they are not written down anywhere
    - for example, promotion rules for categorical columns differ depending on whether the categorical is the left or right key.

Moreover, there are other, correctness, problems. The current type promotion rules admit lossy conversions that can result in false positive matches in merges.

Example:
```
left = cudf.DataFrame({""key"": [1, 2, 2**53]})
right = cudf.DataFrame({""key"": [2**53 + 1, 10]})
right[""key""] = right.key.astype(""uint64"")
left.merge(right, on=""key"", how=""inner"")
#            key
# 0  9.007199e+15
left
#                key
# 0                 1
# 1                 2
# 2  9007199254740992
right
#                key
# 0  9007199254740993
# 1                10
```

Pandas is also susceptible to this, but produces a different wrong result.

I would like to tighten up the rules in cudf, so that it is impossible for the user to get a ""surprising"" result without some explicit intervention on their behalf. We would also try and match pandas more closely where that is possible, but my preference is to be correct in a subset of cases over dubiously correct in a larger set.

**Describe the solution you'd like**

There are, I think, three levels of things we could do:

1. Push the burden of dtype matching completely on to the user: complain (raise) if merge keys do not match dtypes _exactly_
2. Promote keys whose dtypes allow so safely (without needing to inspect values), and raise for cases where that is not possible. The user can still perform the merge by intentionally casting to matching types. But then they must know that it is safe.
3. Try and match pandas promotions as closely as possible and accept that there might be false positives.

I would like to go for (2). (1) is easiest; (3) is difficult, probably a moving target and can result in false positives without the user explicitly ""requesting"" them.

With cudf-pandas (2), I think, skates the line between ease of use and correctness reasonably well. We can run as much on the GPU as possible and raise (possibly providing a warning in pandas-compat mode) with fallback to CPU. When using cudf directly, users will hopefully be willing to accept a few more edge cases in the name of consistency. 


Concretely this would mean:

- No casting for strings
- No casting for lists
- No casting for structs
- Categoricals:
    - if both columns are categorical and match, no casting
    - if both columns are categorical and _do not_ match, raise[^1]
    - if one column is categorical, unwrap, and go round again[^2]
 - No casting for decimals
 - No casting for datetimes[^3]
 - For numeric types, use a type promotion lattice that has lossless least upper bounds for all types[^4]

For numeric types, that means that we would only promote pairs of types where there exists a wider type whose values are uniquely and unambiguously mapped onto from the narrower types.

For example `(int32, uint32) -> int64` would be allowed, but merging a pair `(int32, uint64)` would raise (since there is no signed 128bit int that we could use). Similarly, we would safely be able to promote `(intX, floatY)` pairs (and similarly with `uintX`) as long as the integer type is 32 or fewer bits wide[^5].



[^1]: I could also be convinced to unwrap and go round again, but that would lose information about the categorical nature of the inputs
[^2]: Pandas behaviour in this case depends on whether the left or right key is categorical (and which merge type it is): it casts the non-categorical to object, and the categorical to its underlying dtype, then imperfectly goes through its matching process again
[^3]: I haven't looked at what pandas does here, but I guess the other thing one could do is promote when one can losslessly convert
[^4]: See, for example https://jax.readthedocs.io/en/latest/jep/9407-type-promotion.html though I disagree with their approach of selecting a ""weak"" float64 as the least upper bound for `(int64, uint64)`
[^5]: Merging between float and int columns is kind of weird, so I could also be convinced to raise when merging between mismatching numeric kinds.",2023-12-07T19:04:54Z,0,0,Lawrence Mitchell,,False
714,[BUG] DIV on decimal types appears to lose fractional part,"**Describe the bug**

Dividing two decimal columns with equal scale N results in 0 fractional digits in the output (i.e. the result is truncated). I would expect that if division returns a result of scale N, then we should have N fractional digits of output. Instead, the number of fractional digits computed appears to be `LHS.scale - RHS.scale`, so dividing equal scale decimals always truncates the fractional part. It's possible I'm misunderstanding how scale is handled here, though. (It looks vaguely like cuDF is just doing the raw integer division, then possibly rescaling? IIRC, PyArrow rescales before and after)

**Steps/Code to reproduce bug**

```python
from decimal import Decimal

import cudf
import pyarrow
import pyarrow.compute

print(""cuDF version: "", cudf.__version__)
print(""PyArrow version: "", pyarrow.__version__)

lhs_py = [Decimal(""1.0""), Decimal(""2.0"")]
rhs_py = [Decimal(""2.0""), Decimal(""3.0"")]

print(""* cuDF:"")
lhs = cudf.Series(lhs_py, dtype=cudf.Decimal128Dtype(precision=10, scale=4))
rhs = cudf.Series(rhs_py, dtype=cudf.Decimal128Dtype(precision=10, scale=4))

# There are 0 actual fractional digits in the result, although the dtype
# indicates precision=25, scale=15.
result = lhs / rhs
print(result)
print(repr(result.dtype))

# If we arbitrarily add more fractional digits to the LHS, we get (LHS.scale -
# RHS.scale) fractional digits in the result.
result = lhs.astype(cudf.Decimal128Dtype(precision=10, scale=8)) / rhs
print(result)
print(repr(result.dtype))

print(""* PyArrow"")
lhs = pyarrow.array(lhs_py, type=pyarrow.decimal128(10, 4))
rhs = pyarrow.array(rhs_py, type=pyarrow.decimal128(10, 4))

# PyArrow computes 11 fractional digits, and the result scale is 11.
result = pyarrow.compute.divide(lhs, rhs)
print(result)
print(result.type)
```

The result:

```
cuDF version:  23.12.00a717
PyArrow version:  14.0.1
* cuDF:
0    0E-15
1    0E-15
dtype: decimal128
Decimal128Dtype(precision=25, scale=15)
0    0.5000000000000000000
1    0.6666000000000000000
dtype: decimal128
Decimal128Dtype(precision=25, scale=19)
* PyArrow
[
  0.50000000000,
  0.66666666666
]
decimal128(21, 11)
```

**Expected behavior**
A clear and concise description of what you expected to happen.

**Environment overview (please complete the following information)**
 - Environment location: Bare-metal
 - Method of cuDF install: conda

<details>
<summary><tt>conda list</tt></summary>

```
# packages in environment at /home/lidavidm/miniforge3/envs/cudf:
#
# Name                    Version                   Build  Channel
_libgcc_mutex             0.1                 conda_forge    conda-forge
_openmp_mutex             4.5                       2_gnu    conda-forge
aws-c-auth                0.7.8                h538f98c_2    conda-forge
aws-c-cal                 0.6.9                h5d48c4d_2    conda-forge
aws-c-common              0.9.10               hd590300_0    conda-forge
aws-c-compression         0.2.17               h7f92143_7    conda-forge
aws-c-event-stream        0.3.2                h0bcb0bb_8    conda-forge
aws-c-http                0.7.14               hd268abd_3    conda-forge
aws-c-io                  0.13.36              he0cd244_2    conda-forge
aws-c-mqtt                0.9.10               h35285c7_2    conda-forge
aws-c-s3                  0.4.4                h0448019_0    conda-forge
aws-c-sdkutils            0.1.13               h7f92143_0    conda-forge
aws-checksums             0.1.17               h7f92143_6    conda-forge
aws-crt-cpp               0.24.11              h5bdc202_2    conda-forge
aws-sdk-cpp               1.11.210             h967ea9e_4    conda-forge
bzip2                     1.0.8                hd590300_5    conda-forge
c-ares                    1.23.0               hd590300_0    conda-forge
ca-certificates           2023.11.17           hbcca054_0    conda-forge
cachetools                5.3.2              pyhd8ed1ab_0    conda-forge
cubinlinker               0.3.0           py310hfdf336d_1    rapidsai-nightly
cuda-python               11.8.3          py310h70a93da_0    conda-forge
cuda-version              11.8                 h70ddcb2_2    conda-forge
cudatoolkit               11.8.0              h4ba93d1_12    conda-forge
cudf                      23.12.00a717    cuda11_py310_231212_gfd2f6a6fd1_717    rapidsai-nightly
cupy                      12.3.0          py310hf4db66c_0    conda-forge
dlpack                    0.5                  h9c3ff4c_0    conda-forge
fastrlock                 0.8.2           py310hc6cd4ac_1    conda-forge
fmt                       10.1.1               h00ab1b0_1    conda-forge
fsspec                    2023.12.2          pyhca7485f_0    conda-forge
gflags                    2.2.2             he1b5a44_1004    conda-forge
glog                      0.6.0                h6f12383_0    conda-forge
gmock                     1.14.0               ha770c72_1    conda-forge
gtest                     1.14.0               h00ab1b0_1    conda-forge
icu                       73.2                 h59595ed_0    conda-forge
keyutils                  1.6.1                h166bdaf_0    conda-forge
krb5                      1.21.2               h659d440_0    conda-forge
ld_impl_linux-64          2.40                 h41732ed_0    conda-forge
libabseil                 20230802.1      cxx17_h59595ed_0    conda-forge
libarrow                  14.0.1           h0f82fcc_9_cpu    conda-forge
libarrow-acero            14.0.1           h59595ed_9_cpu    conda-forge
libarrow-dataset          14.0.1           h59595ed_9_cpu    conda-forge
libarrow-flight           14.0.1           h120cb0d_9_cpu    conda-forge
libarrow-flight-sql       14.0.1           h61ff412_9_cpu    conda-forge
libarrow-gandiva          14.0.1           hacb8726_9_cpu    conda-forge
libarrow-substrait        14.0.1           h61ff412_9_cpu    conda-forge
libblas                   3.9.0           20_linux64_openblas    conda-forge
libbrotlicommon           1.1.0                hd590300_1    conda-forge
libbrotlidec              1.1.0                hd590300_1    conda-forge
libbrotlienc              1.1.0                hd590300_1    conda-forge
libcblas                  3.9.0           20_linux64_openblas    conda-forge
libcrc32c                 1.1.2                h9c3ff4c_0    conda-forge
libcudf                   23.12.00a717    cuda11_231212_gfd2f6a6fd1_717    rapidsai-nightly
libcufile                 1.4.0.31                      0    nvidia
libcufile-dev             1.4.0.31                      0    nvidia
libcurl                   8.5.0                hca28451_0    conda-forge
libedit                   3.1.20191231         he28a2e2_2    conda-forge
libev                     4.33                 hd590300_2    conda-forge
libevent                  2.1.12               hf998b51_1    conda-forge
libffi                    3.4.2                h7f98852_5    conda-forge
libgcc-ng                 13.2.0               h807b86a_3    conda-forge
libgfortran-ng            13.2.0               h69a702a_3    conda-forge
libgfortran5              13.2.0               ha4646dd_3    conda-forge
libgomp                   13.2.0               h807b86a_3    conda-forge
libgoogle-cloud           2.12.0               h5206363_4    conda-forge
libgrpc                   1.59.3               hd6c4280_0    conda-forge
libiconv                  1.17                 hd590300_1    conda-forge
libkvikio                 23.12.00a       cuda11_231212_g26efdd1_23    rapidsai-nightly
liblapack                 3.9.0           20_linux64_openblas    conda-forge
libllvm14                 14.0.6               hcd5def8_4    conda-forge
libllvm15                 15.0.7               hb3ce162_4    conda-forge
libnghttp2                1.58.0               h47da74e_1    conda-forge
libnsl                    2.0.1                hd590300_0    conda-forge
libnuma                   2.0.16               h0b41bf4_1    conda-forge
libopenblas               0.3.25          pthreads_h413a1c8_0    conda-forge
libparquet                14.0.1           h352af49_9_cpu    conda-forge
libprotobuf               4.24.4               hf27288f_0    conda-forge
libre2-11                 2023.06.02           h7a70373_0    conda-forge
librmm                    23.12.00             h4725429_0    conda-forge
libsqlite                 3.44.2               h2797004_0    conda-forge
libssh2                   1.11.0               h0841786_0    conda-forge
libstdcxx-ng              13.2.0               h7e041cc_3    conda-forge
libthrift                 0.19.0               hb90f79a_1    conda-forge
libutf8proc               2.8.0                h166bdaf_0    conda-forge
libuuid                   2.38.1               h0b41bf4_0    conda-forge
libxml2                   2.12.2               h232c23b_0    conda-forge
libzlib                   1.2.13               hd590300_5    conda-forge
llvmlite                  0.40.1          py310h1b8f574_0    conda-forge
lz4-c                     1.9.4                hcb278e6_0    conda-forge
markdown-it-py            3.0.0              pyhd8ed1ab_0    conda-forge
mdurl                     0.1.0              pyhd8ed1ab_0    conda-forge
ncurses                   6.4                  h59595ed_2    conda-forge
numba                     0.57.1          py310h0f6aa51_0    conda-forge
numpy                     1.24.4          py310ha4c1d20_0    conda-forge
nvcomp                    3.0.4                h838ba91_1    conda-forge
nvtx                      0.2.8           py310h2372a71_1    conda-forge
openssl                   3.2.0                hd590300_1    conda-forge
orc                       1.9.2                h4b38347_0    conda-forge
packaging                 23.2               pyhd8ed1ab_0    conda-forge
pandas                    1.5.3           py310h9b08913_1    conda-forge
pip                       23.3.1             pyhd8ed1ab_0    conda-forge
protobuf                  4.24.4          py310h620c231_0    conda-forge
ptxcompiler               0.8.1           py310h70a93da_2    conda-forge
pyarrow                   14.0.1          py310hf9e7431_9_cpu    conda-forge
pygments                  2.17.2             pyhd8ed1ab_0    conda-forge
python                    3.10.13         hd12c33a_0_cpython    conda-forge
python-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge
python_abi                3.10                    4_cp310    conda-forge
pytz                      2023.3.post1       pyhd8ed1ab_0    conda-forge
rdma-core                 49.0                 hd3aeb46_1    conda-forge
re2                       2023.06.02           h2873b5e_0    conda-forge
readline                  8.2                  h8228510_1    conda-forge
rich                      13.7.0             pyhd8ed1ab_0    conda-forge
rmm                       23.12.00        cuda11_py310_231206_g2db5cbb3_0    rapidsai
s2n                       1.4.0                h06160fa_0    conda-forge
setuptools                68.2.2             pyhd8ed1ab_0    conda-forge
six                       1.16.0             pyh6c4a22f_0    conda-forge
snappy                    1.1.10               h9fff704_0    conda-forge
spdlog                    1.12.0               hd2e6256_2    conda-forge
tk                        8.6.13          noxft_h4845f30_101    conda-forge
typing_extensions         4.9.0              pyha770c72_0    conda-forge
tzdata                    2023c                h71feb2d_0    conda-forge
ucx                       1.15.0               hae80064_1    conda-forge
wheel                     0.42.0             pyhd8ed1ab_0    conda-forge
xz                        5.2.6                h166bdaf_0    conda-forge
zstd                      1.5.5                hfc55251_0    conda-forge
```

</details>

**Environment details**

<details><summary>Click here to see environment details</summary><pre>
     
     **git***
     Not inside a git repository
     
     ***OS Information***
     PRETTY_NAME=""Debian GNU/Linux 12 (bookworm)""
     NAME=""Debian GNU/Linux""
     VERSION_ID=""12""
     VERSION=""12 (bookworm)""
     VERSION_CODENAME=bookworm
     ID=debian
     HOME_URL=""https://www.debian.org/""
     SUPPORT_URL=""https://www.debian.org/support""
     BUG_REPORT_URL=""https://bugs.debian.org/""
     Linux debian 6.1.0-12-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.52-1 (2023-09-07) x86_64 GNU/Linux
     
     ***GPU Information***
     Tue Dec 12 14:17:51 2023
     +---------------------------------------------------------------------------------------+
     | NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |
     |-----------------------------------------+----------------------+----------------------+
     | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
     | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
     |                                         |                      |               MIG M. |
     |=========================================+======================+======================|
     |   0  Quadro T2000 with Max-Q ...    On  | 00000000:01:00.0 Off |                  N/A |
     | N/A   49C    P8               1W /  40W |      5MiB /  4096MiB |      0%      Default |
     |                                         |                      |                  N/A |
     +-----------------------------------------+----------------------+----------------------+
     
     +---------------------------------------------------------------------------------------+
     | Processes:                                                                            |
     |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
     |        ID   ID                                                             Usage      |
     |=======================================================================================|
     |    0   N/A  N/A      1347      G   /usr/lib/xorg/Xorg                            4MiB |
     +---------------------------------------------------------------------------------------+
     
     ***CPU***
     Architecture:                       x86_64
     CPU op-mode(s):                     32-bit, 64-bit
     Address sizes:                      39 bits physical, 48 bits virtual
     Byte Order:                         Little Endian
     CPU(s):                             16
     On-line CPU(s) list:                0-15
     Vendor ID:                          GenuineIntel
     Model name:                         Intel(R) Core(TM) i9-10885H CPU @ 2.40GHz
     CPU family:                         6
     Model:                              165
     Thread(s) per core:                 2
     Core(s) per socket:                 8
     Socket(s):                          1
     Stepping:                           2
     CPU(s) scaling MHz:                 64%
     CPU max MHz:                        5300.0000
     CPU min MHz:                        800.0000
     BogoMIPS:                           4800.00
     Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx rdseed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp pku ospke md_clear flush_l1d arch_capabilities
     Virtualization:                     VT-x
     L1d cache:                          256 KiB (8 instances)
     L1i cache:                          256 KiB (8 instances)
     L2 cache:                           2 MiB (8 instances)
     L3 cache:                           16 MiB (1 instance)
     NUMA node(s):                       1
     NUMA node0 CPU(s):                  0-15
     Vulnerability Gather data sampling: Mitigation; Microcode
     Vulnerability Itlb multihit:        KVM: Mitigation: VMX disabled
     Vulnerability L1tf:                 Not affected
     Vulnerability Mds:                  Not affected
     Vulnerability Meltdown:             Not affected
     Vulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable
     Vulnerability Retbleed:             Mitigation; Enhanced IBRS
     Vulnerability Spec rstack overflow: Not affected
     Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl
     Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
     Vulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence
     Vulnerability Srbds:                Mitigation; Microcode
     Vulnerability Tsx async abort:      Not affected
     
     ***CMake***
     
     ***g++***
     /usr/bin/g++
     g++ (Debian 12.2.0-14) 12.2.0
     Copyright (C) 2022 Free Software Foundation, Inc.
     This is free software; see the source for copying conditions.  There is NO
     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
     
     
     ***nvcc***
     /usr/local/cuda-11.8/bin/nvcc
     nvcc: NVIDIA (R) Cuda compiler driver
     Copyright (c) 2005-2022 NVIDIA Corporation
     Built on Wed_Sep_21_10:33:58_PDT_2022
     Cuda compilation tools, release 11.8, V11.8.89
     Build cuda_11.8.r11.8/compiler.31833905_0
     
     ***Python***
     /home/lidavidm/miniforge3/envs/cudf/bin/python
     Python 3.10.13
     
     ***Environment Variables***
     PATH                            : /home/lidavidm/miniforge3/envs/cudf/bin:/home/lidavidm/miniforge3/condabin:/usr/local/cuda-11.8/bin:/home/lidavidm/go/bin:/home/lidavidm/.local/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
     LD_LIBRARY_PATH                 :
     NUMBAPRO_NVVM                   :
     NUMBAPRO_LIBDEVICE              :
     CONDA_PREFIX                    : /home/lidavidm/miniforge3/envs/cudf
     PYTHON_PATH                     :
     
     ***conda packages***
     /home/lidavidm/miniforge3/condabin/conda
     # packages in environment at /home/lidavidm/miniforge3/envs/cudf:
     #
     # Name                    Version                   Build  Channel
     _libgcc_mutex             0.1                 conda_forge    conda-forge
     _openmp_mutex             4.5                       2_gnu    conda-forge
     aws-c-auth                0.7.8                h538f98c_2    conda-forge
     aws-c-cal                 0.6.9                h5d48c4d_2    conda-forge
     aws-c-common              0.9.10               hd590300_0    conda-forge
     aws-c-compression         0.2.17               h7f92143_7    conda-forge
     aws-c-event-stream        0.3.2                h0bcb0bb_8    conda-forge
     aws-c-http                0.7.14               hd268abd_3    conda-forge
     aws-c-io                  0.13.36              he0cd244_2    conda-forge
     aws-c-mqtt                0.9.10               h35285c7_2    conda-forge
     aws-c-s3                  0.4.4                h0448019_0    conda-forge
     aws-c-sdkutils            0.1.13               h7f92143_0    conda-forge
     aws-checksums             0.1.17               h7f92143_6    conda-forge
     aws-crt-cpp               0.24.11              h5bdc202_2    conda-forge
     aws-sdk-cpp               1.11.210             h967ea9e_4    conda-forge
     bzip2                     1.0.8                hd590300_5    conda-forge
     c-ares                    1.23.0               hd590300_0    conda-forge
     ca-certificates           2023.11.17           hbcca054_0    conda-forge
     cachetools                5.3.2              pyhd8ed1ab_0    conda-forge
     cubinlinker               0.3.0           py310hfdf336d_1    rapidsai-nightly
     cuda-python               11.8.3          py310h70a93da_0    conda-forge
     cuda-version              11.8                 h70ddcb2_2    conda-forge
     cudatoolkit               11.8.0              h4ba93d1_12    conda-forge
     cudf                      23.12.00a717    cuda11_py310_231212_gfd2f6a6fd1_717    rapidsai-nightly
     cupy                      12.3.0          py310hf4db66c_0    conda-forge
     dlpack                    0.5                  h9c3ff4c_0    conda-forge
     fastrlock                 0.8.2           py310hc6cd4ac_1    conda-forge
     fmt                       10.1.1               h00ab1b0_1    conda-forge
     fsspec                    2023.12.2          pyhca7485f_0    conda-forge
     gflags                    2.2.2             he1b5a44_1004    conda-forge
     glog                      0.6.0                h6f12383_0    conda-forge
     gmock                     1.14.0               ha770c72_1    conda-forge
     gtest                     1.14.0               h00ab1b0_1    conda-forge
     icu                       73.2                 h59595ed_0    conda-forge
     keyutils                  1.6.1                h166bdaf_0    conda-forge
     krb5                      1.21.2               h659d440_0    conda-forge
     ld_impl_linux-64          2.40                 h41732ed_0    conda-forge
     libabseil                 20230802.1      cxx17_h59595ed_0    conda-forge
     libarrow                  14.0.1           h0f82fcc_9_cpu    conda-forge
     libarrow-acero            14.0.1           h59595ed_9_cpu    conda-forge
     libarrow-dataset          14.0.1           h59595ed_9_cpu    conda-forge
     libarrow-flight           14.0.1           h120cb0d_9_cpu    conda-forge
     libarrow-flight-sql       14.0.1           h61ff412_9_cpu    conda-forge
     libarrow-gandiva          14.0.1           hacb8726_9_cpu    conda-forge
     libarrow-substrait        14.0.1           h61ff412_9_cpu    conda-forge
     libblas                   3.9.0           20_linux64_openblas    conda-forge
     libbrotlicommon           1.1.0                hd590300_1    conda-forge
     libbrotlidec              1.1.0                hd590300_1    conda-forge
     libbrotlienc              1.1.0                hd590300_1    conda-forge
     libcblas                  3.9.0           20_linux64_openblas    conda-forge
     libcrc32c                 1.1.2                h9c3ff4c_0    conda-forge
     libcudf                   23.12.00a717    cuda11_231212_gfd2f6a6fd1_717    rapidsai-nightly
     libcufile                 1.4.0.31                      0    nvidia
     libcufile-dev             1.4.0.31                      0    nvidia
     libcurl                   8.5.0                hca28451_0    conda-forge
     libedit                   3.1.20191231         he28a2e2_2    conda-forge
     libev                     4.33                 hd590300_2    conda-forge
     libevent                  2.1.12               hf998b51_1    conda-forge
     libffi                    3.4.2                h7f98852_5    conda-forge
     libgcc-ng                 13.2.0               h807b86a_3    conda-forge
     libgfortran-ng            13.2.0               h69a702a_3    conda-forge
     libgfortran5              13.2.0               ha4646dd_3    conda-forge
     libgomp                   13.2.0               h807b86a_3    conda-forge
     libgoogle-cloud           2.12.0               h5206363_4    conda-forge
     libgrpc                   1.59.3               hd6c4280_0    conda-forge
     libiconv                  1.17                 hd590300_1    conda-forge
     libkvikio                 23.12.00a       cuda11_231212_g26efdd1_23    rapidsai-nightly
     liblapack                 3.9.0           20_linux64_openblas    conda-forge
     libllvm14                 14.0.6               hcd5def8_4    conda-forge
     libllvm15                 15.0.7               hb3ce162_4    conda-forge
     libnghttp2                1.58.0               h47da74e_1    conda-forge
     libnsl                    2.0.1                hd590300_0    conda-forge
     libnuma                   2.0.16               h0b41bf4_1    conda-forge
     libopenblas               0.3.25          pthreads_h413a1c8_0    conda-forge
     libparquet                14.0.1           h352af49_9_cpu    conda-forge
     libprotobuf               4.24.4               hf27288f_0    conda-forge
     libre2-11                 2023.06.02           h7a70373_0    conda-forge
     librmm                    23.12.00             h4725429_0    conda-forge
     libsqlite                 3.44.2               h2797004_0    conda-forge
     libssh2                   1.11.0               h0841786_0    conda-forge
     libstdcxx-ng              13.2.0               h7e041cc_3    conda-forge
     libthrift                 0.19.0               hb90f79a_1    conda-forge
     libutf8proc               2.8.0                h166bdaf_0    conda-forge
     libuuid                   2.38.1               h0b41bf4_0    conda-forge
     libxml2                   2.12.2               h232c23b_0    conda-forge
     libzlib                   1.2.13               hd590300_5    conda-forge
     llvmlite                  0.40.1          py310h1b8f574_0    conda-forge
     lz4-c                     1.9.4                hcb278e6_0    conda-forge
     markdown-it-py            3.0.0              pyhd8ed1ab_0    conda-forge
     mdurl                     0.1.0              pyhd8ed1ab_0    conda-forge
     ncurses                   6.4                  h59595ed_2    conda-forge
     numba                     0.57.1          py310h0f6aa51_0    conda-forge
     numpy                     1.24.4          py310ha4c1d20_0    conda-forge
     nvcomp                    3.0.4                h838ba91_1    conda-forge
     nvtx                      0.2.8           py310h2372a71_1    conda-forge
     openssl                   3.2.0                hd590300_1    conda-forge
     orc                       1.9.2                h4b38347_0    conda-forge
     packaging                 23.2               pyhd8ed1ab_0    conda-forge
     pandas                    1.5.3           py310h9b08913_1    conda-forge
     pip                       23.3.1             pyhd8ed1ab_0    conda-forge
     protobuf                  4.24.4          py310h620c231_0    conda-forge
     ptxcompiler               0.8.1           py310h70a93da_2    conda-forge
     pyarrow                   14.0.1          py310hf9e7431_9_cpu    conda-forge
     pygments                  2.17.2             pyhd8ed1ab_0    conda-forge
     python                    3.10.13         hd12c33a_0_cpython    conda-forge
     python-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge
     python_abi                3.10                    4_cp310    conda-forge
     pytz                      2023.3.post1       pyhd8ed1ab_0    conda-forge
     rdma-core                 49.0                 hd3aeb46_1    conda-forge
     re2                       2023.06.02           h2873b5e_0    conda-forge
     readline                  8.2                  h8228510_1    conda-forge
     rich                      13.7.0             pyhd8ed1ab_0    conda-forge
     rmm                       23.12.00        cuda11_py310_231206_g2db5cbb3_0    rapidsai
     s2n                       1.4.0                h06160fa_0    conda-forge
     setuptools                68.2.2             pyhd8ed1ab_0    conda-forge
     six                       1.16.0             pyh6c4a22f_0    conda-forge
     snappy                    1.1.10               h9fff704_0    conda-forge
     spdlog                    1.12.0               hd2e6256_2    conda-forge
     tk                        8.6.13          noxft_h4845f30_101    conda-forge
     typing_extensions         4.9.0              pyha770c72_0    conda-forge
     tzdata                    2023c                h71feb2d_0    conda-forge
     ucx                       1.15.0               hae80064_1    conda-forge
     wheel                     0.42.0             pyhd8ed1ab_0    conda-forge
     xz                        5.2.6                h166bdaf_0    conda-forge
     zstd                      1.5.5                hfc55251_0    conda-forge
     
</pre></details>

**Additional context**

I'm actually using C++, but chose Python to reproduce.
",2023-12-12T19:27:33Z,0,0,David Li,,False
715,[BUG] `as_column` of pandas timestamps delivers different resolution datetime depending on whether we pass a scalar or list,"**Describe the bug**

```
import pandas as pd
from cudf.core.column import as_column

data = pd.Timestamp(""2000-01-01"")

from_scalar = as_column(data)
from_list = as_column([data])

assert from_scalar.dtype == from_list.dtype # False
```

**Expected behavior**

The resolution should be inferred consistently. Note that `cudf.Scalar(data)` infers the same (nanosecond) resolution as `as_column([data])`.",2023-12-14T11:33:07Z,0,0,Lawrence Mitchell,,False
716,[QST] Calling cudf.dataframe.apply from c++ or porting to libcudf ,"Dear Rapids.Ai Team,

in the cuDF python API documentation there are several methods which are not in libcudf for c++:
cudf.dataframe.apply
cudf.dataframe.applymap
cudf.dataframe.apply_rows
cudf.dataframe.apply_chunks

1) Is there any chance that those functions will be made available in libcudf for c++ ?
2) Is there a way we could call the cuDF python functions from libcudf c++ context or from a general c++ context using pybind11 or  python c-api ?
3) Could you enhance the examples section with such a code which shows how to call python cuDF from c++ ?

Best regards
Developer
",2023-12-14T13:10:54Z,0,0,,Freelancer,False
717,"[BUG] For certain parquet list schemas, the root PageNestingInfo struct can end up uninitialized.","
Inside of the `allocate_nesting_info` function, we allocate PageNestingInfo and PageNestingDecodeInfo structs and initialize them.  However, the logic for traversing the schema in the file can sometimes leave the 0th element uninitialized.  This is a mild bug that leads to a slightly wrong size calculating for output chunk sizes in the chunked reader.  

The easiest way to repro is with the file `python/cudf/cudf/tests/data/parquet/one_level_list.parquet`

",2023-12-14T21:40:52Z,0,0,,,False
718,[QST] How'd this work with multi-threading pandas ? ,"**What is your question?**
I have a multi-thread pandas process to divide large dataset by date ranges  then combine them,  trying cudf now for speed . 

It does something like this ,  args contains the start/end_dt for each segment, run to process segments
while Pool() as pool : 
   result = pool.starmap(run,  args)

However , running in cudf it  threw this error just now. 
__pickle.PicklingError: Can't pickle <built-in function _timedelta_unpickle>: it's not the same object as pandas._libs.tslibs.timedeltas._timedelta_unpickle_
",2023-12-20T07:47:29Z,0,0,Henry Zhang,,False
719,Remove arrow dependency in cuIO by implementing a `host_buffer_source`,"Currently we use `arrow::io::BufferReader` to implement a zero-copy datasource for host buffers. This is a feature we could implement ourselves and remove the last of arrow dependency in cuIO.

A requirement for this source type would be to avoid copying the data when calling `std::unique_ptr<datasource::buffer> host_read(size_t offset, size_t size)`. This is what `device_buffer_source` alrady does in its d`evice_read` implementation.",2023-12-20T21:27:07Z,0,0,Vukasin Milovanovic,NVIDIA,True
720,[QST] Does the read_json() method support GPU acceleration?,"At first, I see this article: [GPU-Accelerated JSON Data Processing with RAPIDS](https://developer.nvidia.com/blog/gpu-accelerated-json-data-processing-with-rapids/)
I follow it to use the cudf.read_json(), but I get the warning
`UserWarning: Using CPU via Pandas to read JSON dataset, this may be GPU accelerated in the future` 
and I use `%%cudf.pandas.line_profile`, it shows there is no GPU TIME.
![image](https://github.com/rapidsai/cudf/assets/76741680/e96c4355-e6f9-430e-be36-71a78159ebd7)

But, when I load the cudf before by running `%load_ext cudf.pandas`
and I change `import cudf as pd` to `import pandas as pd`
It still has the warning, but show the GPU TIME.
So I want to know does the read_json() method support GPU acceleration?
![image](https://github.com/rapidsai/cudf/assets/76741680/5d4465aa-7b41-47af-bfbf-9692ff1025c8)

",2023-12-25T10:13:46Z,0,0,TX,NJUPT,False
721,"[BUG] The read_json() method of cudf can't parse the string like ""5-0""","**Describe the bug**
The read_json() method of cudf can't parse the string like ""5-0"".
It seems that the number in front cannot be larger than the number in the back.

**Steps/Code to reproduce bug**
run code
```
import cudf as pd

data = '''[{""id"":""1"",""Col_01"":""test"",""Col_02"":""77""},

{""id"":""2"",""Col_01"":""test"",""Col_02"":""1355-0652142""}]
'''

df = pd.read_json(data, orient = ""records"")

df
```
get error
![image](https://github.com/rapidsai/cudf/assets/76741680/b8998b5d-a4e6-4ea3-83d0-4474dccfc39b)

If I use the normal pandas, it can get the correct answer.
```
import pandas as pd

data = '''[{""id"":""1"",""Col_01"":""test"",""Col_02"":""77""},

{""id"":""2"",""Col_01"":""test"",""Col_02"":""1355-0652142""}]
'''

df = pd.read_json(data, orient = ""records"")

df
```
If I change the `5-0` to `5-5` and still use cudf, it also right.
```
import cudf as pd

data = '''[{""id"":""1"",""Col_01"":""test"",""Col_02"":""77""},

{""id"":""2"",""Col_01"":""test"",""Col_02"":""1355-5652142""}]
'''

df = pd.read_json(data, orient = ""records"")

df
```

**Environment overview (please complete the following information)**
 - Environment location: Bare-metal
 - Method of cuDF install: pip

",2023-12-25T10:32:51Z,0,0,TX,NJUPT,False
722,[BUG] Type checks fail with cuDF pandas objects,"**Describe the bug**
Some type checks fail with cuDF pandas objects.

**Steps/Code to reproduce bug**
The following examples fail with assertion errors:
```
import cudf.pandas
cudf.pandas.install()
import pandas as pd

freq = ""D""
assert isinstance(pd.tseries.frequencies.to_offset(freq), pd.tseries.offsets.BaseOffset)
```

```
import cudf.pandas
import numpy as np
cudf.pandas.install()
import pandas as pd

df = pd.DataFrame([0, 1, 2])

assert isinstance(df.to_numpy(), np.ndarray)
```

Both of these examples pass if we remove the `cudf.pandas.install()` line.

**Expected behavior**
I expected the code blocks above to run so that I could use the accelerated version of pandas with zero code changes. The errors I'm facing make it difficult to work with cuDF pandas and other libraries (e.g. https://github.com/Nixtla/statsforecast).

**Environment overview (please complete the following information)**
 - Environment location: Bare-metal
 - Method of cuDF install: conda

**Environment details**

<details><summary>Click here to see environment details</summary><pre>
     
     **git***
     Not inside a git repository
     
     ***OS Information***
     DISTRIB_ID=Ubuntu
     DISTRIB_RELEASE=20.04
     DISTRIB_CODENAME=focal
     DISTRIB_DESCRIPTION=""Ubuntu 20.04.4 LTS""
     NAME=""Ubuntu""
     VERSION=""20.04.4 LTS (Focal Fossa)""
     ID=ubuntu
     ID_LIKE=debian
     PRETTY_NAME=""Ubuntu 20.04.4 LTS""
     VERSION_ID=""20.04""
     HOME_URL=""https://www.ubuntu.com/""
     SUPPORT_URL=""https://help.ubuntu.com/""
     BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
     PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
     VERSION_CODENAME=focal
     UBUNTU_CODENAME=focal
     Linux TurinTech-0004 5.15.0-91-generic #101~20.04.1-Ubuntu SMP Thu Nov 16 14:22:28 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
     
     ***GPU Information***
     Wed Dec 27 21:18:49 2023
     +-----------------------------------------------------------------------------+
     | NVIDIA-SMI 525.147.05   Driver Version: 525.147.05   CUDA Version: 12.0     |
     |-------------------------------+----------------------+----------------------+
     | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
     | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
     |                               |                      |               MIG M. |
     |===============================+======================+======================|
     |   0  NVIDIA GeForce ...  Off  | 00000000:09:00.0 Off |                  N/A |
     |  0%   54C    P8    22W / 170W |   1681MiB / 12288MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     
     +-----------------------------------------------------------------------------+
     | Processes:                                                                  |
     |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
     |        ID   ID                                                   Usage      |
     |=============================================================================|
     |    0   N/A  N/A      1512      G   /usr/lib/xorg/Xorg                198MiB |
     |    0   N/A  N/A      2685      G   /usr/lib/xorg/Xorg               1239MiB |
     |    0   N/A  N/A      2814      G   /usr/bin/gnome-shell               22MiB |
     |    0   N/A  N/A      3403      G   ...AAAAAAAAA= --shared-files      198MiB |
     |    0   N/A  N/A      5817      G   gnome-control-center                2MiB |
     +-----------------------------------------------------------------------------+
     
     ***CPU***
     Architecture:                       x86_64
     CPU op-mode(s):                     32-bit, 64-bit
     Byte Order:                         Little Endian
     Address sizes:                      48 bits physical, 48 bits virtual
     CPU(s):                             24
     On-line CPU(s) list:                0-23
     Thread(s) per core:                 2
     Core(s) per socket:                 12
     Socket(s):                          1
     NUMA node(s):                       1
     Vendor ID:                          AuthenticAMD
     CPU family:                         25
     Model:                              33
     Model name:                         AMD Ryzen 9 5900X 12-Core Processor
     Stepping:                           0
     Frequency boost:                    enabled
     CPU MHz:                            3597.987
     CPU max MHz:                        3700.0000
     CPU min MHz:                        2200.0000
     BogoMIPS:                           7386.52
     Virtualisation:                     AMD-V
     L1d cache:                          384 KiB
     L1i cache:                          384 KiB
     L2 cache:                           6 MiB
     L3 cache:                           64 MiB
     NUMA node0 CPU(s):                  0-23
     Vulnerability Gather data sampling: Not affected
     Vulnerability Itlb multihit:        Not affected
     Vulnerability L1tf:                 Not affected
     Vulnerability Mds:                  Not affected
     Vulnerability Meltdown:             Not affected
     Vulnerability Mmio stale data:      Not affected
     Vulnerability Retbleed:             Not affected
     Vulnerability Spec rstack overflow: Mitigation; safe RET, no microcode
     Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp
     Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
     Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected
     Vulnerability Srbds:                Not affected
     Vulnerability Tsx async abort:      Not affected
     Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm
     
     ***CMake***
     
     ***g++***
     /usr/bin/g++
     g++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
     Copyright (C) 2019 Free Software Foundation, Inc.
     This is free software; see the source for copying conditions.  There is NO
     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
     
     
     ***nvcc***
     /home/paul/anaconda3/envs/rapids-23.12/bin/nvcc
     nvcc: NVIDIA (R) Cuda compiler driver
     Copyright (c) 2005-2022 NVIDIA Corporation
     Built on Mon_Oct_24_19:12:58_PDT_2022
     Cuda compilation tools, release 12.0, V12.0.76
     Build cuda_12.0.r12.0/compiler.31968024_0
     
     ***Python***
     /home/paul/anaconda3/envs/rapids-23.12/bin/python
     Python 3.9.18
     
     ***Environment Variables***
     PATH                            : /home/paul/.local/bin:/home/paul/.cargo/bin:/home/paul/anaconda3/envs/rapids-23.12/bin:/home/paul/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
     LD_LIBRARY_PATH                 :
     NUMBAPRO_NVVM                   :
     NUMBAPRO_LIBDEVICE              :
     CONDA_PREFIX                    : /home/paul/anaconda3/envs/rapids-23.12
     PYTHON_PATH                     :
     
     ***conda packages***
     /home/paul/anaconda3/condabin/conda
     # packages in environment at /home/paul/anaconda3/envs/rapids-23.12:
     #
     # Name                    Version                   Build  Channel
     _libgcc_mutex             0.1                 conda_forge    conda-forge
     _openmp_mutex             4.5                       2_gnu    conda-forge
     accelerate                0.19.0                   pypi_0    pypi
     adagio                    0.2.4                    pypi_0    pypi
     aiohttp                   3.9.1            py39hd1e30aa_0    conda-forge
     aiosignal                 1.3.1              pyhd8ed1ab_0    conda-forge
     alabaster                 0.7.13                   pypi_0    pypi
     alembic                   1.13.1                   pypi_0    pypi
     antlr4-python3-runtime    4.11.1                   pypi_0    pypi
     anyio                     4.2.0              pyhd8ed1ab_0    conda-forge
     aom                       3.7.1                h59595ed_0    conda-forge
     appdirs                   1.4.4              pyh9f0ad1d_0    conda-forge
     argon2-cffi               23.1.0             pyhd8ed1ab_0    conda-forge
     argon2-cffi-bindings      21.2.0           py39hd1e30aa_4    conda-forge
     arrow                     1.3.0              pyhd8ed1ab_0    conda-forge
     astroid                   2.15.8                   pypi_0    pypi
     async-timeout             4.0.3              pyhd8ed1ab_0    conda-forge
     attrs                     23.1.0             pyh71513ae_1    conda-forge
     aws-c-auth                0.7.8                hcf8cf63_3    conda-forge
     aws-c-cal                 0.6.9                h5d48c4d_2    conda-forge
     aws-c-common              0.9.10               hd590300_0    conda-forge
     aws-c-compression         0.2.17               h7f92143_7    conda-forge
     aws-c-event-stream        0.3.2                h0bcb0bb_8    conda-forge
     aws-c-http                0.7.15               hd268abd_0    conda-forge
     aws-c-io                  0.13.36              he0cd244_2    conda-forge
     aws-c-mqtt                0.10.0               hbafccad_1    conda-forge
     aws-c-s3                  0.4.5                h47b1690_1    conda-forge
     aws-c-sdkutils            0.1.13               h7f92143_0    conda-forge
     aws-checksums             0.1.17               h7f92143_6    conda-forge
     aws-crt-cpp               0.25.0               h169d4cb_3    conda-forge
     aws-sdk-cpp               1.11.210             h0853bfa_5    conda-forge
     azure-core-cpp            1.10.3               h91d86a7_0    conda-forge
     azure-storage-blobs-cpp   12.10.0              h00ab1b0_0    conda-forge
     azure-storage-common-cpp  12.5.0               hb858b4b_2    conda-forge
     babel                     2.14.0                   pypi_0    pypi
     beautifulsoup4            4.12.2             pyha770c72_0    conda-forge
     black                     23.7.0                   pypi_0    pypi
     bleach                    6.1.0              pyhd8ed1ab_0    conda-forge
     blosc                     1.21.5               h0f2a231_0    conda-forge
     bokeh                     3.3.2              pyhd8ed1ab_0    conda-forge
     branca                    0.7.0              pyhd8ed1ab_1    conda-forge
     brotli                    1.1.0                hd590300_1    conda-forge
     brotli-bin                1.1.0                hd590300_1    conda-forge
     brotli-python             1.1.0            py39h3d6467e_1    conda-forge
     brunsli                   0.1                  h9c3ff4c_0    conda-forge
     bzip2                     1.0.8                hd590300_5    conda-forge
     c-ares                    1.24.0               hd590300_0    conda-forge
     c-blosc2                  2.11.3               hb4ffafa_0    conda-forge
     ca-certificates           2023.11.17           hbcca054_0    conda-forge
     cached-property           1.5.2                hd8ed1ab_1    conda-forge
     cached_property           1.5.2              pyha770c72_1    conda-forge
     cachetools                5.3.2              pyhd8ed1ab_0    conda-forge
     cairo                     1.18.0               h3faef2a_0    conda-forge
     catboost                  1.2.2                    pypi_0    pypi
     category-encoders         2.6.3                    pypi_0    pypi
     certifi                   2023.11.17         pyhd8ed1ab_0    conda-forge
     cffi                      1.16.0           py39h7a31438_0    conda-forge
     cfgv                      3.4.0                    pypi_0    pypi
     cfitsio                   4.3.1                hbdc6101_0    conda-forge
     charls                    2.4.2                h59595ed_0    conda-forge
     charset-normalizer        3.3.2              pyhd8ed1ab_0    conda-forge
     click                     8.0.4                    pypi_0    pypi
     click-plugins             1.1.1                      py_0    conda-forge
     cligj                     0.7.2              pyhd8ed1ab_1    conda-forge
     cloudpickle               3.0.0              pyhd8ed1ab_0    conda-forge
     cmaes                     0.10.0                   pypi_0    pypi
     colorama                  0.4.6              pyhd8ed1ab_0    conda-forge
     colorcet                  3.0.1              pyhd8ed1ab_0    conda-forge
     colorlog                  6.8.0                    pypi_0    pypi
     contourpy                 1.2.0            py39h7633fee_0    conda-forge
     coverage                  7.3.4                    pypi_0    pypi
     cryptography              41.0.7                   pypi_0    pypi
     cucim                     23.12.01        cuda12_py39_231211_ga3445df_0    rapidsai
     cuda-cccl_linux-64        12.0.90              ha770c72_1    conda-forge
     cuda-cudart               12.0.107             hd3aeb46_8    conda-forge
     cuda-cudart-dev           12.0.107             hd3aeb46_8    conda-forge
     cuda-cudart-dev_linux-64  12.0.107             h59595ed_8    conda-forge
     cuda-cudart-static        12.0.107             hd3aeb46_8    conda-forge
     cuda-cudart-static_linux-64 12.0.107             h59595ed_8    conda-forge
     cuda-cudart_linux-64      12.0.107             h59595ed_8    conda-forge
     cuda-nvcc-dev_linux-64    12.0.76              ha770c72_1    conda-forge
     cuda-nvcc-impl            12.0.76              h59595ed_1    conda-forge
     cuda-nvcc-tools           12.0.76              h59595ed_1    conda-forge
     cuda-nvrtc                12.0.76              hd3aeb46_2    conda-forge
     cuda-nvtx                 12.0.76              h59595ed_1    conda-forge
     cuda-profiler-api         12.0.76              ha770c72_0    conda-forge
     cuda-python               12.0.0           py39h2d39e0c_4    conda-forge
     cuda-version              12.0                 hffde075_2    conda-forge
     cudf                      23.12.01        cuda12_py39_231208_g2ce46216b5_0    rapidsai
     cudf_kafka                23.12.01        cuda12_py39_231208_g2ce46216b5_0    rapidsai
     cugraph                   23.12.00        cuda12_py39_231206_g1309813f_0    rapidsai
     cuml                      23.12.00        cuda12_py39_231206_gad2bd2b65_0    rapidsai
     cuproj                    23.12.00        cuda12_py39_231206_g3a357729_0    rapidsai
     cupy                      12.3.0           py39hc7c1505_0    conda-forge
     cuspatial                 23.12.01        cuda12_py39_231207_g16727064_0    rapidsai
     custreamz                 23.12.01        cuda12_py39_231208_g2ce46216b5_0    rapidsai
     cuxfilter                 23.12.00        cuda12_py39_231206_g63dabeb_0    rapidsai
     cycler                    0.12.1             pyhd8ed1ab_0    conda-forge
     cyrus-sasl                2.1.27               h54b06d7_7    conda-forge
     cython                    3.0.7                    pypi_0    pypi
     cytoolz                   0.12.2           py39hd1e30aa_1    conda-forge
     daal                      2023.2.0                 pypi_0    pypi
     daal4py                   2023.2.0                 pypi_0    pypi
     darts                     0.27.1                   pypi_0    pypi
     dask                      2023.11.0          pyhd8ed1ab_0    conda-forge
     dask-core                 2023.11.0          pyhd8ed1ab_0    conda-forge
     dask-cuda                 23.12.00        py39_231206_ge1638ae_0    rapidsai
     dask-cudf                 23.12.01        cuda12_py39_231208_g2ce46216b5_0    rapidsai
     datasets                  2.15.0                   pypi_0    pypi
     datashader                0.16.0             pyhd8ed1ab_0    conda-forge
     dav1d                     1.2.1                hd590300_0    conda-forge
     defusedxml                0.7.1              pyhd8ed1ab_0    conda-forge
     deprecated                1.2.14                   pypi_0    pypi
     dill                      0.3.7                    pypi_0    pypi
     distlib                   0.3.8                    pypi_0    pypi
     distributed               2023.11.0          pyhd8ed1ab_0    conda-forge
     dlpack                    0.5                  h9c3ff4c_0    conda-forge
     docutils                  0.18.1                   pypi_0    pypi
     entrypoints               0.4                pyhd8ed1ab_0    conda-forge
     exceptiongroup            1.2.0              pyhd8ed1ab_0    conda-forge
     execnet                   2.0.2                    pypi_0    pypi
     expat                     2.5.0                hcb278e6_1    conda-forge
     fastrlock                 0.8.2            py39h3d6467e_2    conda-forge
     filelock                  3.13.1                   pypi_0    pypi
     fiona                     1.9.5            py39hcfcd403_2    conda-forge
     flake8                    6.0.0                    pypi_0    pypi
     fmt                       9.1.0                h924138e_0    conda-forge
     folium                    0.15.1             pyhd8ed1ab_0    conda-forge
     font-ttf-dejavu-sans-mono 2.37                 hab24e00_0    conda-forge
     font-ttf-inconsolata      3.000                h77eed37_0    conda-forge
     font-ttf-source-code-pro  2.038                h77eed37_0    conda-forge
     font-ttf-ubuntu           0.83                 h77eed37_1    conda-forge
     fontconfig                2.14.2               h14ed4e7_0    conda-forge
     fonts-conda-ecosystem     1                             0    conda-forge
     fonts-conda-forge         1                             0    conda-forge
     fonttools                 4.47.0           py39hd1e30aa_0    conda-forge
     fqdn                      1.5.1              pyhd8ed1ab_0    conda-forge
     freetype                  2.12.1               h267a509_2    conda-forge
     freexl                    2.0.0                h743c826_0    conda-forge
     frozendict                2.3.10                   pypi_0    pypi
     frozenlist                1.4.1            py39hd1e30aa_0    conda-forge
     fs                        2.4.16                   pypi_0    pypi
     fsspec                    2023.10.0                pypi_0    pypi
     fugue                     0.8.7                    pypi_0    pypi
     fugue-sql-antlr           0.2.0                    pypi_0    pypi
     gdal                      3.8.2            py39h14df8fe_0    conda-forge
     gdk-pixbuf                2.42.10              h829c605_4    conda-forge
     geopandas                 0.14.1             pyhd8ed1ab_0    conda-forge
     geopandas-base            0.14.1             pyha770c72_0    conda-forge
     geos                      3.12.1               h59595ed_0    conda-forge
     geotiff                   1.7.1               h6b2125f_15    conda-forge
     gettext                   0.21.1               h27087fc_0    conda-forge
     gflags                    2.2.2             he1b5a44_1004    conda-forge
     giflib                    5.2.1                h0b41bf4_3    conda-forge
     glog                      0.6.0                h6f12383_0    conda-forge
     gmock                     1.14.0               ha770c72_1    conda-forge
     greenlet                  3.0.3                    pypi_0    pypi
     gtest                     1.14.0               h00ab1b0_1    conda-forge
     hdf4                      4.2.15               h2a13503_7    conda-forge
     hdf5                      1.14.3          nompi_h4f84152_100    conda-forge
     holidays                  0.27                     pypi_0    pypi
     holoviews                 1.18.1             pyhd8ed1ab_0    conda-forge
     huggingface-hub           0.20.1                   pypi_0    pypi
     icu                       73.2                 h59595ed_0    conda-forge
     identify                  2.5.33                   pypi_0    pypi
     idna                      3.6                pyhd8ed1ab_0    conda-forge
     imagecodecs               2023.9.18        py39hf9b8f0e_2    conda-forge
     imageio                   2.33.1             pyh8c1a49c_0    conda-forge
     imagesize                 1.4.1                    pypi_0    pypi
     imbalanced-learn          0.11.0                   pypi_0    pypi
     importlib-metadata        7.0.0              pyha770c72_0    conda-forge
     importlib-resources       6.1.1              pyhd8ed1ab_0    conda-forge
     importlib_metadata        7.0.0                hd8ed1ab_0    conda-forge
     importlib_resources       6.1.1              pyhd8ed1ab_0    conda-forge
     iniconfig                 2.0.0                    pypi_0    pypi
     isoduration               20.11.0            pyhd8ed1ab_0    conda-forge
     isort                     5.13.2                   pypi_0    pypi
     jaraco-classes            3.3.0                    pypi_0    pypi
     jbig                      2.1               h7f98852_2003    conda-forge
     jeepney                   0.8.0                    pypi_0    pypi
     jinja2                    3.1.2              pyhd8ed1ab_1    conda-forge
     joblib                    1.3.2              pyhd8ed1ab_0    conda-forge
     json-c                    0.17                 h7ab15ed_0    conda-forge
     jsonpointer               2.4              py39hf3d152e_3    conda-forge
     jsonschema                4.20.0             pyhd8ed1ab_0    conda-forge
     jsonschema-specifications 2023.11.2          pyhd8ed1ab_0    conda-forge
     jsonschema-with-format-nongpl 4.20.0             pyhd8ed1ab_0    conda-forge
     jupyter-server-proxy      4.1.0              pyhd8ed1ab_0    conda-forge
     jupyter_client            8.6.0              pyhd8ed1ab_0    conda-forge
     jupyter_core              5.5.1            py39hf3d152e_0    conda-forge
     jupyter_events            0.9.0              pyhd8ed1ab_0    conda-forge
     jupyter_server            2.12.1             pyhd8ed1ab_0    conda-forge
     jupyter_server_terminals  0.5.0              pyhd8ed1ab_0    conda-forge
     jupyterlab_pygments       0.3.0              pyhd8ed1ab_0    conda-forge
     jxrlib                    1.1                  h7f98852_2    conda-forge
     kealib                    1.5.3                h2f55d51_0    conda-forge
     keyring                   24.3.0                   pypi_0    pypi
     keyutils                  1.6.1                h166bdaf_0    conda-forge
     kiwisolver                1.4.5            py39h7633fee_1    conda-forge
     krb5                      1.21.2               h659d440_0    conda-forge
     lazy-object-proxy         1.10.0                   pypi_0    pypi
     lazy_loader               0.3                pyhd8ed1ab_0    conda-forge
     lcms2                     2.16                 hb7c19ff_0    conda-forge
     ld_impl_linux-64          2.40                 h41732ed_0    conda-forge
     lerc                      4.0.0                h27087fc_0    conda-forge
     libabseil                 20230802.1      cxx17_h59595ed_0    conda-forge
     libaec                    1.1.2                h59595ed_1    conda-forge
     libarchive                3.7.2                h2aa1ff5_1    conda-forge
     libarrow                  14.0.2           hfb4d3a9_0_cpu    conda-forge
     libarrow-acero            14.0.2           h59595ed_0_cpu    conda-forge
     libarrow-dataset          14.0.2           h59595ed_0_cpu    conda-forge
     libarrow-flight           14.0.2           h120cb0d_0_cpu    conda-forge
     libarrow-flight-sql       14.0.2           h61ff412_0_cpu    conda-forge
     libarrow-gandiva          14.0.2           hacb8726_0_cpu    conda-forge
     libarrow-substrait        14.0.2           h61ff412_0_cpu    conda-forge
     libavif16                 1.0.3                hef5bec9_1    conda-forge
     libblas                   3.9.0           20_linux64_openblas    conda-forge
     libboost-headers          1.84.0               ha770c72_0    conda-forge
     libbrotlicommon           1.1.0                hd590300_1    conda-forge
     libbrotlidec              1.1.0                hd590300_1    conda-forge
     libbrotlienc              1.1.0                hd590300_1    conda-forge
     libcblas                  3.9.0           20_linux64_openblas    conda-forge
     libcrc32c                 1.1.2                h9c3ff4c_0    conda-forge
     libcublas                 12.0.1.189           hd3aeb46_3    conda-forge
     libcublas-dev             12.0.1.189           hd3aeb46_3    conda-forge
     libcucim                  23.12.01        cuda12_231211_ga3445df_0    rapidsai
     libcudf                   23.12.01        cuda12_231208_g2ce46216b5_0    rapidsai
     libcudf_kafka             23.12.01        cuda12_231208_g2ce46216b5_0    rapidsai
     libcufft                  11.0.0.21            hd3aeb46_2    conda-forge
     libcufile                 1.5.0.59             hd3aeb46_1    conda-forge
     libcufile-dev             1.5.0.59             hd3aeb46_1    conda-forge
     libcugraph                23.12.00        cuda12_231206_g1309813f_0    rapidsai
     libcugraph_etl            23.12.00        cuda12_231206_g1309813f_0    rapidsai
     libcugraphops             23.12.00        cuda12_231206_g42d08202_0    nvidia
     libcuml                   23.12.00        cuda12_231206_gad2bd2b65_0    rapidsai
     libcumlprims              23.12.00        cuda12_231206_gc120fe0_0    nvidia
     libcurand                 10.3.1.50            hd3aeb46_1    conda-forge
     libcurand-dev             10.3.1.50            hd3aeb46_1    conda-forge
     libcurl                   8.5.0                hca28451_0    conda-forge
     libcusolver               11.4.2.57            hd3aeb46_2    conda-forge
     libcusolver-dev           11.4.2.57            hd3aeb46_2    conda-forge
     libcusparse               12.0.0.76            hd3aeb46_2    conda-forge
     libcusparse-dev           12.0.0.76            hd3aeb46_2    conda-forge
     libcuspatial              23.12.01        cuda12_231207_g16727064_0    rapidsai
     libdeflate                1.19                 hd590300_0    conda-forge
     libedit                   3.1.20191231         he28a2e2_2    conda-forge
     libev                     4.33                 hd590300_2    conda-forge
     libevent                  2.1.12               hf998b51_1    conda-forge
     libexpat                  2.5.0                hcb278e6_1    conda-forge
     libffi                    3.4.2                h7f98852_5    conda-forge
     libgcc-ng                 13.2.0               h807b86a_3    conda-forge
     libgdal                   3.8.2                hed8bd54_0    conda-forge
     libgfortran-ng            13.2.0               h69a702a_3    conda-forge
     libgfortran5              13.2.0               ha4646dd_3    conda-forge
     libglib                   2.78.3               h783c2da_0    conda-forge
     libgomp                   13.2.0               h807b86a_3    conda-forge
     libgoogle-cloud           2.12.0               h5206363_4    conda-forge
     libgrpc                   1.59.3               hd6c4280_0    conda-forge
     libiconv                  1.17                 hd590300_2    conda-forge
     libjpeg-turbo             3.0.0                hd590300_1    conda-forge
     libkml                    1.3.0             h01aab08_1018    conda-forge
     libkvikio                 23.12.00        cuda12_231206_gf90bfbe_0    rapidsai
     liblapack                 3.9.0           20_linux64_openblas    conda-forge
     libllvm14                 14.0.6               hcd5def8_4    conda-forge
     libllvm15                 15.0.7               hb3ce162_4    conda-forge
     libnetcdf                 4.9.2           nompi_h9612171_113    conda-forge
     libnghttp2                1.58.0               h47da74e_1    conda-forge
     libnl                     3.9.0                hd590300_0    conda-forge
     libnsl                    2.0.1                hd590300_0    conda-forge
     libntlm                   1.4               h7f98852_1002    conda-forge
     libnuma                   2.0.16               h0b41bf4_1    conda-forge
     libnvjitlink              12.0.76              hd3aeb46_2    conda-forge
     libnvjpeg                 12.0.0.28            h59595ed_1    conda-forge
     libopenblas               0.3.25          pthreads_h413a1c8_0    conda-forge
     libparquet                14.0.2           h352af49_0_cpu    conda-forge
     libpng                    1.6.39               h753d276_0    conda-forge
     libpq                     16.1                 h33b98f1_7    conda-forge
     libprotobuf               4.24.4               hf27288f_0    conda-forge
     libraft                   23.12.00        cuda12_231206_g9e2d6277_0    rapidsai
     libraft-headers           23.12.00        cuda12_231206_g9e2d6277_0    rapidsai
     libraft-headers-only      23.12.00        cuda12_231206_g9e2d6277_0    rapidsai
     librdkafka                1.9.2                ha5a0de0_2    conda-forge
     libre2-11                 2023.06.02           h7a70373_0    conda-forge
     librmm                    23.12.00        cuda12_231206_g2db5cbb3_0    rapidsai
     librttopo                 1.1.0               h8917695_15    conda-forge
     libsodium                 1.0.18               h36c2ea0_1    conda-forge
     libspatialindex           1.9.3                h9c3ff4c_4    conda-forge
     libspatialite             5.1.0                h7bd4643_4    conda-forge
     libsqlite                 3.44.2               h2797004_0    conda-forge
     libssh2                   1.11.0               h0841786_0    conda-forge
     libstdcxx-ng              13.2.0               h7e041cc_3    conda-forge
     libthrift                 0.19.0               hb90f79a_1    conda-forge
     libtiff                   4.6.0                ha9c0a0a_2    conda-forge
     libutf8proc               2.8.0                h166bdaf_0    conda-forge
     libuuid                   2.38.1               h0b41bf4_0    conda-forge
     libuv                     1.46.0               hd590300_0    conda-forge
     libwebp                   1.3.2                h658648e_1    conda-forge
     libwebp-base              1.3.2                hd590300_0    conda-forge
     libxcb                    1.15                 h0b41bf4_0    conda-forge
     libxgboost                1.7.6           rapidsai_h52ede06_7    rapidsai
     libxml2                   2.12.3               h232c23b_0    conda-forge
     libzip                    1.10.1               h2629f0a_3    conda-forge
     libzlib                   1.2.13               hd590300_5    conda-forge
     libzopfli                 1.0.3                h9c3ff4c_0    conda-forge
     lightgbm                  3.3.5                    pypi_0    pypi
     lightning-utilities       0.10.0                   pypi_0    pypi
     linkify-it-py             2.0.0              pyhd8ed1ab_0    conda-forge
     llvmlite                  0.41.1                   pypi_0    pypi
     locket                    1.0.0              pyhd8ed1ab_0    conda-forge
     lz4                       4.3.2            py39h79d96da_1    conda-forge
     lz4-c                     1.9.4                hcb278e6_0    conda-forge
     lzo                       2.10              h516909a_1000    conda-forge
     mako                      1.3.0                    pypi_0    pypi
     mapclassify               2.6.1              pyhd8ed1ab_0    conda-forge
     markdown                  3.5.1              pyhd8ed1ab_0    conda-forge
     markdown-it-py            3.0.0              pyhd8ed1ab_0    conda-forge
     markupsafe                2.1.3            py39hd1e30aa_1    conda-forge
     matplotlib-base           3.8.2            py39he9076e7_0    conda-forge
     mccabe                    0.7.0                    pypi_0    pypi
     mdit-py-plugins           0.4.0              pyhd8ed1ab_0    conda-forge
     mdurl                     0.1.0              pyhd8ed1ab_0    conda-forge
     metaml                    1.0.19rc0                pypi_0    pypi
     minizip                   4.0.3                h0ab5242_0    conda-forge
     mistune                   3.0.2              pyhd8ed1ab_0    conda-forge
     more-itertools            10.1.0                   pypi_0    pypi
     mpmath                    1.3.0                    pypi_0    pypi
     mrmr-selection            0.2.8                    pypi_0    pypi
     msgpack-python            1.0.7            py39h7633fee_0    conda-forge
     multidict                 6.0.4            py39hd1e30aa_1    conda-forge
     multipledispatch          0.6.0                      py_0    conda-forge
     multiprocess              0.70.15                  pypi_0    pypi
     munkres                   1.1.4              pyh9f0ad1d_0    conda-forge
     mypy                      1.4.1                    pypi_0    pypi
     mypy-extensions           1.0.0                    pypi_0    pypi
     nbclient                  0.8.0              pyhd8ed1ab_0    conda-forge
     nbconvert-core            7.13.1             pyhd8ed1ab_0    conda-forge
     nbformat                  5.9.2              pyhd8ed1ab_0    conda-forge
     nccl                      2.19.4.1             h3a97aeb_0    conda-forge
     ncurses                   6.4                  h59595ed_2    conda-forge
     networkx                  3.2                      pypi_0    pypi
     nfoursid                  1.0.1                    pypi_0    pypi
     nh3                       0.2.15                   pypi_0    pypi
     nodeenv                   1.8.0                    pypi_0    pypi
     nodejs                    20.9.0               hb753e55_0    conda-forge
     nspr                      4.35                 h27087fc_0    conda-forge
     nss                       3.96                 h1d7d5a4_0    conda-forge
     nuitka                    1.7.5                    pypi_0    pypi
     numba                     0.58.1                   pypi_0    pypi
     numpy                     1.24.4           py39h6183b62_0    conda-forge
     nvcomp                    3.0.4                h10b603f_1    conda-forge
     nvidia-cublas-cu12        12.1.3.1                 pypi_0    pypi
     nvidia-cuda-cupti-cu12    12.1.105                 pypi_0    pypi
     nvidia-cuda-nvrtc-cu12    12.1.105                 pypi_0    pypi
     nvidia-cuda-runtime-cu12  12.1.105                 pypi_0    pypi
     nvidia-cudnn-cu12         8.9.2.26                 pypi_0    pypi
     nvidia-cufft-cu12         11.0.2.54                pypi_0    pypi
     nvidia-curand-cu12        10.3.2.106               pypi_0    pypi
     nvidia-cusolver-cu12      11.4.5.107               pypi_0    pypi
     nvidia-cusparse-cu12      12.1.0.106               pypi_0    pypi
     nvidia-nccl-cu12          2.18.1                   pypi_0    pypi
     nvidia-nvjitlink-cu12     12.3.101                 pypi_0    pypi
     nvidia-nvtx-cu12          12.1.105                 pypi_0    pypi
     nvtx                      0.2.8            py39hd1e30aa_1    conda-forge
     openjpeg                  2.5.0                h488ebb8_3    conda-forge
     openslide                 3.4.1               h58ba908_12    conda-forge
     openssl                   3.2.0                hd590300_1    conda-forge
     optuna                    3.3.0                    pypi_0    pypi
     orc                       1.9.2                h4b38347_0    conda-forge
     ordered-set               4.1.0                    pypi_0    pypi
     overrides                 7.4.0              pyhd8ed1ab_0    conda-forge
     packaging                 23.2               pyhd8ed1ab_0    conda-forge
     pandas                    1.5.3            py39h2ad29b5_1    conda-forge
     pandocfilters             1.5.0              pyhd8ed1ab_0    conda-forge
     panel                     1.3.4              pyhd8ed1ab_0    conda-forge
     param                     2.0.1              pyhca7485f_0    conda-forge
     partd                     1.4.1              pyhd8ed1ab_0    conda-forge
     pathspec                  0.12.1                   pypi_0    pypi
     patsy                     0.5.4                    pypi_0    pypi
     pcre2                     10.42                hcad00b1_0    conda-forge
     pillow                    10.1.0           py39had0adad_0    conda-forge
     pip                       23.3.2             pyhd8ed1ab_0    conda-forge
     pipdeptree                2.10.2                   pypi_0    pypi
     pixman                    0.42.2               h59595ed_0    conda-forge
     pkginfo                   1.9.6                    pypi_0    pypi
     pkgutil-resolve-name      1.3.10             pyhd8ed1ab_1    conda-forge
     platformdirs              4.1.0              pyhd8ed1ab_0    conda-forge
     plotly                    5.18.0                   pypi_0    pypi
     pluggy                    1.3.0                    pypi_0    pypi
     pmdarima                  2.0.4                    pypi_0    pypi
     polars                    0.20.2                   pypi_0    pypi
     poppler                   23.12.0              h590f24d_0    conda-forge
     poppler-data              0.4.12               hd8ed1ab_0    conda-forge
     portion                   2.4.2                    pypi_0    pypi
     postgresql                16.1                 h7387d8b_7    conda-forge
     pre-commit                3.3.3                    pypi_0    pypi
     proj                      9.3.1                h1d62c97_0    conda-forge
     prometheus_client         0.19.0             pyhd8ed1ab_0    conda-forge
     protobuf                  3.20.3                   pypi_0    pypi
     psutil                    5.9.7            py39hd1e30aa_0    conda-forge
     pthread-stubs             0.4               h36c2ea0_1001    conda-forge
     ptyprocess                0.7.0              pyhd3deb0d_0    conda-forge
     py-xgboost                1.7.6           rapidsai_py39h84d37f7_7    rapidsai
     pyarrow                   14.0.2          py39h6925388_0_cpu    conda-forge
     pyarrow-hotfix            0.6                pyhd8ed1ab_0    conda-forge
     pycodestyle               2.10.0                   pypi_0    pypi
     pycparser                 2.21               pyhd8ed1ab_0    conda-forge
     pyct                      0.5.0              pyhd8ed1ab_0    conda-forge
     pydantic                  1.10.13                  pypi_0    pypi
     pyee                      8.1.0              pyhd8ed1ab_0    conda-forge
     pyflakes                  3.0.1                    pypi_0    pypi
     pygments                  2.17.2             pyhd8ed1ab_0    conda-forge
     pylibcugraph              23.12.00        cuda12_py39_231206_g1309813f_0    rapidsai
     pylibraft                 23.12.00        cuda12_py39_231206_g9e2d6277_0    rapidsai
     pylint                    2.17.4                   pypi_0    pypi
     pynndescent               0.5.11                   pypi_0    pypi
     pynvml                    11.4.1             pyhd8ed1ab_0    conda-forge
     pyod                      1.1.2                    pypi_0    pypi
     pyparsing                 3.1.1              pyhd8ed1ab_0    conda-forge
     pyppeteer                 1.0.2              pyhd8ed1ab_0    conda-forge
     pyproj                    3.6.1            py39h15b0fa6_5    conda-forge
     pysocks                   1.7.1              pyha2e5f31_6    conda-forge
     pytest                    7.4.0                    pypi_0    pypi
     pytest-cov                4.1.0                    pypi_0    pypi
     pytest-split              0.8.1                    pypi_0    pypi
     pytest-xdist              3.3.1                    pypi_0    pypi
     python                    3.9.18          h0755675_0_cpython    conda-forge
     python-confluent-kafka    1.9.2            py39hb9d737c_2    conda-forge
     python-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge
     python-fastjsonschema     2.19.0             pyhd8ed1ab_0    conda-forge
     python-graphviz           0.20.1                   pypi_0    pypi
     python-json-logger        2.0.7              pyhd8ed1ab_0    conda-forge
     python_abi                3.9                      4_cp39    conda-forge
     pytorch-lightning         2.1.3                    pypi_0    pypi
     pytz                      2023.3.post1       pyhd8ed1ab_0    conda-forge
     pyviz_comms               3.0.0              pyhd8ed1ab_0    conda-forge
     pywavelets                1.4.1            py39h44dd56e_1    conda-forge
     pyyaml                    6.0.1            py39hd1e30aa_1    conda-forge
     pyzmq                     25.1.2           py39h8c080ef_0    conda-forge
     qpd                       0.4.4                    pypi_0    pypi
     raft-dask                 23.12.00        cuda12_py39_231206_g9e2d6277_0    rapidsai
     rapids                    23.12.00        cuda12_py39_231206_g1d8bed4_0    rapidsai
     rapids-dask-dependency    23.12.01                      0    rapidsai
     rapids-xgboost            23.12.00        cuda12_py39_231206_g1d8bed4_0    rapidsai
     rav1e                     0.6.6                he8a937b_2    conda-forge
     rdma-core                 49.0                 hd3aeb46_2    conda-forge
     re2                       2023.06.02           h2873b5e_0    conda-forge
     readline                  8.2                  h8228510_1    conda-forge
     readme-renderer           42.0                     pypi_0    pypi
     referencing               0.32.0             pyhd8ed1ab_0    conda-forge
     regex                     2023.10.3                pypi_0    pypi
     requests                  2.31.0             pyhd8ed1ab_0    conda-forge
     requests-toolbelt         1.0.0                    pypi_0    pypi
     rfc3339-validator         0.1.4              pyhd8ed1ab_0    conda-forge
     rfc3986                   2.0.0                    pypi_0    pypi
     rfc3986-validator         0.1.1              pyh9f0ad1d_0    conda-forge
     rich                      13.7.0             pyhd8ed1ab_0    conda-forge
     rmm                       23.12.00        cuda12_py39_231206_g2db5cbb3_0    rapidsai
     rpds-py                   0.15.2           py39h9fdd4d6_0    conda-forge
     rtree                     1.1.0            py39hb102c33_0    conda-forge
     ruamel-yaml               0.18.5                   pypi_0    pypi
     ruamel-yaml-clib          0.2.8                    pypi_0    pypi
     s2n                       1.4.0                h06160fa_0    conda-forge
     scikit-image              0.21.0           py39h3d6467e_0    conda-forge
     scikit-learn              1.1.3                    pypi_0    pypi
     scikit-learn-intelex      2023.2.0                 pypi_0    pypi
     scipy                     1.11.4           py39h474f0d3_0    conda-forge
     secretstorage             3.3.3                    pypi_0    pypi
     send2trash                1.8.2              pyh41d4057_0    conda-forge
     setuptools                68.2.2             pyhd8ed1ab_0    conda-forge
     shap                      0.44.0                   pypi_0    pypi
     shapely                   2.0.2            py39h6404dd3_1    conda-forge
     simpervisor               1.0.0              pyhd8ed1ab_0    conda-forge
     six                       1.16.0             pyh6c4a22f_0    conda-forge
     sklearn-contrib-lightning 0.6.2.post0              pypi_0    pypi
     sktime                    0.17.1                   pypi_0    pypi
     slicer                    0.0.7                    pypi_0    pypi
     snappy                    1.1.10               h9fff704_0    conda-forge
     sniffio                   1.3.0              pyhd8ed1ab_0    conda-forge
     snowballstemmer           2.2.0                    pypi_0    pypi
     sortedcontainers          2.4.0              pyhd8ed1ab_0    conda-forge
     soupsieve                 2.5                pyhd8ed1ab_1    conda-forge
     spdlog                    1.11.0               h9b3ece8_1    conda-forge
     sphinx                    6.2.1                    pypi_0    pypi
     sphinx-rtd-theme          1.2.2                    pypi_0    pypi
     sphinxcontrib-applehelp   1.0.7                    pypi_0    pypi
     sphinxcontrib-devhelp     1.0.5                    pypi_0    pypi
     sphinxcontrib-htmlhelp    2.0.4                    pypi_0    pypi
     sphinxcontrib-jquery      4.1                      pypi_0    pypi
     sphinxcontrib-jsmath      1.0.1                    pypi_0    pypi
     sphinxcontrib-qthelp      1.0.6                    pypi_0    pypi
     sphinxcontrib-serializinghtml 1.1.9                    pypi_0    pypi
     sqlalchemy                2.0.23                   pypi_0    pypi
     sqlglot                   20.4.0                   pypi_0    pypi
     sqlite                    3.44.2               h2c6b66d_0    conda-forge
     statsforecast             1.7.0                    pypi_0    pypi
     statsmodels               0.14.1                   pypi_0    pypi
     streamz                   0.6.4              pyh6c4a22f_0    conda-forge
     svt-av1                   1.8.0                h59595ed_0    conda-forge
     sympy                     1.12                     pypi_0    pypi
     tbats                     1.1.3                    pypi_0    pypi
     tbb                       2021.11.0                pypi_0    pypi
     tblib                     3.0.0              pyhd8ed1ab_0    conda-forge
     tenacity                  8.2.3                    pypi_0    pypi
     tensorboardx              2.6                      pypi_0    pypi
     terminado                 0.18.0             pyh0d859eb_0    conda-forge
     threadpoolctl             3.2.0              pyha21a80b_0    conda-forge
     tifffile                  2023.12.9          pyhd8ed1ab_0    conda-forge
     tiledb                    2.18.3               hc1131af_1    conda-forge
     tinycss2                  1.2.1              pyhd8ed1ab_0    conda-forge
     tk                        8.6.13          noxft_h4845f30_101    conda-forge
     tokenizers                0.13.3                   pypi_0    pypi
     tomli                     2.0.1                    pypi_0    pypi
     tomlkit                   0.12.3                   pypi_0    pypi
     toolz                     0.12.0             pyhd8ed1ab_0    conda-forge
     torch                     2.1.2                    pypi_0    pypi
     torchmetrics              1.2.1                    pypi_0    pypi
     tornado                   6.3.3            py39hd1e30aa_1    conda-forge
     tqdm                      4.66.1             pyhd8ed1ab_0    conda-forge
     traitlets                 5.14.0             pyhd8ed1ab_0    conda-forge
     transformers              4.28.1                   pypi_0    pypi
     treelite                  3.9.1            py39h9b5fa3e_0    conda-forge
     treelite-runtime          3.9.1                    pypi_0    pypi
     triad                     0.9.3                    pypi_0    pypi
     triton                    2.1.0                    pypi_0    pypi
     twine                     4.0.2                    pypi_0    pypi
     types-python-dateutil     2.8.19.14          pyhd8ed1ab_0    conda-forge
     typing-extensions         4.9.0                hd8ed1ab_0    conda-forge
     typing_extensions         4.9.0              pyha770c72_0    conda-forge
     typing_utils              0.1.0              pyhd8ed1ab_0    conda-forge
     tzcode                    2023c                h0b41bf4_0    conda-forge
     tzdata                    2023c                h71feb2d_0    conda-forge
     uc-micro-py               1.0.1              pyhd8ed1ab_0    conda-forge
     ucx                       1.15.0               h6d2d1ec_2    conda-forge
     ucx-proc                  1.0.0                       gpu    rapidsai
     ucx-py                    0.35.00         py39_231206_gb5f60ca_0    rapidsai
     umap-learn                0.5.5                    pypi_0    pypi
     unicodedata2              15.1.0           py39hd1e30aa_0    conda-forge
     uri-template              1.3.0              pyhd8ed1ab_0    conda-forge
     uriparser                 0.9.7                hcb278e6_1    conda-forge
     urllib3                   1.26.18            pyhd8ed1ab_0    conda-forge
     utilsforecast             0.0.23                   pypi_0    pypi
     vecstack                  0.4.0                    pypi_0    pypi
     virtualenv                20.25.0                  pypi_0    pypi
     webcolors                 1.13               pyhd8ed1ab_0    conda-forge
     webencodings              0.5.1              pyhd8ed1ab_2    conda-forge
     websocket-client          1.7.0              pyhd8ed1ab_0    conda-forge
     websockets                10.4             py39hb9d737c_1    conda-forge
     wheel                     0.42.0             pyhd8ed1ab_0    conda-forge
     wrapt                     1.16.0                   pypi_0    pypi
     xarray                    2023.12.0          pyhd8ed1ab_0    conda-forge
     xerces-c                  3.2.5                hac6953d_0    conda-forge
     xgboost                   1.7.6           rapidsai_py39h0b6c2bb_7    rapidsai
     xorg-kbproto              1.0.7             h7f98852_1002    conda-forge
     xorg-libice               1.1.1                hd590300_0    conda-forge
     xorg-libsm                1.2.4                h7391055_0    conda-forge
     xorg-libx11               1.8.7                h8ee46fc_0    conda-forge
     xorg-libxau               1.0.11               hd590300_0    conda-forge
     xorg-libxdmcp             1.1.3                h7f98852_0    conda-forge
     xorg-libxext              1.3.4                h0b41bf4_2    conda-forge
     xorg-libxrender           0.9.11               hd590300_0    conda-forge
     xorg-renderproto          0.11.1            h7f98852_1002    conda-forge
     xorg-xextproto            7.3.0             h0b41bf4_1003    conda-forge
     xorg-xproto               7.0.31            h7f98852_1007    conda-forge
     xxhash                    3.4.1                    pypi_0    pypi
     xyzservices               2023.10.1          pyhd8ed1ab_0    conda-forge
     xz                        5.2.6                h166bdaf_0    conda-forge
     yaml                      0.2.5                h7f98852_2    conda-forge
     yarl                      1.9.3            py39hd1e30aa_0    conda-forge
     zeromq                    4.3.5                h59595ed_0    conda-forge
     zfp                       1.0.1                h59595ed_0    conda-forge
     zict                      3.0.0              pyhd8ed1ab_0    conda-forge
     zipp                      3.17.0             pyhd8ed1ab_0    conda-forge
     zlib                      1.2.13               hd590300_5    conda-forge
     zlib-ng                   2.0.7                h0b41bf4_0    conda-forge
     zstandard                 0.22.0                   pypi_0    pypi
     zstd                      1.5.5                hfc55251_0    conda-forge



**Additional context**
Add any other context about the problem here.
",2023-12-27T21:20:01Z,0,0,Paul Brookes,UCL,False
723,[FEA] Leak Tracking for java Table instances,"**Is your feature request related to a problem? Please describe.**
This is very similar to https://github.com/rapidsai/cudf/issues/8227 except this is for instances of Table. Table is not nearly as important as Scalar because it does not directly reference GPU memory. But it would still be very nice to not leak them as they can cause off heap memory to grow.

**Describe the solution you'd like**
The same leak tracking and debugging done for ColumnVector should be applied to Table. Except Table does not do reference counting so it should be much simpler.",2023-12-27T22:38:47Z,0,0,Robert (Bobby) Evans,Nvidia,True
724,[BUG] `str.character_ngrams` produces <NA> with strings < ngram length,"**Describe the bug**
The `str.character_ngrams` function produces token `<NA>` for strings which are lesser than the provided `n` (shown in image for the case of bigrams).
![result output](https://github.com/rapidsai/cudf/assets/68988130/946aeebb-6be3-4719-91e7-25eb9e2c0091)

I have debugged this and as far as I understand it, it is being caused by an empty list returned by the `libstrings.generate_character_ngrams` function. This causes <NA> to be a part of the result when it is exploded in the problematic function.
This issue causes several bugs in downstream tasks (like when using cuml for `CountVectorizer` etc).


**Steps/Code to reproduce bug**
Minimum code required to reproduce the bug:
```
import cudf
str_series = cudf.Series(['1744', '4'])
str_series.str.character_ngrams(2)
```

**Expected behavior**
<NA> should not be a part of the output. This causes several downstream tasks to fail because <NA> is not a valid token in the actual input string series.

**Environment overview (please complete the following information)**
 - Environment location: Cloud GCP
 - Method of cuDF install: pip
 
**Environment details**
```
**git***
     Not inside a git repository
     
     ***OS Information***
     PRETTY_NAME=""Debian GNU/Linux 11 (bullseye)""
     NAME=""Debian GNU/Linux""
     VERSION_ID=""11""
     VERSION=""11 (bullseye)""
     VERSION_CODENAME=bullseye
     ID=debian
     HOME_URL=""https://www.debian.org/""
     SUPPORT_URL=""https://www.debian.org/support""
     BUG_REPORT_URL=""https://bugs.debian.org/""
     Linux janmey-gpu-c2 5.10.0-26-cloud-amd64 #1 SMP Debian 5.10.197-1 (2023-09-29) x86_64 GNU/Linux
     
     ***GPU Information***
     Fri Dec 29 10:21:54 2023
     +-----------------------------------------------------------------------------+
     | NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |
     |-------------------------------+----------------------+----------------------+
     | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
     | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
     |                               |                      |               MIG M. |
     |===============================+======================+======================|
     |   0  Tesla T4            On   | 00000000:00:04.0 Off |                    0 |
     | N/A   70C    P0    33W /  70W |    459MiB / 15360MiB |      0%      Default |
     |                               |                      |                  N/A |
     +-------------------------------+----------------------+----------------------+
     
     +-----------------------------------------------------------------------------+
     | Processes:                                                                  |
     |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
     |        ID   ID                                                   Usage      |
     |=============================================================================|
     |    0   N/A  N/A    316341      C   ..._log_ner/.venv/bin/python      454MiB |
     +-----------------------------------------------------------------------------+
     
     ***CPU***
     Architecture:                       x86_64
     CPU op-mode(s):                     32-bit, 64-bit
     Byte Order:                         Little Endian
     Address sizes:                      46 bits physical, 48 bits virtual
     CPU(s):                             16
     On-line CPU(s) list:                0-15
     Thread(s) per core:                 2
     Core(s) per socket:                 8
     Socket(s):                          1
     NUMA node(s):                       1
     Vendor ID:                          GenuineIntel
     CPU family:                         6
     Model:                              79
     Model name:                         Intel(R) Xeon(R) CPU @ 2.20GHz
     Stepping:                           0
     CPU MHz:                            2199.998
     BogoMIPS:                           4399.99
     Hypervisor vendor:                  KVM
     Virtualization type:                full
     L1d cache:                          256 KiB
     L1i cache:                          256 KiB
     L2 cache:                           2 MiB
     L3 cache:                           55 MiB
     NUMA node0 CPU(s):                  0-15
     Vulnerability Gather data sampling: Not affected
     Vulnerability Itlb multihit:        Not affected
     Vulnerability L1tf:                 Mitigation; PTE Inversion
     Vulnerability Mds:                  Mitigation; Clear CPU buffers; SMT Host state unknown
     Vulnerability Meltdown:             Mitigation; PTI
     Vulnerability Mmio stale data:      Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown
     Vulnerability Retbleed:             Mitigation; IBRS
     Vulnerability Spec rstack overflow: Not affected
     Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp
     Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
     Vulnerability Spectre v2:           Mitigation; IBRS, IBPB conditional, STIBP conditional, RSB filling, PBRSB-eIBRS Not affected
     Vulnerability Srbds:                Not affected
     Vulnerability Tsx async abort:      Mitigation; Clear CPU buffers; SMT Host state unknown
     Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities
     
     ***CMake***
     /usr/bin/cmake
     cmake version 3.18.4
     
     CMake suite maintained and supported by Kitware (kitware.com/cmake).
     
     ***g++***
     /usr/bin/g++
     g++ (Debian 10.2.1-6) 10.2.1 20210110
     Copyright (C) 2020 Free Software Foundation, Inc.
     This is free software; see the source for copying conditions.  There is NO
     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
     
     
     ***nvcc***
     /usr/local/cuda/bin/nvcc
     nvcc: NVIDIA (R) Cuda compiler driver
     Copyright (c) 2005-2022 NVIDIA Corporation
     Built on Wed_Sep_21_10:33:58_PDT_2022
     Cuda compilation tools, release 11.8, V11.8.89
     Build cuda_11.8.r11.8/compiler.31833905_0
     
     ***Python***
     /home/janmeysandeepshukla/datasci/transaction_log_ner/.venv/bin/python
     Python 3.10.13
     
     ***Environment Variables***
     PATH                            : /home/janmeysandeepshukla/datasci/transaction_log_ner/.venv/bin:/home/janmeysandeepshukla/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/usr/local/cuda/bin:/opt/conda/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
     LD_LIBRARY_PATH                 : /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64
     NUMBAPRO_NVVM                   :
     NUMBAPRO_LIBDEVICE              :
     CONDA_PREFIX                    : /opt/conda
     PYTHON_PATH                     :
     
     ***conda packages***
     /opt/conda/bin/conda
     # packages in environment at /opt/conda:
     #
     # Name                    Version                   Build  Channel
     _libgcc_mutex             0.1                 conda_forge    conda-forge
     _openmp_mutex             4.5                       2_gnu    conda-forge
     absl-py                   2.0.0                    pypi_0    pypi
     aiofiles                  22.1.0                   pypi_0    pypi
     aiohttp                   3.9.1                    pypi_0    pypi
     aiohttp-cors              0.7.0                    pypi_0    pypi
     aiorwlock                 1.3.0                    pypi_0    pypi
     aiosignal                 1.3.1                    pypi_0    pypi
     aiosqlite                 0.19.0                   pypi_0    pypi
     anyio                     3.7.1                    pypi_0    pypi
     archspec                  0.2.2              pyhd8ed1ab_0    conda-forge
     argon2-cffi               23.1.0             pyhd8ed1ab_0    conda-forge
     argon2-cffi-bindings      21.2.0          py310h2372a71_4    conda-forge
     arrow                     1.3.0              pyhd8ed1ab_0    conda-forge
     asttokens                 2.4.1              pyhd8ed1ab_0    conda-forge
     async-lru                 2.0.4              pyhd8ed1ab_0    conda-forge
     async-timeout             4.0.3                    pypi_0    pypi
     attrs                     23.1.0             pyh71513ae_1    conda-forge
     babel                     2.13.1             pyhd8ed1ab_0    conda-forge
     backoff                   2.2.1                    pypi_0    pypi
     beatrix-jupyterlab        2023.128.151533          pypi_0    pypi
     beautifulsoup4            4.12.2             pyha770c72_0    conda-forge
     bleach                    6.1.0              pyhd8ed1ab_0    conda-forge
     blessed                   1.20.0                   pypi_0    pypi
     boltons                   23.0.0             pyhd8ed1ab_0    conda-forge
     brotli-python             1.1.0           py310hc6cd4ac_1    conda-forge
     bzip2                     1.0.8                hd590300_5    conda-forge
     c-ares                    1.23.0               hd590300_0    conda-forge
     ca-certificates           2023.11.17           hbcca054_0    conda-forge
     cached-property           1.5.2                hd8ed1ab_1    conda-forge
     cached_property           1.5.2              pyha770c72_1    conda-forge
     cachetools                5.3.2                    pypi_0    pypi
     certifi                   2023.11.17         pyhd8ed1ab_0    conda-forge
     cffi                      1.16.0          py310h2fee648_0    conda-forge
     charset-normalizer        3.3.2              pyhd8ed1ab_0    conda-forge
     click                     8.1.7                    pypi_0    pypi
     cloud-tpu-client          0.10                     pypi_0    pypi
     cloudpickle               3.0.0                    pypi_0    pypi
     colorama                  0.4.6              pyhd8ed1ab_0    conda-forge
     colorful                  0.5.5                    pypi_0    pypi
     comm                      0.2.0                    pypi_0    pypi
     conda                     23.11.0         py310hff52083_1    conda-forge
     conda-libmamba-solver     23.11.1            pyhd8ed1ab_0    conda-forge
     conda-package-handling    2.2.0              pyh38be061_0    conda-forge
     conda-package-streaming   0.9.0              pyhd8ed1ab_0    conda-forge
     contourpy                 1.2.0                    pypi_0    pypi
     cryptography              41.0.7                   pypi_0    pypi
     cycler                    0.12.1                   pypi_0    pypi
     cython                    3.0.6                    pypi_0    pypi
     dacite                    1.8.1                    pypi_0    pypi
     dataproc-jupyter-plugin   0.1.59                   pypi_0    pypi
     db-dtypes                 1.1.1                    pypi_0    pypi
     debugpy                   1.8.0           py310hc6cd4ac_1    conda-forge
     decorator                 5.1.1              pyhd8ed1ab_0    conda-forge
     defusedxml                0.7.1              pyhd8ed1ab_0    conda-forge
     deprecated                1.2.14                   pypi_0    pypi
     distlib                   0.3.7                    pypi_0    pypi
     distro                    1.8.0              pyhd8ed1ab_0    conda-forge
     dlenv-base                1.0.20231210            py310_0    file:///tmp/conda-pkgs
     dm-tree                   0.1.8                    pypi_0    pypi
     docker                    7.0.0                    pypi_0    pypi
     docstring-parser          0.15                     pypi_0    pypi
     entrypoints               0.4                pyhd8ed1ab_0    conda-forge
     exceptiongroup            1.2.0              pyhd8ed1ab_0    conda-forge
     executing                 2.0.1              pyhd8ed1ab_0    conda-forge
     farama-notifications      0.0.4                    pypi_0    pypi
     fastapi                   0.104.1                  pypi_0    pypi
     filelock                  3.13.1                   pypi_0    pypi
     fmt                       10.1.1               h00ab1b0_1    conda-forge
     fonttools                 4.46.0                   pypi_0    pypi
     fqdn                      1.5.1              pyhd8ed1ab_0    conda-forge
     frozenlist                1.4.0                    pypi_0    pypi
     fsspec                    2023.12.1                pypi_0    pypi
     gcsfs                     2023.12.1                pypi_0    pypi
     gitdb                     4.0.11                   pypi_0    pypi
     gitpython                 3.1.40                   pypi_0    pypi
     google-api-core           1.34.0                   pypi_0    pypi
     google-api-python-client  1.8.0                    pypi_0    pypi
     google-auth               2.25.2                   pypi_0    pypi
     google-auth-httplib2      0.1.1                    pypi_0    pypi
     google-auth-oauthlib      1.1.0                    pypi_0    pypi
     google-cloud-aiplatform   1.37.0                   pypi_0    pypi
     google-cloud-artifact-registry 1.10.0                   pypi_0    pypi
     google-cloud-bigquery     3.13.0                   pypi_0    pypi
     google-cloud-bigquery-storage 2.23.0                   pypi_0    pypi
     google-cloud-core         2.4.1                    pypi_0    pypi
     google-cloud-datastore    1.15.5                   pypi_0    pypi
     google-cloud-jupyter-config 0.0.5                    pypi_0    pypi
     google-cloud-language     2.12.0                   pypi_0    pypi
     google-cloud-monitoring   2.17.0                   pypi_0    pypi
     google-cloud-resource-manager 1.11.0                   pypi_0    pypi
     google-cloud-storage      2.13.0                   pypi_0    pypi
     google-crc32c             1.5.0                    pypi_0    pypi
     google-resumable-media    2.6.0                    pypi_0    pypi
     googleapis-common-protos  1.62.0                   pypi_0    pypi
     gpustat                   1.0.0                    pypi_0    pypi
     greenlet                  3.0.2                    pypi_0    pypi
     grpc-google-iam-v1        0.13.0                   pypi_0    pypi
     grpcio                    1.60.0                   pypi_0    pypi
     grpcio-status             1.48.2                   pypi_0    pypi
     gymnasium                 0.28.1                   pypi_0    pypi
     h11                       0.14.0                   pypi_0    pypi
     htmlmin                   0.1.12                   pypi_0    pypi
     httplib2                  0.22.0                   pypi_0    pypi
     httptools                 0.6.1                    pypi_0    pypi
     icu                       73.2                 h59595ed_0    conda-forge
     idna                      3.6                pyhd8ed1ab_0    conda-forge
     imagehash                 4.3.1                    pypi_0    pypi
     imageio                   2.33.0                   pypi_0    pypi
     importlib-metadata        6.11.0                   pypi_0    pypi
     importlib_metadata        7.0.0                hd8ed1ab_0    conda-forge
     importlib_resources       6.1.1              pyhd8ed1ab_0    conda-forge
     ipykernel                 6.27.1                   pypi_0    pypi
     ipython                   8.18.1             pyh707e725_3    conda-forge
     ipython-genutils          0.2.0                    pypi_0    pypi
     ipython-sql               0.5.0                    pypi_0    pypi
     ipywidgets                8.1.1                    pypi_0    pypi
     isoduration               20.11.0            pyhd8ed1ab_0    conda-forge
     jaraco-classes            3.3.0                    pypi_0    pypi
     jax-jumpy                 1.0.0                    pypi_0    pypi
     jedi                      0.19.1             pyhd8ed1ab_0    conda-forge
     jeepney                   0.8.0                    pypi_0    pypi
     jinja2                    3.1.2              pyhd8ed1ab_1    conda-forge
     joblib                    1.3.2                    pypi_0    pypi
     json5                     0.9.14             pyhd8ed1ab_0    conda-forge
     jsonpatch                 1.33               pyhd8ed1ab_0    conda-forge
     jsonpointer               2.4             py310hff52083_3    conda-forge
     jsonschema                4.20.0             pyhd8ed1ab_0    conda-forge
     jsonschema-specifications 2023.11.2          pyhd8ed1ab_0    conda-forge
     jsonschema-with-format-nongpl 4.20.0             pyhd8ed1ab_0    conda-forge
     jupyter-client            7.4.9                    pypi_0    pypi
     jupyter-http-over-ws      0.0.8                    pypi_0    pypi
     jupyter-lsp               2.2.1              pyhd8ed1ab_0    conda-forge
     jupyter-server-fileid     0.9.0                    pypi_0    pypi
     jupyter-server-mathjax    0.2.6                    pypi_0    pypi
     jupyter-server-proxy      4.1.0                    pypi_0    pypi
     jupyter-server-ydoc       0.8.0                    pypi_0    pypi
     jupyter-ydoc              0.2.5                    pypi_0    pypi
     jupyter_client            8.6.0              pyhd8ed1ab_0    conda-forge
     jupyter_core              5.5.0           py310hff52083_0    conda-forge
     jupyter_events            0.9.0              pyhd8ed1ab_0    conda-forge
     jupyter_server            2.12.1             pyhd8ed1ab_0    conda-forge
     jupyter_server_terminals  0.4.4              pyhd8ed1ab_1    conda-forge
     jupyterlab                3.6.6                    pypi_0    pypi
     jupyterlab-git            0.44.0                   pypi_0    pypi
     jupyterlab-widgets        3.0.9                    pypi_0    pypi
     jupyterlab_pygments       0.3.0              pyhd8ed1ab_0    conda-forge
     jupyterlab_server         2.25.2             pyhd8ed1ab_0    conda-forge
     jupytext                  1.16.0                   pypi_0    pypi
     kernels-mixer             0.0.7                    pypi_0    pypi
     keyring                   24.3.0                   pypi_0    pypi
     keyrings-google-artifactregistry-auth 1.1.2                    pypi_0    pypi
     keyutils                  1.6.1                h166bdaf_0    conda-forge
     kfp                       2.4.0                    pypi_0    pypi
     kfp-pipeline-spec         0.2.2                    pypi_0    pypi
     kfp-server-api            2.0.5                    pypi_0    pypi
     kiwisolver                1.4.5                    pypi_0    pypi
     krb5                      1.21.2               h659d440_0    conda-forge
     kubernetes                26.1.0                   pypi_0    pypi
     lazy-loader               0.3                      pypi_0    pypi
     ld_impl_linux-64          2.40                 h41732ed_0    conda-forge
     libarchive                3.7.2                h2aa1ff5_1    conda-forge
     libcurl                   8.5.0                hca28451_0    conda-forge
     libedit                   3.1.20191231         he28a2e2_2    conda-forge
     libev                     4.33                 hd590300_2    conda-forge
     libffi                    3.4.2                h7f98852_5    conda-forge
     libgcc-ng                 13.2.0               h807b86a_3    conda-forge
     libgomp                   13.2.0               h807b86a_3    conda-forge
     libiconv                  1.17                 h166bdaf_0    conda-forge
     libmamba                  1.5.4                had39da4_0    conda-forge
     libmambapy                1.5.4           py310h39ff949_0    conda-forge
     libnghttp2                1.58.0               h47da74e_1    conda-forge
     libnsl                    2.0.1                hd590300_0    conda-forge
     libsodium                 1.0.18               h36c2ea0_1    conda-forge
     libsolv                   0.7.27               hfc55251_0    conda-forge
     libsqlite                 3.44.2               h2797004_0    conda-forge
     libssh2                   1.11.0               h0841786_0    conda-forge
     libstdcxx-ng              13.2.0               h7e041cc_3    conda-forge
     libuuid                   2.38.1               h0b41bf4_0    conda-forge
     libuv                     1.46.0               hd590300_0    conda-forge
     libxml2                   2.12.2               h232c23b_0    conda-forge
     libzlib                   1.2.13               hd590300_5    conda-forge
     llvmlite                  0.41.1                   pypi_0    pypi
     lz4                       4.3.2                    pypi_0    pypi
     lz4-c                     1.9.4                hcb278e6_0    conda-forge
     lzo                       2.10              h516909a_1000    conda-forge
     markdown-it-py            3.0.0                    pypi_0    pypi
     markupsafe                2.1.3           py310h2372a71_1    conda-forge
     matplotlib                3.7.3                    pypi_0    pypi
     matplotlib-inline         0.1.6              pyhd8ed1ab_0    conda-forge
     mdit-py-plugins           0.4.0                    pypi_0    pypi
     mdurl                     0.1.2                    pypi_0    pypi
     menuinst                  2.0.0           py310hff52083_1    conda-forge
     mistune                   3.0.2              pyhd8ed1ab_0    conda-forge
     more-itertools            10.1.0                   pypi_0    pypi
     msgpack                   1.0.7                    pypi_0    pypi
     multidict                 6.0.4                    pypi_0    pypi
     multimethod               1.10                     pypi_0    pypi
     nb_conda                  2.2.1                    unix_6    conda-forge
     nb_conda_kernels          2.3.1              pyhd8ed1ab_3    conda-forge
     nbclassic                 1.0.0                    pypi_0    pypi
     nbclient                  0.9.0                    pypi_0    pypi
     nbconvert-core            7.12.0             pyhd8ed1ab_0    conda-forge
     nbdime                    3.2.0                    pypi_0    pypi
     nbformat                  5.9.2              pyhd8ed1ab_0    conda-forge
     ncurses                   6.4                  h59595ed_2    conda-forge
     nest-asyncio              1.5.8              pyhd8ed1ab_0    conda-forge
     networkx                  3.2.1                    pypi_0    pypi
     nodejs                    20.9.0               hb753e55_0    conda-forge
     notebook                  6.5.6                    pypi_0    pypi
     notebook-executor         0.2                      pypi_0    pypi
     notebook-shim             0.2.3              pyhd8ed1ab_0    conda-forge
     numba                     0.58.1                   pypi_0    pypi
     numpy                     1.25.2                   pypi_0    pypi
     nvidia-ml-py              11.495.46                pypi_0    pypi
     oauth2client              4.1.3                    pypi_0    pypi
     oauthlib                  3.2.2                    pypi_0    pypi
     opencensus                0.11.3                   pypi_0    pypi
     opencensus-context        0.1.3                    pypi_0    pypi
     openssl                   3.2.0                hd590300_1    conda-forge
     opentelemetry-api         1.21.0                   pypi_0    pypi
     opentelemetry-exporter-otlp 1.21.0                   pypi_0    pypi
     opentelemetry-exporter-otlp-proto-common 1.21.0                   pypi_0    pypi
     opentelemetry-exporter-otlp-proto-grpc 1.21.0                   pypi_0    pypi
     opentelemetry-exporter-otlp-proto-http 1.21.0                   pypi_0    pypi
     opentelemetry-proto       1.21.0                   pypi_0    pypi
     opentelemetry-sdk         1.21.0                   pypi_0    pypi
     opentelemetry-semantic-conventions 0.42b0                   pypi_0    pypi
     overrides                 7.4.0              pyhd8ed1ab_0    conda-forge
     packaging                 23.2               pyhd8ed1ab_0    conda-forge
     pandas                    2.0.3                    pypi_0    pypi
     pandas-profiling          3.6.6                    pypi_0    pypi
     pandocfilters             1.5.0              pyhd8ed1ab_0    conda-forge
     papermill                 2.5.0                    pypi_0    pypi
     parso                     0.8.3              pyhd8ed1ab_0    conda-forge
     patsy                     0.5.4                    pypi_0    pypi
     pexpect                   4.9.0                    pypi_0    pypi
     phik                      0.12.3                   pypi_0    pypi
     pickleshare               0.7.5                   py_1003    conda-forge
     pillow                    10.1.0                   pypi_0    pypi
     pip                       23.3.1             pyhd8ed1ab_0    conda-forge
     pkgutil-resolve-name      1.3.10             pyhd8ed1ab_1    conda-forge
     platformdirs              3.11.0                   pypi_0    pypi
     plotly                    5.18.0                   pypi_0    pypi
     pluggy                    1.3.0              pyhd8ed1ab_0    conda-forge
     prettytable               3.9.0                    pypi_0    pypi
     prometheus_client         0.19.0             pyhd8ed1ab_0    conda-forge
     prompt-toolkit            3.0.41             pyha770c72_0    conda-forge
     proto-plus                1.23.0                   pypi_0    pypi
     protobuf                  3.20.3                   pypi_0    pypi
     psutil                    5.9.3                    pypi_0    pypi
     ptyprocess                0.7.0              pyhd3deb0d_0    conda-forge
     pure_eval                 0.2.2              pyhd8ed1ab_0    conda-forge
     py-spy                    0.3.14                   pypi_0    pypi
     pyarrow                   14.0.1                   pypi_0    pypi
     pyasn1                    0.5.1                    pypi_0    pypi
     pyasn1-modules            0.3.0                    pypi_0    pypi
     pybind11-abi              4                    hd8ed1ab_3    conda-forge
     pycosat                   0.6.6           py310h2372a71_0    conda-forge
     pycparser                 2.21               pyhd8ed1ab_0    conda-forge
     pydantic                  1.10.13                  pypi_0    pypi
     pygments                  2.17.2             pyhd8ed1ab_0    conda-forge
     pyjwt                     2.8.0                    pypi_0    pypi
     pyparsing                 3.1.1                    pypi_0    pypi
     pysocks                   1.7.1              pyha2e5f31_6    conda-forge
     python                    3.10.13         hd12c33a_0_cpython    conda-forge
     python-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge
     python-dotenv             1.0.0                    pypi_0    pypi
     python-fastjsonschema     2.19.0             pyhd8ed1ab_0    conda-forge
     python-json-logger        2.0.7              pyhd8ed1ab_0    conda-forge
     python_abi                3.10                    4_cp310    conda-forge
     pytz                      2023.3.post1       pyhd8ed1ab_0    conda-forge
     pywavelets                1.5.0                    pypi_0    pypi
     pyyaml                    6.0.1           py310h2372a71_1    conda-forge
     pyzmq                     24.0.1                   pypi_0    pypi
     ray                       2.8.1                    pypi_0    pypi
     ray-cpp                   2.8.1                    pypi_0    pypi
     readline                  8.2                  h8228510_1    conda-forge
     referencing               0.32.0             pyhd8ed1ab_0    conda-forge
     reproc                    14.2.4.post0         hd590300_1    conda-forge
     reproc-cpp                14.2.4.post0         h59595ed_1    conda-forge
     requests                  2.31.0             pyhd8ed1ab_0    conda-forge
     requests-oauthlib         1.3.1                    pypi_0    pypi
     requests-toolbelt         0.10.1                   pypi_0    pypi
     retrying                  1.3.4                    pypi_0    pypi
     rfc3339-validator         0.1.4              pyhd8ed1ab_0    conda-forge
     rfc3986-validator         0.1.1              pyh9f0ad1d_0    conda-forge
     rich                      13.7.0                   pypi_0    pypi
     rpds-py                   0.13.2          py310hcb5633a_0    conda-forge
     ruamel.yaml               0.18.5          py310h2372a71_0    conda-forge
     ruamel.yaml.clib          0.2.7           py310h2372a71_2    conda-forge
     scikit-image              0.22.0                   pypi_0    pypi
     scikit-learn              1.3.2                    pypi_0    pypi
     scipy                     1.11.4                   pypi_0    pypi
     seaborn                   0.12.2                   pypi_0    pypi
     secretstorage             3.3.3                    pypi_0    pypi
     send2trash                1.8.2              pyh41d4057_0    conda-forge
     setuptools                68.2.2             pyhd8ed1ab_0    conda-forge
     shapely                   2.0.2                    pypi_0    pypi
     simpervisor               1.0.0                    pypi_0    pypi
     six                       1.16.0             pyh6c4a22f_0    conda-forge
     smart-open                6.4.0                    pypi_0    pypi
     smmap                     5.0.1                    pypi_0    pypi
     sniffio                   1.3.0              pyhd8ed1ab_0    conda-forge
     soupsieve                 2.5                pyhd8ed1ab_1    conda-forge
     sqlalchemy                2.0.23                   pypi_0    pypi
     sqlparse                  0.4.4                    pypi_0    pypi
     stack-data                0.6.3                    pypi_0    pypi
     stack_data                0.6.2              pyhd8ed1ab_0    conda-forge
     starlette                 0.27.0                   pypi_0    pypi
     statsmodels               0.14.0                   pypi_0    pypi
     tabulate                  0.9.0                    pypi_0    pypi
     tangled-up-in-unicode     0.2.0                    pypi_0    pypi
     tenacity                  8.2.3                    pypi_0    pypi
     tensorboardx              2.6.2.2                  pypi_0    pypi
     terminado                 0.18.0             pyh0d859eb_0    conda-forge
     threadpoolctl             3.2.0                    pypi_0    pypi
     tifffile                  2023.12.9                pypi_0    pypi
     tinycss2                  1.2.1              pyhd8ed1ab_0    conda-forge
     tk                        8.6.13          noxft_h4845f30_101    conda-forge
     toml                      0.10.2                   pypi_0    pypi
     tomli                     2.0.1              pyhd8ed1ab_0    conda-forge
     tornado                   6.3.3           py310h2372a71_1    conda-forge
     tqdm                      4.66.1             pyhd8ed1ab_0    conda-forge
     traitlets                 5.14.0             pyhd8ed1ab_0    conda-forge
     truststore                0.8.0              pyhd8ed1ab_0    conda-forge
     typeguard                 4.1.5                    pypi_0    pypi
     typer                     0.9.0                    pypi_0    pypi
     types-python-dateutil     2.8.19.14          pyhd8ed1ab_0    conda-forge
     typing-extensions         4.8.0                hd8ed1ab_0    conda-forge
     typing_extensions         4.8.0              pyha770c72_0    conda-forge
     typing_utils              0.1.0              pyhd8ed1ab_0    conda-forge
     tzdata                    2023.3                   pypi_0    pypi
     uri-template              1.3.0              pyhd8ed1ab_0    conda-forge
     uritemplate               3.0.1                    pypi_0    pypi
     urllib3                   1.26.18                  pypi_0    pypi
     uvicorn                   0.24.0.post1             pypi_0    pypi
     uvloop                    0.19.0                   pypi_0    pypi
     virtualenv                20.21.0                  pypi_0    pypi
     visions                   0.7.5                    pypi_0    pypi
     watchfiles                0.21.0                   pypi_0    pypi
     wcwidth                   0.2.12             pyhd8ed1ab_0    conda-forge
     webcolors                 1.13               pyhd8ed1ab_0    conda-forge
     webencodings              0.5.1              pyhd8ed1ab_2    conda-forge
     websocket-client          1.7.0              pyhd8ed1ab_0    conda-forge
     websockets                12.0                     pypi_0    pypi
     wheel                     0.42.0             pyhd8ed1ab_0    conda-forge
     widgetsnbextension        4.0.9                    pypi_0    pypi
     wordcloud                 1.9.3                    pypi_0    pypi
     wrapt                     1.16.0                   pypi_0    pypi
     xz                        5.2.6                h166bdaf_0    conda-forge
     y-py                      0.6.2                    pypi_0    pypi
     yaml                      0.2.5                h7f98852_2    conda-forge
     yaml-cpp                  0.8.0                h59595ed_0    conda-forge
     yarl                      1.9.4                    pypi_0    pypi
     ydata-profiling           4.6.0                    pypi_0    pypi
     ypy-websocket             0.8.4                    pypi_0    pypi
     zeromq                    4.3.5                h59595ed_0    conda-forge
     zipp                      3.17.0             pyhd8ed1ab_0    conda-forge
     zlib                      1.2.13               hd590300_5    conda-forge
     zstandard                 0.22.0          py310h1275a96_0    conda-forge
     zstd                      1.5.5                hfc55251_0    conda-forge
     ```
",2023-12-29T10:26:09Z,0,0,Janmey Shukla,,False
725,pd.concat() of dictionaries [FEA],"**Missing Pandas Feature Request**

Please implement concatenation of dictionaries, as shown at the very bottom of the page in the [Pandas package](https://pandas.pydata.org/docs/reference/api/pandas.concat.html). Currently, cudf only supports DataFrame and Series objects; dictionaries would be a very useful addition.


**Additional context**

The following should return a cudf DataFrame stored in GPU ram.

Input >> `cudf.concat({'a': 1.1, 'b': 2.2}, axis=1)`

Stored variable >>

'a' | 1.1
'b' | 2.2

",2024-01-10T19:34:43Z,0,0,,,False
726,[FEA] libcudf should not rely on the ABI stability of Thrust types ,"**Is your feature request related to a problem? Please describe.**

libcudf's public API currently includes use of Thrust types like `thrust::device_vector`. This means libcudf implicitly depends on the ABI of those Thrust types to be stable not only across Thrust versions, but even within the same version (e.g., host TUs vs device TUs).

This is problematic because [Thrust and CUB symbols make no ABI stability guarantees](https://github.com/nvidia/cccl?tab=readme-ov-file#application-binary-interface-abi). 

**Describe the solution you'd like**
Thrust types in libcudf's public API should be replaced with types that have ABI guarantees. This likely means replacing with equivalent `cuda::std::` types. 

**Additional context**

This is related to https://github.com/rapidsai/cudf/issues/14734 in a roundabout way that isn't worth going into the details. 

See also:
https://github.com/NVIDIA/cccl/issues/1246
https://github.com/NVIDIA/cccl/issues/1262
",2024-01-10T19:57:45Z,0,0,Jake Hemstad,@NVIDIA,True
727,[DOC] Improve Sphinx C++ documentation,"## Report needed documentation
As of #13846 we will start publishing the C++ documentation as part of the Sphinx doc build as well as via doxygen. The long-term plan is to unify these two and remove the latter, but before we do so we should make sure that we're happy with the Sphinx output replacing the doxygen. This  issue is for tracking improvements that we want to see to the Sphinx docs.

Improvements:
- [ ] Starting list

Once we're happy, the final migration from doxygen will involve the following tasks that cannot be done until the end:
- [ ] Update inline tables in doxygen to a style supported by Sphinx's text builder (see https://github.com/rapidsai/cudf/pull/13846/commits/b2dae66314829ed6c75d10a8dddaea47321dc68c)
- [ ] Update anchors in developer documentation so that cross-linking works via Myst (see https://github.com/rapidsai/cudf/pull/13846/commits/b2dae66314829ed6c75d10a8dddaea47321dc68c)
- [ ] Add developer documentation pages to Sphinx docs (see https://github.com/rapidsai/cudf/pull/13846/commits/b2dae66314829ed6c75d10a8dddaea47321dc68c)",2024-01-11T19:14:11Z,0,0,Vyas Ramasubramani,@rapidsai,True
728,Initial pool size should not be hard coded,"Now that `rmm::pool_memory_resource` does not by default use half of available memory for the initial pool size, cuDF has hard-coded 50% of free memory as the initial size to match previous behavior for benchmarks and tests. This should be configured globally somewhere.

             I'm thinking about whether we should parse `percent_of_free_device_memory` from some env variable, instead of hard coding like this in many places. By doing so we can control the number we want at runtime.

_Originally posted by @ttnghia in https://github.com/rapidsai/cudf/pull/14741#discussion_r1448304244_
            ",2024-01-15T20:47:27Z,0,0,Mark Harris,@NVIDIA,True
729,[BUG] Using swifter with cudf.pandas causes CPU fallack to fail for `apply` ,"[Swifter](https://github.com/jmcarpenter2/swifter) is a [popular tool](https://pypistats.org/packages/swifter) that ""efficiently applies any function to a pandas dataframe or series in the fastest available manner"" (e.g., using multiprocessing and more under the hood).

When we use swifter with cudf.pandas, things work as expected for UDFs that we can execute on the GPU:

```python
%load_ext cudf.pandas

import pandas as pd
import swifter

def func(x):
    return x + 5

s = pd.Series(range(5))
s.swifter.apply(func)
0    5
1    6
2    7
3    8
4    9
dtype: int64
```

But if we need to fallback to the CPU, something about the way swifter interfaces with pandas (and cudf.pandas) is causing the error to not be caught and trigger CPU fallback:

```python
%load_ext cudf.pandas

import pandas as pd
import swifter

def func(x):
    return str(x + 5)

s = pd.Series(range(5))
s.swifter.apply(func)
---------------------------------------------------------------------------
NumbaNotImplementedError                  Traceback (most recent call last)
File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/cudf/core/indexed_frame.py:2344, in IndexedFrame._apply(self, func, kernel_getter, *args, **kwargs)
   2343 try:
-> 2344     kernel, retty = _compile_or_get(
   2345         self, func, args, kernel_getter=kernel_getter
   2346     )
   2347 except Exception as e:

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/nvtx/nvtx.py:115, in annotate.__call__.<locals>.inner(*args, **kwargs)
    114 libnvtx_push_range(self.attributes, self.domain.handle)
--> 115 result = func(*args, **kwargs)
    116 libnvtx_pop_range(self.domain.handle)

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/cudf/core/udf/utils.py:276, in _compile_or_get(frame, func, args, kernel_getter, suffix)
    273 # precompile the user udf to get the right return type.
    274 # could be a MaskedType or a scalar type.
--> 276 kernel, scalar_return_type = kernel_getter(frame, func, args)
    277 np_return_type = (
    278     numpy_support.as_dtype(scalar_return_type)
    279     if scalar_return_type.is_internal
    280     else scalar_return_type.np_dtype
    281 )

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/cudf/core/udf/scalar_function.py:55, in _get_scalar_kernel(sr, func, args)
     52 sr_type = MaskedType(
     53     string_view if sr.dtype == ""O"" else numpy_support.from_dtype(sr.dtype)
     54 )
---> 55 scalar_return_type = _get_udf_return_type(sr_type, func, args)
     57 sig = _construct_signature(sr, scalar_return_type, args=args)

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/nvtx/nvtx.py:115, in annotate.__call__.<locals>.inner(*args, **kwargs)
    114 libnvtx_push_range(self.attributes, self.domain.handle)
--> 115 result = func(*args, **kwargs)
    116 libnvtx_pop_range(self.domain.handle)

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/cudf/core/udf/utils.py:96, in _get_udf_return_type(argty, func, args)
     95 with _CUDFNumbaConfig():
---> 96     ptx, output_type = cudautils.compile_udf(func, compile_sig)
     98 if not isinstance(output_type, MaskedType):

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/cudf/utils/cudautils.py:130, in compile_udf(udf, type_signature)
    129 if not isinstance(return_type, cudf.core.udf.masked_typing.MaskedType):
--> 130     output_type = numpy_support.as_dtype(return_type).type
    131 else:

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/numba/np/numpy_support.py:159, in as_dtype(nbtype)
    158 msg = f""{nbtype} cannot be represented as a NumPy dtype""
--> 159 raise errors.NumbaNotImplementedError(msg)

NumbaNotImplementedError: unicode_type cannot be represented as a NumPy dtype

The above exception was the direct cause of the following exception:

ValueError                                Traceback (most recent call last)
File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/swifter/swifter.py:312, in SeriesAccessor.apply(self, func, convert_dtype, args, **kwds)
    311 tmp_df = func(sample, *args, **kwds)
--> 312 sample_df = sample.apply(func, convert_dtype=convert_dtype, args=args, **kwds)
    313 self._validate_apply(
    314     np.array_equal(sample_df, tmp_df) & (hasattr(tmp_df, ""shape"")) & (sample_df.shape == tmp_df.shape),
    315     error_message=(""Vectorized function sample doesn't match pandas apply sample.""),
    316 )

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/nvtx/nvtx.py:115, in annotate.__call__.<locals>.inner(*args, **kwargs)
    114 libnvtx_push_range(self.attributes, self.domain.handle)
--> 115 result = func(*args, **kwargs)
    116 libnvtx_pop_range(self.domain.handle)

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/cudf/core/series.py:2632, in Series.apply(self, func, convert_dtype, args, **kwargs)
   2630     raise ValueError(""Series.apply only supports convert_dtype=True"")
-> 2632 result = self._apply(func, _get_scalar_kernel, *args, **kwargs)
   2633 result.name = self.name

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/contextlib.py:79, in ContextDecorator.__call__.<locals>.inner(*args, **kwds)
     78 with self._recreate_cm():
---> 79     return func(*args, **kwds)

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/nvtx/nvtx.py:115, in annotate.__call__.<locals>.inner(*args, **kwargs)
    114 libnvtx_push_range(self.attributes, self.domain.handle)
--> 115 result = func(*args, **kwargs)
    116 libnvtx_pop_range(self.domain.handle)

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/cudf/core/indexed_frame.py:2348, in IndexedFrame._apply(self, func, kernel_getter, *args, **kwargs)
   2347 except Exception as e:
-> 2348     raise ValueError(
   2349         ""user defined function compilation failed.""
   2350     ) from e
   2352 # Mask and data column preallocated

ValueError: user defined function compilation failed.

During handling of the above exception, another exception occurred:

NumbaNotImplementedError                  Traceback (most recent call last)
File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/cudf/core/indexed_frame.py:2344, in IndexedFrame._apply(self, func, kernel_getter, *args, **kwargs)
   2343 try:
-> 2344     kernel, retty = _compile_or_get(
   2345         self, func, args, kernel_getter=kernel_getter
   2346     )
   2347 except Exception as e:

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/nvtx/nvtx.py:115, in annotate.__call__.<locals>.inner(*args, **kwargs)
    114 libnvtx_push_range(self.attributes, self.domain.handle)
--> 115 result = func(*args, **kwargs)
    116 libnvtx_pop_range(self.domain.handle)

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/cudf/core/udf/utils.py:276, in _compile_or_get(frame, func, args, kernel_getter, suffix)
    273 # precompile the user udf to get the right return type.
    274 # could be a MaskedType or a scalar type.
--> 276 kernel, scalar_return_type = kernel_getter(frame, func, args)
    277 np_return_type = (
    278     numpy_support.as_dtype(scalar_return_type)
    279     if scalar_return_type.is_internal
    280     else scalar_return_type.np_dtype
    281 )

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/cudf/core/udf/scalar_function.py:55, in _get_scalar_kernel(sr, func, args)
     52 sr_type = MaskedType(
     53     string_view if sr.dtype == ""O"" else numpy_support.from_dtype(sr.dtype)
     54 )
---> 55 scalar_return_type = _get_udf_return_type(sr_type, func, args)
     57 sig = _construct_signature(sr, scalar_return_type, args=args)

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/nvtx/nvtx.py:115, in annotate.__call__.<locals>.inner(*args, **kwargs)
    114 libnvtx_push_range(self.attributes, self.domain.handle)
--> 115 result = func(*args, **kwargs)
    116 libnvtx_pop_range(self.domain.handle)

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/cudf/core/udf/utils.py:96, in _get_udf_return_type(argty, func, args)
     95 with _CUDFNumbaConfig():
---> 96     ptx, output_type = cudautils.compile_udf(func, compile_sig)
     98 if not isinstance(output_type, MaskedType):

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/cudf/utils/cudautils.py:130, in compile_udf(udf, type_signature)
    129 if not isinstance(return_type, cudf.core.udf.masked_typing.MaskedType):
--> 130     output_type = numpy_support.as_dtype(return_type).type
    131 else:

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/numba/np/numpy_support.py:159, in as_dtype(nbtype)
    158 msg = f""{nbtype} cannot be represented as a NumPy dtype""
--> 159 raise errors.NumbaNotImplementedError(msg)

NumbaNotImplementedError: unicode_type cannot be represented as a NumPy dtype

The above exception was the direct cause of the following exception:

ValueError                                Traceback (most recent call last)
Input In [5], in <cell line: 10>()
      7     return str(x + 5)
      9 s = pd.Series(range(5))
---> 10 s.swifter.apply(func)

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/swifter/swifter.py:320, in SeriesAccessor.apply(self, func, convert_dtype, args, **kwds)
    318 except ERRORS_TO_HANDLE:  # if can't vectorize, estimate time to pandas apply
    319     wrapped = self._wrapped_apply(func, convert_dtype=convert_dtype, args=args, **kwds)
--> 320     timed = timeit.timeit(wrapped, number=N_REPEATS)
    321     sample_proc_est = timed / N_REPEATS
    322     est_apply_duration = sample_proc_est / self._SAMPLE_SIZE * self._nrows

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/timeit.py:234, in timeit(stmt, setup, timer, number, globals)
    231 def timeit(stmt=""pass"", setup=""pass"", timer=default_timer,
    232            number=default_number, globals=None):
    233     """"""Convenience function to create Timer object and call timeit method.""""""
--> 234     return Timer(stmt, setup, timer, globals).timeit(number)

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/timeit.py:178, in Timer.timeit(self, number)
    176 gc.disable()
    177 try:
--> 178     timing = self.inner(it, self.timer)
    179 finally:
    180     if gcold:

File <timeit-src>:6, in inner(_it, _timer, _stmt)

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/swifter/swifter.py:228, in SeriesAccessor._wrapped_apply.<locals>.wrapped()
    226 def wrapped():
    227     with suppress_stdout_stderr_logging():
--> 228         self._obj.iloc[self._SAMPLE_INDEX].apply(func, convert_dtype=convert_dtype, args=args, **kwds)

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/nvtx/nvtx.py:115, in annotate.__call__.<locals>.inner(*args, **kwargs)
    112 @wraps(func)
    113 def inner(*args, **kwargs):
    114     libnvtx_push_range(self.attributes, self.domain.handle)
--> 115     result = func(*args, **kwargs)
    116     libnvtx_pop_range(self.domain.handle)
    117     return result

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/cudf/core/series.py:2632, in Series.apply(self, func, convert_dtype, args, **kwargs)
   2629 if convert_dtype is not True:
   2630     raise ValueError(""Series.apply only supports convert_dtype=True"")
-> 2632 result = self._apply(func, _get_scalar_kernel, *args, **kwargs)
   2633 result.name = self.name
   2634 return result

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/contextlib.py:79, in ContextDecorator.__call__.<locals>.inner(*args, **kwds)
     76 @wraps(func)
     77 def inner(*args, **kwds):
     78     with self._recreate_cm():
---> 79         return func(*args, **kwds)

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/nvtx/nvtx.py:115, in annotate.__call__.<locals>.inner(*args, **kwargs)
    112 @wraps(func)
    113 def inner(*args, **kwargs):
    114     libnvtx_push_range(self.attributes, self.domain.handle)
--> 115     result = func(*args, **kwargs)
    116     libnvtx_pop_range(self.domain.handle)
    117     return result

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/cudf/core/indexed_frame.py:2348, in IndexedFrame._apply(self, func, kernel_getter, *args, **kwargs)
   2344     kernel, retty = _compile_or_get(
   2345         self, func, args, kernel_getter=kernel_getter
   2346     )
   2347 except Exception as e:
-> 2348     raise ValueError(
   2349         ""user defined function compilation failed.""
   2350     ) from e
   2352 # Mask and data column preallocated
   2353 ans_col = _return_arr_from_dtype(retty, len(self))

ValueError: user defined function compilation failed.
``` 

CPU fallback works as expected using the regular `.apply` interface:

```python
%%cudf.pandas.profile

import pandas as pd
import swifter

def func(x):
    return str(x + 5)

s = pd.Series(range(5))
s.apply(func)
0    5
1    6
2    7
3    8
4    9
dtype: object

                                                                                                     
                                      Total time elapsed: 0.384 seconds                              
                                    3 GPU function calls in 0.022 seconds                            
                                    1 CPU function calls in 0.199 seconds                            
                                                                                                     
                                                    Stats                                            
                                                                                                     
┏━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓
┃ Function        ┃ GPU ncalls ┃ GPU cumtime ┃ GPU percall ┃ CPU ncalls ┃ CPU cumtime ┃ CPU percall ┃
┡━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩
│ Series          │ 2          │ 0.018       │ 0.009       │ 0          │ 0.000       │ 0.000       │
│ Series.apply    │ 0          │ 0.000       │ 0.000       │ 1          │ 0.199       │ 0.199       │
│ Series.__repr__ │ 1          │ 0.003       │ 0.003       │ 0          │ 0.000       │ 0.000       │
└─────────────────┴────────────┴─────────────┴─────────────┴────────────┴─────────────┴─────────────┘
```

This is the same error we'd get using this UDF with raw cuDF

```python
import cudf

def func(x):
    return str(x + 5)

s = cudf.Series(range(5))
s.apply(func)
---------------------------------------------------------------------------
NumbaNotImplementedError                  Traceback (most recent call last)
File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/cudf/core/indexed_frame.py:2344, in IndexedFrame._apply(self, func, kernel_getter, *args, **kwargs)
   2343 try:
-> 2344     kernel, retty = _compile_or_get(
   2345         self, func, args, kernel_getter=kernel_getter
   2346     )
   2347 except Exception as e:

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/nvtx/nvtx.py:115, in annotate.__call__.<locals>.inner(*args, **kwargs)
    114 libnvtx_push_range(self.attributes, self.domain.handle)
--> 115 result = func(*args, **kwargs)
    116 libnvtx_pop_range(self.domain.handle)

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/cudf/core/udf/utils.py:276, in _compile_or_get(frame, func, args, kernel_getter, suffix)
    273 # precompile the user udf to get the right return type.
    274 # could be a MaskedType or a scalar type.
--> 276 kernel, scalar_return_type = kernel_getter(frame, func, args)
    277 np_return_type = (
    278     numpy_support.as_dtype(scalar_return_type)
    279     if scalar_return_type.is_internal
    280     else scalar_return_type.np_dtype
    281 )

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/cudf/core/udf/scalar_function.py:55, in _get_scalar_kernel(sr, func, args)
     52 sr_type = MaskedType(
     53     string_view if sr.dtype == ""O"" else numpy_support.from_dtype(sr.dtype)
     54 )
---> 55 scalar_return_type = _get_udf_return_type(sr_type, func, args)
     57 sig = _construct_signature(sr, scalar_return_type, args=args)

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/nvtx/nvtx.py:115, in annotate.__call__.<locals>.inner(*args, **kwargs)
    114 libnvtx_push_range(self.attributes, self.domain.handle)
--> 115 result = func(*args, **kwargs)
    116 libnvtx_pop_range(self.domain.handle)

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/cudf/core/udf/utils.py:96, in _get_udf_return_type(argty, func, args)
     95 with _CUDFNumbaConfig():
---> 96     ptx, output_type = cudautils.compile_udf(func, compile_sig)
     98 if not isinstance(output_type, MaskedType):

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/cudf/utils/cudautils.py:130, in compile_udf(udf, type_signature)
    129 if not isinstance(return_type, cudf.core.udf.masked_typing.MaskedType):
--> 130     output_type = numpy_support.as_dtype(return_type).type
    131 else:

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/numba/np/numpy_support.py:159, in as_dtype(nbtype)
    158 msg = f""{nbtype} cannot be represented as a NumPy dtype""
--> 159 raise errors.NumbaNotImplementedError(msg)

NumbaNotImplementedError: unicode_type cannot be represented as a NumPy dtype

The above exception was the direct cause of the following exception:

ValueError                                Traceback (most recent call last)
Input In [7], in <cell line: 7>()
      4     return str(x + 5)
      6 s = cudf.Series(range(5))
----> 7 s.apply(func)

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/nvtx/nvtx.py:115, in annotate.__call__.<locals>.inner(*args, **kwargs)
    112 @wraps(func)
    113 def inner(*args, **kwargs):
    114     libnvtx_push_range(self.attributes, self.domain.handle)
--> 115     result = func(*args, **kwargs)
    116     libnvtx_pop_range(self.domain.handle)
    117     return result

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/cudf/core/series.py:2632, in Series.apply(self, func, convert_dtype, args, **kwargs)
   2629 if convert_dtype is not True:
   2630     raise ValueError(""Series.apply only supports convert_dtype=True"")
-> 2632 result = self._apply(func, _get_scalar_kernel, *args, **kwargs)
   2633 result.name = self.name
   2634 return result

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/contextlib.py:79, in ContextDecorator.__call__.<locals>.inner(*args, **kwds)
     76 @wraps(func)
     77 def inner(*args, **kwds):
     78     with self._recreate_cm():
---> 79         return func(*args, **kwds)

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/nvtx/nvtx.py:115, in annotate.__call__.<locals>.inner(*args, **kwargs)
    112 @wraps(func)
    113 def inner(*args, **kwargs):
    114     libnvtx_push_range(self.attributes, self.domain.handle)
--> 115     result = func(*args, **kwargs)
    116     libnvtx_pop_range(self.domain.handle)
    117     return result

File ~/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/cudf/core/indexed_frame.py:2348, in IndexedFrame._apply(self, func, kernel_getter, *args, **kwargs)
   2344     kernel, retty = _compile_or_get(
   2345         self, func, args, kernel_getter=kernel_getter
   2346     )
   2347 except Exception as e:
-> 2348     raise ValueError(
   2349         ""user defined function compilation failed.""
   2350     ) from e
   2352 # Mask and data column preallocated
   2353 ans_col = _return_arr_from_dtype(retty, len(self))

ValueError: user defined function compilation failed.
```

Environment:

```
conda list | grep ""rapids\|pandas\|numba""
# packages in environment at /home/nicholasb/miniconda3/envs/rapids-24.02:
cudf_kafka                24.02.00a203    cuda11_py310_240117_g9acddc08cc_203    rapidsai-nightly
geopandas                 0.14.2             pyhd8ed1ab_0    conda-forge
geopandas-base            0.14.2             pyha770c72_0    conda-forge
libcucim                  24.02.00a23     cuda11_240117_g0bb1dd6_23    rapidsai-nightly
libcudf                   24.02.00a203    cuda11_240117_g9acddc08cc_203    rapidsai-nightly
libcudf_kafka             24.02.00a203    cuda11_240117_g9acddc08cc_203    rapidsai-nightly
libcugraph                24.02.00a72     cuda11_240117_geacdf587_72    rapidsai-nightly
libcugraph_etl            24.02.00a72     cuda11_240117_geacdf587_72    rapidsai-nightly
libcugraphops             24.02.00a10     cuda11_240117_g62fe19da_10    rapidsai-nightly
libcuml                   24.02.00a40     cuda11_240117_gbb09e545c_40    rapidsai-nightly
libcumlprims              24.02.00a       cuda11_240117_ge9ee121_5    rapidsai-nightly
libcuspatial              24.02.00a20     cuda11_240117_g43bb8f30_20    rapidsai-nightly
libkvikio                 24.02.00a       cuda11_240117_gf8f5858_16    rapidsai-nightly
libraft                   24.02.00a68     cuda11_240117_g1e4961e2_68    rapidsai-nightly
libraft-headers           24.02.00a68     cuda11_240117_g1e4961e2_68    rapidsai-nightly
libraft-headers-only      24.02.00a68     cuda11_240117_g1e4961e2_68    rapidsai-nightly
librmm                    24.02.00a34     cuda11_240117_gbb8fdf1e_34    rapidsai-nightly
libxgboost                1.7.6           rapidsai_h3ea025e_8    rapidsai-nightly
pandas                    1.5.3                    pypi_0    pypi
py-xgboost                1.7.6           rapidsai_py310h09713c5_8    rapidsai-nightly
rapids                    24.02.00a       cuda11_py310_240112_g757f4e1_3    rapidsai-nightly
rapids-dask-dependency    24.02.00a11                   0    rapidsai-nightly
rapids-xgboost            24.02.00a       cuda11_py310_240112_g757f4e1_3    rapidsai-nightly
ucx-proc                  1.0.0                       gpu    rapidsai-nightly

pip freeze | grep swifter
swifter==1.4.0
```",2024-01-17T23:40:25Z,0,0,Nick Becker,@NVIDIA,True
730,[FEA][JNI] Adopt rmm::pinned_host_memory_resource in java,"RMM has introduced a pinned host memory resource, and is in the process to make changes for the pool memory resource to be compatible (https://github.com/rapidsai/rmm/pull/1392).

I have tested gutting `PinnedMemoryPool`, turning into a thin wrapper to a `pool_memory_resource<pinned_host_memory_resource>` in a draft PR, that uses some initial work that @jbrennan333 started. Here's what we have so far: https://github.com/rapidsai/cudf/compare/branch-24.02...abellina:cudf:ab-pinned-pool.

We would like to make architecture decisions on how we could use the pinned pool, perhaps the single one created for the JNI side of things, to be shared with cuIO https://github.com/rapidsai/cudf/issues/14314.

It would be great or us to try and repro this https://github.com/rapidsai/cudf/issues/12341. And see if the new approach removes the slowness we have occasionally seen.
",2024-01-18T17:29:41Z,0,0,Alessandro Bellina,NVIDIA,True
731,MixedTypeError when there is no mixed type [BUG],"**Describe the bug**
I load a pandas dataframe into cudf using cudf.from_pandas(originaldataframe) and it gives me a mixed type error.

**Steps/Code to reproduce bug**
Original Dataframe: 
```
     symbol                                               name STOCK_TYPE  first_date   last_date    AGE                INDUSTRY   marketcap
0     WHFBZ      WhiteHorse Finance, Inc. 6.50% Notes due 2025     common  2018-11-30  2021-12-16   1112                 Unknown    0.000000
1       ANH                 Anworth Mortgage Asset Corporation     common  1998-03-12  2021-03-19   8408                 Unknown    0.000000
2       CEE          The Central and Eastern Europe Fund, Inc.     common  1990-02-28  2024-01-09  12368        Asset Management    0.062059
3      SEMR                             SEMrush Holdings, Inc.     common  2021-03-24  2024-01-09   1021  Software - Application    1.780361
4      BWMX  Betterware de Mexico, S.A.P.I. de C.V. Ordinar...     common  2020-03-16  2024-01-09   1394        Specialty Retail    0.470934
...     ...                                                ...        ...         ...         ...    ...                     ...         ...
9281    GHI  Greystone Housing Impact Investors LP Benefici...     common  1986-04-02  2024-01-09  13796        Mortgage Finance    0.387465
9282    LMT                              Lockheed Martin Corp.     common  1977-01-03  2024-01-09  17172     Aerospace & Defense  113.205101
9283   ^DJI                                          Dow Jones      index  1970-01-02  2024-01-08  19729                   Index    0.000000
9284   ^INX                                            S&P 500      index  1970-01-02  2024-01-08  19729                   Index    0.000000
9285  ^IXIC                                             NASDAQ      index  1971-02-05  2024-01-08  19330                   Index    0.000000
```
I created the following function to create a dictionary of all the unique datatypes found for each column, even if there are more than one type in a single column.  Here's the function:
```
def get_column_data_types(dataframe):
    column_data_types = {}
    
    for column in dataframe.columns:
        unique_types = set(type(value) for value in dataframe[column])
        column_data_types[column] = unique_types
    
    return column_data_types
```
Here's the output of the column data types:
```
{
    'AGE': set([<class 'int'>]),
    'INDUSTRY': set([<class 'str'>]),
    'STOCK_TYPE': set([<class 'str'>]),
    'first_date': set([<class 'datetime.date'>]),
    'last_date': set([<class 'datetime.date'>]),
    'marketcap': set([<class 'float'>]),
    'name': set([<class 'str'>]),
    'symbol': set([<class 'str'>]),
}
```

As you can see, the function only found one datatype for each column.

Alternatively, if I use pandas built in datatype command `dataframe.dtypes` I get the following:
```
symbol         object
name           object
STOCK_TYPE     object
first_date     object
last_date      object
AGE             int64
INDUSTRY       object
marketcap     float64
dtype: object
```
So by both tests, each column has only one data type.  Though the .dtypes command shows ""object"" as the datatype.  Perhaps that's causing cudf to throw the error?

Here is another example:
```
            date        NVDA
0     1999-01-22    0.376356
1     1999-01-25    0.415730
2     1999-01-26    0.383428
3     1999-01-27    0.382281
4     1999-01-28    0.381134
...          ...         ...
6277  2024-01-03  475.690000
6278  2024-01-04  479.980000
6279  2024-01-05  490.970000
6280  2024-01-08  522.530000
6281  2024-01-09  531.400000
```
Running the following:

```
pprint(get_column_data_types(df))
pprint(df.dtypes)
```
gives you the following:

```
{'NVDA': set([<class 'float'>]), 'date': set([<class 'datetime.date'>])}

date     object
NVDA    float64
dtype: object
```
As you can see, each column has only one data type.  Yet when I try to convert df to a cudf using cudf.from_pandas(df), it throws the same mixed type error.


**Expected behavior**
There's no apparent mixtype column in the dataframe so it should be able to open the dataframe without throwing the mixedtype error.

**Environment overview (please complete the following information)**
 - Environment location: Local machine.  PC running Win 11 using WSL2 Ubuntu
 - Method of cuDF install: pip

CUDA 12 installed
NVIDIA GTX 1080 graphics card


**Environment details**
Error thrown in detail:
```
Traceback (most recent call last):
  File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code
    exec(code, run_globals)
  File ""/home/notwopr/.local/lib/python3.10/site-packages/cudf/pandas/__main__.py"", line 91, in <module>
    main()
  File ""/home/notwopr/.local/lib/python3.10/site-packages/cudf/pandas/__main__.py"", line 87, in main
    runpy.run_path(args.args[0], run_name=""__main__"")
  File ""/usr/lib/python3.10/runpy.py"", line 289, in run_path
    return _run_module_code(code, init_globals, run_name,
  File ""/usr/lib/python3.10/runpy.py"", line 96, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code
    exec(code, run_globals)
  File ""scratchp7.py"", line 29, in <module>
    stockdatastats = FileOperations().readpkl(oldfn, DirPaths().full_info_db)
  File ""/home/notwopr/beluga/beluga3/file_functions.py"", line 57, in readpkl
    data = cudf.from_pandas(data)
  File ""/home/notwopr/.local/lib/python3.10/site-packages/nvtx/nvtx.py"", line 115, in inner
    result = func(*args, **kwargs)
  File ""/home/notwopr/.local/lib/python3.10/site-packages/cudf/core/dataframe.py"", line 7891, in from_pandas
    return DataFrame.from_pandas(obj, nan_as_null=nan_as_null)
  File ""/home/notwopr/.local/lib/python3.10/site-packages/nvtx/nvtx.py"", line 115, in inner
    result = func(*args, **kwargs)
  File ""/home/notwopr/.local/lib/python3.10/site-packages/cudf/core/dataframe.py"", line 5237, in from_pandas
    data[col_name] = column.as_column(
  File ""/home/notwopr/.local/lib/python3.10/site-packages/cudf/core/column/column.py"", line 2279, in as_column
    raise MixedTypeError(""Cannot create column with mixed types"")
cudf.errors.MixedTypeError: Cannot create column with mixed types
```

**Additional context**
Add any other context about the problem here.
",2024-01-19T06:53:54Z,0,0,David Choi,,False
732,[BUG] sort_values fails on groupby aggregation,"**Describe the bug**
This bug was found while playing around with the [One Billion Row Challenge](https://medium.com/coiled-hq/1brc-in-python-with-dask-3cdee6a56a2d).

**Steps/Code to reproduce bug**
```python
import cudf
import cupy as cp

# Generate some sample data similar to the data in 1BRC
stations = cudf.Series(['Ankara', 'Kampala', 'Tallinn', 'Gjoa Haven', 'Luanda', 'Cairo', 'Phnom Penh', 'Thessaloniki', 'Split', 'Palermo', 'Ouarzazate', 'Mandalay'])
df = cudf.DataFrame({        
        ""station"": cp.random.randint(0, len(stations)-1, 10_000_000),
        ""measure"": cp.random.normal(0, 10.0, 10_000_000)
    })
df.station = df.station.map(stations)

# Perform the grouby aggregation
df = df.groupby(""station"").agg([""min"", ""max"", ""mean""])
df.columns = df.columns.droplevel()

# Sort
df.sort_values(""station"") # <------------ Raises a KeyError

# It works in Pandas
df.to_pandas().sort_values(""station"")  # Works!
```

```pytb
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
Cell In[10], line 1
----> 1 df.sort_values(""station"")

File /opt/conda/lib/python3.10/site-packages/cudf/core/indexed_frame.py:2459, in IndexedFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position, ignore_index)
   2454     return self
   2456 # argsort the `by` column
   2457 out = self._gather(
   2458     GatherMap.from_column_unchecked(
-> 2459         self._get_columns_by_label(by)._get_sorted_inds(
   2460             ascending=ascending, na_position=na_position
   2461         ),
   2462         len(self),
   2463         nullify=False,
   2464     ),
   2465     keep_index=not ignore_index,
   2466 )
   2467 if (
   2468     isinstance(self, cudf.core.dataframe.DataFrame)
   2469     and self._data.multiindex
   2470 ):
   2471     out.columns = self._data.to_pandas_index()

File /opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py:115, in annotate.__call__.<locals>.inner(*args, **kwargs)
    112 @wraps(func)
    113 def inner(*args, **kwargs):
    114     libnvtx_push_range(self.attributes, self.domain.handle)
--> 115     result = func(*args, **kwargs)
    116     libnvtx_pop_range(self.domain.handle)
    117     return result

File /opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py:1959, in DataFrame._get_columns_by_label(self, labels, downcast)
   1950 @_cudf_nvtx_annotate
   1951 def _get_columns_by_label(
   1952     self, labels, *, downcast=False
   1953 ) -> Self | Series:
   1954     """"""
   1955     Return columns of dataframe by `labels`
   1956 
   1957     If downcast is True, try and downcast from a DataFrame to a Series
   1958     """"""
-> 1959     ca = self._data.select_by_label(labels)
   1960     if downcast:
   1961         if is_scalar(labels):

File /opt/conda/lib/python3.10/site-packages/cudf/core/column_accessor.py:381, in ColumnAccessor.select_by_label(self, key)
    379     if any(isinstance(k, slice) for k in key):
    380         return self._select_by_label_with_wildcard(key)
--> 381 return self._select_by_label_grouped(key)

File /opt/conda/lib/python3.10/site-packages/cudf/core/column_accessor.py:536, in ColumnAccessor._select_by_label_grouped(self, key)
    535 def _select_by_label_grouped(self, key: Any) -> ColumnAccessor:
--> 536     result = self._grouped_data[key]
    537     if isinstance(result, cudf.core.column.ColumnBase):
    538         return self.__class__({key: result}, multiindex=self.multiindex)

KeyError: 'station'
```

**Expected behavior**
The same code works in Pandas/Numpy.

**Environment overview (please complete the following information)**

```bash
docker run --gpus all --pull always --rm -it \
    --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 \
    -p 8888:8888 -p 8787:8787 -p 8786:8786 \
    rapidsai/notebooks:23.12-cuda12.0-py3.10
```

**Additional context**
Add any other context about the problem here.
",2024-01-19T11:27:04Z,0,0,Jacob Tomlinson,@nvidia,True
733,[BUG] Can't append cudf pandas dataframe to an open file with UnicodeEncodeError,"**Describe the bug**
This bug was found while playing around with the [One Billion Row Challenge](https://medium.com/coiled-hq/1brc-in-python-with-dask-3cdee6a56a2d).

If I have a string column that contains utf-8 characters and I try and append that DataFrame to an open file I get a `UnicodeEncodeError`. It only happens if I use CuPy to generate the random data, it works with NumPy but is much slower at the scale I'm working at.

**Steps/Code to reproduce bug**

```python
%load_ext cudf.pandas
import pandas as pd
import cupy as cp

# Generate some data
stations = pd.Series(['San José', 'Ankara', 'Kampala', 'Tallinn', 'Gjoa Haven', 'Luanda', 'Cairo', 'Phnom Penh', 'Thessaloniki', 'Split', 'Palermo', 'Ouarzazate', 'Mandalay'])
df = pd.DataFrame({""station"": cp.random.randint(0, len(stations)-1, 10_000)})
df.station = df.station.map(stations)

# Append to the output file
with open(""foo.txt"", ""a"") as fh:
    df.to_csv(fh, sep="";"", header=False, index=False)
```

```pytb
---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
File /opt/conda/lib/python3.10/site-packages/cudf/pandas/fast_slow_proxy.py:836, in _fast_slow_function_call(func, *args, **kwargs)
    831 with nvtx.annotate(
    832     ""EXECUTE_FAST"",
    833     color=_CUDF_PANDAS_NVTX_COLORS[""EXECUTE_FAST""],
    834     domain=""cudf_pandas"",
    835 ):
--> 836     fast_args, fast_kwargs = _fast_arg(args), _fast_arg(kwargs)
    837     result = func(*fast_args, **fast_kwargs)

File /opt/conda/lib/python3.10/site-packages/cudf/pandas/fast_slow_proxy.py:955, in _fast_arg(arg)
    954 seen: Set[int] = set()
--> 955 return _transform_arg(arg, ""_fsproxy_fast"", seen)

File /opt/conda/lib/python3.10/site-packages/cudf/pandas/fast_slow_proxy.py:882, in _transform_arg(arg, attribute_name, seen)
    880 if type(arg) is tuple:
    881     # Must come first to avoid infinite recursion
--> 882     return tuple(_transform_arg(a, attribute_name, seen) for a in arg)
    883 elif hasattr(arg, ""__getnewargs_ex__""):
    884     # Partial implementation of to reconstruct with
    885     # transformed pieces
    886     # This handles scipy._lib._bunch._make_tuple_bunch

File /opt/conda/lib/python3.10/site-packages/cudf/pandas/fast_slow_proxy.py:882, in <genexpr>(.0)
    880 if type(arg) is tuple:
    881     # Must come first to avoid infinite recursion
--> 882     return tuple(_transform_arg(a, attribute_name, seen) for a in arg)
    883 elif hasattr(arg, ""__getnewargs_ex__""):
    884     # Partial implementation of to reconstruct with
    885     # transformed pieces
    886     # This handles scipy._lib._bunch._make_tuple_bunch

File /opt/conda/lib/python3.10/site-packages/cudf/pandas/fast_slow_proxy.py:882, in _transform_arg(arg, attribute_name, seen)
    880 if type(arg) is tuple:
    881     # Must come first to avoid infinite recursion
--> 882     return tuple(_transform_arg(a, attribute_name, seen) for a in arg)
    883 elif hasattr(arg, ""__getnewargs_ex__""):
    884     # Partial implementation of to reconstruct with
    885     # transformed pieces
    886     # This handles scipy._lib._bunch._make_tuple_bunch

File /opt/conda/lib/python3.10/site-packages/cudf/pandas/fast_slow_proxy.py:882, in <genexpr>(.0)
    880 if type(arg) is tuple:
    881     # Must come first to avoid infinite recursion
--> 882     return tuple(_transform_arg(a, attribute_name, seen) for a in arg)
    883 elif hasattr(arg, ""__getnewargs_ex__""):
    884     # Partial implementation of to reconstruct with
    885     # transformed pieces
    886     # This handles scipy._lib._bunch._make_tuple_bunch

File /opt/conda/lib/python3.10/site-packages/cudf/pandas/fast_slow_proxy.py:938, in _transform_arg(arg, attribute_name, seen)
    933 elif isinstance(arg, Iterator) and attribute_name == ""_fsproxy_fast"":
    934     # this may include consumable objects like generators or
    935     # IOBase objects, which we don't want unavailable to the slow
    936     # path in case of fallback. So, we raise here and ensure the
    937     # slow path is taken:
--> 938     raise Exception()
    939 elif isinstance(arg, types.FunctionType):

Exception: 

During handling of the above exception, another exception occurred:

UnicodeEncodeError                        Traceback (most recent call last)
Cell In[1], line 13
     11 # Append to the output file
     12 with open(""foo.txt"", ""a"") as fh:
---> 13     df.to_csv(fh, sep="";"", header=False, index=False)

File /opt/conda/lib/python3.10/site-packages/cudf/pandas/fast_slow_proxy.py:785, in _CallableProxyMixin.__call__(self, *args, **kwargs)
    784 def __call__(self, *args, **kwargs) -> Any:
--> 785     result, _ = _fast_slow_function_call(
    786         # We cannot directly call self here because we need it to be
    787         # converted into either the fast or slow object (by
    788         # _fast_slow_function_call) to avoid infinite recursion.
    789         # TODO: When Python 3.11 is the minimum supported Python version
    790         # this can use operator.call
    791         lambda fn, args, kwargs: fn(*args, **kwargs),
    792         self,
    793         args,
    794         kwargs,
    795     )
    796     return result

File /opt/conda/lib/python3.10/site-packages/cudf/pandas/fast_slow_proxy.py:850, in _fast_slow_function_call(func, *args, **kwargs)
    848         slow_args, slow_kwargs = _slow_arg(args), _slow_arg(kwargs)
    849         with disable_module_accelerator():
--> 850             result = func(*slow_args, **slow_kwargs)
    851 return _maybe_wrap_result(result, func, *args, **kwargs), fast

File /opt/conda/lib/python3.10/site-packages/cudf/pandas/fast_slow_proxy.py:791, in _CallableProxyMixin.__call__.<locals>.<lambda>(fn, args, kwargs)
    784 def __call__(self, *args, **kwargs) -> Any:
    785     result, _ = _fast_slow_function_call(
    786         # We cannot directly call self here because we need it to be
    787         # converted into either the fast or slow object (by
    788         # _fast_slow_function_call) to avoid infinite recursion.
    789         # TODO: When Python 3.11 is the minimum supported Python version
    790         # this can use operator.call
--> 791         lambda fn, args, kwargs: fn(*args, **kwargs),
    792         self,
    793         args,
    794         kwargs,
    795     )
    796     return result

File /opt/conda/lib/python3.10/site-packages/pandas/util/_decorators.py:211, in deprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper(*args, **kwargs)
    209     else:
    210         kwargs[new_arg_name] = new_arg_value
--> 211 return func(*args, **kwargs)

File /opt/conda/lib/python3.10/site-packages/pandas/core/generic.py:3720, in NDFrame.to_csv(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)
   3709 df = self if isinstance(self, ABCDataFrame) else self.to_frame()
   3711 formatter = DataFrameFormatter(
   3712     frame=df,
   3713     header=header,
   (...)
   3717     decimal=decimal,
   3718 )
-> 3720 return DataFrameRenderer(formatter).to_csv(
   3721     path_or_buf,
   3722     lineterminator=lineterminator,
   3723     sep=sep,
   3724     encoding=encoding,
   3725     errors=errors,
   3726     compression=compression,
   3727     quoting=quoting,
   3728     columns=columns,
   3729     index_label=index_label,
   3730     mode=mode,
   3731     chunksize=chunksize,
   3732     quotechar=quotechar,
   3733     date_format=date_format,
   3734     doublequote=doublequote,
   3735     escapechar=escapechar,
   3736     storage_options=storage_options,
   3737 )

File /opt/conda/lib/python3.10/site-packages/pandas/util/_decorators.py:211, in deprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper(*args, **kwargs)
    209     else:
    210         kwargs[new_arg_name] = new_arg_value
--> 211 return func(*args, **kwargs)

File /opt/conda/lib/python3.10/site-packages/pandas/io/formats/format.py:1189, in DataFrameRenderer.to_csv(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)
   1168     created_buffer = False
   1170 csv_formatter = CSVFormatter(
   1171     path_or_buf=path_or_buf,
   1172     lineterminator=lineterminator,
   (...)
   1187     formatter=self.fmt,
   1188 )
-> 1189 csv_formatter.save()
   1191 if created_buffer:
   1192     assert isinstance(path_or_buf, StringIO)

File /opt/conda/lib/python3.10/site-packages/pandas/io/formats/csvs.py:261, in CSVFormatter.save(self)
    241 with get_handle(
    242     self.filepath_or_buffer,
    243     self.mode,
   (...)
    249 
    250     # Note: self.encoding is irrelevant here
    251     self.writer = csvlib.writer(
    252         handles.handle,
    253         lineterminator=self.lineterminator,
   (...)
    258         quotechar=self.quotechar,
    259     )
--> 261     self._save()

File /opt/conda/lib/python3.10/site-packages/pandas/io/formats/csvs.py:266, in CSVFormatter._save(self)
    264 if self._need_to_save_header:
    265     self._save_header()
--> 266 self._save_body()

File /opt/conda/lib/python3.10/site-packages/pandas/io/formats/csvs.py:304, in CSVFormatter._save_body(self)
    302 if start_i >= end_i:
    303     break
--> 304 self._save_chunk(start_i, end_i)

File /opt/conda/lib/python3.10/site-packages/pandas/io/formats/csvs.py:315, in CSVFormatter._save_chunk(self, start_i, end_i)
    312 data = [res.iget_values(i) for i in range(len(res.items))]
    314 ix = self.data_index[slicer]._format_native_types(**self._number_format)
--> 315 libwriters.write_csv_rows(
    316     data,
    317     ix,
    318     self.nlevels,
    319     self.cols,
    320     self.writer,
    321 )

File /opt/conda/lib/python3.10/site-packages/pandas/_libs/writers.pyx:72, in pandas._libs.writers.write_csv_rows()

UnicodeEncodeError: 'ascii' codec can't encode character '\xe9' in position 7: ordinal not in range(128)
```

**Expected behavior**

The dataframe should be appended to the file.

**Environment overview (please complete the following information)**

```bash
docker run --gpus all --pull always --rm -it \
    --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 \
    -p 8888:8888 -p 8787:8787 -p 8786:8786 \
    rapidsai/notebooks:23.12-cuda12.0-py3.10
```

**Additional context**

If I do a plain `df.to_csv(""foo.txt"", ...` it works, but I need to be able to append to the file.
",2024-01-19T12:17:38Z,0,0,Jacob Tomlinson,@nvidia,True
734,[BUG] deprecated warnings should be enabled,"**Describe the bug**
deprecated warnings are disabled by default in build.sh. Dev containers have them enabled. 
in rapids-compose also, it's disabled by default in bash-utils.sh

PR https://github.com/rapidsai/cudf/pull/14771 fixes deprecated warnings introduced when https://github.com/rapidsai/cudf/pull/14202 merged. These warnings didn't showup during development (used rapids-compose)

**Expected behavior**
deprecated warnings should be enabled by default.
deprecated APIs usage are typically removed from unit tests, benchmarks and wrapper APIs as well.

**Environment overview (please complete the following information)**
 - Environment location: Bare-metal, rapids-compose
 - Method of cuDF install: from source

**Additional context**
https://github.com/rapidsai/cudf/pull/14771#issuecomment-1899129821
",2024-01-22T16:48:10Z,0,0,Karthikeyan,NVIDIA,True
735,[DOC] Python chunked writers are not documented with other io APIs,ORCWriter and ParquetWriter are not included in the docs at ioutils.py,2024-01-22T20:02:38Z,0,0,Vukasin Milovanovic,NVIDIA,True
736,[BUG] `RollingGroupBy` objects cannot be pickled,"I noticed that `RollingGroupby` objects in cuDF cannot be pickled:

```python
In [5]: df = cudf.DataFrame({'a': [1, 1, 2], 'b': [1, 2, 3]})

In [6]: pickle.dumps(df.groupby('a').rolling(2))
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[6], line 1
----> 1 pickle.dumps(df.groupby('a').rolling(2))

ValueError: ctypes objects containing pointers cannot be pickled
```

This has something to do with the `.window` attribute being a numba DeviceArray, and the latter being unpicklable. ",2024-01-23T20:18:08Z,0,0,Ashwin Srinath,Voltron Data,False
737,[BUG][JNI] java.lang.IllegalArgumentException: Unknown scalar type: STRUCT,"**Describe the bug**
cuDF Java supports creating Scalar types to represent structs, but printing them for debug purposes causes an exception.

```
java.lang.IllegalArgumentException: Unknown scalar type: STRUCT
```

**Steps/Code to reproduce bug**
Create a scalar using `Scalar.structFromNull` or `Scalar.structFromColumnViews` and then call its `toString` method or try and print it using `System.out.println`

**Expected behavior**
Should print some useful debug info as per other scalar types, rather than throw an exception.

**Environment overview (please complete the following information)**
N/A

**Environment details**

**Additional context**
",2024-01-23T23:50:46Z,0,0,Andy Grove,@Apple,False
738,[BUG] Incorrect `NaN` ignoring in GroupBy JIT Reductions,"**Describe the bug**
Pandas series and dataframe reductions default to `skipna=True`, our JIT operators don't fully support this behavior. 


**Steps/Code to reproduce bug**
```python
import cudf
df = cudf.DataFrame({
    'key1': [0,0],
    'val1':[1, float('nan')]
}, nan_as_null=False)

def func(df):
    return df['val1'].sum()

expect = df.to_pandas().groupby('key1').apply(func)
got = df.groupby('key1').apply(func, engine='jit')

print(expect)
print(got)
```
```
key1
0    1.0
dtype: float64
key1
0   NaN
dtype: float64
```


**Expected behavior**
`NaN` values should be ignored by default and the same numerical value should be returned from cuDF as pandas.

**Environment overview (please complete the following information)**
 - Environment location: Bare-metal
 - Method of cuDF install: Source

**Additional context**
The current operators have _some_ `NaN` handling, but it's mostly to cover cases where the entire sequence of values is `NaN` or for edge cases in some operators where we'd end up dividing by zero. However general `NaN` _skipping_ is not implemented leading to the buggy behavior. ",2024-01-24T13:57:57Z,0,0,,NVIDIA,True
739,[BUG] `.loc` with `DatetimeIndex` needs to be have stricter checks similar to `pandas-2.x`,"**Describe the bug**
Pandas-2.x introduced stricter checks for `.loc` when there is a `DatetimeIndex` present. We need to adapt similar checks and match the behavior in `pandas_2.0_feature_branch`

Following are the failing pytests:
```
FAILED python/cudf/cudf/tests/test_indexing.py::test_loc_datetime_index[True-sli0]
FAILED python/cudf/cudf/tests/test_indexing.py::test_loc_datetime_index[True-sli1]
FAILED python/cudf/cudf/tests/test_indexing.py::test_loc_datetime_index[True-sli2]
FAILED python/cudf/cudf/tests/test_indexing.py::test_loc_datetime_index[True-sli3]
FAILED python/cudf/cudf/tests/test_indexing.py::test_loc_datetime_index[False-sli0]
FAILED python/cudf/cudf/tests/test_indexing.py::test_loc_datetime_index[False-sli1]
FAILED python/cudf/cudf/tests/test_indexing.py::test_loc_datetime_index[False-sli2]
FAILED python/cudf/cudf/tests/test_indexing.py::test_loc_datetime_index[False-sli3]
```

",2024-01-24T17:55:49Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
740,[BUG] byte-range logic can be confused with empty rows,"**Describe the bug**
When using the `byte_range=` argument, a range that:
1. Includes trailing empty rows
2. Does not include the file end (last byte)

The read can lose the last row within the range.

**Steps/Code to reproduce bug**

```python
import cudf

filename = ""/tmp/bad_file.csv""

with open(filename, ""w"") as f:
    f.write(""a\n1\n2\n\n\n\n\n3\n"")

print(""Read first row:"")
print(cudf.read_csv(filename, byte_range=(0, 3)))  # read up to 1
print(""Attempt to read second, but not everything *WRONG*:"")
print(cudf.read_csv(filename, byte_range=(3, 5), header=-1))  # should read the 2!
print(""Read second, but include more *CORRECT*:"")
print(cudf.read_csv(filename, byte_range=(3, 6), header=-1))  # does read the 2!
```
(It is not clear to me what the magic is here, w.r.t. to the larger read seeing enough newlines?)

I can reproduce the same with `f.write(""a,b\n1,2\n2,3\n\n\n\n\n3,4\n"")` and reading the range `(5, 9)` (still fails) vs. `(5, 10)` where the second chunk includes the row.

*EDIT:* I have confirmed that the size of the row (containing the 2) itself is not relevant.  Using `f.write(f""a\n1\n2{'2' * pad}\n\n\n\n\n3\n"")` and adding `pad` to the bytes read behaves the same.

**Expected behavior**
Chunk which includes the `\n` before the 2 should always read it (independent of the bytes read).

**Environment overview (please complete the following information)**
conda cudf install `'23.04.01'`

**Environment details**

<details><summary>Click here to see environment details</summary><pre>
     
     ***OS Information***
     DISTRIB_ID=Ubuntu
     DISTRIB_RELEASE=22.04
     DISTRIB_CODENAME=jammy
     DISTRIB_DESCRIPTION=""Ubuntu 22.04.2 LTS""
     PRETTY_NAME=""Ubuntu 22.04.2 LTS""
     NAME=""Ubuntu""
     VERSION_ID=""22.04""
     VERSION=""22.04.2 LTS (Jammy Jellyfish)""
     VERSION_CODENAME=jammy
     ID=ubuntu
     ID_LIKE=debian
     HOME_URL=""https://www.ubuntu.com/""
     SUPPORT_URL=""https://help.ubuntu.com/""
     BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
     PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
     UBUNTU_CODENAME=jammy
     Linux dt05 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
     
     ***GPU Information***
     Thu Jan 25 11:50:45 2024
     +---------------------------------------------------------------------------------------+
     | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
     |-----------------------------------------+----------------------+----------------------+
     | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
     | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
     |                                         |                      |               MIG M. |
     |=========================================+======================+======================|
     |   0  Tesla T4                       On  | 00000000:3B:00.0 Off |                    0 |
     | N/A   45C    P8              10W /  70W |      2MiB / 15360MiB |      0%      Default |
     |                                         |                      |                  N/A |
     +-----------------------------------------+----------------------+----------------------+
     |   1  Tesla T4                       On  | 00000000:5E:00.0 Off |                    0 |
     | N/A   36C    P8              10W /  70W |      2MiB / 15360MiB |      0%      Default |
     |                                         |                      |                  N/A |
     +-----------------------------------------+----------------------+----------------------+
     |   2  Tesla T4                       On  | 00000000:AF:00.0 Off |                    0 |
     | N/A   30C    P8               9W /  70W |      2MiB / 15360MiB |      0%      Default |
     |                                         |                      |                  N/A |
     +-----------------------------------------+----------------------+----------------------+
     |   3  Tesla T4                       On  | 00000000:D8:00.0 Off |                    0 |
     | N/A   30C    P8               9W /  70W |      2MiB / 15360MiB |      0%      Default |
     |                                         |                      |                  N/A |
     +-----------------------------------------+----------------------+----------------------+
     
     +---------------------------------------------------------------------------------------+
     | Processes:                                                                            |
     |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
     |        ID   ID                                                             Usage      |
     |=======================================================================================|
     |  No running processes found                                                           |
     +---------------------------------------------------------------------------------------+
     
     ***CPU***
     Architecture:                       x86_64
     CPU op-mode(s):                     32-bit, 64-bit
     Address sizes:                      46 bits physical, 48 bits virtual
     Byte Order:                         Little Endian
     CPU(s):                             64
     On-line CPU(s) list:                0-63
     Vendor ID:                          GenuineIntel
     Model name:                         Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz
     CPU family:                         6
     Model:                              85
     Thread(s) per core:                 2
     Core(s) per socket:                 16
     Socket(s):                          2
     Stepping:                           4
     CPU max MHz:                        3700.0000
     CPU min MHz:                        1000.0000
     BogoMIPS:                           4200.00
     Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts pku ospke md_clear flush_l1d arch_capabilities
     Virtualization:                     VT-x
     L1d cache:                          1 MiB (32 instances)
     L1i cache:                          1 MiB (32 instances)
     L2 cache:                           32 MiB (32 instances)
     L3 cache:                           44 MiB (2 instances)
     NUMA node(s):                       2
     NUMA node0 CPU(s):                  0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62
     NUMA node1 CPU(s):                  1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63
     Vulnerability Gather data sampling: Mitigation; Microcode
     Vulnerability Itlb multihit:        KVM: Mitigation: VMX disabled
     Vulnerability L1tf:                 Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable
     Vulnerability Mds:                  Mitigation; Clear CPU buffers; SMT vulnerable
     Vulnerability Meltdown:             Mitigation; PTI
     Vulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable
     Vulnerability Retbleed:             Mitigation; IBRS
     Vulnerability Spec rstack overflow: Not affected
     Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp
     Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
     Vulnerability Spectre v2:           Mitigation; IBRS, IBPB conditional, STIBP conditional, RSB filling, PBRSB-eIBRS Not affected
     Vulnerability Srbds:                Not affected
     Vulnerability Tsx async abort:      Mitigation; Clear CPU buffers; SMT vulnerable
     
     ***CMake***
     /nvme/0/sebastianb/mambaforge/envs/.../bin/cmake
     cmake version 3.26.4
     
     CMake suite maintained and supported by Kitware (kitware.com/cmake).
     
     ***g++***
     /nvme/0/sebastianb/mambaforge/envs/.../bin/g++
     g++ (conda-forge gcc 11.3.0-19) 11.3.0
     Copyright (C) 2021 Free Software Foundation, Inc.
     This is free software; see the source for copying conditions.  There is NO
     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
     
     
     ***nvcc***
     /usr/local/cuda/bin/nvcc
     nvcc: NVIDIA (R) Cuda compiler driver
     Copyright (c) 2005-2023 NVIDIA Corporation
     Built on Fri_Nov__3_17:16:49_PDT_2023
     Cuda compilation tools, release 12.3, V12.3.103
     Build cuda_12.3.r12.3/compiler.33492891_0
     
     ***Python***
     /nvme/0/sebastianb/mambaforge/envs/.../bin/python
     Python 3.10.9
     
     ***Environment Variables***
     PATH                            : /usr/local/cuda/bin:/nvme/0/sebastianb/mambaforge/envs/.../bin:/nvme/0/sebastianb/mambaforge/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
     LD_LIBRARY_PATH                 :
     NUMBAPRO_NVVM                   :
     NUMBAPRO_LIBDEVICE              :
     CONDA_PREFIX                    : /nvme/0/sebastianb/mambaforge/envs/...
     PYTHON_PATH                     :
     
     ***conda packages***
     /nvme/0/sebastianb/mambaforge/condabin/conda
     # packages in environment at /nvme/0/sebastianb/mambaforge/envs/...:
     #
     # Name                    Version                   Build  Channel
     _libgcc_mutex             0.1                 conda_forge    conda-forge
     _openmp_mutex             4.5                  2_kmp_llvm    conda-forge
     accessible-pygments       0.0.3                    pypi_0    pypi
     adwaita-icon-theme        43                       unix_0    conda-forge
     alabaster                 0.7.13                   pypi_0    pypi
     alsa-lib                  1.2.8                h166bdaf_0    conda-forge
     arrow-cpp                 10.0.1          ha770c72_24_cpu    conda-forge
     asttokens                 2.2.1                    pypi_0    pypi
     at-spi2-atk               2.38.0               h0630a04_3    conda-forge
     at-spi2-core              2.40.3               h0630a04_0    conda-forge
     atk-1.0                   2.38.0               hd4edc92_1    conda-forge
     attr                      2.5.1                h166bdaf_1    conda-forge
     attrs                     22.2.0             pyh71513ae_0    conda-forge
     aws-c-auth                0.6.27               he072965_1    conda-forge
     aws-c-cal                 0.5.26               hf677bf3_1    conda-forge
     aws-c-common              0.8.19               hd590300_0    conda-forge
     aws-c-compression         0.2.16               hbad4bc6_7    conda-forge
     aws-c-event-stream        0.2.20               hb4b372c_7    conda-forge
     aws-c-http                0.7.7                h2632f9a_4    conda-forge
     aws-c-io                  0.13.21              h9fef7b8_5    conda-forge
     aws-c-mqtt                0.8.11               h2282364_1    conda-forge
     aws-c-s3                  0.3.0                hcb5a9b2_2    conda-forge
     aws-c-sdkutils            0.1.9                hbad4bc6_2    conda-forge
     aws-checksums             0.1.14               hbad4bc6_7    conda-forge
     aws-crt-cpp               0.20.1               he0fdcb3_3    conda-forge
     aws-sdk-cpp               1.10.57             hb0b1f3a_12    conda-forge
     babel                     2.12.1                   pypi_0    pypi
     backcall                  0.2.0                    pypi_0    pypi
     beautifulsoup4            4.11.2                   pypi_0    pypi
     binutils                  2.39                 hdd6e379_1    conda-forge
     binutils_impl_linux-64    2.39                 he00db2b_1    conda-forge
     binutils_linux-64         2.39                h5fc0e48_11    conda-forge
     bleach                    6.0.0                    pypi_0    pypi
     bokeh                     2.4.3              pyhd8ed1ab_3    conda-forge
     brotli                    1.0.9                h166bdaf_8    conda-forge
     brotli-bin                1.0.9                h166bdaf_8    conda-forge
     brotlipy                  0.7.0           py310h5764c6d_1005    conda-forge
     bzip2                     1.0.8                h7f98852_4    conda-forge
     c-ares                    1.25.0               hd590300_0    conda-forge
     c-compiler                1.5.2                h0b41bf4_0    conda-forge
     ca-certificates           2023.11.17           hbcca054_0    conda-forge
     cachetools                5.3.2              pyhd8ed1ab_0    conda-forge
     cairo                     1.16.0            ha61ee94_1014    conda-forge
     certifi                   2023.11.17         pyhd8ed1ab_0    conda-forge
     cffi                      1.15.1          py310h255011f_3    conda-forge
     cfgv                      3.3.1              pyhd8ed1ab_0    conda-forge
     charset-normalizer        2.1.1              pyhd8ed1ab_0    conda-forge
     clang                     15.0.7               ha770c72_1    conda-forge
     clang-15                  15.0.7          default_had23c3d_1    conda-forge
     clang-format              15.0.7          default_had23c3d_1    conda-forge
     clang-format-15           15.0.7          default_had23c3d_1    conda-forge
     clang-tools               15.0.7          default_had23c3d_1    conda-forge
     click                     8.1.7           unix_pyh707e725_0    conda-forge
     cloudpickle               3.0.0              pyhd8ed1ab_0    conda-forge
     cmake                     3.26.4               hcfe8598_0    conda-forge
     colorama                  0.4.6              pyhd8ed1ab_0    conda-forge
     contourpy                 1.0.7           py310hdf3cbec_0    conda-forge
     coverage                  7.2.1           py310h1fa729e_0    conda-forge
     cryptography              39.0.1          py310h34c0648_0    conda-forge
     cubinlinker               0.3.0           py310hfdf336d_0    rapidsai
     cuda-python               11.8.3          py310h70a93da_0    conda-forge
     cuda-version              11.7                 h67201e3_2    conda-forge
     cudatoolkit               11.7.0              hd8887f6_11    conda-forge
     cudf                      23.04.01        cuda_11_py310_230421_g7e070fce16_0    rapidsai
     cupy                      11.6.0          py310h9216885_0    conda-forge
     curl                      8.1.2                h409715c_0    conda-forge
     cutensor                  1.6.2.3              h12f7317_0    conda-forge
     cxx-compiler              1.5.2                hf52228f_0    conda-forge
     cycler                    0.11.0             pyhd8ed1ab_0    conda-forge
     cytoolz                   0.12.2          py310h2372a71_1    conda-forge
     dask                      2023.3.2           pyhd8ed1ab_0    conda-forge
     dask-core                 2023.3.2           pyhd8ed1ab_0    conda-forge
     dask-cudf                 23.04.01        cuda_11_py310_230421_g7e070fce16_0    rapidsai
     dbus                      1.13.6               h5008d03_3    conda-forge
     decorator                 5.1.1                    pypi_0    pypi
     deepmerge                 1.1.1              pyhd8ed1ab_0    conda-forge
     defusedxml                0.7.1                    pypi_0    pypi
     distlib                   0.3.6              pyhd8ed1ab_0    conda-forge
     distributed               2023.3.2.1         pyhd8ed1ab_0    conda-forge
     distro                    1.8.0              pyhd8ed1ab_0    conda-forge
     dlpack                    0.5                  h9c3ff4c_0    conda-forge
     docutils                  0.19                     pypi_0    pypi
     emacs                     28.2                 hcaca9e6_1    conda-forge
     epoxy                     1.5.10               h166bdaf_1    conda-forge
     exceptiongroup            1.1.0              pyhd8ed1ab_0    conda-forge
     executing                 1.2.0                    pypi_0    pypi
     expat                     2.5.0                h27087fc_0    conda-forge
     fastavro                  1.9.3           py310h2372a71_0    conda-forge
     fastjsonschema            2.16.3                   pypi_0    pypi
     fastrlock                 0.8.2           py310hc6cd4ac_2    conda-forge
     fftw                      3.3.10          nompi_hc118613_107    conda-forge
     filelock                  3.9.0              pyhd8ed1ab_0    conda-forge
     fmt                       9.1.0                h924138e_0    conda-forge
     font-ttf-dejavu-sans-mono 2.37                 hab24e00_0    conda-forge
     font-ttf-inconsolata      3.000                h77eed37_0    conda-forge
     font-ttf-source-code-pro  2.038                h77eed37_0    conda-forge
     font-ttf-ubuntu           0.83                 hab24e00_0    conda-forge
     fontconfig                2.14.2               h14ed4e7_0    conda-forge
     fonts-conda-ecosystem     1                             0    conda-forge
     fonts-conda-forge         1                             0    conda-forge
     fonttools                 4.39.3          py310h1fa729e_0    conda-forge
     freetype                  2.12.1               hca18f0e_1    conda-forge
     fribidi                   1.0.10               h36c2ea0_0    conda-forge
     fsspec                    2023.12.2          pyhca7485f_0    conda-forge
     gcc                       11.3.0              h02d0930_11    conda-forge
     gcc_impl_linux-64         11.3.0              hab1b70f_19    conda-forge
     gcc_linux-64              11.3.0              he6f903b_11    conda-forge
     gdk-pixbuf                2.42.10              h05c8ddd_0    conda-forge
     gettext                   0.21.1               h27087fc_0    conda-forge
     gflags                    2.2.2             he1b5a44_1004    conda-forge
     giflib                    5.2.1                h0b41bf4_3    conda-forge
     git                       2.39.2          pl5321h693f4a3_0    conda-forge
     glib                      2.74.1               h6239696_1    conda-forge
     glib-tools                2.74.1               h6239696_1    conda-forge
     glog                      0.6.0                h6f12383_0    conda-forge
     gmock                     1.10.0               h4bd325d_7    conda-forge
     gmp                       6.2.1                h58526e2_0    conda-forge
     gnutls                    3.7.8                hf3e180e_0    conda-forge
     graphite2                 1.3.13            h58526e2_1001    conda-forge
     gst-plugins-base          1.22.0               h4243ec0_2    conda-forge
     gstreamer                 1.22.0               h25f0c4b_2    conda-forge
     gstreamer-orc             0.4.33               h166bdaf_0    conda-forge
     gtest                     1.10.0               h4bd325d_7    conda-forge
     gtk3                      3.24.37              h1e7d460_0    conda-forge
     gxx                       11.3.0              h02d0930_11    conda-forge
     gxx_impl_linux-64         11.3.0              hab1b70f_19    conda-forge
     gxx_linux-64              11.3.0              hc203a17_11    conda-forge
     harfbuzz                  6.0.0                h8e241bc_0    conda-forge
     hicolor-icon-theme        0.17                 ha770c72_2    conda-forge
     icu                       70.1                 h27087fc_0    conda-forge
     identify                  2.5.33             pyhd8ed1ab_0    conda-forge
     idna                      3.4                pyhd8ed1ab_0    conda-forge
     imagesize                 1.4.1                    pypi_0    pypi
     importlib-metadata        6.0.0              pyha770c72_0    conda-forge
     importlib_metadata        6.0.0                hd8ed1ab_0    conda-forge
     iniconfig                 2.0.0              pyhd8ed1ab_0    conda-forge
     ipython                   8.11.0                   pypi_0    pypi
     jack                      1.9.22               h11f4161_0    conda-forge
     jedi                      0.18.2                   pypi_0    pypi
     jinja2                    3.1.2                    pypi_0    pypi
     jpeg                      9e                   h166bdaf_2    conda-forge
     jsonschema                4.17.3                   pypi_0    pypi
     jupyter-client            8.0.3                    pypi_0    pypi
     jupyter-core              5.2.0                    pypi_0    pypi
     jupyterlab-pygments       0.2.2                    pypi_0    pypi
     kernel-headers_linux-64   2.6.32              he073ed8_15    conda-forge
     keyutils                  1.6.1                h166bdaf_0    conda-forge
     kiwisolver                1.4.4           py310hbf28c38_1    conda-forge
     krb5                      1.20.1               h81ceb04_0    conda-forge
     lame                      3.100             h166bdaf_1003    conda-forge
     lcms2                     2.15                 hfd0df8a_0    conda-forge
     ld_impl_linux-64          2.39                 hcc3a1bd_1    conda-forge
     lerc                      4.0.0                h27087fc_0    conda-forge
     libabseil                 20230125.3      cxx17_h59595ed_0    conda-forge
     libarrow                  10.0.1          h3e0c18e_24_cpu    conda-forge
     libblas                   3.9.0           16_linux64_openblas    conda-forge
     libbrotlicommon           1.0.9                h166bdaf_8    conda-forge
     libbrotlidec              1.0.9                h166bdaf_8    conda-forge
     libbrotlienc              1.0.9                h166bdaf_8    conda-forge
     libcap                    2.67                 he9d0100_0    conda-forge
     libcblas                  3.9.0           16_linux64_openblas    conda-forge
     libclang                  15.0.7          default_had23c3d_1    conda-forge
     libclang-cpp15            15.0.7          default_had23c3d_1    conda-forge
     libclang13                15.0.7          default_h3e3d535_1    conda-forge
     libcrc32c                 1.1.2                h9c3ff4c_0    conda-forge
     libcudf                   23.04.01        cuda11_230421_g7e070fce16_0    rapidsai
     libcups                   2.3.3                h36d4200_3    conda-forge
     libcurl                   8.1.2                h409715c_0    conda-forge
     libdb                     6.2.32               h9c3ff4c_0    conda-forge
     libdeflate                1.17                 h0b41bf4_0    conda-forge
     libdrm                    2.4.114              h166bdaf_0    conda-forge
     libedit                   3.1.20191231         he28a2e2_2    conda-forge
     libev                     4.33                 h516909a_1    conda-forge
     libevent                  2.1.10               h28343ad_4    conda-forge
     libexpat                  2.5.0                hcb278e6_1    conda-forge
     libffi                    3.4.2                h7f98852_5    conda-forge
     libflac                   1.4.2                h27087fc_0    conda-forge
     libgcc-devel_linux-64     11.3.0              h210ce93_19    conda-forge
     libgcc-ng                 12.2.0              h65d4601_19    conda-forge
     libgcrypt                 1.10.1               h166bdaf_0    conda-forge
     libgfortran-ng            12.2.0              h69a702a_19    conda-forge
     libgfortran5              12.2.0              h337968e_19    conda-forge
     libglib                   2.74.1               h606061b_1    conda-forge
     libgomp                   12.2.0              h65d4601_19    conda-forge
     libgoogle-cloud           2.10.1               hac9eb74_1    conda-forge
     libgpg-error              1.46                 h620e276_0    conda-forge
     libgrpc                   1.54.3               hb20ce57_0    conda-forge
     libiconv                  1.17                 h166bdaf_0    conda-forge
     libidn2                   2.3.4                h166bdaf_0    conda-forge
     libjpeg-turbo             2.1.4                h166bdaf_0    conda-forge
     liblapack                 3.9.0           16_linux64_openblas    conda-forge
     libllvm11                 11.1.0               he0ac6c6_5    conda-forge
     libllvm15                 15.0.7               hadd5161_0    conda-forge
     libnghttp2                1.58.0               h47da74e_1    conda-forge
     libnsl                    2.0.0                h7f98852_0    conda-forge
     libnuma                   2.0.16               h0b41bf4_1    conda-forge
     libogg                    1.3.4                h7f98852_1    conda-forge
     libopenblas               0.3.21          openmp_h74cd887_3    conda-forge
     libopus                   1.3.1                h7f98852_1    conda-forge
     libpciaccess              0.17                 h166bdaf_0    conda-forge
     libpng                    1.6.39               h753d276_0    conda-forge
     libpq                     15.2                 hb675445_0    conda-forge
     libprotobuf               3.21.12              h3eb15da_0    conda-forge
     librmm                    23.04.01        cuda11_230421_geab50f46_0    rapidsai
     librsvg                   2.54.4               h7abd40a_0    conda-forge
     libsanitizer              11.3.0              h239ccf8_19    conda-forge
     libsndfile                1.2.0                hb75c966_0    conda-forge
     libsqlite                 3.40.0               h753d276_0    conda-forge
     libssh2                   1.10.0               hf14f497_3    conda-forge
     libstdcxx-devel_linux-64  11.3.0              h210ce93_19    conda-forge
     libstdcxx-ng              12.2.0              h46fd767_19    conda-forge
     libsystemd0               253                  h8c4010b_1    conda-forge
     libtasn1                  4.19.0               h166bdaf_0    conda-forge
     libthrift                 0.18.1               h5e4af38_0    conda-forge
     libtiff                   4.5.0                h6adf6a1_2    conda-forge
     libtool                   2.4.7                h27087fc_0    conda-forge
     libudev1                  253                  h0b41bf4_1    conda-forge
     libunistring              0.9.10               h7f98852_0    conda-forge
     libutf8proc               2.8.0                h166bdaf_0    conda-forge
     libuuid                   2.32.1            h7f98852_1000    conda-forge
     libuv                     1.44.2               h166bdaf_0    conda-forge
     libvorbis                 1.3.7                h9c3ff4c_0    conda-forge
     libwebp-base              1.3.0                h0b41bf4_0    conda-forge
     libxcb                    1.13              h7f98852_1004    conda-forge
     libxkbcommon              1.5.0                h79f4944_1    conda-forge
     libxml2                   2.10.3               h7463322_0    conda-forge
     libzlib                   1.2.13               h166bdaf_4    conda-forge
     llvm-openmp               15.0.7               h0cdce71_0    conda-forge
     llvmlite                  0.39.1          py310h58363a5_1    conda-forge
     locket                    1.0.0              pyhd8ed1ab_0    conda-forge
     lz4                       4.3.3           py310h350c4a5_0    conda-forge
     lz4-c                     1.9.4                hcb278e6_0    conda-forge
     make                      4.3                  hd18ef5c_1    conda-forge
     markdown                  3.3.7                    pypi_0    pypi
     markdown-it-py            2.2.0                    pypi_0    pypi
     markupsafe                2.1.2                    pypi_0    pypi
     matplotlib                3.7.1           py310hff52083_0    conda-forge
     matplotlib-base           3.7.1           py310he60537e_0    conda-forge
     matplotlib-inline         0.1.6                    pypi_0    pypi
     mdit-py-plugins           0.3.4                    pypi_0    pypi
     mdurl                     0.1.2                    pypi_0    pypi
     mistune                   2.0.5                    pypi_0    pypi
     mock                      5.0.1              pyhd8ed1ab_0    conda-forge
     mpg123                    1.31.3               hcb278e6_0    conda-forge
     mpi                       1.0                     openmpi    conda-forge
     mpmath                    1.3.0              pyhd8ed1ab_0    conda-forge
     msgpack-python            1.0.7           py310hd41b1e2_0    conda-forge
     munkres                   1.1.4              pyh9f0ad1d_0    conda-forge
     mypy                      1.0.1           py310h1fa729e_0    conda-forge
     mypy_extensions           1.0.0              pyha770c72_0    conda-forge
     mysql-common              8.0.32               ha901b37_1    conda-forge
     mysql-libs                8.0.32               hd7da12d_1    conda-forge
     myst-parser               0.19.0                   pypi_0    pypi
     nbclient                  0.7.2                    pypi_0    pypi
     nbconvert                 7.2.9                    pypi_0    pypi
     nbformat                  5.7.3                    pypi_0    pypi
     nbsphinx                  0.8.12                   pypi_0    pypi
     nccl                      2.14.3.1             h0800d71_0    conda-forge
     ncurses                   6.4                  h59595ed_2    conda-forge
     nettle                    3.8.1                hc379101_1    conda-forge
     ninja                     1.11.1               h924138e_0    conda-forge
     nodeenv                   1.7.0              pyhd8ed1ab_0    conda-forge
     nspr                      4.35                 h27087fc_0    conda-forge
     nss                       3.89                 he45b914_0    conda-forge
     numba                     0.56.4          py310h0e39c9b_1    conda-forge
     numpy                     1.23.5          py310h53a5b5f_0    conda-forge
     nvtop                     3.0.2                hbcb9423_0    conda-forge
     nvtx                      0.2.8           py310h2372a71_1    conda-forge
     openblas                  0.3.21          openmp_h53a8fd6_3    conda-forge
     openjpeg                  2.5.0                hfec8fc6_2    conda-forge
     openmpi                   4.1.5              h9a2ec32_100    conda-forge
     openssl                   3.2.0                hd590300_1    conda-forge
     opt_einsum                3.3.0              pyhd8ed1ab_1    conda-forge
     orc                       1.8.3                h2f23424_1    conda-forge
     p11-kit                   0.24.1               hc5aa10d_0    conda-forge
     packaging                 23.0               pyhd8ed1ab_0    conda-forge
     pandas                    1.5.3           py310h9b08913_1    conda-forge
     pandoc                    2.19.2               h32600fe_1    conda-forge
     pandocfilters             1.5.0                    pypi_0    pypi
     pango                     1.50.14              hd33c08f_0    conda-forge
     parquet-cpp               1.5.1                         2    conda-forge
     parso                     0.8.3                    pypi_0    pypi
     partd                     1.4.1              pyhd8ed1ab_0    conda-forge
     pcre2                     10.40                hc3806b6_0    conda-forge
     perl                      5.32.1          2_h7f98852_perl5    conda-forge
     pexpect                   4.8.0                    pypi_0    pypi
     pickleshare               0.7.5                    pypi_0    pypi
     pillow                    9.4.0           py310h023d228_1    conda-forge
     pip                       23.0.1             pyhd8ed1ab_0    conda-forge
     pixman                    0.40.0               h36c2ea0_0    conda-forge
     platformdirs              3.0.0              pyhd8ed1ab_0    conda-forge
     pluggy                    1.0.0              pyhd8ed1ab_5    conda-forge
     ply                       3.11                       py_1    conda-forge
     pooch                     1.7.0              pyhd8ed1ab_0    conda-forge
     pre-commit                3.6.0              pyha770c72_0    conda-forge
     prompt-toolkit            3.0.38                   pypi_0    pypi
     protobuf                  4.21.12         py310heca2aa9_0    conda-forge
     psutil                    5.9.4           py310h5764c6d_0    conda-forge
     pthread-stubs             0.4               h36c2ea0_1001    conda-forge
     ptxcompiler               0.8.1           py310h70a93da_2    conda-forge
     ptyprocess                0.7.0                    pypi_0    pypi
     pulseaudio                16.1                 hcb278e6_3    conda-forge
     pulseaudio-client         16.1                 h5195f5e_3    conda-forge
     pulseaudio-daemon         16.1                 ha8d29e2_3    conda-forge
     pure-eval                 0.2.2                    pypi_0    pypi
     pyarrow                   10.0.1          py310he6bfd7f_24_cpu    conda-forge
     pycparser                 2.21               pyhd8ed1ab_0    conda-forge
     pydata-sphinx-theme       0.13.0                   pypi_0    pypi
     pygments                  2.14.0                   pypi_0    pypi
     pynvml                    11.5.0             pyhd8ed1ab_0    conda-forge
     pyopenssl                 23.0.0             pyhd8ed1ab_0    conda-forge
     pyparsing                 3.0.9              pyhd8ed1ab_0    conda-forge
     pyqt                      5.15.7          py310hab646b1_3    conda-forge
     pyqt5-sip                 12.11.0         py310heca2aa9_3    conda-forge
     pyrsistent                0.19.3                   pypi_0    pypi
     pysocks                   1.7.1              pyha2e5f31_6    conda-forge
     pytest                    7.2.1              pyhd8ed1ab_0    conda-forge
     pytest-cov                4.0.0              pyhd8ed1ab_0    conda-forge
     pytest-lazy-fixture       0.6.3                      py_0    conda-forge
     pytest-mock               3.10.0             pyhd8ed1ab_0    conda-forge
     python                    3.10.9          he550d4f_0_cpython    conda-forge
     python-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge
     python_abi                3.10                    3_cp310    conda-forge
     pytz                      2023.3.post1       pyhd8ed1ab_0    conda-forge
     pyyaml                    6.0             py310h5764c6d_5    conda-forge
     pyzmq                     25.0.0                   pypi_0    pypi
     qt-main                   5.15.8               h5d23da1_6    conda-forge
     rapids-dependency-file-generator 1.7.1                    pypi_0    pypi
     rdma-core                 28.9                 h59595ed_1    conda-forge
     re2                       2023.03.02           h8c504da_0    conda-forge
     readline                  8.1.2                h0f457ee_0    conda-forge
     requests                  2.28.2             pyhd8ed1ab_0    conda-forge
     rhash                     1.4.3                h166bdaf_0    conda-forge
     rmm                       23.04.01        cuda11_py310_230421_geab50f46_0    rapidsai
     rust                      1.67.1               hefb9f0a_0    conda-forge
     rust-std-x86_64-unknown-linux-gnu 1.67.1               hc1431ca_0    conda-forge
     s2n                       1.3.44               h06160fa_0    conda-forge
     scikit-build              0.16.7             pyh56297ac_0    conda-forge
     scipy                     1.10.1          py310h8deb116_0    conda-forge
     setuptools                67.4.0             pyhd8ed1ab_0    conda-forge
     sip                       6.7.9           py310hc6cd4ac_0    conda-forge
     six                       1.16.0             pyh6c4a22f_0    conda-forge
     snappy                    1.1.10               h9fff704_0    conda-forge
     snowballstemmer           2.2.0                    pypi_0    pypi
     sortedcontainers          2.4.0              pyhd8ed1ab_0    conda-forge
     soupsieve                 2.4                      pypi_0    pypi
     spdlog                    1.11.0               h9b3ece8_1    conda-forge
     sphinx                    6.1.3                    pypi_0    pypi
     sphinx-copybutton         0.5.1                    pypi_0    pypi
     sphinxcontrib-applehelp   1.0.4                    pypi_0    pypi
     sphinxcontrib-devhelp     1.0.2                    pypi_0    pypi
     sphinxcontrib-htmlhelp    2.0.1                    pypi_0    pypi
     sphinxcontrib-jsmath      1.0.1                    pypi_0    pypi
     sphinxcontrib-qthelp      1.0.3                    pypi_0    pypi
     sphinxcontrib-serializinghtml 1.1.5                    pypi_0    pypi
     stack-data                0.6.2                    pypi_0    pypi
     sysroot_linux-64          2.12                he073ed8_15    conda-forge
     tblib                     3.0.0              pyhd8ed1ab_0    conda-forge
     tifffile                  2023.2.28                pypi_0    pypi
     tinycss2                  1.2.1                    pypi_0    pypi
     tk                        8.6.12               h27826a3_0    conda-forge
     toml                      0.10.2             pyhd8ed1ab_0    conda-forge
     tomli                     2.0.1              pyhd8ed1ab_0    conda-forge
     tomlkit                   0.12.3                   pypi_0    pypi
     toolz                     0.12.0             pyhd8ed1ab_0    conda-forge
     tornado                   6.2                      pypi_0    pypi
     traitlets                 5.9.0                    pypi_0    pypi
     types-docutils            0.19.1.6           pyhd8ed1ab_0    conda-forge
     typing-extensions         4.4.0                hd8ed1ab_0    conda-forge
     typing_extensions         4.4.0              pyha770c72_0    conda-forge
     tzdata                    2022g                h191b570_0    conda-forge
     ucx                       1.14.1               h64cca9d_5    conda-forge
     ukkonen                   1.0.1           py310hbf28c38_3    conda-forge
     unicodedata2              15.0.0          py310h5764c6d_0    conda-forge
     urllib3                   1.26.14            pyhd8ed1ab_0    conda-forge
     virtualenv                20.20.0            pyhd8ed1ab_0    conda-forge
     wcwidth                   0.2.6                    pypi_0    pypi
     webencodings              0.5.1                    pypi_0    pypi
     wheel                     0.38.4             pyhd8ed1ab_0    conda-forge
     xcb-util                  0.4.0                h516909a_0    conda-forge
     xcb-util-image            0.4.0                h166bdaf_0    conda-forge
     xcb-util-keysyms          0.4.0                h516909a_0    conda-forge
     xcb-util-renderutil       0.3.9                h166bdaf_0    conda-forge
     xcb-util-wm               0.4.1                h516909a_0    conda-forge
     xkeyboard-config          2.38                 h0b41bf4_0    conda-forge
     xorg-compositeproto       0.4.2             h7f98852_1001    conda-forge
     xorg-damageproto          1.2.1             h7f98852_1002    conda-forge
     xorg-fixesproto           5.0               h7f98852_1002    conda-forge
     xorg-inputproto           2.3.2             h7f98852_1002    conda-forge
     xorg-kbproto              1.0.7             h7f98852_1002    conda-forge
     xorg-libice               1.0.10               h7f98852_0    conda-forge
     xorg-libsm                1.2.3             hd9c2040_1000    conda-forge
     xorg-libx11               1.6.12               h36c2ea0_0    conda-forge
     xorg-libxau               1.0.9                h7f98852_0    conda-forge
     xorg-libxaw               1.0.14               h7f98852_0    conda-forge
     xorg-libxcomposite        0.4.5                h7f98852_0    conda-forge
     xorg-libxcursor           1.2.0                h516909a_0    conda-forge
     xorg-libxdamage           1.1.5                h7f98852_0    conda-forge
     xorg-libxdmcp             1.1.3                h7f98852_0    conda-forge
     xorg-libxext              1.3.4                h516909a_0    conda-forge
     xorg-libxfixes            5.0.3             h516909a_1004    conda-forge
     xorg-libxft               2.3.4                hc534e41_1    conda-forge
     xorg-libxi                1.7.10               h516909a_0    conda-forge
     xorg-libxinerama          1.1.5                h27087fc_0    conda-forge
     xorg-libxmu               1.1.3                h516909a_0    conda-forge
     xorg-libxpm               3.5.13               h516909a_0    conda-forge
     xorg-libxrandr            1.5.2                h516909a_1    conda-forge
     xorg-libxrender           0.9.10            h516909a_1002    conda-forge
     xorg-libxt                1.1.5             h516909a_1003    conda-forge
     xorg-libxtst              1.2.3             h516909a_1002    conda-forge
     xorg-randrproto           1.5.0             h7f98852_1001    conda-forge
     xorg-recordproto          1.14.2            h7f98852_1002    conda-forge
     xorg-renderproto          0.11.1            h7f98852_1002    conda-forge
     xorg-util-macros          1.19.3               h7f98852_0    conda-forge
     xorg-xextproto            7.3.0             h0b41bf4_1003    conda-forge
     xorg-xineramaproto        1.2.1             h7f98852_1001    conda-forge
     xorg-xproto               7.0.31            h7f98852_1007    conda-forge
     xz                        5.2.6                h166bdaf_0    conda-forge
     yaml                      0.2.5                h7f98852_2    conda-forge
     zict                      3.0.0              pyhd8ed1ab_0    conda-forge
     zipp                      3.15.0             pyhd8ed1ab_0    conda-forge
     zlib                      1.2.13               h166bdaf_4    conda-forge
     zstd                      1.5.2                h3eb15da_6    conda-forge
     
</pre></details>

(I have reproduced it in a different docker environment as well)

**Additional context**

This seems relatively niche, although not nice to lose a row.  I stumbled across it by accident writing unrelated tests.",2024-01-25T11:56:26Z,0,0,Sebastian Berg,Nvidia,True
741,[FEA] Consolidate python code-style checking and formatting to just use `ruff`,"**Is your feature request related to a problem? Please describe.**

We have already migrated to using [ruff](https://docs.astral.sh/ruff/) to replace flake8 and pyflakes. A previous barrier to using ruff's `isort` lint was lack of support for custom sections, [that is now supported](https://docs.astral.sh/ruff/settings/#isort-sections). Similarly, `ruff format` (which produces effectively black-compatible formatting) is now in ""stable"" beta and very usable.

We should consider migrating the separate `isort` and `black` configs to use `ruff check --fix` and `ruff format` respectively.

The advantage here is that ruff is orders of magnitude faster than both isort and black. For those of us that use format-on-save this is a significant quality of life improvement (formatting a large python file when editing cudf can easily take a few seconds with black).

We would also reduce our tool configuration options.

We might also at the same time consider increasing the default line length from the current (somewhat miserly) 79 characters.",2024-01-25T12:30:01Z,0,0,Lawrence Mitchell,,False
742,[FEA] Properly interface with `dask` `object` -> `arrow[string]` conversions,"**Is your feature request related to a problem? Please describe.**
`dask` has a behavioral change when `pandas-2.x` & `pyarrow` are installed. Some APIs try switching `object` type columns to `arrow[string]` for efficiencies arrow strings provide over object types in pandas.  This causes a lot of pytest failures in `dask_cudf`, that is currently being prevented by configured a `dask.config`: `dataframe.convert-string` to `False`.  However, this only happens properly for `dask.DataFrame`'s and not for `dask_cudf.DataFrame`'s. 

**Describe the solution you'd like**
We need to update `cudf` & `dask` upstream to get this support for `dask_cudf.DataFrame` as well when the device object is converted to host objects.

",2024-01-29T13:04:21Z,1,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
743,[FEA] Produce and Consume ArrowDeviceArray struct from cudf::table / cudf::column,"**Is your feature request related to a problem? Please describe.**
I would like to generate Arrow IPC payloads from a `cudf::table` without copying the data off of the GPU device. Currently the `to_arrow` and `from_arrow` functions explicitly perform copies to and from the GPU device. There is not currently any efficient way to generate Arrow IPC payloads from libcudf without copying all of the data off of the device.

**Describe the solution you'd like**
In addition to the existing `to_arrow` and `from_arrow` functions, we could have a `to_arrow_device_arr` function that populates an `ArrowDeviceArray` struct from a `cudf::table` or `cudf::column`. We'd also create a `from_arrow_device_arr` function that could construct a `cudf::table` / `cudf::column` from an `ArrowDeviceArray` that describes Arrow data which is already on the device. Once you have the `ArrowDeviceArray` struct, the Arrow C++ library itself can be used to generate the IPC payloads without needing to copy the data off the device. This would also increase the interoperability options that libcudf has, as anything which produces or consumes `ArrowDeviceArray` structs could hand data off to libcudf and vice versa.

**Describe alternatives you've considered**
An alternative would be to implement Arrow IPC creating inside of the libcudf library, but I saw that this was explicitly removed from libcudf due to the requirement of linking against `libarrow_cuda.so`. (https://github.com/rapidsai/cudf/issues/10994). Implementing conversions to and from `ArrowDeviceArray` wouldn't require linking against `libarrow_cuda.so` at all and would provide an easy way to allow any consumers to create Arrow IPC payloads, or whatever else they want to do with the resulting Arrow data. Such as leveraging CUDA IPC with the data.

**Additional context**
When designing the `ArrowDeviceArray` struct, I created https://github.com/zeroshade/arrow-non-cpu as a POC which used Python numba to generate and operate on some GPU data before handing it off to libcudf, and then getting it back without copying off the device. Using `ArrowDeviceArray` as the way it handed the data off.

More recently I've been working on creating a protocol for sending Arrow IPC data that is located on GPUs across high-performance transports like UCX. To this end, I created a POC using libcudf to pass the data. As a result I have a partial implementation of the `to_arrow_device_arr` which can be found [here](https://github.com/zeroshade/cudf-flight-ucx/blob/main/to_arrow.cc). There's likely better ways than what I'm doing in there, but at least for my POC it was working.

The contribution guidelines say I should file this issue first for discussion rather than just submitting a PR, so that's where I'm at. I plan on trying to create a full implementation that I can contribute but wanted to have this discussion and get feedback here first. 

Thanks for hearing me out everyone!
",2024-01-29T21:39:04Z,0,0,Matt Topol,@voltrondata,False
744,[BUG] dask_cudf pivot_table function is broken: TypeError: StringIndex object is not iterable.,"**Describe the bug**
Pivot_table fails on a dask_cudf dataframe due to an unimplemented Index iteration function:

**Steps/Code to reproduce bug**
```python
ddf = dask_cudf.from_cudf(cudf.DataFrame(
    data={
        ""A"": [""foo"", ""bar"", ""bar""],
        ""B"": [""one"", ""two"", ""one""],
        ""C"": [1, 2, 3]
    }
), npartitions=1)
ddf = ddf.categorize(""B"")
ddf.pivot_table(index=""A"", columns=""B"", values=""C"")
```

Error:
```python
TypeError                                 Traceback (most recent call last)
Cell In[3], line 9
      1 ddf = dask_cudf.from_cudf(cudf.DataFrame(
      2     data={
      3         ""A"": [""foo"", ""bar"", ""bar""],
   (...)
      6     }
      7 ), npartitions=1)
      8 ddf = ddf.categorize(""B"")
----> 9 ddf.pivot_table(index=""A"", columns=""B"", values=""C"")

File lib/python3.10/site-packages/dask/dataframe/core.py:6373, in DataFrame.pivot_table(self, index, columns, values, aggfunc)
   6352 """"""
   6353 Create a spreadsheet-style pivot table as a DataFrame. Target ``columns``
   6354 must have category dtype to infer result's ``columns``.
   (...)
   6369 table : DataFrame
   6370 """"""
   6371 from dask.dataframe.reshape import pivot_table
-> 6373 return pivot_table(
   6374     self, index=index, columns=columns, values=values, aggfunc=aggfunc
   6375 )

File lib/python3.10/site-packages/dask/dataframe/reshape.py:233, in pivot_table(df, index, columns, values, aggfunc)
    226     raise ValueError(
    227         ""aggfunc must be either "" + "", "".join(f""'{x}'"" for x in available_aggfuncs)
    228     )
    230 # _emulate can't work for empty data
    231 # the result must have CategoricalIndex columns
--> 233 columns_contents = pd.CategoricalIndex(df[columns].cat.categories, name=columns)
    234 if is_scalar(values):
    235     new_columns = columns_contents

File lib/python3.10/site-packages/pandas/core/indexes/category.py:234, in CategoricalIndex.__new__(cls, data, categories, ordered, dtype, copy, name)
    231 if is_scalar(data):
    232     raise cls._scalar_data_error(data)
--> 234 data = Categorical(
    235     data, categories=categories, ordered=ordered, dtype=dtype, copy=copy
    236 )
    238 return cls._simple_new(data, name=name)

File lib/python3.10/site-packages/pandas/core/arrays/categorical.py:410, in Categorical.__init__(self, values, categories, ordered, dtype, fastpath, copy)
    408         dtype = CategoricalDtype(values.categories, dtype.ordered)
    409 elif not isinstance(values, (ABCIndex, ABCSeries, ExtensionArray)):
--> 410     values = com.convert_to_list_like(values)
    411     if isinstance(values, list) and len(values) == 0:
    412         # By convention, empty lists result in object dtype:
    413         values = np.array([], dtype=object)

File lib/python3.10/site-packages/pandas/core/common.py:541, in convert_to_list_like(values)
    539     return values
    540 elif isinstance(values, abc.Iterable) and not isinstance(values, str):
--> 541     return list(values)
    543 return [values]

File lib/python3.10/site-packages/cudf/utils/utils.py:242, in NotIterable.__iter__(self)
    235 def __iter__(self):
    236     """"""
    237     Iteration is unsupported.
    238 
    239     See :ref:`iteration <pandas-comparison/iteration>` for more
    240     information.
    241     """"""
--> 242     raise TypeError(
    243         f""{self.__class__.__name__} object is not iterable. ""
    244         f""Consider using `.to_arrow()`, `.to_pandas()` or `.values_host` ""
    245         f""if you wish to iterate over the values.""
    246     )

TypeError: StringIndex object is not iterable. Consider using `.to_arrow()`, `.to_pandas()` or `.values_host` if you wish to iterate over the values.
```

**Expected behavior**
Pivot_table succeeds as documented.

**Environment overview (please complete the following information)**
Installed cuDF using pip, using the stable release:
```bash
pip install \
    --extra-index-url=https://pypi.nvidia.com \
    cudf-cu12==23.12.* dask-cudf-cu12==23.12.* cuml-cu12==23.12.* \
    cugraph-cu12==23.12.* cuspatial-cu12==23.12.* cuproj-cu12==23.12.* \
    cuxfilter-cu12==23.12.* cucim-cu12==23.12.* pylibraft-cu12==23.12.* \
    raft-dask-cu12==23.12.*
```

**Environment details**
```
<details><summary>Click here to see environment details</summary><pre>
     
     **git***
fatal: your current branch 'master' does not have any commits yet
     **git submodules***
     
     ***OS Information***
     NAME=""Red Hat Enterprise Linux""
     VERSION=""8.8 (Ootpa)""
     ID=""rhel""
     ID_LIKE=""fedora""
     VERSION_ID=""8.8""
     PLATFORM_ID=""platform:el8""
     PRETTY_NAME=""Red Hat Enterprise Linux 8.8 (Ootpa)""
     ANSI_COLOR=""0;31""
     CPE_NAME=""cpe:/o:redhat:enterprise_linux:8::baseos""
     HOME_URL=""https://www.redhat.com/""
     DOCUMENTATION_URL=""https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8""
     BUG_REPORT_URL=""https://bugzilla.redhat.com/""
     
     REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 8""
     REDHAT_BUGZILLA_PRODUCT_VERSION=8.8
     REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux""
     REDHAT_SUPPORT_PRODUCT_VERSION=""8.8""
     Red Hat Enterprise Linux release 8.8 (Ootpa)
     Red Hat Enterprise Linux release 8.8 (Ootpa)
     Linux c1000a-s23.ufhpc 4.18.0-477.27.1.el8_8.x86_64 #1 SMP Thu Aug 31 10:29:22 EDT 2023 x86_64 x86_64 x86_64 GNU/Linux
     
     ***GPU Information***
     Tue Jan 30 11:09:21 2024
     +---------------------------------------------------------------------------------------+
     | NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |
     |-----------------------------------------+----------------------+----------------------+
     | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
     | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
     |                                         |                      |               MIG M. |
     |=========================================+======================+======================|
     |   0  NVIDIA A100-SXM4-80GB          On  | 00000000:07:00.0 Off |                    0 |
     | N/A   25C    P0              56W / 400W |      4MiB / 81920MiB |      0%      Default |
     |                                         |                      |             Disabled |
     +-----------------------------------------+----------------------+----------------------+
     |   1  NVIDIA A100-SXM4-80GB          On  | 00000000:0F:00.0 Off |                    0 |
     | N/A   26C    P0              57W / 400W |      4MiB / 81920MiB |      0%      Default |
     |                                         |                      |             Disabled |
     +-----------------------------------------+----------------------+----------------------+
     |   2  NVIDIA A100-SXM4-80GB          On  | 00000000:47:00.0 Off |                    0 |
     | N/A   24C    P0              54W / 400W |      4MiB / 81920MiB |      0%      Default |
     |                                         |                      |             Disabled |
     +-----------------------------------------+----------------------+----------------------+
     |   3  NVIDIA A100-SXM4-80GB          On  | 00000000:4E:00.0 Off |                    0 |
     | N/A   24C    P0              56W / 400W |      4MiB / 81920MiB |      0%      Default |
     |                                         |                      |             Disabled |
     +-----------------------------------------+----------------------+----------------------+
     |   4  NVIDIA A100-SXM4-80GB          On  | 00000000:87:00.0 Off |                    0 |
     | N/A   29C    P0              67W / 400W |    583MiB / 81920MiB |     40%      Default |
     |                                         |                      |             Disabled |
     +-----------------------------------------+----------------------+----------------------+
     |   5  NVIDIA A100-SXM4-80GB          On  | 00000000:90:00.0 Off |                    0 |
     | N/A   45C    P0             177W / 400W |    775MiB / 81920MiB |     94%      Default |
     |                                         |                      |             Disabled |
     +-----------------------------------------+----------------------+----------------------+
     |   6  NVIDIA A100-SXM4-80GB          On  | 00000000:B7:00.0 Off |                    0 |
     | N/A   60C    P0             338W / 400W |  76523MiB / 81920MiB |    100%      Default |
     |                                         |                      |             Disabled |
     +-----------------------------------------+----------------------+----------------------+
     |   7  NVIDIA A100-SXM4-80GB          On  | 00000000:BD:00.0 Off |                    0 |
     | N/A   28C    P0              54W / 400W |      4MiB / 81920MiB |      0%      Default |
     |                                         |                      |             Disabled |
     +-----------------------------------------+----------------------+----------------------+
     
     +---------------------------------------------------------------------------------------+
     | Processes:                                                                            |
     |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
     |        ID   ID                                                             Usage      |
     |=======================================================================================|
     |    4   N/A  N/A   2669759      C   python3                                     570MiB |
     |    5   N/A  N/A   1903237      C   pmemd.cuda_SPFP                             762MiB |
     |    6   N/A  N/A   1446394      C   python                                    76510MiB |
     +---------------------------------------------------------------------------------------+
     
     ***CPU***
     Architecture:        x86_64
     CPU op-mode(s):      32-bit, 64-bit
     Byte Order:          Little Endian
     CPU(s):              128
     On-line CPU(s) list: 0-127
     Thread(s) per core:  1
     Core(s) per socket:  64
     Socket(s):           2
     NUMA node(s):        8
     Vendor ID:           AuthenticAMD
     CPU family:          23
     Model:               49
     Model name:          AMD EPYC 7742 64-Core Processor
     Stepping:            0
     CPU MHz:             3386.055
     CPU max MHz:         2250.0000
     CPU min MHz:         1500.0000
     BogoMIPS:            4491.84
     Virtualization:      AMD-V
     L1d cache:           32K
     L1i cache:           32K
     L2 cache:            512K
     L3 cache:            16384K
     NUMA node0 CPU(s):   0-15
     NUMA node1 CPU(s):   16-31
     NUMA node2 CPU(s):   32-47
     NUMA node3 CPU(s):   48-63
     NUMA node4 CPU(s):   64-79
     NUMA node5 CPU(s):   80-95
     NUMA node6 CPU(s):   96-111
     NUMA node7 CPU(s):   112-127
     Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es
     
     ***CMake***
     /apps/jupyter/6.5.4/bin/cmake
./print_env.sh: /apps/jupyter/6.5.4/bin/cmake: /apps/jupyter/6.5.4/bin/python3.11: bad interpreter: No such file or directory
     
     ***g++***
     /usr/bin/g++
     g++ (GCC) 8.5.0 20210514 (Red Hat 8.5.0-18)
     Copyright (C) 2018 Free Software Foundation, Inc.
     This is free software; see the source for copying conditions.  There is NO
     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
     
     
     ***nvcc***
     /apps/compilers/cuda/12.2.2/bin/nvcc
     nvcc: NVIDIA (R) Cuda compiler driver
     Copyright (c) 2005-2023 NVIDIA Corporation
     Built on Tue_Aug_15_22:02:13_PDT_2023
     Cuda compilation tools, release 12.2, V12.2.140
     Build cuda_12.2.r12.2/compiler.33191640_0
     
     ***Python***
     /blue/ptighe-rapidsai/pvnick/rapids-test/rapids-test/bin/python
     Python 3.10.12
     
     ***Environment Variables***
     PATH                            : /apps/compilers/cuda/12.2.2/bin:/blue/ptighe-rapidsai/pvnick/rapids-test/rapids-test/bin:/opt/slurm/bin:/usr/local/cuda/bin:/opt/bin:/apps/jupyter/6.5.4/bin:/apps/ufrc/ufhpc/bin:/apps/git/2.30.1/bin:/home/pvnick/.local/bin:/home/pvnick/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/puppetlabs/bin:/bin
     LD_LIBRARY_PATH                 : /apps/compilers/cuda/12.2.2/lib64:/opt/slurm/lib64::
     NUMBAPRO_NVVM                   :
     NUMBAPRO_LIBDEVICE              :
     CONDA_PREFIX                    :
     PYTHON_PATH                     :
     
     conda not found
     ***pip packages***
     /blue/ptighe-rapidsai/pvnick/rapids-test/rapids-test/bin/pip
     Package                   Version
     ------------------------- ---------------
     aiohttp                   3.9.3
     aiosignal                 1.3.1
     anyio                     4.2.0
     argon2-cffi               23.1.0
     argon2-cffi-bindings      21.2.0
     arrow                     1.3.0
     asttokens                 2.4.1
     async-lru                 2.0.4
     async-timeout             4.0.3
     attrs                     23.2.0
     Babel                     2.14.0
     beautifulsoup4            4.12.3
     bleach                    6.1.0
     bokeh                     3.3.4
     cachetools                5.3.2
     certifi                   2023.11.17
     cffi                      1.16.0
     charset-normalizer        3.3.2
     click                     8.1.7
     click-plugins             1.1.1
     cligj                     0.7.2
     cloudpickle               3.0.0
     colorcet                  3.0.1
     comm                      0.2.1
     contourpy                 1.2.0
     cucim-cu12                23.12.1
     cuda-python               12.3.0
     cudf-cu12                 23.12.1
     cugraph-cu12              23.12.0
     cuml-cu12                 23.12.0
     cuproj-cu12               23.12.1
     cupy-cuda12x              13.0.0
     cuspatial-cu12            23.12.1
     cuxfilter-cu12            23.12.0
     dask                      2023.11.0
     dask-cuda                 23.12.0
     dask-cudf-cu12            23.12.0
     datashader                0.16.0
     debugpy                   1.8.0
     decorator                 5.1.1
     defusedxml                0.7.1
     distributed               2023.11.0
     exceptiongroup            1.2.0
     executing                 2.0.1
     fastjsonschema            2.19.1
     fastrlock                 0.8.2
     fiona                     1.9.5
     fqdn                      1.5.1
     frozenlist                1.4.1
     fsspec                    2023.12.2
     geopandas                 0.14.2
     holoviews                 1.18.1
     idna                      3.6
     imageio                   2.33.1
     importlib-metadata        7.0.1
     ipykernel                 6.29.0
     ipython                   8.20.0
     ipywidgets                8.1.1
     isoduration               20.11.0
     jedi                      0.19.1
     Jinja2                    3.1.3
     joblib                    1.3.2
     json5                     0.9.14
     jsonpointer               2.4
     jsonschema                4.21.1
     jsonschema-specifications 2023.12.1
     jupyter                   1.0.0
     jupyter_client            8.6.0
     jupyter-console           6.6.3
     jupyter_core              5.7.1
     jupyter-events            0.9.0
     jupyter-lsp               2.2.2
     jupyter_server            2.12.5
     jupyter_server_proxy      4.1.0
     jupyter_server_terminals  0.5.2
     jupyterlab                4.0.11
     jupyterlab_pygments       0.3.0
     jupyterlab_server         2.25.2
     jupyterlab-widgets        3.0.9
     lazy_loader               0.3
     linkify-it-py             2.0.2
     llvmlite                  0.40.1
     locket                    1.0.0
     Markdown                  3.5.2
     markdown-it-py            3.0.0
     MarkupSafe                2.1.4
     matplotlib-inline         0.1.6
     mdit-py-plugins           0.4.0
     mdurl                     0.1.2
     mistune                   3.0.2
     msgpack                   1.0.7
     multidict                 6.0.4
     multipledispatch          1.0.0
     nbclient                  0.9.0
     nbconvert                 7.14.2
     nbformat                  5.9.2
     nest-asyncio              1.6.0
     networkx                  3.2.1
     notebook                  7.0.7
     notebook_shim             0.2.3
     numba                     0.57.1
     numpy                     1.24.4
     nvtx                      0.2.8
     overrides                 7.7.0
     packaging                 23.2
     pandas                    1.5.3
     pandocfilters             1.5.1
     panel                     1.3.8
     param                     2.0.2
     parso                     0.8.3
     partd                     1.4.1
     pexpect                   4.9.0
     pillow                    10.2.0
     pip                       23.0.1
     platformdirs              4.1.0
     prometheus-client         0.19.0
     prompt-toolkit            3.0.43
     protobuf                  4.25.2
     psutil                    5.9.8
     ptyprocess                0.7.0
     pure-eval                 0.2.2
     pyarrow                   14.0.2
     pycparser                 2.21
     pyct                      0.5.0
     Pygments                  2.17.2
     pylibcugraph-cu12         23.12.0
     pylibraft-cu12            23.12.0
     pynvml                    11.4.1
     pyproj                    3.6.1
     python-dateutil           2.8.2
     python-json-logger        2.0.7
     pytz                      2023.4
     pyviz_comms               3.0.1
     PyWavelets                1.5.0
     PyYAML                    6.0.1
     pyzmq                     25.1.2
     qtconsole                 5.5.1
     QtPy                      2.4.1
     raft-dask-cu12            23.12.0
     rapids-dask-dependency    23.12.1
     referencing               0.33.0
     requests                  2.31.0
     rfc3339-validator         0.1.4
     rfc3986-validator         0.1.1
     rich                      13.7.0
     rmm-cu12                  23.12.0
     rpds-py                   0.17.1
     scikit-image              0.21.0
     scipy                     1.12.0
     Send2Trash                1.8.2
     setuptools                65.5.0
     shapely                   2.0.2
     simpervisor               1.0.0
     six                       1.16.0
     sniffio                   1.3.0
     sortedcontainers          2.4.0
     soupsieve                 2.5
     stack-data                0.6.3
     tblib                     3.0.0
     terminado                 0.18.0
     tifffile                  2024.1.30
     tinycss2                  1.2.1
     tomli                     2.0.1
     toolz                     0.12.1
     tornado                   6.4
     tqdm                      4.66.1
     traitlets                 5.14.1
     treelite                  3.9.1
     treelite-runtime          3.9.1
     types-python-dateutil     2.8.19.20240106
     typing_extensions         4.9.0
     uc-micro-py               1.0.2
     ucx-py-cu12               0.35.0
     uri-template              1.3.0
     urllib3                   2.1.0
     wcwidth                   0.2.13
     webcolors                 1.13
     webencodings              0.5.1
     websocket-client          1.7.0
     widgetsnbextension        4.0.9
     xarray                    2024.1.1
     xyzservices               2023.10.1
     yarl                      1.9.4
     zict                      3.0.0
     zipp                      3.17.0

[notice] A new release of pip is available: 23.0.1 -> 23.3.2
[notice] To update, run: pip install --upgrade pip
     
</pre></details>
```
",2024-01-30T16:11:19Z,0,0,paul,,False
745,[FEA] parquet: rle_stream for dictionary pages,"I've been looking at the `rle_stream` class in order to decode dictionary streams in addition to repetition streams in the parquet decoder. This is a component of the work that @nvdbaranec has done here https://github.com/rapidsai/cudf/pull/13622, where we'd like to separate out at least a ""fixed width"" and a ""fixed width dictionary encoded"" pair of kernels. 

With the changes in `rle_stream`, the core of the logic is able to use more threads for the RLE stream decoder. Specifically, a first warp is in charge of generating a set of runs, and other warps are able to take each one of the runs and decode them in parallel. As part of the micro kernel work, we feel that focusing on `rle_stream` decoder and its effects on `gpuComputeStringPageBounds`, `gpuComputePageSizes` and the use in the new fixed kernels, is a good first step to get the micro kernel work merged. 

This issue then is to get a new `rle_stream` into cuDF that can handle both repetition AND dictionary streams, and show that the performance impact is same or better than what we have now. We hope that having this decoder will help centralize code, helping cleanup the parquet code base.",2024-02-01T15:45:30Z,0,0,Alessandro Bellina,NVIDIA,True
746,[FEA] Implement a templated parquet decoding kernel suitable for reuse in micro-kernel optimization approach.,"As part of the drive towards implementing the micro-kernel parquet decoding strategy, we would like to start centralizing the core parquet decoding loop into a generic templated implementation that can be reused.  At the high level, all of various parquet kernels are structured similar to this:
```
kernel(PageInfo p)
{
    // page setup, bounds checking (for skip_rows/num_rows), etc
    setup_code();

   while(there are still values to decode in p){
      def_levels = def_stream.decode(def_levels);
      rep_levels = p.has_lists ? rep_stream.decode(rep_levels);
      dict_indices = p.has_dict ? dict_stream.decode(dict_indices);
      decode_general_outputs(def_levels, rep_levels, dict_indices);

      PROCESS(p, def_levels, rep_levels, dict_indices);
   }
}
```
The various *_stream.decode() functions are the key bottleneck in decoding parquet data. At the moment, the kernels we have mostly utilize the older/slower way of decoding these streams.  The `rle_stream` class was developed to do this in a more parallel (and more confiurable) way, but only a few kernels use it at the moment because it does not currently handle dictionaries.  The work for that is underway and very close to completion (https://github.com/rapidsai/cudf/issues/14950)

`decode_general_outputs` is a function that produces validity, list offset information and the mapping of source data (location in the parquet data page) to destination data (location in the final cudf column).  The amount of work this function has to do varies greatly based on the characteristics of the input data - nullability, presence of lists, etc.

PROCESS is something that varies from kernel-to-kernel.  Essentially, the user-provided function that actually does the final data decoding.

We would like to implement this high level loop as a templated function that can be tailored to produce multiple, more optimal kernels based on they key data characteristics. For example:

```
template<// page data characteristics
                bool nullable,
                bool has_lists,
                bool has_dictionary,
                etc

                // parameters which can be tuned 
                int decode_buffer_size,
                int decode_warp_count,
                etc,
                
                // user provided PROCESS functor
                ProcessFunc Proc>
```

There are several reasons to do this:
- The `rle_stream` class uses shared memory, so it is a big advantage to be able to have kernels that don't need a given feature (say, list decoding) to be able to use less.
- It is useful to be able to tune block size per kernel as they tend to get bottlenecked in different ways.  
- It would allow us to eliminate the old level decoding path.

The first candidates for using this would be two new micro-kernels:  Fixed-width and fixed-width-with-dictionaries (the non-list case for both of them). We would like to get these in for 24.04 and then later on we can start refactoring the larger mass of existing kernels (especially the general-purpose `gpuDecodePageData` and `gpuDecodeStringPageData`",2024-02-01T19:09:50Z,0,0,,,False
747,[FEA] Zero-copy nested types with other GPU libraries (like Awkward array),"In a conversation with @martindurant and @jpivarski, it came up that there's no supported way to exchange data zero copy between cuDF and [Awkward Array](https://awkward-array.org/doc/main/) (which has GPU support).

The standard 0-copy mechanisms like dlpack and `__cuda_array_interface__` don't support nested types like lists or structs. And our `to/from_arrow()` methods convert to and from _host_ data so they're not useful when we want to 0-copy _device_ data.

## Option 1

We support a `gpu=True` (or similar) keyword argument in `to_arrow()` which would then return a PyArrow array backed by device data. Now, PyArrow does not seemingly support it, but it's _possible_ to create a PyArrow array backed by device data:

```python
In [5]: a = cp.asarray([1, 2, 3])

In [6]: buf = pa.foreign_buffer(a.data.ptr, a.nbytes, a)

In [7]: type(buf)
Out[7]: pyarrow.lib.Buffer

In [8]: print(buf)
<pyarrow.Buffer address=0x7f2f6fa00200 size=24 is_cpu=True is_mutable=False>
```

The problem (as can be seen above) is that PyArrow thinks this is a CPU-backed buffer. So attempting to do anything with it segfaults:

```python
In [9]: arr = pa.Array.from_buffers(pa.int64(), len(a), buffers=[None, buf])

In [10]: print(arr)  # segfault
```

## Option 2

We could expose new `Series.to_buffers()` and `Series.from_buffers()` functions that would produce and consume GPU buffers (along with a schema), presumably in the same order as arrow's [from_buffers](https://arrow.apache.org/docs/python/generated/pyarrow.Array.html#pyarrow.Array.from_buffers) and [buffers](https://arrow.apache.org/docs/python/generated/pyarrow.Array.html#pyarrow.Array.buffers) methods. We could use CuPy arrays to represent the buffers.

---

Curious what folks think? Interested also in @kkraus14's thoughts here if any.",2024-02-02T21:21:47Z,0,0,Ashwin Srinath,Voltron Data,False
748,[BUG] Consider cleaning up rmm global state modification in spilling,"**Describe the bug**
Currently the SpillManager [modifies rmm's current device resource](https://github.com/rapidsai/cudf/blob/branch-24.04/python/cudf/cudf/core/buffer/spill_manager.py#L251) on construction to allow itself to detect when allocations fail and trigger spilling. However, this memory resource change is not reverted if the manager goes out of scope. This causes problems in situations where managers may be created and then deleted as was discovered in #14958.

**Expected behavior**
Either the spill manager should attempt to remove the modification of the mr when it is garbage collected, or the test that was skipped in #14958 should be removed. If we attempt the former, one of the challenges will be that the user could have set a different memory resource themselves.

In fact, this reveals a flaw in the current spilling logic whereby a user could enable spilling but then set the memory resource, which would invalidate the spilling approach. I'm not sure that there is a safe way to handle this right now other than modifying cudf to store a default memory resource that is passed to every algorithm rather than relying on rmm's default memory resource. That way, when spilling is enabled cudf could modify its default memory resource, and users modifying rmm's memory resource would have no effect, while attempting to modify cudf's memory resource would either raise an error or handle doing this in a spilling-safe manner (i.e. by wrapping the new memory resource in the callback adaptor).

This dovetails with some discussions in #14229 around the fact that default memory resources in libcudf's function signatures open us up to some pretty subtle ways because cuDF Python often leverages libcudf and [lib]rmm in nontrivial ways.",2024-02-02T23:25:37Z,1,0,Vyas Ramasubramani,@rapidsai,True
749,[FEA] Incorporate chunked parquet reading into cuDF-python,"**Is your feature request related to a problem? Please describe.**
libcudf provides a `chunked_parquet_reader` in its public API. This reader uses new reader options to process the data in a parquet file in sub-file units. The `chunk_read_limit` option limits the table size in bytes to be returned per read by only decoding a subset of pages per chunked read. The `pass_read_limit` option limits the memory used for reading and decompressing data by only decompressing a subset of pages per chunked read.

The chunked parquet reader allows cuDF-python to expose two types of useful functionality:
1. an API that acts as an iterator to yield dataframe chunks. This is similar to the `iter_row_groups` behavior in [fastparquet](https://fastparquet.readthedocs.io/en/latest/api.html). This approach would let users work with parquet files that contain more rows than 2.1B rows (see #13159 for more information about the row limit in libcudf). 
2. a ""low_memory"" mode that reads the full file, but has a lower peak memory footprint thanks to the smaller sizes of intermediate allocations. This is similar to the the `low_memory` argument in [polars](https://docs.pola.rs/py-polars/html/reference/api/polars.read_parquet.html). This approach would make it easier to read large parquet datasets with limited GPU memory.

**Describe the solution you'd like**
We should make chunked parquet reading available to cuDF-python users. Perhaps this functionality could be made available to `cudf.pandas` users as well. 


**Additional context**
Pandas does not seem to have a method for chunking parquet reads, and I'm not sure if pandas makes use of the `iter_row_groups` behavior in fastparquet as a pass-through parameter.


API docs references:
* pandas: [read_parquet](https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html)
* pyarrow: [parquet.read_table](https://arrow.apache.org/docs/python/generated/pyarrow.parquet.read_table.html)
* fastparquet: [read_parquet](https://fastparquet.readthedocs.io/en/latest/api.html)
* polars: [read_parquet](https://docs.pola.rs/py-polars/html/reference/api/polars.read_parquet.html)
* cudf: [read_parquet](https://docs.rapids.ai/api/cudf/nightly/user_guide/api_docs/api/cudf.read_parquet/)
",2024-02-04T19:10:28Z,0,0,Gregory Kimball,,False
750,[FEA] Offer more control over CPU fallback in cudf.pandas,"**Is your feature request related to a problem? Please describe.**
The default execution model for `cudf.pandas` is to try to execute an operation on the GPU, then fall back to the CPU if it fails for any reason. This approach is desirable for end-users to maximize the number of cases where `cudf.pandas` ""just works"", but it makes it difficult to analyze when failures are occurring and why. The former can be addressed by running under the profiler, but that is more cumbersome than we would like in many cases where we would rather get a quick signal in the form of failure (e.g. when running a workflow or a test suite to analyze unsupported cases). Furthermore, there is no easy way to determine whether cudf and pandas return the same results for a given operation, which is a different failure mode that is currently not possible to capture.

**Describe the solution you'd like**
We should generalize `_fast_slow_function_call` to support a wider range of fallback options. These options could be configurable by an environment variable, or by some global configuration option (the former is probably fine to start with). The different behaviors we would want to support are:
- Error on fallback. We could then run the pandas test suite with this turned on and get a sense of how many tests cudf passes on its own.
- Error on specific types of fallback. This would allow us to analyze the types of fallback that are occurring. Some of the most obvious error modes I can foresee (there are certainly others) are:
    - Out of memory errors, for the sake of planning No OOM related work
    - AttributeErrors for missing functionality
    - TypeErrors for differing function signatures
- Error when cudf and pandas produce different outputs. This would be an extra branch within the fast path where the slow path is run even if the fast path succeeds, and then the fast and slow paths are compared for equivalence.

We may want to support warning instead of raising errors in some cases, but I don't think that's critical to start.

**Describe alternatives you've considered**
This could be configured by the cudf.pandas profiler, or a similar context manager?

**Additional context**
Feedback from @ianozsvald and @lmeyerov would be welcome!",2024-02-06T17:03:52Z,0,0,Bradley Dice,@NVIDIA @rapidsai,True
751,[BUG] Output of ``GroupBy.apply`` is missing the grouping index,"**Describe the bug**
There seems to be a subtle difference between cudf and pandas for `groupby(...).apply(...)`

**Steps/Code to reproduce bug**
```python
import cudf

df = cudf.DataFrame({""a"": [""cat"", ""dog"", ""cat""], ""b"": [0, 1, 0]})
df.groupby([""a"", ""b""]).apply(lambda x: x)
```
```
     a  b
0  cat  0
2  cat  0
1  dog  1
```
**Pandas Result**:
```
           a  b
a   b          
cat 0 0  cat  0
      2  cat  0
dog 1 1  dog  1
```

**Expected behavior**
I would expect the behavior of cudf and pandas to be consistent.

**Additional context**
I found this issue while debugging TPC-h query 16 and 21 with dask-cudf + dask-expr.",2024-02-06T19:51:54Z,0,0,Richard (Rick) Zamora,@NVIDIA,True
752,"[FEA] Support ""dataframe.query-planning"" config in ``dask.dataframe``","## PSA

To unblock CI failures related to the dask-expr migration, down-stream RAPIDS libraries can set the following environment variable in CI (before `dask.dataframe`/`dask_cudf` is ever imported):

```
export DASK_DATAFRAME__QUERY_PLANNING=False
```

**If you do this, please be sure to comment on the change, and link it to this meta issue.** (So I can make the necessary changes/fixes, and turn query-planning back on)


____

## Background

The `2024.2.0` release of Dask has deprecated the ""legacy"" `dask.dataframe` API. Given that dask-cudf (and much of RAPIDS) is tightly integrated with `dask.dataframe`, it is critical that `dask_cudf` be updated to use the new `dask_expr` backend smoothly.

Most of the heavy lifting is already being done in https://github.com/rapidsai/cudf/pull/14805. However, there will also be some follow-up work to expand coverage/examples/documentation/benchmarks. We will also need to update dask-cuda/explicit-comms.

## Action Items

**Basics** (to be covered by https://github.com/rapidsai/cudf/pull/14805):
- [x] Add dask-expr `DataFrameBackendEntrypoint` entrypoint for ""cudf""
- [x] Align top-level `dask_cudf` imports with `dask.dataframe` for `""dataframe.query-planning""` support

**Expected Follow-up**:
- [x] Add `read_json` support (https://github.com/rapidsai/cudf/pull/15408)
- [x] Add `read_orc` support (https://github.com/rapidsai/cudf/pull/15439)
- [ ] `read_parquet` should always return DataFrame (not currently the case in dask-expr if `columns=<str>`)
- [ ] Remove outdated `check_file_size` functionality from `dask_cudf.read_parquet`
- [x] Add s3 testing/support (https://github.com/rapidsai/cudf/pull/15408)
- [x] Add `read_text` support (https://github.com/rapidsai/cudf/pull/15439)
- [x] Fix unexplained test failures for categorical accessors (https://github.com/rapidsai/cudf/pull/15591)
- [x] Deprecate `to_dask_dataframe` API in favor of `to_backend` (https://github.com/rapidsai/cudf/pull/15592)
- [x] Deprecate `set_index(..., divisions=""quantile"")` (https://github.com/rapidsai/cudf/pull/15804)
- [x] Add `describe` support (seems to be working now? Just need to remove `xfail` markers)
- [x] Add `groupby` ""collect"" support (https://github.com/rapidsai/cudf/pull/15593)
- [ ] (Maybe?) add `as_index` support to `groupby`
- [x] Fix `get_dummy` support (https://github.com/dask/dask-expr/pull/1053)
- [ ] Fix sorting by categorical columns (https://github.com/rapidsai/cudf/pull/15701; **Related Issues**: https://github.com/rapidsai/cudf/issues/15641 & https://github.com/dask/dask/issues/11090 & https://github.com/rapidsai/cudf/pull/15801)
- [x] Fix sorting with nulls (https://github.com/rapidsai/cudf/pull/15639)
- [ ] `leftanti` merge support (Likely an error message in 24.06 and support in 24.08+)
- [x] `to_datetime` support (https://github.com/dask/dask-expr/pull/1035)
- [x] Add `melt` support (https://github.com/dask/dask-expr/pull/1049 & https://github.com/dask/dask/pull/11088)

**cuDF / Dask cuDF doc build**:
- [x] Revert https://github.com/rapidsai/cudf/pull/15343 (see: https://github.com/rapidsai/cudf/pull/15347)

**cuML support**:
- [x] [General debugging PR](https://github.com/rapidsai/cuml/pull/5835)

**cuxfilter support**:
- [x] (https://github.com/rapidsai/cuxfilter/pull/583, https://github.com/rapidsai/cuxfilter/pull/593)

**cugraph support**:
- [x] [General debugging PR](https://github.com/rapidsai/cugraph/pull/4325)

**Dask CUDA**:
- [ ] Explicit comms support (https://github.com/rapidsai/dask-cuda/issues/1311)

**Dask SQL**:
- [ ] Migrate predicate pushdown to dask-expr

**NeMo Curator**:
- [ ] Migrate custom-graph code and test against latest dask/cudf

**Merlin**:
- [ ] Port Merlin/NVTabular (Heavy lift - Aiming for 24.08)
",2024-02-12T17:33:04Z,0,0,Richard (Rick) Zamora,@NVIDIA,True
753,[FEA] Update JSON reader benchmarks to include JSON lines and normalization,"**Is your feature request related to a problem? Please describe.**

First pass changes:
- [ ] I believe [this line](https://github.com/rapidsai/cudf/blob/bb6ae07079f3c36dca8387bab578b75d06be6b33/cpp/benchmarks/io/json/nested_json.cpp#L91) in the benchmark `nested_json.cpp` should use `max_list_size` instead of `max_struct_size`. We should also add `int64` nvbench axes for these two size values, sticking with a standard value of `{10}`, and adding the ability to sweep these parameters in custom tests.
- [ ] Add JSON versus JSON Lines benchmark. We have a `parquet_reader_options` benchmark and we could add something similar e.g. `json_reader_options`. This benchmark can start by choosing a single data type and a device buffer data source. As a follow-on step we would want to allow data type and IO source to be nvbench enum axes.
- [ ] Add `_normalize_single_quotes` and `_normalize_whitespace` to the `json_reader_options` benchmark. Since the JSON writer can't generate single quotes or extra whitespace, these normalization steps will not change the resulting table, but we should track the added runtime.
- [ ] Add `_recovery_mode` and `_mixed_types_as_string` to the `json_reader_options` benchmark as ""no-op"" tests. The benchmark would use the the existing data generator without invalid records and without mixed types.
- [ ] Add post-processing to the generated data to introduce mixed types, and then benchmark against similar data without mixed types. The approach could be using the existing data generator, but then changing one list entry into a struct entry, e.g. `[1,2,3]` => `{""a"": [1,2,3]}`

Lower priority ideas. If we have reason to believe these benchmarks would highlight performance issues, then we should raise their priority.
- [ ] For the quote and whitespace normalization options, create a modified data generator or character buffer post-processing to introduce un-normalized data. For instance, we could replace `""` with `'` for quote normalization and `:` with ` : ` for whitespace normalization.
- [ ] Update the data generator to introduce invalid JSON lines and exercises the `_recovery_mode` as nulls code path. We could add a fraction of invalid records as well as valid records followed by invalid characters.
- [ ] Add a normalization benchmark into the `benchmarks/io/json/` suite that measures the runtime of `detail::normalize_single_quotes` and the upcoming detil API for whitespace normalization. This benchmark would not test the overall reader, but only the FST-based normalization functions.

",2024-02-13T21:15:10Z,0,0,Gregory Kimball,,False
754,[FEA] Word ngram based minhashes,"**Is your feature request related to a problem? Please describe.**
The current minhashes functionality/API #12961  computes minhashes by taking a fixed character width sliding window and computes the minhashes for a document using character ngrams.

Another alternative approach to minhashing documents is to instead use word based ngrams instead of char based and research seems to suggest it leads to better quality results (lower false positives during Locality sensitive hashing). 

**Describe the solution you'd like**
Add a word ngram based minhash support that's accelerated on GPUs.

**Describe alternatives you've considered**
Currently most CPU libraries achieve this by first tokenizing the string into word ngrams, and then looping over these tokens computing the minhash. 
It is possible to somewhat mimic that with a `str.word_tokenize + str.hash_values + groupby + min` in cuDF but requires a lot more intermediate memory and reduces the batch size of documents processed. It is also slower than a custom minhash implementation.

Another challenge is that the definition of a ""word"" can be different for different languages and there's often different approaches to create word_ngrams based on language.

**Additional context**
Add any other context, code examples, or references to existing implementations about the feature request here.
",2024-02-14T19:43:47Z,1,0,Ayush Dattagupta,Nvidia,True
755,[FEA] Update chunked parquet reader benchmarks to include `pass_read_limit`,"**Is your feature request related to a problem? Please describe.**
The `BM_parquet_read_chunks` benchmark in `benchmarks/io/parquet/parquet_reader_input.cpp` includes a `byte_limit` nvbench axis. This axis controls the `chunk_read_limit`. With the new features added in #14360, there is a new `chunked_parquet_reader` API that exposes both `chunk_read_limit` and `pass_read_limit` parameters to control reader behavior. We currently do not have a method for benchmarking `pass_read_limit` values.

**Describe the solution you'd like**
- [ ] Add a new benchmark, such as `BM_parquet_read_subrowgroup_chunks`, that provides nvbench axes for both `chunk_read_limit` and `pass_read_limit`
- [ ] Rename `byte_limit` to `chunk_read_limit` in `BM_parquet_read_chunks` for clarity, now that we have both input and output byte limits in chunked parquet reading.
- [ ] Also, please consider adding an nvbench axis for `data_size` for at least the chunked parquet reader benchmarks. It would be useful to allow the benchmarks to operate on tables larger than 536 MB.

**Describe alternatives you've considered**
n/a",2024-02-14T21:27:40Z,0,0,Gregory Kimball,,False
756,[FEA] Implement center for offset based windows,"Suppose I have a Series as follows:
```python
s = cudf.Series(range(100), index=cudf.date_range('2024', periods=100, freq='D'))
```
If I want to perform 3-day rolling window mean, I can do:
```python
window_size = 3
s.rolling(f'{window_size}D').mean()
```
This is not centered. If I want to set the window labels as the center of the window index (like in pandas):
```python
a = s.rolling(f'{window_size}D', center=True).mean()
```
then I get a NotImplementedError.

I wish I could do this in cudf.

Right now, I can just compute the rolling mean, shift it by half the window size and fill in the NaN values by using a loop over a variable window but it's a little ugly.
```python
shift = -(window_size-1)//2
b = s.rolling(f'{window_size}D').mean().shift(shift)
b.iloc[shift:] = [s.loc[i:].mean() for i in s.index[-window_size+1:-window_size-shift+1]]
```

Pandas' `DataFrame.rolling` uses a cython optimized function to implement `center` (and `closed`) parameters. Its function to get variable window indexers is `pandas._libs.window.indexers.calculate_variable_window_bounds`. My suggestion is to implement this function in cudf.",2024-02-19T07:50:52Z,0,0,Manlai Amar,,False
757,"[BUG] Unlike its pandas counterpart, `cudf.date_range` doesn't include the end if it's specified","If I create a DatetimeIndex object using `cudf.date_range()` by passing `start`, `end` and `freq` parameters:
```python
cudf.date_range('2020-01-01', '2020-01-05', freq='D')

DatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04'], 
              dtype='datetime64[ns]', freq='D')
```
As you can see, the result is an object with length 4. The same code in pandas results in an object with length 5:
```python
pd.date_range('2020-01-01', '2020-01-05', freq='D')

DatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04', '2020-01-05'],
              dtype='datetime64[ns]', freq='D')
```

Is it possible to make it consistent with pandas' `date_range()` in that the result includes both ends? Because, `closed` parameter is nonfunctional at the moment, to make `cudf.date_range()` produce the same output as its pandas counterpart, we need to add 1 `freq` to `end`, which creates a whole lot of work: convert the string to datetime, convert freq to timedelta and add them to define a new end.

Anyway, is it possible to make it clear in the documentation that this is different from its pandas counterpart?

<sup>I didn't know how to tag this since it's not really a bug. It's just different from pandas API that results in a subtle bug _in my code_.</sup>",2024-02-22T04:35:02Z,0,0,Manlai Amar,,False
758,[FEA] NamedAgg in groupby context,"**Is your feature request related to a problem? Please describe.**

please support namedagg in groupby(...).agg

**Describe the solution you'd like**

to be able to write
```python
df.groupby(""A"").agg(
    b_min=cudf.NamedAgg(column=""B"", aggfunc=""min""),
    c_sum=cudf.NamedAgg(column=""C"", aggfunc=""sum"")
)
```

**Describe alternatives you've considered**


**Additional context**

https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.agg.html
",2024-02-22T12:58:24Z,0,0,Marco Edward Gorelli,Quansight,False
759,[BUG] Rolling window aggregations are very slow with large windows,"With large windows, the `.rolling()` function in cuDF can be pathologically slow:

```python
In [6]: dt = cudf.date_range(""2001-01-01"", ""2002-01-01"", freq=""1s"")
In [7]: df = cudf.DataFrame({""x"": np.random.rand(len(dt))}, index=dt)
In [8]: %time df.rolling(""1D"").sum()
CPU times: user 10.3 s, sys: 57.1 ms, total: 10.3 s
Wall time: 10.4 s
Out[8]:
                                x
2001-01-01 00:00:00      0.815418
2001-01-01 00:00:01      1.238151
2001-01-01 00:00:02      1.811390
2001-01-01 00:00:03      2.065794
2001-01-01 00:00:04      2.195230
...                           ...
2001-12-31 23:59:55  43308.909704
2001-12-31 23:59:56  43309.098228
2001-12-31 23:59:57  43308.658888
2001-12-31 23:59:58  43308.790256
2001-12-31 23:59:59  43308.915838

[31536000 rows x 1 columns]
```

## Why is it slow?

Of the 10s of execution time above, about 8s is spent in computing the window sizes, which is done in a hand-rolled numba CUDA kernel: https://github.com/rapidsai/cudf/blob/6f6e521257dce5732eea7b6b9d56243f8b0a69cc/python/cudf/cudf/utils/cudautils.py#L17. Note that running the code through a profiler will show execution time being spent in the _next_ CUDA kernel (`column.full`) - but that's a red herring I think, because there's no synchronization after the numba kernel call.

## What can we do about it?

I see a couple of options here:

1. I wonder if there's a better way to write that kernel. Currently, it naively launches one thread per element, and does a linear search for the next element that would exceed the window bounds. 
2. We could make it `libcudf`'s responsibility to compute the window sizes. I believe they already do window sizes computation in the context of _grouped_ rolling window aggreagations: see [`grouped_range_rolling_window()`](https://github.com/rapidsai/cudf/blob/6f6e521257dce5732eea7b6b9d56243f8b0a69cc/cpp/include/cudf/rolling.hpp#L542).",2024-02-22T16:04:49Z,0,0,Ashwin Srinath,Voltron Data,False
760,[BUG] parquet reader::impl::decode_page_data error_code checking slowness,"While benchmarking some code we found that about 5% worth of time are being lost due to this line of code:
https://github.com/rapidsai/cudf/blob/branch-24.04/cpp/src/io/parquet/reader_impl.cpp#L248

This is on our perf cluster (A100) for NDS @3k. It explains some of a dip in perf we have seen since 23.10 but we haven't gotten around to testing. 

If I stop obtaining the value from `error_code` (e.g. I don't perform the pageable memcpy essentially) we gain 20 seconds locally. I am filing this because it may be a good idea to remove this or look into how to improve it (would a pinned copy help)?",2024-02-22T23:05:41Z,0,0,Alessandro Bellina,NVIDIA,True
761,[FEA] Pass column indices as `index_col` in `read_csv`,"If I want to use the `index_col` parameter to set certain columns as indices when reading a csv file, I cannot pass a list of column indices (like in pandas). I can pass a list of column labels though:
```python
cudf.read_csv(filepath, index_col=[0])
KeyError: 'None of [0] are in the columns'

cudf.read_csv(filepath, index_col=['family'])
```
While this is not a huge issue, I imagine the following is a common scenario: You have know that the first 3 columns are index columns, but you don't exactly know how each are spelt (`'date'` vs `'Date'` etc.). In this case, if passing a list of column indices were possible, `index_col=[0,1,2]` would have worked fine; otherwise, you will have to read the file without specifying index columns and set index later (or require trial and error to guess the column labels).

Is it possible for `index_col` to accept list of indices like in pandas?",2024-02-23T08:21:24Z,0,0,Manlai Amar,,False
762,[FEA] Simplify the pylibcudf groupby-aggregation API,"**Is your feature request related to a problem? Please describe.**
The current pylibcudf groupby-aggregation API maps to libcudf's. It is very expressive, allowing the specification of an arbitrary number of aggregations for every column to be aggregated, and an arbitrary number of aggregation columns per groupby table. However, this API is also fairly verbose and cumbersome to work with. Writing a groupby-aggregation in pylibcudf currently requires ~10 lines of code, as compared the concise single line version of the pandas API. Ideally we would like to offer the same level of convenience via a simpler API without sacrificing the flexibility and performance of the more general API where necessary.

**Describe the solution you'd like**
We should consider making the following changes:
1. Every aggregation should be default-constructible. That essentially means that every parameter should have a default parameter. This is true of most but not all aggregations already. Where appropriate, we may also want to push some of these defaults down to libcudf, but I would be OK with the small deviation of different default values if necessary.
2. We should add an API roughly like `GroupBy.aggregate_simple` (name TBD) that accepts a `List[Tuple[Column, List[str]]]` and handles the construction of the GroupByRequest objects under the hood. This only works once every agg is default-constructible. It may not be terribly useful for the heavily parametrized aggregations, but it will simplify working with the most common unparametrized aggregations (sum, prod, min, max, etc).
3. We should consider adding a functional API `groupby_agg(data: Table, group_columns : List[int], aggs : List[Tuple[Column, List[str]]])` that effectively functions as a simple one-line wrapper around step 2.

**Describe alternatives you've considered**
None yet. I am not yet fully sold on the exact APIs that I proposed above, but I do think that the current API is too cumbersome for direct use most of the time except by other library developers and needs improvement. I do not want to lose the purity of mirroring libcudf APIs, especially not by replacing those APIs with higher-overhead alternatives, so the current API does need to exist. I've listed the three steps above in decreasing order of importance/quality, and we may go a completely different direction with 3. This issue is largely intended as a starting point for a discussion capturing the problems with the current API.",2024-02-24T00:05:41Z,0,0,Vyas Ramasubramani,@rapidsai,True
763,[FEA] Add standard data ingestion pipelines to pylibcudf,"**Is your feature request related to a problem? Please describe.**
Currently core pylibcudf owning objects are difficult to construct. Columns are constructed using a complete signature involving nearly raw pointers, while Scalars are really only constructible from pyarrow scalars. While this approach is sufficient for now due to the interop layers baked into cuDF's Cython, in the long run this will not be a reasonable API for users. We need to define simpler ways for users to create these objects, ideally without sacrificing the performance of the lowest level APIs in cases where power users want to access them.

**Describe the solution you'd like**
We should define a `singledispatch` `classmethod` factory `from_any` that accepts an arbitrary input and attempts to construct from it. Each specialization should be a trivial one-line passthrough to a separate `classmethod` factory of the appropriate type, such that users who know what input they have could always call the appropriate factory themselves. For instance:

```
class Column:
    @singledispatchmethod
    @classmethod
    def from_any(cls, obj):
        raise ValueError(""Not supported"")

    @from_any.register(np.ndarray)
    def from_any(cls, obj):
        return cls.from_numpy_array(obj)

    @classmethod
    def from_numpy_array(cls, obj : np.ndarray):
        ...
```

**Describe alternatives you've considered**
We could wrap the `singledispatch` function in a higher-level fused-type Cython function that could do compile-time dispatch on known C types. That would offer a minor performance benefit in a limited number of cases, but at the cost of unnecessary complexity IMO, especially since the vast majority of user inputs are not going to be Cython-typed objects but will instead come in as PyObjects that need to be introspected at runtime via `isinstance` anyway (which is what `singledispatch` does).

**Additional context**
The current Column constructor is not the most usable and we may eventually want to make the current constructor into a factor and instead have the constructor be something more user-friendly. ",2024-02-24T00:19:14Z,0,0,Vyas Ramasubramani,@rapidsai,True
764,[FEA] Determine the best way to emulate keyword-only parameters in cpdef functions,"**Is your feature request related to a problem? Please describe.**
Numerous pylibcudf APIs currently accept optional arguments because the corresponding libcudf APIs do. In C++, libcudf essentially forbids boolean parameters in favor of enums, which are at least self-documenting at the call site (unlike boolean parameters). However, if there are multiple such parameters the caller still has to remember the order of these parameters. In Python, we can solve this problem using keyword-only arguments, which forces the caller to access the parameters by name and therefore automatically protects against misuse by incorrect ordering. However, this solution does not work for `cdef` (and therefore `cpdef`) functions in Cython because such functions are effectively C functions and all arguments are therefore ultimately passed in order to these functions.

In a related vein, some libcudf APIs could be naturally exposed in pylibcudf using a single API with an additional parameter. For instance the stable variants on libcudf's sorting APIs would be very natural to expose in pylibcudf with an extra parameter `sort(..., stable : bool)`. However, this again runs into the same issue above where we would want to use a keyword-only argument but are stymied by the fact that this is unsupported.

**Describe the solution you'd like**
We should determine a standard practice for this kind of API in pylibcudf and document it in the developer guide. It is not clear to me what a good solution is though. One option would be to make these APIs `def` functions. That would allow using keyword-only arguments, but at the cost of not being able to take advantage of Cython call syntax with typed arguments in Cython contexts, which I'd like to avoid at this stage since at the moment the entire pylibcudf API supports usage in a pure Cython context and removing that is a broader conversation. We can revisit that option if we can't come up with any alternatives, though",2024-02-24T01:08:38Z,0,0,Vyas Ramasubramani,@rapidsai,True
765,[FEA] Explore using Cython language features that allow minimizing overhead and simplifying code,"**Is your feature request related to a problem? Please describe.**
Cython offers various small bits of functionality that reduce overhead in various scenarios. We should test some of these out.

**Describe the solution you'd like**
Features to test:
- [ ] Apply `cython.final` decorator to classes. This will reduce the overhead of `cpdef` methods by removing the need to look up child classes.
- [ ] Use `cython.freelist` for objects that may be created and freed often. Good examples would be lightweight objects like `pylibcudf.aggregation.Aggregation`s
- [ ] Add `not None` to parameters that must be provided so that they can be checked cheaply at runtime.",2024-02-24T01:14:36Z,0,0,Vyas Ramasubramani,@rapidsai,True
766,[FEA] Add python bindings in the parquet reader for `num_rows`/`skiprows`,"**Is your feature request related to a problem? Please describe.**
Unfortunately there has been churn in libcudf around support for `num_rows`/`skiprows` in the Parquet and ORC readers. In 22.08 we deprecated these parameters in the parquet reader (#11218) and then in 22.10 we removed them from C++ (#11503) and python (#11480). We also deprecated `num_rows`/`skiprows` in the ORC reader (#11522, see issue #11519).

At this point, we realized that chunked parquet reading (#11867) would require adding `num_rows`/`skiprows` back to the C++ implementation (#11657).

Let's stabilize row selection APIs in libcudf by completing these tasks:
- [ ] Add python bindings in the parquet reader for `num_rows`/`skiprows`
- [ ] Remove the deprecation notice in the ORC reader for `num_rows`/`skiprows` (#11522)

**Additional context**
We also dropped `num_rows`/`skiprows` support in the cuDF-python fuzz tests (#11505). My preference is to not include any python fuzz testing changes in the scope of this issue.
",2024-02-26T19:36:33Z,0,0,Gregory Kimball,,False
767,[FEA] Implement all libcudf modules required by cuDF Python in pylibcudf,"**Is your feature request related to a problem? Please describe.**
pylibcudf is intended to provide a low-level Python interface to the libcudf C++ API. cuDF's internals will ultimately be refactored to depend on pylibcudf. As a first step, we need to expose all libcudf algorithms used by cuDF Cython in pylibcudf.

**Describe the solution you'd like**
This is a tracking issue for APIs to expose in Cython. The APIs are grouped based on the pxd file exposing libcudf APIs in Cython, which roughly corresponds to namespaces in libcudf.

| Module | PRs (or assignees) | Notes |
|---|---|---|
| aggregation.pxd | #14945, #14970  | |
| binaryop.pxd | #14821 | |
| column/column.pxd | #13562 | pylibcudf Columns share ownership |
| column/column_factories.pxd | | |
| column/column_view.pxd | #13562 | pylibcudf Columns share ownership |
| concatenate.pxd | #15011 | |
| contiguous_split.pxd | | |
| copying.pxd | #13562, #14508, #14640| |
| datetime.pxd | | |
| expressions.pxd | | |
| filling.pxd | #15225 | |
| groupby.pxd | #14945 | |
| hash.pxd | #15418  | |
| interop.pxd | | |
| io/arrow_io_source.pxd | | |
| io/avro.pxd | | |
| io/csv.pxd | | |
| io/data_sink.pxd | | |
| io/datasource.pxd | | |
| io/json.pxd | | |
| io/orc.pxd | | |
| io/orc_metadata.pxd | | |
| io/parquet.pxd | | |
| io/text.pxd | | |
| io/timezone.pxd | | |
| io/types.pxd | | |
| join.pxd | #14972 | |
| labeling.pxd | | |
| lists/combine.pxd | @Matt711 | |
| lists/contains.pxd | | |
| lists/count_elements.pxd | | |
| lists/explode.pxd | #15011 | |
| lists/extract.pxd | | |
| lists/gather.pxd | | |
| lists/lists_column_view.pxd | | |
| lists/sorting.pxd | | |
| lists/stream_compaction.pxd | | |
| merge.pxd | #15011 | |
| null_mask.pxd | @charlesbluca | |
| nvtext/byte_pair_encode.pxd | | |
| nvtext/edit_distance.pxd | | |
| nvtext/generate_ngrams.pxd | | |
| nvtext/jaccard.pxd | | |
| nvtext/minhash.pxd | | |
| nvtext/ngrams_tokenize.pxd | | |
| nvtext/normalize.pxd | | |
| nvtext/replace.pxd | | |
| nvtext/stemmer.pxd | | |
| nvtext/subword_tokenize.pxd | | |
| nvtext/tokenize.pxd | | |
| partitioning.pxd | | |
| quantiles.pxd | #15874 | |
| reduce.pxd | #14970 | |
| replace.pxd | #15005 | |
| reshape.pxd | #15827| sans byte_cast which is only used by cpp/java |
| rolling.pxd | #14982 | |
| round.pxd | #15863 | |
| scalar/scalar.pxd | #14133 | |
| search.pxd | #15166 | |
| sorting.pxd | #15011 | |
| stream_compaction.pxd | #15011 | |
| strings/convert/convert_booleans.pxd | | |
| strings/convert/convert_datetime.pxd | | |
| strings/convert/convert_durations.pxd | | |
| strings/convert/convert_fixed_point.pxd | | |
| strings/convert/convert_floats.pxd | | |
| strings/convert/convert_integers.pxd | | |
| strings/convert/convert_ipv4.pxd | | |
| strings/convert/convert_lists.pxd | | |
| strings/convert/convert_urls.pxd | | |
| strings/split/partition.pxd | | |
| strings/split/split.pxd | | |
| strings/attributes.pxd | | |
| strings/capitalize.pxd | | |
| strings/case.pxd |https://github.com/rapidsai/cudf/pull/15489 | |
| strings/char_types.pxd | | |
| strings/combine.pxd | | |
| strings/contains.pxd | | |
| strings/extract.pxd | | |
| strings/find.pxd | https://github.com/rapidsai/cudf/pull/15604 | |
| strings/find_multiple.pxd | | |
| strings/findall.pxd | | |
| strings/json.pxd | | |
| strings/padding.pxd | | |
| strings/regex_flags.pxd | | |
| strings/regex_program.pxd | | |
| strings/repeat.pxd | | |
| strings/replace.pxd | #15839 | |
| strings/replace_re.pxd | | |
| strings/side_type.pxd | | |
| strings/strip.pxd | | |
| strings/substring.pxd | | |
| strings/translate.pxd | | |
| strings/wrap.pxd | | |
| strings_udf.pxd | | |
| table/table.pxd | #13562 | Tables share column ownership in pylibcudf |
| table/table_view.pxd | #13562 | Tables share column ownership in pylibcudf |
| transform.pxd | | |
| transpose.pxd | | |
| types.pxd | #13562 | More types added as needed in other PRs |
| unary.pxd | #14850 | |
| utilities/host_span.pxd | | |
| wrappers/decimals.pxd | | |
| wrappers/durations.pxd | | |
| wrappers/timestamps.pxd | | |",2024-02-27T23:31:08Z,0,0,Vyas Ramasubramani,@rapidsai,True
768,[FEA] Expose stream-ordered APIs in pylibcudf,"**Is your feature request related to a problem? Please describe.**
There is currently no way to run cuDF operations in a stream-ordered manner. Since cuDF is deeply tied to the pandas API, there are also limits to how much stream-ordering may be exposed in the public API. pylibcudf has no such restrictions and should allow complete control over streams.

**Describe the solution you'd like**
libcudf APIs are being incrementally modified to support stream-ordering. Once #13744 is complete, pylibcudf should expose the same functionality in its APIs.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
#13087 and #13509 are examples of where stream-ordered Python APIs could be useful.
",2024-02-27T23:49:33Z,0,0,Vyas Ramasubramani,@rapidsai,True
769,[FEA] Expose memory_resource arguments in pylibcudf,"**Is your feature request related to a problem? Please describe.**

As discussed in #14229, cudf currently relies on fate (working well so far) to ensure there are no use-after-free bugs when calling into, and taking ownership of return values from, libcudf.

In cudf-classic cython wrappers, the memory resource argument is never exposed to cython land. In pylibcudf, it is not currently exposed either.

**Describe the solution you'd like**

All pylibcudf calls should take a memory resource argument, that is then called with the memory resource cudf considers to be currently active. This needs to go hand-in-hand with #15163, since when both a stream and memory resource are exposed in the public libcudf API, the memory resource is second, so we can't rely on overloaded defaulting for the stream argument.

**Describe alternatives you've considered**

None.",2024-02-28T12:48:36Z,0,0,Lawrence Mitchell,,False
770,[FEA] Implement `observed` parameter for `groupby`,"Doing `DataFrame.groupby(..., observed=False)` currently produces:

```
NotImplementedError: observed parameter is not yet implemented
```

**Additional context**
Not sure if this is already being tracked somewhere, but the missing argument is requiring some extra code in https://github.com/rapidsai/cudf/pull/14805 that I'd like to remove later, and I'd like to have an issue to link to in the code.
",2024-02-28T17:30:37Z,0,0,Richard (Rick) Zamora,@NVIDIA,True
771,[BUG] Discrepancy between cudf and pandas for cumulative operations,"**Describe the bug**
For cumulative operations (e.g. `cummax`, `cummin`, `cumsum`, `cumprod`), Pandas defines `axis` and `skipna` as optional positional arguments, while cudf requires these to be key-word arguments.

**Steps/Code to reproduce bug**
```python
import cudf
import pandas as pd

df = pd.DataFrame({""a"": [1, 2]})
axis = 0
skipna = False
df.cumsum(axis, skipna)
```
```
   a
0  1
1  3
```
```python
cudf.from_pandas(df).cumsum(axis, skipna)
```
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[34], line 7
      5 axis = 0
      6 skipna = False
----> 7 cudf.from_pandas(df).cumsum(axis, skipna)

File /.../site-packages/cudf/core/mixins/mixin_factory.py:11, in _partialmethod.<locals>.wrapper(self, *args2, **kwargs2)
     10 def wrapper(self, *args2, **kwargs2):
---> 11     return method(self, *args1, *args2, **kwargs1, **kwargs2)

File /.../site-packages/nvtx/nvtx.py:115, in annotate.__call__.<locals>.inner(*args, **kwargs)
    112 @wraps(func)
    113 def inner(*args, **kwargs):
    114     libnvtx_push_range(self.attributes, self.domain.handle)
--> 115     result = func(*args, **kwargs)
    116     libnvtx_pop_range(self.domain.handle)
    117     return result

TypeError: DataFrame._scan() got multiple values for argument 'op'
```

**Expected behavior**
I'd expect cudf behavior to be consistent with pandas in this case.",2024-02-28T19:01:04Z,0,0,Richard (Rick) Zamora,@NVIDIA,True
772,"[FEA] Consider implementing standard operators on pylibcudf Columns, Scalars, and possibly Tables","**Is your feature request related to a problem? Please describe.**
pylibcudf objects support many of the standard operations that Python objects expect to work via dunder methods that map to language operator -- such as `len(x) == x.__len__()` -- via methods instead. The reason for preferring methods is that methods are typed and as such are preferable in pure Cython contexts because they can immediately produce typed outputs and because they can avoid Python function call overhead when invoked on Cython-typed inputs. The downside of this approach is that when pylibcudf is used as a Python library it is more verbose and less idiomatic to have to use `col.size()` than `len(col)`. 

**Describe the solution you'd like**
I'm not entirely convinced that implementing all operators is worthwhile given the potential (albeit minor) typing issues/performance footguns it introduces in Cython code, but I think we should at least consider it and can use this issue to document our conclusions one way or another. If we do choose to move forward, I think it makes sense to implement things like binary operators on Columns and Scalars. Tables are a harder sell; most likely we will only want to implement simple operators like `len` and require users to manually handle binary operations on a per-column basis since the isomorphism between a Table and a 2D array is weak at best and binary operations likely have far too many edge cases to be worth pursuing.",2024-02-28T23:55:40Z,0,0,Vyas Ramasubramani,@rapidsai,True
773,Reduce IO when `byte_range` option is used in `read_json`,"When reading a byte range from a file, JSON reader has to read data beyond the actual byte range to get the remainder of the last row that starts within the range. In order to find the end of this row, current implementation reads the next byte range. Once the reader finds the delimiter, it reads the full required range of data, discarding everything previously read.
This leads to 3x data read in most cases: 
https://github.com/rapidsai/cudf/blob/branch-24.04/cpp/src/io/json/read_json.cu#L162-L202
We could implement a solution that reads additional data in smaller chunks and does not discard the byte range that was initially read. Once we find the next delimiter, we can concatenate all required data into a single buffer. This does include a D2D copy of all data, but this will be a lot faster than IO or the H2D copy that we now make. With this, we can limit the IO to just the requested byte range + few extra KBs. ",2024-02-29T02:13:59Z,1,0,Vukasin Milovanovic,NVIDIA,True
774,Memory mapped datasource does not allow reading data beyond the mapped range,"When creating a memory mapped datasource, we optionally pass a range within the file that we want mapped. Primary use for this is to avoid memory mapping the entire file when using a byte_range option in CSV/JSON.
Because we often need data beyond the exact byte_range, the mapped source add padding to the mapped range. However, we cannot guarantee that the reads will fall into this range.
Currently the source does not read beyond the mapped range and this can lead to incorrect output when the padding is not sufficient.
https://github.com/rapidsai/cudf/blob/branch-24.04/cpp/src/io/utilities/datasource.cpp#L163

Desired behavior: 
Memory mapped datasource should read from the file when the mapping is not sufficient instead of clamping the returned data to the mapped range.",2024-02-29T02:21:11Z,0,0,Vukasin Milovanovic,NVIDIA,True
775,[FEA] Provide type stubs for pylibcudf package,"**Is your feature request related to a problem? Please describe.**

As we build out the pylibcudf API, having type stubs for lsp and type-checking integration becomes increasingly useful.

**Describe the solution you'd like**

Provide type stubs (with docstrings) so that lsp/type-checker integration works.

We don't want to replicate docstrings in more than one place, I don't know if the right place for them is the type stub file.

**Describe alternatives you've considered**

Having the relevant pyx file open in an editor at the same time.

**Additional context**

There are a number of half-working auto-generation engines for stubs, but none of them seem to work that well, so we should probably just do this by hand.
",2024-02-29T12:38:48Z,0,0,Lawrence Mitchell,,False
776,[FEA] Implement `closed=` parameter for rolling window,"The `closed=` parameter to rolling windows is currently unsupported:

https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rolling.html

",2024-02-29T15:09:37Z,0,0,Ashwin Srinath,Voltron Data,False
777,[FEA] Reduce arrow library dependencies in cudf,"**Is your feature request related to a problem? Please describe.**
Arrow is in general a difficult dependency to work with, increasing build system complexity and fragility on its own while simultaneously expanding the full dependency tree, which particularly complicates use cases like conda where it leads to meaningful constraints on core system packages like protobuf, abseil, or the AWS SDK. This often hinders developer velocity when builds or CI are broken, but can also have far-reaching impacts when it creates problems with installation or running in specific environments. To prevent this, we would like to reduce or remove our dependence on Arrow libraries entirely.

Currently cudf makes use of Arrow in various ways at different levels of the stack. The primary uses of Arrow boil down to interop with host Arrow data and I/O with specific types of files. This involves interaction at both the Python layer via pyarrow, at the Cython layer (also via pyarrow), and in C++. Both Cython and C++ interactions are particularly problematic because they involve C-level interactions, which sets ABI-level constraints that are significantly tighter than we would like while also significantly complicating build (CMake, Python builds) and packaging (narrow Arrow version support ranges leading to limited support of other packages in the dependency tree). Python interactions are generally less difficult to work around, especially since Python code can be written to dynamically adapt to the pyarrow version.

**Describe the solution you'd like**

We should look to remove the Arrow dependencies from the various layers of cudf (Java, Python, Cython, C++) to the greatest extent possible, ideally entirely.

For Arrow Array interop code, this can be accomplished by using the Arrow C Data Interface (see #5097), which provides an ABI-stable way to interchange Arrow data without directly using Arrow libraries. To make this even easier, the [nanoarrow](https://github.com/apache/arrow-nanoarrow) library was created to support clients that wish to produce or interpret [Arrow C Data](https://arrow.apache.org/docs/format/CDataInterface.html) and [Arrow C Streams](https://arrow.apache.org/docs/format/CStreamInterface.html) structures, without having to include a dependency on libarrow. We can make use of that (see also #13678 which discusses this in depth). For Python interaction we can use [Arrow's pycapsule interface](https://arrow.apache.org/docs/format/CDataInterface/PyCapsuleInterface.html), which provides a standard way to interchange this data from Python. We can write Cython code leveraging this interface to get Arrow C Data from pyarrow objects without relying directly on pyarrow's Cython, therefore also allowing us to remove this dependency from the Cython layer.

For I/O, the question is a bit trickier. We currently have limited usage of libarrow headers in our C++, and those features largely exist only for Python support for reading Arrow's NativeFiles. We could in principle remove those from the C++ entirely, which would in turn allow us to remove libarrow as a dependency of libcudf. However, libcudf tests would still need libarrow (removing that dependency would require significant additional work). Moreover, those features would still be used by cudf Cython, so we would just be limiting the dependency. However, this could at least allow us to remove Arrow as a build-time dependency for both libcudf and the low-level pylibcudf Python API (#13921) that we are currently developing, which would still be a significant improvement since it would avoid imposing the Arrow dependency on low-level consumers of our APIs at the Python level. Then we could come back to working on replacing the cudf Cython usage.

Based on the above, the current plan is the following:
1. Remove libarrow as a dependency of libcudf/pylibcudf:
    a. Remove the compiled parts of `arrow_io_source.cpp` and make `arrow_io_source.hpp` a standalone header not compiled by anything in libcudf.
    b. Rewrite cudf Cython to use the arrow headers directly.
    c. Add new interop code that uses the Arrow C Data interface (see #15047)
    d. Rewrite Python interop code to call through to the new interfaces
    e. Remove the old Cython bindings for interop
2. Remove pyarrow Cython linkages from cudf Cython
    a. This will require some exploration as to how we can maintain performant file reading. We may have to implement our own minimal version of something like Arrow's NativeFile reader interface.
    b. Once the above is done, we'll need to rewrite cuIO C++ to consume this interface and remove the current functions.
3. Rewrite libcudf tests to remove libarrow dependence.
    a. This will require further investigation into how tests could be rewritten without Arrow. One possibility would be rewriting these tests as pylibcudf tests (see #15133) that use pyarrow instead (only the Python API, no Cython). That would give us access to the same functionality without tying us to linking to the libarrow library

**Additional context**

Code pointers where libarrow is used in 24.04
| Source file | Arrow include | Notes |
|---|---|---|
| `detail/interop.hpp` | `api.h` | `to_arrow_array` uses many array classes: `arrow::*Array`, `arrow::TimeUnit::*`, `arrow::*Type` also `arrow::MemoryPool`, `arrow::Scalar`, `arrow::Table`. I believe all of these are covered by nanoarrow |
| `include/cudf/interop.hpp`  | `api.h` | uses `arrow::Table`, `arrow::MemoryPool`, `arrow::default_memory_pool`, `arrow::Scalar`. I believe all of these are covered by nanoarrow |
| `include/cudf/io/arrow_io_source.hpp` | `filesystem/filesystem.h` <br> `io/interfaces.h`  | uses `arrow::io:RandomAccessFile`, `arrow::fs::FileSystem`. See #13698 for the work to refactor `arrow_io_source` out of `datasource` |
| `include/cudf/io/arrow_io_source.cpp` | `buffer.h` <br> `filesystem/filesystem.h` <br> `result.h`  | uses `arrow::Buffer`, `arrow::fs::FileSystemFromUri`, |
| `src/io/utilities/datasource.cpp` | `io/memory.h` | to be solved by #15189 |

| Test file | Arrow include | Notes |
|---|---|---|
| `tests/interop/arrow_utils.hpp` | `util/bitmap_builders.h` for `arrow::internal::BytesToBits` | Also uses many arrow types such as: `arrow::Array`, `arrow:DictionaryArray`, `arrow::dictionary`, `arrow::Table`,  `arrow::Decimal128Builder`, `arrow::decimal`, `arrow::default_memory_pool`, `arrow::ListArray`, `arrow::list` , `arrow::Buffer`, `arrow::StringBuilder`, `arrow::StringArray` , `arrow::BooleanArray`, `arrow::BooleanBuilder` <br> needs research - can all of these references be migrated to nanoarrow? |
| `tests/io/arrow_io_source_test.cpp`  | `io/api.h`  <br> `filesystem/filesystem.h` <br> `filesystem/s3fs.h` <br> `util/config.h` | uses `arrow::fs::FileSystemFromUri`, `arrow::fs::EnsureS3Finalized` |
| `tests/io/json_test.cpp` | `io/api.h` | Uses `arrow::io::ReadableFile` as part of a test for reading from an `ArrowFileSource` |
| `tests/io/csv_test.cpp` | `io/api.h` | uses `arrow::io::ReadableFile` |
| `tests/quantiles/percentile_approx_test.cpp` | `util/tdigest.h` | uses `arrow::internal::TDigest`. presumably we could replace this with our own limited implementation |
",2024-02-29T17:13:26Z,0,0,Gregory Kimball,,False
778,[FEA] Be consistent in handling of default parameter values in pylibcudf,"**Is your feature request related to a problem? Please describe.**
Currently some pylibcudf APIs have default values for parameters while others do not. Defaults are a double-edged sword because while they are convenient, the underlying libcudf APIs also have defaults and the two could diverge.

**Describe the solution you'd like**
pylibcudf APIs should decide whether they offer defaults. The two options would be to either offer no defaults and require users to provide all parameters, or offer the same defaults as libcudf, in which case it would be the responsibility of pylibcudf devs to keep those defaults in sync with libcudf. 

**Describe alternatives you've considered**
There isn't really any cost to the defaults in pylibcudf diverging from those in libcudf, so as long as they're documented it might be fine to have defaults for user convenience and not be overly concerned about ensuring that they remain identical to libcudf's in perpetuity.",2024-02-29T20:36:13Z,0,0,Vyas Ramasubramani,@rapidsai,True
779,[FEA] Add Parquet-to-Arrow dictionary transcoding to the parquet reader,"**Is your feature request related to a problem? Please describe.**
Using a parquet reader option, we could allow the user to specify columns that they would like to receive as dictionary-encoded in the output table. For the specified columns, the Parquet reader would transcode multiple Parquet dictionary-encoded column chunks into an Arrow dictionary-encoded column. 

**Describe the solution you'd like**
### Part 1 - Confirm correct and efficient dictionary processing in libcudf ###
1. Add benchmarks for dictionary `encode` and `decode` with axes including data type, cardinality and row count. Add checks that data is correctly round-tripped through dictionary encoding and decoding.
2. Expand unit testing when using dictionary types for reductions, join keys, aggregation keys, aggregation values and other operations. Include string and numeric types as dictionary values. Please note that although libcudf can represent dictionaries of lists (needs to be checked), in Parquet only leaf values can be dictionary-encoded.
3. Expand benchmarks for dictionary operations.  As of 24.04 we only have a [dictionary reduction](https://github.com/rapidsai/cudf/blob/branch-24.04/cpp/benchmarks/reduction/dictionary.cpp) benchmarks on `int32` and `float` value types. Benchmarks should include strings data type and axes for varying cardinality and row count.
4. Consider signed int for index type. Revisit the int types that can be used as indices. Revisit compatibility differences between libcudf dictionary and Arrow dictionary.
5. Consider dropping the sorted key requirement for improved python compatibility. We use natural order of index today and we could add a mapping layer to indexes to stop constraining the indices.

### Part 2 - Parquet-to-Arrow dictionary transcoding ###
1. Estimate the performance of transcoding Parquet dictionary-encoded column chunks into arrow dictionary-encoded columns. Each Parquet dictionary-encoded column chunk with begins with a dictionary page. To create an Arrow-compliant dictionary column, we need to merge the values from the dictionary page in each column chunk into a single set of values for the arrow dictionary-encoded column. Then to generate the indices data, we need to re-map the indices from each column chunk against the indices in the combined values. 
2. Please note that [PLAIN_DICTIONARY](https://parquet.apache.org/docs/file-format/data-pages/encodings/#dictionary-encoding-plain_dictionary--2-and-rle_dictionary--8) encoding is deprecated in Parquet 2.0. To support the new default [RLE_DICTIONARY](https://parquet.apache.org/docs/file-format/data-pages/encodings/#dictionary-encoding-plain_dictionary--2-and-rle_dictionary--8), we will need to add a conversion step from Parquet bit-packed indices into Arrow fixed-width indices.
3. The parquet format allows different encodings for each column chunk within a column. In the case of dictionaries, the Parquet specification describes cases where PLAIN encoding will be mixed with DICTIONARY encoding, ""If the dictionary grows too big, whether in size or number of distinct values, the encoding will fall back to the plain encoding"". To support this case we would need to add special handling.

**Describe alternatives you've considered**
Use `dictionary::encode` to encode target columns immediately after materialization by the Parquet reader. This approach will realize the downstream benefits of dictionary encoding, at the cost of additional work in Parquet decode and dictionary encode. We would benefit from sample queries and profiles that compare materialized column versus dictionary column processing in libcudf workflows. Such profiles could be used to estimate the performance improvement from adding Parquet-to-Arrow dictionary transcoding to the libcudf Parquet reader.

### Part 3 - Introduce run-end encoded type in libcudf, and then add Parquet-to-Arrow run-length/run-end transcoding
The Parquet format supports a [run-length encoding / bit-packing hybrid](https://parquet.apache.org/docs/file-format/data-pages/encodings/#run-length-encoding--bit-packing-hybrid-rle--3) and this could be transcoded into a [run-end encoded](https://arrow.apache.org/docs/format/Columnar.html#run-end-encoded-layout) Arrow type. To begin this project, we need to add run-end encoding as a new type to libcudf, introduce decode and encode functions, confirm correctness across libcudf APIs and audit for performance hotspots. A run-end encoded type in libcudf would allow us to support ""constant"" or ""scalar"" columns as requested in #15308. If libcudf supported a run-end encoded type, transcoding into this type from Parquet run-length encoded data would not be a zero-copy operation and would require converting the Parquet bit-packed ""lengths"" to Arrow fixed-width ""ends"". 

",2024-03-01T00:10:23Z,0,0,Gregory Kimball,,False
780,[BUG] Add support for `force_ascii=False` when writing to JSON with cuDF engine,"**Describe the bug**
Ideally, we should eventually support `engine=""cudf""` and `force_ascii=False` together with `to_json`. For now, we should update the documentation and/or provide a warning for users.

**Steps/Code to reproduce bug**
```
import cudf

df = cudf.DataFrame({""a"": [1,2,3], ""b"": [""4"",""5"",""🌱""]})
df.to_json(""test.jsonl"", orient=""records"", lines=True, engine=""cudf"", force_ascii=False)
```

produces a `TypeError: write_json() got an unexpected keyword argument 'force_ascii'`.

I can do a `df.to_json(""test.jsonl"", orient=""records"", lines=True, force_ascii=False)` and see the emoji in the `.jsonl` file, and I can also do a `df.to_json(""test.jsonl"", orient=""records"", lines=True, engine=""cudf"")` and see the emoji represented as ""\ud83c\udf31"" in the `.jsonl` file. But I am unable to see the emoji represented as is in the file, while also writing with the cuDF engine.

**Environment details**
I tested this with the latest cuDF version.
",2024-03-01T22:06:31Z,1,0,Sarah Yurick,@NVIDIA,True
781,"[BUG] memcheck and racecheck errors in avro reader with `codec=""deflate""`","**Describe the bug**

```python
import cudf
import fastavro
import io

total_rows = num_rows = rows_per_block = 2048
total_bytes_per_block = rows_per_block * 7

records = [{""0"": f""{i:0>6}""} for i in range(total_rows)]
schema = {
    ""name"": ""root"",
    ""type"": ""record"",
    ""fields"": [
        {""name"": ""0"", ""type"": ""string""},
    ],
}

buffer = io.BytesIO()
fastavro.writer(buffer, schema, records, sync_interval=total_bytes_per_block, codec=""deflate"")
buffer.seek(0)

actual_df = cudf.read_avro(buffer, skiprows=0, num_rows=num_rows)
```

Extracted from `test_avro_reader_fastavro_integration.py::test_avro_reader_multiblock`.

Neither
```
compute-sanitizer --tool=memcheck python bug.py
```
nor
```
compute-sanitizer --tool=racecheck python bug.py
```

are clean.

Exemplar stack traces:

<details>
<summary> memcheck </summary>

```
========= COMPUTE-SANITIZER
========= Invalid __global__ read of size 1 bytes
=========     at 0x2080 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:807:cudf::io::process_symbols(cudf::io::inflate_state_s *, int)
=========     by thread (32,0,0) in block (0,0,0)
=========     Address 0x7f6078604cb3 is out of bounds
=========     and is 2,356 bytes after the nearest allocation at 0x7f6078601600 of size 11,648 bytes
=========     Device Frame:/home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:1109:void cudf::io::inflate_kernel<(int)128>(cudf::device_span<const cudf::device_span<const unsigned char, (unsigned long)18446744073709551615>, (unsigned long)18446744073709551615>, cudf::device_span<const cudf::device_span<unsigned char, (unsigned long)18446744073709551615>, (unsigned long)18446744073709551615>, cudf::device_span<cudf::io::compression_result, (unsigned long)18446744073709551615>, cudf::io::gzip_header_included) [0x6050]
=========     Saved host backtrace up to driver entry point at kernel launch time
=========     Host Frame: [0x332470]
=========                in /usr/lib/x86_64-linux-gnu/libcuda.so.1
=========     Host Frame: [0x14fb4]
=========                in /home/coder/.conda/envs/rapids/lib/libcudart.so.12
=========     Host Frame:cudaLaunchKernel [0x70aae]
=========                in /home/coder/.conda/envs/rapids/lib/libcudart.so.12
=========     Host Frame:/home/coder/.conda/envs/rapids/targets/x86_64-linux/include/cuda_runtime.h:216:cudaError cudaLaunchKernel<char>(char const*, dim3, dim3, void**, unsigned long, CUstream_st*) [0x12a5605]
=========                in /home/coder/cudf/cpp/build/release/libcudf.so
=========     Host Frame:/tmp/tmpxft_0003da43_00000000-6_gpuinflate.compute_90.cudafe1.stub.c:1:__device_stub__ZN4cudf2io14inflate_kernelILi128EEEvNS_11device_spanIKNS2_IKhLm18446744073709551615EEELm18446744073709551615EEENS2_IKNS2_IhLm18446744073709551615EEELm18446744073709551615EEENS2_INS0_18compression_resultELm18446744073709551615EEENS0_20gzip_header_includedE(cudf::device_span<cudf::device_span<unsigned char const, 18446744073709551615ul> const, 18446744073709551615ul>&, cudf::device_span<cudf::device_span<unsigned char, 18446744073709551615ul> const, 18446744073709551615ul>&, cudf::device_span<cudf::io::compression_result, 18446744073709551615ul>&, cudf::io::gzip_header_included) [0x12a4de6]
=========                in /home/coder/cudf/cpp/build/release/libcudf.so
=========     Host Frame:/tmp/tmpxft_0003da43_00000000-6_gpuinflate.compute_90.cudafe1.stub.c:4:void cudf::io::__wrapper__device_stub_inflate_kernel<128>(cudf::device_span<cudf::device_span<unsigned char const, 18446744073709551615ul> const, 18446744073709551615ul>&, cudf::device_span<cudf::device_span<unsigned char, 18446744073709551615ul> const, 18446744073709551615ul>&, cudf::device_span<cudf::io::compression_result, 18446744073709551615ul>&, cudf::io::gzip_header_included&) [0x12a4e1e]
=========                in /home/coder/cudf/cpp/build/release/libcudf.so
=========     Host Frame:/home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:1145:void cudf::io::inflate_kernel<128>(cudf::device_span<cudf::device_span<unsigned char const, 18446744073709551615ul> const, 18446744073709551615ul>, cudf::device_span<cudf::device_span<unsigned char, 18446744073709551615ul> const, 18446744073709551615ul>, cudf::device_span<cudf::io::compression_result, 18446744073709551615ul>, cudf::io::gzip_header_included) [0x12a5598]
=========                in /home/coder/cudf/cpp/build/release/libcudf.so
=========     Host Frame:/home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:1214:cudf::io::gpuinflate(cudf::device_span<cudf::device_span<unsigned char const, 18446744073709551615ul> const, 18446744073709551615ul>, cudf::device_span<cudf::device_span<unsigned char, 18446744073709551615ul> const, 18446744073709551615ul>, cudf::device_span<cudf::io::compression_result, 18446744073709551615ul>, cudf::io::gzip_header_included, rmm::cuda_stream_view) [0x12a49ef]
=========                in /home/coder/cudf/cpp/build/release/libcudf.so
=========     Host Frame:/home/coder/cudf/cpp/src/io/avro/reader_impl.cu:227:cudf::io::detail::avro::decompress_data(cudf::io::datasource&, cudf::io::detail::avro::metadata&, rmm::device_buffer const&, rmm::cuda_stream_view) [0x123db3c]
=========                in /home/coder/cudf/cpp/build/release/libcudf.so
=========     Host Frame:/home/coder/cudf/cpp/src/io/avro/reader_impl.cu:528:cudf::io::detail::avro::read_avro(std::unique_ptr<cudf::io::datasource, std::default_delete<cudf::io::datasource> >&&, cudf::io::avro_reader_options const&, rmm::cuda_stream_view, rmm::mr::device_memory_resource*) [0x123fa1f]
=========                in /home/coder/cudf/cpp/build/release/libcudf.so
=========     Host Frame:cudf::io::read_avro(cudf::io::avro_reader_options const&, rmm::mr::device_memory_resource*) [0x13019ee]
=========                in /home/coder/cudf/cpp/build/release/libcudf.so
=========     Host Frame: [0x2ba3c]
=========                in /home/coder/.conda/envs/rapids/lib/python3.10/site-packages/cudf/_lib/avro.cpython-310-x86_64-linux-gnu.so
=========     Host Frame: [0x2d29f]
=========                in /home/coder/.conda/envs/rapids/lib/python3.10/site-packages/cudf/_lib/avro.cpython-310-x86_64-linux-gnu.so
=========     Host Frame:/usr/local/src/conda/python-3.10.13/Python/ceval.c:4181:_PyEval_EvalFrameDefault [0x139022]
=========                in /home/coder/.conda/envs/rapids/bin/python
=========     Host Frame:/usr/local/src/conda/python-3.10.13/Objects/call.c:342:_PyFunction_Vectorcall [0x1448cc]
=========                in /home/coder/.conda/envs/rapids/bin/python
=========     Host Frame:/usr/local/src/conda/python-3.10.13/Python/ceval.c:4231:_PyEval_EvalFrameDefault [0x1357dc]
=========                in /home/coder/.conda/envs/rapids/bin/python
=========     Host Frame:/usr/local/src/conda/python-3.10.13/Python/ceval.c:5067:_PyEval_Vector [0x1d7870]
=========                in /home/coder/.conda/envs/rapids/bin/python
=========     Host Frame:/usr/local/src/conda/python-3.10.13/Python/ceval.c:1135:PyEval_EvalCode [0x1d77b7]
=========                in /home/coder/.conda/envs/rapids/bin/python
=========     Host Frame:/usr/local/src/conda/python-3.10.13/Python/pythonrun.c:1292:run_eval_code_obj [0x207d1a]
=========                in /home/coder/.conda/envs/rapids/bin/python
=========     Host Frame:/usr/local/src/conda/python-3.10.13/Python/pythonrun.c:1313:run_mod [0x203123]
=========                in /home/coder/.conda/envs/rapids/bin/python
=========     Host Frame:/usr/local/src/conda/python-3.10.13/Python/pythonrun.c:1208:pyrun_file.cold [0x9a4d1]
=========                in /home/coder/.conda/envs/rapids/bin/python
=========     Host Frame:/usr/local/src/conda/python-3.10.13/Python/pythonrun.c:456:_PyRun_SimpleFileObject [0x1fd60e]
=========                in /home/coder/.conda/envs/rapids/bin/python
=========     Host Frame:/usr/local/src/conda/python-3.10.13/Python/pythonrun.c:90:_PyRun_AnyFileObject [0x1fd1a4]
=========                in /home/coder/.conda/envs/rapids/bin/python
=========     Host Frame:/usr/local/src/conda/python-3.10.13/Modules/main.c:670:Py_RunMain [0x1fa39b]
=========                in /home/coder/.conda/envs/rapids/bin/python
=========     Host Frame:/usr/local/src/conda/python-3.10.13/Modules/main.c:1091:Py_BytesMain [0x1cae17]
=========                in /home/coder/.conda/envs/rapids/bin/python
=========     Host Frame: [0x29d90]
=========                in /usr/lib/x86_64-linux-gnu/libc.so.6
=========     Host Frame:__libc_start_main [0x29e40]
=========                in /usr/lib/x86_64-linux-gnu/libc.so.6
=========     Host Frame: [0x1cad11]
=========                in /home/coder/.conda/envs/rapids/bin/python
========= 
```

</details>

<details>
<summary> racecheck </summary>

```
========= COMPUTE-SANITIZER
========= Error: Race reported between Read access at 0xe00 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:789:cudf::io::process_symbols(cudf::io::inflate_state_s *, int)
=========     and Write access at 0x1930 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:543:cudf::io::decode_symbols(cudf::io::inflate_state_s *) [16132 hazards]
=========     and Write access at 0x5660 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:661:cudf::io::decode_symbols(cudf::io::inflate_state_s *) [16156 hazards]
========= 
========= Error: Race reported between Write access at 0xd90 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:957:cudf::io::prefetch_warp(volatile cudf::io::inflate_state_s *, int)
=========     and Read access at 0x33c0 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:590:cudf::io::decode_symbols(cudf::io::inflate_state_s *) [1144 hazards]
=========     and Read access at 0x5250 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:642:cudf::io::decode_symbols(cudf::io::inflate_state_s *) [6592 hazards]
========= 
========= Error: Race reported between Read access at 0x810 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:954:cudf::io::prefetch_warp(volatile cudf::io::inflate_state_s *, int)
=========     and Write access at 0x59c0 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:665:cudf::io::decode_symbols(cudf::io::inflate_state_s *) [1032 hazards]
========= 
========= Error: Race reported between Read access at 0xa70 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:784:cudf::io::process_symbols(cudf::io::inflate_state_s *, int)
=========     and Write access at 0x5930 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:663:cudf::io::decode_symbols(cudf::io::inflate_state_s *) [1028 hazards]
=========     and Write access at 0x5f90 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:671:cudf::io::decode_symbols(cudf::io::inflate_state_s *) [4 hazards]
========= 
========= Error: Race reported between Write access at 0x11c0 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:793:cudf::io::process_symbols(cudf::io::inflate_state_s *, int)
=========     and Read access at 0xf90 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:523:cudf::io::decode_symbols(cudf::io::inflate_state_s *) [500 hazards]
=========     and Read access at 0x5dd0 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:670:cudf::io::decode_symbols(cudf::io::inflate_state_s *) [4 hazards]
========= 
========= Error: Race reported between Write access at 0xf60 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:962:cudf::io::prefetch_warp(volatile cudf::io::inflate_state_s *, int)
=========     and Read access at 0xdb0 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:522:cudf::io::decode_symbols(cudf::io::inflate_state_s *) [272 hazards]
========= 
========= Error: Race reported between Write access at 0x5d70 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:1104:void cudf::io::inflate_kernel<(int)128>(cudf::device_span<const cudf::device_span<const unsigned char, (unsigned long)18446744073709551615>, (unsigned long)18446744073709551615>, cudf::device_span<const cudf::device_span<unsigned char, (unsigned long)18446744073709551615>, (unsigned long)18446744073709551615>, cudf::device_span<cudf::io::compression_result, (unsigned long)18446744073709551615>, cudf::io::gzip_header_included)
=========     and Read access at 0x5d0 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:951:cudf::io::prefetch_warp(volatile cudf::io::inflate_state_s *, int) [8 hazards]
========= 
========= Warning: Race reported between Read access at 0x3b0 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:775:cudf::io::process_symbols(cudf::io::inflate_state_s *, int)
=========     and Write access at 0x3000 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:826:cudf::io::process_symbols(cudf::io::inflate_state_s *, int) [8 hazards]
========= 
========= Warning: Race reported between Read access at 0x31a0 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:1068:void cudf::io::inflate_kernel<(int)128>(cudf::device_span<const cudf::device_span<const unsigned char, (unsigned long)18446744073709551615>, (unsigned long)18446744073709551615>, cudf::device_span<const cudf::device_span<unsigned char, (unsigned long)18446744073709551615>, (unsigned long)18446744073709551615>, cudf::device_span<cudf::io::compression_result, (unsigned long)18446744073709551615>, cudf::io::gzip_header_included)
=========     and Write access at 0x4900 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:1081:void cudf::io::inflate_kernel<(int)128>(cudf::device_span<const cudf::device_span<const unsigned char, (unsigned long)18446744073709551615>, (unsigned long)18446744073709551615>, cudf::device_span<const cudf::device_span<unsigned char, (unsigned long)18446744073709551615>, (unsigned long)18446744073709551615>, cudf::device_span<cudf::io::compression_result, (unsigned long)18446744073709551615>, cudf::io::gzip_header_included) [4 hazards]
========= 
========= Error: Race reported between Read access at 0xe00 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:789:cudf::io::process_symbols(cudf::io::inflate_state_s *, int)
=========     and Write access at 0x1930 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:543:cudf::io::decode_symbols(cudf::io::inflate_state_s *) [16132 hazards]
=========     and Write access at 0x5660 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:661:cudf::io::decode_symbols(cudf::io::inflate_state_s *) [16156 hazards]
========= 
========= Error: Race reported between Write access at 0xd90 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:957:cudf::io::prefetch_warp(volatile cudf::io::inflate_state_s *, int)
=========     and Read access at 0x33c0 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:590:cudf::io::decode_symbols(cudf::io::inflate_state_s *) [1144 hazards]
=========     and Read access at 0x5250 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:642:cudf::io::decode_symbols(cudf::io::inflate_state_s *) [6592 hazards]
========= 
========= Error: Race reported between Read access at 0x810 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:954:cudf::io::prefetch_warp(volatile cudf::io::inflate_state_s *, int)
=========     and Write access at 0x59c0 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:665:cudf::io::decode_symbols(cudf::io::inflate_state_s *) [1032 hazards]
========= 
========= Error: Race reported between Read access at 0xa70 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:784:cudf::io::process_symbols(cudf::io::inflate_state_s *, int)
=========     and Write access at 0x5930 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:663:cudf::io::decode_symbols(cudf::io::inflate_state_s *) [1028 hazards]
=========     and Write access at 0x5f90 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:671:cudf::io::decode_symbols(cudf::io::inflate_state_s *) [4 hazards]
========= 
========= Error: Race reported between Write access at 0x11c0 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:793:cudf::io::process_symbols(cudf::io::inflate_state_s *, int)
=========     and Read access at 0xf90 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:523:cudf::io::decode_symbols(cudf::io::inflate_state_s *) [500 hazards]
=========     and Read access at 0x5dd0 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:670:cudf::io::decode_symbols(cudf::io::inflate_state_s *) [4 hazards]
========= 
========= Error: Race reported between Write access at 0xf60 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:962:cudf::io::prefetch_warp(volatile cudf::io::inflate_state_s *, int)
=========     and Read access at 0xdb0 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:522:cudf::io::decode_symbols(cudf::io::inflate_state_s *) [272 hazards]
========= 
========= Error: Race reported between Write access at 0x5d70 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:1104:void cudf::io::inflate_kernel<(int)128>(cudf::device_span<const cudf::device_span<const unsigned char, (unsigned long)18446744073709551615>, (unsigned long)18446744073709551615>, cudf::device_span<const cudf::device_span<unsigned char, (unsigned long)18446744073709551615>, (unsigned long)18446744073709551615>, cudf::device_span<cudf::io::compression_result, (unsigned long)18446744073709551615>, cudf::io::gzip_header_included)
=========     and Read access at 0x5d0 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:951:cudf::io::prefetch_warp(volatile cudf::io::inflate_state_s *, int) [8 hazards]
========= 
========= Warning: Race reported between Read access at 0x3b0 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:775:cudf::io::process_symbols(cudf::io::inflate_state_s *, int)
=========     and Write access at 0x3000 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:826:cudf::io::process_symbols(cudf::io::inflate_state_s *, int) [8 hazards]
========= 
========= Warning: Race reported between Read access at 0x31a0 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:1068:void cudf::io::inflate_kernel<(int)128>(cudf::device_span<const cudf::device_span<const unsigned char, (unsigned long)18446744073709551615>, (unsigned long)18446744073709551615>, cudf::device_span<const cudf::device_span<unsigned char, (unsigned long)18446744073709551615>, (unsigned long)18446744073709551615>, cudf::device_span<cudf::io::compression_result, (unsigned long)18446744073709551615>, cudf::io::gzip_header_included)
=========     and Write access at 0x4900 in /home/coder/cudf/cpp/src/io/comp/gpuinflate.cu:1081:void cudf::io::inflate_kernel<(int)128>(cudf::device_span<const cudf::device_span<const unsigned char, (unsigned long)18446744073709551615>, (unsigned long)18446744073709551615>, cudf::device_span<const cudf::device_span<unsigned char, (unsigned long)18446744073709551615>, (unsigned long)18446744073709551615>, cudf::device_span<cudf::io::compression_result, (unsigned long)18446744073709551615>, cudf::io::gzip_header_included) [4 hazards]
========= 
========= RACECHECK SUMMARY: 18 hazards displayed (14 errors, 4 warnings)
```

</details>

I do not know if the racecheck warnings are as problematic as the memcheck ones, `gpuinflate.cu` is littered with `volatile` accesses to the inter-warp communication queue without (AFAICT) any synchronisation, but perhaps there are enough spin-waits that it is ""OK""?",2024-03-04T11:42:07Z,0,0,Lawrence Mitchell,,False
782,[FEA] Options to validate JSON fields,"**Is your feature request related to a problem? Please describe.**
Apache Spark optionally validates several things according to the JSON spec that the CUDF parser does not currently validate.  

https://www.json.org/json-en.html

The reason this is a problem is that Spark will return a null for any JSON input that violates the spec. So if there is a row where part of it is not valid we need a way to make sure that we return a null for that.

Ideally we want to do something like https://github.com/rapidsai/cudf/pull/14996 which is a huge performance win for us, or ask for CUDF to return nested types as strings for us. If CUDF does not do the validation in those cases we will not even see that data and end up returning an incorrect value. But even without this there is some string validation that involves escape sequences and we cannot validate it ourselves because CUDF has already processed the escape sequences in many cases.

There are a few places where CUDF is not validating the JSON, and it appears to really be in values.

According to the spec a value that is not a string, object, or array must be `true`, `false`, `null`, or a *number*. It appears that CUDF accepts most unquoted value as valid. Spaces in the middle of an entry appears to make it invalid.

Spark does not have any configs to enable/disable this type of validation.

Again according to the spec a *number* should match the regular expression ""^-?(?:(?:[1-9][0-9]*)|0)(?:\\.[0-9]+)?(?:[eE][\\-\\+]?[0-9]+)?$"" 
Spark does have a few options related to numbers.  
  1. They have an option to enable leading zeros, which changes the regular expression to look more like ""^-?[0-9]+(?:\\.[0-9]+)?(?:[eE][\\-\\+]?[0-9]+)?$"". This is not on by default so it is okay if CUDF does not try to support this, but I did want to call it out.
  2. Spark also has an option to include `NaN`, ""+INF"", ""-INF"", ""+Infinity"", ""Infinity"", and ""-Infinity"". Sadly this is on by default. Not sure if we could just include an allow list similar to how CSV handles boolean values.
 
According to the JSON spec a quoted string is allowed to only have a very small number of things that can be escaped with a backslash. `""`, `\`, `/`, `b`, `f`, `n`, `r`, `t`, and `u` followed by 4 hex digits.  Spark has a config to disable this and allows escaping of any character, including `\u` without the hex digits. As this is enabled by default in Spark we are fine if this check is not implemented, but I wanted to document it.

The JSON Spec also says that a quoted string cannot have ""control character"" in it. Here a control character appears to be anything between `\^@` and `\^_` inclusive. Spark does enforce this by default, but it varies by the JSON command used, like `get_json_object` has the check disabled. This is something that we eventually will need support for.


**Describe the solution you'd like**
I would like a few configs for the JSON reader that would let us pass in options to enable/disable validation based on things similar to what Spark does today.

We already support this more, or less for single quoted strings, and it would be great to extend it to include validation of numbers with/without leading zeros, and with/without an allow list of special cases; and validation of unescaped control characters.",2024-03-04T17:37:14Z,0,0,Robert (Bobby) Evans,Nvidia,True
783,Standardize docstring typing when pylibcudf is split out,"Currently the types used in pylibcudf docstrings are inconsistent in how they're namespaced, and they're often poorly linked in the rendered docs as a result. It's probably not worth fixing this until after we split pylibcudf into a separate library, though.",2024-03-05T21:15:42Z,0,0,Vyas Ramasubramani,@rapidsai,True
784,[BUG] Unable to update array column for subset of rows,"**Describe the bug**
Unable to update values of a dataframe for a column of type array for a subset of rows via `.loc` with a `list[list]`. 

Tested with versions: 24.02.02 & nightly 24.04.00a508

Updating the values for all rows works:
```py
df['a'] = [[0,0,0], [9, 10, 11], [20, 21, 22]]
```
however updating a subset of rows with a bool mask fails:
```py
df.loc[mask, 'a'] = new_values
```

I believe at least part of the problem is (python/cudf/cudf/core/column/lists.py) ~line 90:
```
    def __setitem__(self, key, value):
        if isinstance(value, list):
            value = cudf.Scalar(value)
        if isinstance(value, cudf.Scalar):
            if value.dtype != self.dtype:
                raise TypeError(""list nesting level mismatch"")
```
When the column is `ListDtype(int64)` and the incoming values look like `[[9, 10, 11], [20, 21, 22]]`
It will create a `cudf.Scalar` which will have a dtype of `ListDtype(ListDtype(int64))` .

Causing the `value.dtype != self.dtype` to fail.


**Steps/Code to reproduce bug**
```py
import os

import cupy
import numpy
import pandas as pd
import cudf

data = {'apple': ['pie', 'cake', 'candy'], 'a': [[3,2,1], [4,5,6], [8,7,9]], 'b': [10, 20, 30]}


if os.environ.get('USE_PANDAS') != None:
    df = pd.DataFrame(data)
else:
    df = cudf.DataFrame(data)

mask = [False, True, True]
new_values = [[9, 10, 11], [20, 21, 22]]

# Other datatypes work
df.loc[mask, 'b'] = [25, 35]
df['b'].loc[mask] = [26, 36]
print(df)

print(f""array column type: {repr(df['a'].dtype)}"", flush=True)

try:
    df.loc[mask, 'a'] = new_values
except ValueError as e:
    print(f""Encoundered error setting new values ({e}) trying another way"")

    try:
        df['a'].loc[mask] = new_values
    except TypeError as e:
        print(f""Encoundered error setting new values ({e})"")

print(df)
```

**Expected behavior**
Update the values

**Environment overview (please complete the following information)**
 - Environment location: [Bare-metal]
 - Method of cuDF install: [conda]
",2024-03-05T21:16:00Z,0,0,David Gardner,@Nvidia,True
785,"[API]: ""Private"" (imported) Python APIs used in across other RAPIDS projects","This is the result of a quick audit and does not include private APIs on public objects e.g. `cudf.Series._column`

TLDR observations:

1. There seems to be a common desire to create a `Column` object (this might be solved with `pylibcudf`)?
2. There seems to be a secondary desire to create a `Buffer` object


**cuspatial**

`cudf.core.copy_types.BooleanMask` (for typing)
`cudf.core.copy_types.GatherMap` (for typing)
`cudf.core.column.ColumnBase`
`cudf.core.column.ListColumn`
`cudf.core.column.as_column`
`cudf.core.column.column_empty`
`cudf.core.buffer.acquire_spill_lock`

`from cudf._lib.column cimport Column, column`
`from cudf._lib.cpp.column.column cimport column`
`from cudf._lib.cpp.column.column_view cimport column_view`
`from cudf._lib.cpp.table.table_view cimport table_view`
`from cudf._lib.cpp.table.table cimport table`
`from cudf._lib.utils cimport columns_from_table_view`
`from cudf._lib.utils cimport columns_from_unique_ptr`
`from cudf._lib.cpp.types cimport size_type`
`from cudf._lib.utils cimport table_view_from_table`

**cugraph**

`cudf.core.buffer.as_buffer`
`cudf.core.column.build_column` 
`cudf.core.column.column_empty`
`cudf.core.column.as_column`
`cudf.core.column.build_categorical_column`
`cudf.core.column.ListColumn`
`cudf._lib.transform.bools_to_mask`

**cuml**

`cudf.core.buffer.acquire_spill_lock`
`cudf.core.buffer.Buffer`
`cudf.utils.dtypes.min_signed_type`

**Morpheus**

`cudf.core.frame.Frame`
`cudf.core.index._index_from_data`
`cudf.core.subword_tokenizer.SubwordTokenizer`

`from cudf._lib.column cimport Column`
`from cudf._lib.cpp.io.types cimport table_with_metadata`
`from cudf._lib.cpp.table.table_view cimport table_view`
`from cudf._lib.cpp.types cimport size_type`
`from cudf._lib.utils cimport data_from_unique_ptr`
`from cudf._lib.utils cimport get_column_names`
`from cudf._lib.utils cimport table_view_from_table`

**Merlin**

`cudf.tests.utils.assert_eq`

**cuopt**

`cudf.core.buffer.as_buffer`
`cudf.core.column.build_column`

",2024-03-06T18:32:51Z,0,0,Matthew Roeschke,@rapidsai ,True
786,"[QST] Returning from multi-thread. TypeError: a bytes-like object is required, not 'dict'","When running my code with `cudf`, I got `TypeError: a bytes-like object is required, not 'dict'` in the multi-thread returning part.
1. Running the code without `-m cudf.pandas` option is *fine*.
2. It's *okay* if each multi-thread branch returns merely a scalar.
3. Program **CRUSHES** if a multi-thread branch returns a dataframe.

This is the code message:
```
concurrent.futures.process._RemoteTraceback:
'''
Traceback (most recent call last):
  File ""/usr/lib64/python3.9/concurrent/futures/process.py"", line 387, in wait_result_broken_or_wakeup
    result_item = result_reader.recv()
  File ""/usr/lib64/python3.9/multiprocessing/connection.py"", line 255, in recv
    return _ForkingPickler.loads(buf.getbuffer())
  File ""/usr/local/lib64/python3.9/site-packages/cudf/pandas/fast_slow_proxy.py"", line 742, in __setstate__
    unpickled_wrapped_obj = pickle.loads(state)
TypeError: a bytes-like object is required, not 'dict'
'''

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/usr/lib64/python3.9/runpy.py"", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/usr/lib64/python3.9/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/usr/local/lib64/python3.9/site-packages/cudf/pandas/__main__.py"", line 91, in <module>
    main()
  File ""/usr/local/lib64/python3.9/site-packages/cudf/pandas/__main__.py"", line 87, in main
    runpy.run_path(args.args[0], run_name=""__main__"")
  File ""/usr/lib64/python3.9/runpy.py"", line 288, in run_path
    return _run_module_code(code, init_globals, run_name,
  File ""/usr/lib64/python3.9/runpy.py"", line 97, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File ""/usr/lib64/python3.9/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""clean_header.py"", line 48, in <module>
    main()
  File ""clean_header.py"", line 45, in main
    my_func()
  File ""clean_header.py"", line 39, in my_func
    for obj in r:
  File ""/usr/lib64/python3.9/concurrent/futures/process.py"", line 562, in _chain_from_iterable_of_lists
    for element in iterable:
  File ""/usr/lib64/python3.9/concurrent/futures/_base.py"", line 609, in result_iterator
    yield fs.pop().result()
  File ""/usr/lib64/python3.9/concurrent/futures/_base.py"", line 439, in result
    return self.__get_result()
  File ""/usr/lib64/python3.9/concurrent/futures/_base.py"", line 391, in __get_result
    raise self._exception
concurrent.futures.process.BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending.
```


Here is my code.
```
from datetime import datetime, timedelta, date
import numpy as np
import pandas as pd
from random import randint
import swifter
import json, sys, os
from cudf.pandas.module_accelerator import disable_module_accelerator

from functools import partial
from concurrent.futures import ProcessPoolExecutor as Pool
from multiprocessing import set_start_method


def data_generation(nRows: int):
################## unimportant, for reproducing purpose ###################
# This function generates the dataframe obj, which has 5 columns, and the data are sorted by WorkingDay and Minute ascendingly
    my_df = pd.DataFrame(data={'WorkingDay': ['2019-01-02', '2018-01-02', '2019-05-02', '2020-01-02', '2021-01-02'], 'name': ['albert', 'alex', 'alice', 'ben', 'bob'], 'Minute': ['09:00:00', '09:20:00', '08:00:00', '07:00:00', '09:30:00'], 'aaa': np.random.rand(5), 'bbb': np.    random.rand(5)})
    my_df = pd.concat([my_df for i in range(int(nRows/5))], axis=0)
    my_df['WorkingDay'] = my_df['WorkingDay'].map(lambda x: (date(randint(2010,2020), randint(1,4), randint(1,5))).strftime('%Y-%m-%d'))
    my_df['Minute'] = np.random.permutation(my_df['Minute'].values)
    my_df = my_df.sort_values(by=['WorkingDay', 'Minute'], inplace=False).reset_index(drop=True,inplace=False)
    return my_df

def my_func_single(branchIndex: int):
    my_df = data_generation(20-5*branchIndex)
# data generated
#############################################################################
    # The multi-thread return is problematic
#############################################################################
    #return my_df.shape[0]
    return my_df


def my_func():
    set_start_method('spawn')
    my_func_partial = partial(my_func_single)
    with Pool(max_workers=2) as pool:
        r = pool.map(my_func_partial, range(4))
    for obj in r:
        #print('df has length: {}.'.format(obj))
        print('df has length: {}.'.format(obj.shape[0]))

def main():
    print('-------------------- program starts -----------------------')
    my_func()

if __name__ == '__main__':
    main()
```

Relevant dependencies:
```
cuda-python==12.4.0
cudf-cu12==24.4.0a516
cugraph-cu12==24.4.0a69
cuml-cu12==24.4.0a37
dask==2024.1.1
dask-cuda==24.4.0a11
dask-cudf-cu12==24.4.0a516
pylibcugraph-cu12==24.4.0a69
pylibraft-cu12==24.4.0a70
```
",2024-03-07T07:19:32Z,0,0,,,False
787,[FEA] Couple null mask and null count in all APIs,"**Is your feature request related to a problem? Please describe.**
We currently have APIs that accept a null mask and a null count as parameters. Historically, these APIs made sense because the count was loosely coupled to the mask in the sense that it could be omitted and it would be inferred. As of #13372, this is no longer possible and the null count must be known when a column is constructed. Therefore, allowing the null mask and the null count to be provided separately is no longer possible. From an API design perspective, it would make more sense to have them tightly coupled.

**Describe the solution you'd like**
We should update all APIs involving a mask and count to instead accept the two as a pair (or as a simple POD struct if we prefer) that encodes both. This change would more clearly signal to users that the two must be provided and used together.

**Describe alternatives you've considered**
None

**Additional context**
Implementing this would effectively fix #13154, but it would be a breaking change to the existing API rather than adding an overload.

See https://github.com/rapidsai/cudf/pull/13311#discussion_r1188969201 for some of the discussion that originally sparked this idea.
",2024-03-08T18:41:37Z,1,0,Vyas Ramasubramani,@rapidsai,True
788,Nightly memcheck failure caused by compute-sanitizer bug,"**Describe the issue**
Nightly builds are failing due to memcheck errors in specific gtests. The error appears to be `compute-sanitizer` tool issue which has been opened as nvbug 4553815.
This issue is to document the issue while working on possible workarounds until the bug is fixed.

The 2 errors appear as follows:
```
[ RUN      ] NumericValueIteratorTest/1.non_null_iterator
========= Invalid __shared__ read of size 16 bytes
=========     at 0x9670 in void cub::CUB_200200_700_750_800_860_900_NS::DeviceReduceSingleTileKernel<cub::CUB_200200_700_750_800_860_900_NS::DeviceReducePolicy<short, unsigned int, thrust::minimum<void>>::Policy600, short *, short *, unsigned int, thrust::minimum<void>, short, short>(T2, T3, T4, T5, T6)
=========     by thread (0,0,0) in block (0,0,0)
=========     Address 0x8 is misaligned
=========     Saved host backtrace up to driver entry point at kernel launch time
=========     Host Frame: [0x331d50]
=========                in /usr/lib/x86_64-linux-gnu/libcuda.so.1
=========     Host Frame: [0x14fb4]
=========                in /usr/local/cuda/targets/x86_64-linux/lib/libcudart.so.12
=========     Host Frame:cudaLaunchKernel [0x70aae]
=========                in /usr/local/cuda/targets/x86_64-linux/lib/libcudart.so.12
=========     Host Frame:cudaError cub::CUB_200200_700_750_800_860_900_NS::DeviceReduce::Reduce<short*, short*, thrust::minimum<void>, short, int>(void*, unsigned long&, short*, short*, int, thrust::minimum<void>, short, CUstream_st*) [clone .isra.0] [0x2fa199]
=========                in /opt/conda/envs/test/bin/gtests/libcudf/ITERATOR_TEST
```

```
[ RUN      ] MinMaxReductionTest/0.MinMaxTypes
========= Invalid __shared__ read of size 16 bytes
=========     at 0x4310 in void cub::CUB_200200_700_750_800_860_900_NS::DeviceReduceSingleTileKernel<cub::CUB_200200_700_750_800_860_900_NS::DeviceReducePolicy<short, unsigned int, cudf::detail::cast_functor_fn<short, cudf::DeviceMin>>::Policy600, thrust::transform_iterator<thrust::identity<short>, thrust::transform_iterator<cudf::detail::value_accessor<short>, thrust::counting_iterator<int, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::use_default, thrust::use_default>, thrust::use_default, thrust::use_default>, short *, unsigned int, cudf::detail::cast_functor_fn<short, cudf::DeviceMin>, short, short>(T2, T3, T4, T5, T6)
=========     by thread (0,0,0) in block (0,0,0)
=========     Address 0x8 is misaligned
=========     Saved host backtrace up to driver entry point at kernel launch time
=========     Host Frame: [0x331d50]
=========                in /usr/lib/x86_64-linux-gnu/libcuda.so.1
=========     Host Frame: [0x14fb4]
=========                in /usr/local/cuda/targets/x86_64-linux/lib/libcudart.so.12
=========     Host Frame:cudaLaunchKernel [0x70aae]
=========                in /usr/local/cuda/targets/x86_64-linux/lib/libcudart.so.12
=========     Host Frame:cudaError cub::CUB_200200_700_750_800_860_900_NS::DeviceReduce::Reduce<thrust::transform_iterator<thrust::identity<short>, thrust::transform_iterator<cudf::detail::value_accessor<short>, thrust::counting_iterator<int, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::use_default, thrust::use_default>, thrust::use_default, thrust::use_default>, short*, cudf::detail::cast_functor_fn<short, cudf::DeviceMin>, short, int>(void*, unsigned long&, thrust::transform_iterator<thrust::identity<short>, thrust::transform_iterator<cudf::detail::value_accessor<short>, thrust::counting_iterator<int, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::use_default, thrust::use_default>, thrust::use_default, thrust::use_default>, short*, int, cudf::detail::cast_functor_fn<short, cudf::DeviceMin>, short, CUstream_st*) [clone .isra.0] [0x18950ae]
=========                in /opt/conda/envs/test/bin/gtests/libcudf/../../../lib/libcudf.so
=========     Host Frame:cudf::reduction::simple::detail::simple_reduction<short, short, cudf::reduction::detail::op::min>(cudf::column_view const&, std::optional<std::reference_wrapper<cudf::scalar const> >, rmm::cuda_stream_view, rmm::mr::device_memory_resource*)::{lambda()#2}::operator()() const [0x18984c3]
=========                in /opt/conda/envs/test/bin/gtests/libcudf/../../../lib/libcudf.so
=========     Host Frame:std::unique_ptr<cudf::scalar, std::default_delete<cudf::scalar> > cudf::reduction::simple::detail::simple_reduction<short, short, cudf::reduction::detail::op::min>(cudf::column_view const&, std::optional<std::reference_wrapper<cudf::scalar const> >, rmm::cuda_stream_view, rmm::mr::device_memory_resource*) [0x1898a70]
=========                in /opt/conda/envs/test/bin/gtests/libcudf/../../../lib/libcudf.so
=========     Host Frame:cudf::reduction::detail::min(cudf::column_view const&, cudf::data_type, std::optional<std::reference_wrapper<cudf::scalar const> >, rmm::cuda_stream_view, rmm::mr::device_memory_resource*) [0x187ea46]
=========                in /opt/conda/envs/test/bin/gtests/libcudf/../../../lib/libcudf.so
=========     Host Frame:decltype(auto) cudf::detail::aggregation_dispatcher<cudf::reduction::detail::reduce_dispatch_functor, cudf::reduce_aggregation const&>(cudf::aggregation::Kind, cudf::reduction::detail::reduce_dispatch_functor&&, cudf::reduce_aggregation const&) [0x193431e]
=========                in /opt/conda/envs/test/bin/gtests/libcudf/../../../lib/libcudf.so
=========     Host Frame:cudf::reduction::detail::reduce(cudf::column_view const&, cudf::reduce_aggregation const&, cudf::data_type, std::optional<std::reference_wrapper<cudf::scalar const> >, rmm::cuda_stream_view, rmm::mr::device_memory_resource*) [0x1934d71]
=========                in /opt/conda/envs/test/bin/gtests/libcudf/../../../lib/libcudf.so
=========     Host Frame:cudf::reduce(cudf::column_view const&, cudf::reduce_aggregation const&, cudf::data_type, rmm::mr::device_memory_resource*) [0x193583f]
=========                in /opt/conda/envs/test/bin/gtests/libcudf/../../../lib/libcudf.so
=========     Host Frame:std::pair<short, bool> ReductionTest<short>::reduction_test<short>(cudf::column_view const&, cudf::reduce_aggregation const&, std::optional<cudf::data_type>) [clone .constprop.0] [0x28ec47]
=========                in /opt/conda/envs/test/bin/gtests/libcudf/./REDUCTIONS_TEST
```

If these were real errors the should appear when running without `compute-sanitizer`.
The nvbug report includes a small reproducer that shows the error without any libcudf-specific code.

**Steps/Code to reproduce**

```
compute-sanitizer --tool memcheck gtests/ITERATOR_TEST --gtest_filter=NumericValueIteratorTest/1.non_null_iterator --rmm_mode=cuda
compute-sanitizer --tool memcheck gtests/REDUCTIONS_TEST--gtest_filter=MinMaxReductionTest/0.MinMaxTypes --rmm_mode=cuda

```
Note the failure only occurs on int16 (short) integer types when doing a min-reduction through CUB.

**Additional context**
The error occurs as follows on various `compute-sanitizer` versions:
```
2022.3.0    ok
2022.4.0    ok
2022.4.1    fail
2023.1.1    fail
2023.2.2.0  fail
2023.3.1    fail
```
In general, it fails only with 12.0 and above.

",2024-03-08T19:52:34Z,0,0,David Wendt,NVIDIA,True
789,[BUG] mixed_type_as_string throws exception for nested data with nested STRING schema request,"**Describe the bug**
This is very similar to https://github.com/rapidsai/cudf/issues/14239, and because that is not done, then it is fine for this to be a dupe of that.

In Spark we are handed a read schema and some JSON data. Our goal is to pull out the parts of the JSON data that match the read schema.  But for strings, this gets to be a little complicated, and any type can be coerced into a string. If the data is an array it is coerced into a string by converting the tokens to a JSON formatted string, if the data is a dict it is coerced into a string the same way. 

`mixed_types_as_string` was added in part to help make this happen, especially in the case of nested types. But that appears to only work at a top level column.

```
  std::string data = ""{\""data\"": {\""A\"": 0, \""B\"": 1}}\n{\""data\"": [1,0]}\n"";

  std::map<std::string, cudf::io::schema_element> data_types;
  std::map<std::string, cudf::io::schema_element> child_types;
  child_types.insert(std::pair{""LIST"", cudf::io::schema_element{cudf::data_type{cudf::type_id::STRING, 0}, {}}});
  data_types.insert(std::pair{""data"", cudf::io::schema_element{cudf::data_type{cudf::type_id::LIST, 0}, child_types}});

  cudf::io::json_reader_options in_options =
    cudf::io::json_reader_options::builder(cudf::io::source_info{data.data(), data.size()})
      .dtypes(data_types)
      .recovery_mode(cudf::io::json_recovery_mode_t::RECOVER_WITH_NULL)
      .normalize_single_quotes(true)
      .normalize_whitespace(true)
      .mixed_types_as_string(true)
      .keep_quotes(true)
      .lines(true);
  cudf::io::table_with_metadata result = cudf::io::read_json(in_options);
```

Throws an exception about trying to create a nested column using a fixed width column factory.

```
C++ exception with description ""CUDF failure at: .../cpp/include/cudf/column/column_factories.hpp:342: Invalid, non-fixed-width type."" thrown in the test body.
```",2024-03-08T22:02:39Z,1,0,Robert (Bobby) Evans,Nvidia,True
790,[FEA] Add shared memory hash map for low-cardinality aggregations,"**Is your feature request related to a problem? Please describe.**
libcudf aggregations show lower throughput when data cardinality is less than ~1000 distinct values. This is due to serializing atomic operations over a small range of global memory. We received some projections that use hash maps that begin in shared memory and then spill to global if they exceed a certain size. The projections indicate 2-10x speedup for cardinalities below 100.

![image](https://github.com/rapidsai/cudf/assets/12725111/f28d02c5-f107-4c4a-b44d-687094a0a7a8)
(Aggregation throughput data was collected for groupby max over 20M rows of int64 key and int64 payload, based on benchmarks introduced in https://github.com/rapidsai/cudf/pull/15134 and using A100 hardware. Projections were provided as speedup versus cardinality data and were applied to the A100 measured throughput to yield projected throughput.)

**Describe the solution you'd like**
We could provide an implementation that uses shared memory hash maps when cardinality is low. [Shared memory as storage](https://github.com/NVIDIA/cuCollections/blob/dev/examples/static_set/shared_memory_example.cu) is supported in [cuCollections](https://github.com/NVIDIA/cuCollections), so we could leverage this option to offer a higher throughput code path when cardinality is low.

As far as the API design, we could add an optional `cardinality` parameter to the `aggregate` API. When [hyperloglog](https://github.com/NVIDIA/cuCollections/pull/429) cardinality estimates are available in cuCollections, we may want to support cardinality estimates as well. Some open questions include:
* What is the throughput difference between hyperloglog and count distinct? We expect the memory footprint of hyperloglog to be much lower, but I don't believe throughput has had controlled measurements.
* If we accept cardinality estimates, what happens if the cardinality is underestimated and the shared memory hash map fails? 
* Does it make sense for column objects to track cardinality, or should the application layer track cardinality?


**Describe alternatives you've considered**
We aren't sure how common low cardinality aggregation keys are in customer workloads. Are there cases where cardinality will be known ahead of time, or will it always need to be computed or estimated before triggering the aggregation? Could we instrument NDS to log cardinality and row count before each aggregation node?

**Additional context**
We could also consider using shared memory hash maps for low-cardinality distinct-key joins. This optimization is mentioned in https://github.com/rapidsai/cudf/issues/14948.
",2024-03-08T22:50:19Z,0,0,Gregory Kimball,,False
791,DataFrame.pivot_table not supported in Cudf,"**Missing Pandas Feature Request**
A clear and concise summary of the pandas function(s) you'd like to be able run with cuDF.
DataFrame.pivot_table not supported in Cudf

**Profiler Output**
If you used the profiler in pandas accelerator mode, please provide the full output of your profiling report.
```
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓
┃ Function                  ┃ GPU ncalls ┃ GPU cumtime ┃ GPU percall ┃ CPU ncalls ┃ CPU cumtime ┃ CPU percall ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩
│ DataFrame.pivot_table     │ 0          │ 0.000       │ 0.000       │ 1          │ 0.076       │ 0.076       │
│ DataFrame.reset_index     │ 1          │ 0.003       │ 0.003       │ 0          │ 0.000       │ 0.000       │
│ merge                     │ 1          │ 1.164       │ 1.164       │ 0          │ 0.000       │ 0.000       │
│ DataFrame.drop_duplicates │ 1          │ 0.170       │ 0.170       │ 0          │ 0.000       │ 0.000       │
│ DataFrame                 │ 1          │ 0.000       │ 0.000       │ 0          │ 0.000       │ 0.000       │
│ DataFrame.__repr__        │ 1          │ 0.539       │ 0.539       │ 0          │ 0.000       │ 0.000       │
└───────────────────────────┴────────────┴─────────────┴─────────────┴────────────┴─────────────┴─────────────┘
```
Not all pandas operations ran on the GPU. The following functions required CPU fallback:

- DataFrame.pivot_table

**Additional context**
Add any other context, code examples, or references to existing implementations about the feature request here.
",2024-03-09T07:00:31Z,0,0,,,False
792,[FEA] simplify `page_state_s` and possibly other structures for specialized parquet kernels,"@vuule brought up the point here https://github.com/rapidsai/cudf/pull/15159#discussion_r1505126922 that `page_state_s` could be simplified for the fixed/fixed dictionary kernels, as they don't need all the members of this structure and it might help with shared memory use. 

I also think there are other shared structures that we should audit and clean up. I am filing this issue so we don't loose track of it.",2024-03-11T02:05:05Z,0,0,Alessandro Bellina,NVIDIA,True
793,[FEA] Implement `__hash__` and `__eq__` for pylibcudf Aggregation objecs.,"libcudf `Aggregation` objects have implementations of hash and equality. We should expose these in pylibcudf so we can put the aggregation objects into dictionaries correctly.

",2024-03-11T18:27:02Z,1,0,Lawrence Mitchell,,False
794,[FEA] Find a way to support String column input/fixup for JSON parsing,"**Is your feature request related to a problem? Please describe.**
In Spark we have a requirement to be able to pass in a column of strings and parse them as JSON. Ideally we would just pass this directly to CUDF, but none of the input formats really support this, and neither do any of the pre-processing steps that the JSON reader has put in for us. What we do today is first check to see if a line separator (carriage return) is in the data set. If there is one, then we throw an exception. If not, then we concat the lines together into a single buffer with a line separator in between the inputs. (we do some fixup for NULLs/empty rows too).

This has the problem that we throw an exception when we see a bad character in the data, which is valid for Spark to have in the data.

I think that there are a few options that we have to fix this kind of a problem.

1. Expose the API that removes unneeded white space. We could then remove the unneeded data from the buffer and replace any remaining line separators with '\n' because then they should only be in quoted strings. (we might need to do single quote normalization too because I am not sure which one comes first)
2. Provide a way to set a different line separator (Ideally something really unlikely to show up NUL \0). This would not fix the problem 100%, but it would make it super rare, and I would feel okay with a solution like this.
3. Do nothing and we just take the hit when we see a line with this in it. We would then have to pull back those lines to the CPU and process them on the CPU, and push them back to the GPU afterwards.

I personally like option 2, but I am likely to implement option 3 in the short term unless I hear from CUDF that this is simple to do and can be done really quickly.",2024-03-12T14:59:44Z,0,0,Robert (Bobby) Evans,Nvidia,True
795,[FEA] Support casting of LIST type to STRING in JSON,"**Is your feature request related to a problem? Please describe.**

This is a follow on issue for https://github.com/rapidsai/cudf/pull/14936#discussion_r1516854731

Really we just want to be able to request that a column or child column, be returned as a string and it works, even if the data is a nested type.",2024-03-12T16:23:15Z,0,0,Robert (Bobby) Evans,Nvidia,True
796,[BUG] JSON white space normalization removes too much for unquoted values,"**Describe the bug**
I don't consider this to be too critical, but it is a regression compared to not turning on white space normalization.

An unquoted JSON value is not allowed to have white space in the middle of it, but it looks like the white space normalization is cleaning it up, and converting it from invalid JSON to valid JSON, which makes some of my tests fail.

**Steps/Code to reproduce bug**

```
TEST_F(JsonWSNormalizationTest, Unquoted_Bad_Boolean_Spaces)
{
  std::string input  = R""({""A"": tr ue })"";
  // but it actually is {""A"":true}
  std::string output = R""({""A"":tr ue})"";
  run_test(input, output);
}

TEST_F(JsonWSNormalizationTest, Unquoted_Bad_Float_Spaces)
{
  std::string input  = R""({""A"": 1 . 0 })"";
  // but it actually is {""A"":1.0}
  std::string output = R""({""A"":1 . 0})"";
  run_test(input, output);
}

TEST_F(JsonWSNormalizationTest, Unquoted_Bad_Sci_Spaces)
{
  std::string input  = R""({""A"": 1 E 1 })"";
  // but it actually is {""A"":1E1}
  std::string output = R""({""A"":1 E 1})"";
  run_test(input, output);
}
```

The above tests fail, because there are too many white space values being removed.",2024-03-12T18:56:35Z,0,0,Robert (Bobby) Evans,Nvidia,True
797,[FEA] Accelerate conversion from `arrow::StringViewType` to `arrow::StringType` in libcudf interop,"
**Is your feature request related to a problem? Please describe.**
The Arrow 15 specification includes a definition of ""[arrow::StringViewType](https://arrow.apache.org/docs/cpp/api/datatype.html#classarrow_1_1_string_view_type)"" - an alternate representation of the ""[arrow::StringType](https://arrow.apache.org/docs/cpp/api/datatype.html#classarrow_1_1_string_type)"". You may find ""String view"" also referred to as [Umbra string](https://www.cidrdb.org/cidr2020/papers/p29-neumann-cidr20.pdf) or prefix string. 

A string view consists of two columns:
1. A column of 16 byte fixed-width elements. First 4 bytes contain the string size
* If size < 12, then the string is stored inline in the remaining 12 bytes (short string optimization)
* If size > 12, then the string is stored separately in the second column. Remaining 12 bytes are 8 bytes for pointer to the string + 4 bytes for the first 4 chars of the string
2. A column of characters storing the suffix strings

String view type enables some performance optimizations:
* ability to slice strings (e.g. `left(10)`) in place without a copy
* ability to replace with smaller strings (e.g. `replace(""aa"", ""a"")`) in place without a copy
* inlined strings can be written in any order and without knowing the column size
* better memory access patterns for the first 4 bytes (e.g. `startswith(""a"")`)

**Describe the solution you'd like**
Let's add interop support for string view in `from_arrow` with CUDA C++ code to accept string views and convert them to libcudf strings columns. We may also want to add string view compatibility to `to_arrow`, so we can hand off libcudf strings columns to host libraries that expect string views. We should be able to write CUDA C++ code to efficiently transform `arrow::StringViewType` buffers in to `arrow::StringType` buffers.

**Describe alternatives you've considered**
Force libcudf users to convert their string views into strings on the host before passing the data to the device.

**Additional context**
Velox supports a string view type ([ref1](https://facebookincubator.github.io/velox/develop/vectors.html#flat-vectors-scalar-types), [ref2](https://engineering.fb.com/2024/02/20/developer-tools/velox-apache-arrow-15-composable-data-management/)), [Polars has switched](https://pola.rs/posts/polars-string-type/) to a string view representation, and [DuckDB supports](https://15721.courses.cs.cmu.edu/spring2023/slides/22-duckdb.pdf) string view.

We may choose to investigate using string views in libcudf at some point, but for the foreseeable future string view refactoring will be lower priority than [supporting large strings](https://github.com/rapidsai/cudf/issues/13733) and [improving performance with long strings](https://github.com/rapidsai/cudf/issues/13048).",2024-03-13T23:40:41Z,0,0,Gregory Kimball,,False
798,[FEA] Convert attribute access of pylibcudf objects from getters to properties,"**Is your feature request related to a problem? Please describe.**
Currently most pylibcudf object attributes are accessed via nullary getter methods that return attributes. The main reason for this is that it allows typing of these functions for convenience in Cython, but also because there is a small performance penalty associated with both the loss of typing and the use of a `def` function to make it a property. However, the tradeoff is that it results in unpythonic code when actually using pylibcudf, e.g. you have to do write things like `table.columns()[i]` instead of `table.columns[i]`. The tradeoff is almost certainly not worthwhile, especially since from a performance perspective the two should be nearly equivalent since Cython should handle properties fairly intelligently for us up to the actual function call. Note that for setters there would be some additional overhead for unboxing the value, but that's not relevant since essentially all properties in pylibcudf will be getters only.

**Describe the solution you'd like**
We should replace the getters with properties where appropriate.

If we are concerned with the performance difference in Cython, the other alternative would be to expose these members directly as `readonly` attributes. That would have the same effect in Python as making them properties with only setters, not getters. In Cython the attributes would still be freely writeable, but that is already true and the only ""protection"" we would be losing is the naming convention of an underscore indicating an internal value.",2024-03-14T01:09:13Z,0,0,Vyas Ramasubramani,@rapidsai,True
799,[FEA] Support a Scalar only column,"**Is your feature request related to a problem? Please describe.**

In an offline discussion with some people in CUDF we expressed how it could be a huge memory savings for the Spark team if we could get columns that we could put into a table/etc but they are really just a scalar with a count. We end up doing this all over the place in all kinds of different situations.

We don't currently use dictionary columns at all but we would be okay with that as an alternative if we could get dictionary columns working more broadly. But even then they still have issues.

In many cases they are more expensive to use for computation if they do work like concat two columns together requires merging the dictionary column instead of a simple memory operation. Yes, a concat of two scalar columns is going to likely be more expensive than two regular columns, but it should not be that bad, and might be faster than generating the fully columns and then concat-ing them.

It also is not always a win from a memory standpoint. With a DICTIONARY32 only values that are on average larger than a 32-bit value result in memory savings, for a scalar column replacement. This can get into really odd cases where an INT32 is not a win from a memory standpoint unless it is null (because the null would add 1 bit per row so 33 bits instead of just 32).

**Describe the solution you'd like**
Ideally https://github.com/rapidsai/cudf/blob/769c1bd6c05f3734044762c9efe3c65ef22cddbd/cpp/include/cudf/column/column_factories.hpp#L546 would just return this new type, or we could have a new API like is used to create a dictionary column from a scalar. 

Eventually we might be able to automatically do some things with them, like if we are reading parquet and determine that all of the values in the column are a single thing (like from a dictionary), then we could automatically replace them with a scalar column.",2024-03-14T21:27:50Z,0,0,Robert (Bobby) Evans,Nvidia,True
800,[FEA] JSON number normalization when returned as a string,"**Is your feature request related to a problem? Please describe.**
I am filing this to capture what Spark does, but it feels very Spark specific, which could be problematic. The solution here might be related to a solution to #15222 so we don't cause too much performance impact to others.

https://github.com/NVIDIA/spark-rapids/issues/10458 is the corresponding issue in the Spark Plugin and https://github.com/NVIDIA/spark-rapids/issues/10218 is related to it.

I don't really know 100% the solution I would like. This is where it gets to be kind of ugly/difficult.

When Spark processes JSON it parses the JSON into tokens, and then converts that back to a String when it is done. This results in things like numbers being converted to integers, doubles or java BigDecimal values, and then converted back to a String. For integers and BigDecimal values (numbers that do not include a decimal point or scientific notation) The processing is mostly a noop.

-0 becomes just 0. If there are any leading zeros on the number, then they are removed (but only if validations didn't already mark that as a problem #15222)

For floating point numbers it is more complicated, and I need to get some more specifics to put in here. The hard part is detecting overflow and converting the number to +/- Infinity. Conversion from scientific notation to regular floating point notation and back. Then there is also making sure that the number fits the actual floating point notation.

I almost want to have a way for me to provide my own code for this to happen, but I'm not sure if there is any good way to do that, because I am nervous that Spark will change some of these things over time.",2024-03-15T15:50:44Z,0,0,Robert (Bobby) Evans,Nvidia,True
801,[FEA] Update to CCCL 2.3 or 2.4,"Currently RAPIDS is built with CCCL 2.2. This issue lists tasks that we can follow up on once we have upgraded to CCCL 2.3 or 2.4. (We haven't decided on an exact timeline for updating, so RAPIDS could target CCCL 2.3 or 2.4 depending on that timing.)

### CCCL 2.3
- If we upgrade to 2.3, we will need a patch for https://github.com/NVIDIA/cccl/pull/1499 which will be fixed in 2.4. 
- CCCL 2.3 performance may be a motivating factor:
  - Up to 60% performance improvements of `cub::DeviceSelect::UniqueByKey`, `cub::DeviceScan::ExclusiveSumByKey`, and `cub::DeviceReduce::ReduceByKey` on A100. `cub::DeviceSegmentedReduce` now supports 64-bit indexing.
- Replace device uses of `thrust::optional` with `cuda::std::optional`
  - https://github.com/rapidsai/cudf/pull/15091#issuecomment-2004286213

### CCCL 2.4
- See notes on patch above

### Additional Context
- https://github.com/NVIDIA/cccl/releases
- Test PR: #14704


cc: @miscco @jrhemstad @robertmaynard ",2024-03-18T15:57:37Z,0,0,Bradley Dice,@NVIDIA @rapidsai,True
802,[JNI] Cleanup MemoryBuffer/ColumnVector close methods to use common code,"@gerashegalov had a comment in https://github.com/rapidsai/cudf/pull/15351 that I thought would make sense as a follow on. Specifically, we'd like to make a utility available in the JNI code so that classes that need to invoke `onClosed` can do so with the same common code.

              This is now inconsistent with (Host)ColumnVector. If we cannot consistently move eventHandler.onClosed into a finally please add a comment, but hopefully we can and it could be generalized via some utls withEventHandler(eventHandler) or base class implementation.

_Originally posted by @gerashegalov in https://github.com/rapidsai/cudf/pull/15351#discussion_r1532615825_
            ",2024-03-20T19:18:53Z,0,0,Alessandro Bellina,NVIDIA,True
803,[FEA] Consider exploring JIT compilation/LTO to replace AST evaluation,"**Is your feature request related to a problem? Please describe.**
The AST machinery is fundamentally limited in the performance that we can achieve. We should explore using JIT compilation/linking instead. 

**Describe the solution you'd like**

We'd explored JIT compilation in the past, but was too slow at the time. A number of things have changed since then:
- NVRTC has made significant improvements in runtime compilation (150ms -> 25ms fixed overhead)
- JIT LTO is a thing now
- NVRTC supports pre-compiled headers now

All of these things contribute to the potential for significantly faster runtime compilation.

The basic idea would be we pre-compile the mixed join kernel and treat the equality comparator like an extern function.

Then at runtime, we JIT compile only the comparator. Then we JIT LTO the comparator into the kernel to avoid the cost of the extern function call.

That way we aren't JIT compiling the entire kernel, which should further reduce the runtime cost.
We could even do this without forcing any user-facing changes. We could take the expression tree that a user gives us today and translate that into a string of C++ code that does the operation expressed by the AST.

Furthermore, by pre-compiling the relevant headers, we can further reduce the runtime costs.  

**Additional context**

There's potential for extending this idea beyond AST stuff.

Any of the places where we're currently dispatching to different comparator instantiations based on the presence of nested types would also be a prime target for JIT compilation/LTO.

The primary benefit there would mostly be compile time/binary size reduction to avoid statically instantiating as many independent code paths.

There could be opportunity for performance benefits as well by JIT compiling for only the exact types needed and eliminating the type dispatcher from the critical path in the row-based operators.",2024-03-21T16:44:54Z,0,0,Jake Hemstad,@NVIDIA,True
804,[BUG] Empty DataFrame object `columns` property doesn't match pandas for `data=None` or `data={}`.,"**Describe the bug**

When constructing an empty dataframe where one does not explicitly specify the column names, pandas produces a `RangeIndex` for the `.columns` property.

In contrast, cudf produces an `Index(dtype=object)` if `data={}` or `data=None`.

**Steps/Code to reproduce bug**

```python
import cudf
import pandas as pd

for data in [{}, None]:
    columns = cudf.DataFrame(data=data).columns
    expect = pd.DataFrame(data=data).columns

    assert type(columns) == type(expect)
```

**Expected behavior**

Matching pandas. This works if `data` is an empty list-like object (e.g. `data=[]`) so it's probably just another condition to handle.",2024-03-22T11:45:37Z,0,0,Lawrence Mitchell,,False
805,[QST] How can the performance of chunked reading in Parquet be improved?,"**What is your question?**
![image](https://github.com/rapidsai/cudf/assets/36735914/08a2c7ce-226d-4230-b2a3-ddecb0c5b92c)

I am working on a project to improve the performance of reading parquet files using the libcudf library. As shown in the Nsight Systems screenshot, the decompress_page_data event consumes the most time in the read_chunk operation, taking 46.877ms and 41.987ms out of 123.117ms, respectively. I am trying to reduce this decompression time but have found limited material, documentation, or GitHub issues on the subject. Do you have any suggestions? I am also considering using stream technology to accelerate the process but am unsure where to begin. Attached is my code for your reference. Thank you.
[parquet_chuncked.txt](https://github.com/rapidsai/cudf/files/14726563/parquet_chuncked.txt)
",2024-03-22T17:44:35Z,0,0,Guangyu Meng,University of Notre Dame,False
806,[BUG] ORC writer can't write files with more than 65535 row groups,"Some of the ORC writer kernels use the grid size of `(num_cols, num_row_groups)`. The Y dimension limit is 65535 so when trying to encode more row groups than that, the kernel fails to launch.
In addition, there are no checks so writer keeps going and end up writing garbage.

Proposed solution: 

- [ ] grid-stride or 1D kernels;
- [ ] Add `CUDF_CHECK_CUDA` checks so we at least catch launch failures.",2024-03-22T23:19:01Z,0,0,Vukasin Milovanovic,NVIDIA,True
807,[FEA] pandas DatetimeIndex.indexer_between_time,"I watched @shwina's GTC talk (https://register.nvidia.com/flow/nvidia/gtcs24/attendeeportaldigital/page/sessioncatalog/session/1695219773174001AmnA thanks Ashwin I really enjoyed it!)

**Is your feature request related to a problem? Please describe.**
No. I just noticed there wasn't an issue for [`DatetimeIndex.indexer_between_time`](https://github.com/pandas-dev/pandas/blob/main/pandas/core/indexes/datetimes.py#L764). I also enjoyed the user experience that `%%cudf.pandas.profile` points users to raise issues to highlight pandas API that falls back to CPU (https://github.com/rapidsai/cudf/blob/branch-24.06/python/cudf/cudf/pandas/profiler.py#L300).

**Describe the solution you'd like**
```
import pandas as pd
pd.date_range(""2023-01-01"", ""2023-01-02"", freq=""1h"").indexer_between_time(""09:00"", ""16:00"")
import cudf
cudf.date_range(""2023-01-01"", ""2023-01-02"", freq=""1h"").indexer_between_time(""09:00"", ""16:00"")
```

**Describe alternatives you've considered**
There may be a cudf work around in the meantime for a user who needs indexer_between_time that could be captured at https://docs.rapids.ai/api/cudf/stable/cudf_pandas/

**Additional context**
~~Could create a new issue template with the ""pandas"" label (https://github.com/rapidsai/cudf/issues?q=is%3Aopen+is%3Aissue+label%3Apandas) to be used at https://github.com/rapidsai/cudf/blob/branch-24.06/python/cudf/cudf/pandas/profiler.py#L297~~ fixed a bug at https://github.com/rapidsai/cudf/pull/15381
",2024-03-23T03:05:32Z,0,0,Ray Bell,DTN,False
808,"[BUG] In cudf.pandas mode, `.array` or `.values` don't actually return views to the underlying data","This seems a fundamental issue with the way cuDF is architected and possibly a `wontfix`, but it's important enough that we should consider solutions - and at the very least document the behaviour.

In pandas, `Series.values` (or `Series.array`) gives a reference to the underlying data as some kind of array-like object. Mutations to this object are reflected in the original `Series`:

```python

In [1]: import pandas as pd

In [2]: s = pd.Series([1, 2, pd.NA])

In [3]: a = s.array

In [4]: a
Out[4]:
<PandasArray>
[1, 2, <NA>]
Length: 3, dtype: object

In [5]: a[:2] = 3

In [6]: a
Out[6]:
<PandasArray>
[3, 3, <NA>]
Length: 3, dtype: object

In [7]: s
Out[7]:
0       3
1       3
2    <NA>
dtype: object
```

This doesn't always work when cudf.pandas is enabled:

```

In [1]: %load_ext cudf.pandas

In [2]: import pandas as pd

In [3]: s = pd.Series([1, 2, pd.NA])

In [4]: a = s.array  # this executes on CPU (because we don't support `.array` for null ints in cuDF)

In [5]: a
Out[5]:
<PandasArray>
[1.0, 2.0, nan]
Length: 3, dtype: float64

In [6]: s.max()  # this moves `s` from CPU to GPU, but `a` is still on CPU
Out[6]: 2.0

In [7]: a[:2] = 3  # this mutates `a`, but since `s` now lives on the GPU it doesn't see that mutation

In [8]: s  # `s` is unchanged
Out[8]:
0    1.0
1    2.0
2    NaN
dtype: float64

In [9]: a  # `a` is changed
Out[9]:
<PandasArray>
[3.0, 3.0, nan]
Length: 3, dtype: float64
```",2024-03-25T17:20:07Z,0,0,Ashwin Srinath,Voltron Data,False
809,[BUG] Array proxy in cudf.pandas don't include special casing for `ndarray.flat`,"The `arr.flat` attribute should return a `flatiter` object, but they currently return a generator. Unlike `flatiter`, generator objects cannot be written to:

```python
In [1]: %load_ext cudf.pandas

In [2]: import pandas as pd

In [3]: s = pd.Series([1,2 , 3])

In [4]: arr = s.values

In [5]: arr.flat
Out[5]: <generator object _maybe_wrap_result.<locals>.<genexpr> at 0x7f70a82cc310>
```

In contrast:

```python
import numpy as np

arr = np.arange(5)
print(type(arr.flat))
arr.flat[:3] = 100
print(arr)
<class 'numpy.flatiter'>
[100 100 100   3   4]
```

```python
import cupy as cp
arr = cp.arange(5)
print(type(arr.flat))
arr.flat[:3] = 100
print(arr)
<class 'cupy._indexing.iterate.flatiter'>
[100 100 100   3   4]
```",2024-03-25T19:06:52Z,0,0,Ashwin Srinath,Voltron Data,False
810,[FEA] Report the number of rows read per file in libcudf's Parquet reader ,"**Is your feature request related to a problem? Please describe.**
I wish libcudf's parquet reader reports the number of rows read per file.

Consider the following example, 
```c++
  std::vector<std::string> file_paths;  // defined elsewhere
  std::vector<std::string> column_names;  // defined elsewhere

  auto source  = cudf::io::source_info(file_paths);
  auto options = cudf::io::parquet_reader_options::builder(source);
  options.columns(column_names);
  auto result = cudf::io::read_parquet(options);
```

Here, `result` is of type [`table_with_metadata`](https://github.com/rapidsai/cudf/blob/branch-24.02/cpp/include/cudf/io/types.hpp#L249), but the metadata doesn't contain the number of rows read from each file. I wish libcudf can add this functionality.

**Describe the solution you'd like**
Report the number of rows read from each file in `table_with_metadata`.

**Describe alternatives you've considered**
I have tried `cudf::io::read_parquet_metadata` out-of-band, like the following snippet.

```c++
  std::vector<cudf::size_type> rows_per_file;
  rows_per_file.reserve(file_paths.size());

  for (auto const& file_path : file_paths) {
    auto file_source = cudf::io::source_info(file_path);
    auto metadata    = cudf::io::read_parquet_metadata(file_source);
    rows_per_file.push_back(metadata.num_rows());
  }
  result.rows_per_file = std::move(rows_per_file);
```

But this has nontrivial overhead in my use case. I believe we can get it for free as part of the Parquet reading process, since the Parquet reader needs to decode the file footers anyway.
",2024-03-26T06:34:07Z,0,0,,,False
811,[DOC] update CONTRIBUTING.md to mention devcontainers?,"**Suggested fix for documentation**
I learnt about the https://github.com/rapidsai/devcontainers from @dantegd's GTC talk (https://register.nvidia.com/flow/nvidia/gtcs24/attendeeportaldigital/page/sessioncatalog/session/1697766189600001T2p3) wonder if the build instructions in https://github.com/rapidsai/cudf/blob/branch-24.06/CONTRIBUTING.md could be updated to point to the devcontainers and how to use them.",2024-03-26T21:13:37Z,0,0,Ray Bell,DTN,False
812,[FEA] Improve support or failure modes for numpy and other libraries with C APIs in cudf.pandas,"Currently we proxy numpy.ndarray to ensure that cupy arrays are produced instead where possible. However, this behavior only partially addresses the possible ways to interact with numpy. Similarly to how we handle pandas in cudf.pandas, we may want to install a proxied library for numpy itself in such cases to ensure that we get the desired behavior.

Unfortunately, this is far more challenging with numpy than with pandas due to the fact that numpy exposes a C API. This issue is not unique to numpy, but is also present for other libraries (like torch) that rely on being able to translate Python objects from standard vocabulary types like numpy arrays down to C representations to leverage in their own C code. In general, our approach for proxying is incompatible with code that relies on converting Python objects into their C representations since we cannot mimic the latter, only the former.

We should collect cases where we observe failures like this and see if we can, at minimum, improve the ways in which the code fails. If possible, we can also try to come up with more robust strategies for consuming libraries to use such that they won't accidentally go down such bad code paths with cudf.pandas objects.

In an ideal world, we would come up with a solution that actually enables support for such use cases, but at present I don't see how we could manage doing so without doing something crazy like hooking every function call with `sys.setprofile`, and even if that worked (I'm not sure that it would) the cure might be worse than the disease because we'd most likely slow down every function call in Python enough to overcome any gains from using cudf.pandas.",2024-03-26T22:59:54Z,0,0,Vyas Ramasubramani,@rapidsai,True
813,[FEA] Support pandas flags in cudf.pandas,"**Is your feature request related to a problem? Please describe.**
pandas has a `set_flags` API that disallows duplicate labels from existing on a pandas object. Therefore, subsequent operations on the could/should (this feature isn't really maintained/thorough in pandas) fail if duplicate labels are introduced. 

```python
In [4]: import pandas

In [5]: df = pandas.DataFrame({""A"": [0, 1, 2, 3]}, index=[""x"", ""y"", ""X"", ""Y""]).set_flags(
   ...: 
   ...:     allows_duplicate_labels=False
   ...: 
   ...: )

In [6]: df.rename(str.upper)
DuplicateLabelError: Index has duplicates.
      positions
label          
X        [0, 2]
Y        [1, 3]
```

**Describe the solution you'd like**
It would be nice to add some support for this behavior in cudf.pandas, probably hidden behind the pandas compatibility mode option in cudf.

**Describe alternatives you've considered**
None

**Additional context**
The extra overhead of this mode of operation could be pretty high since every operation would need to be checked after the fact. It's probably still OK to include, but we should probably benchmark to understand the implications when we do it.",2024-03-27T00:24:32Z,0,0,Vyas Ramasubramani,@rapidsai,True
814,[FEA] Improved performance for strings finder_warp_parallel_fn / contains_warp_parallel_fn kernels,"**Is your feature request related to a problem? Please describe.**
A customer has a query that performs many string find/contains operations, often on long strings.  Nsight traces show most of the GPU time is being spent in finder_warp_parallel_fn or contains_warp_parallel_fn, significantly more than Parquet decompress and decode which are typically the top GPU kernels.

**Describe the solution you'd like**
Improved performance for these kernels.
",2024-03-27T19:11:15Z,0,0,Jason Lowe,NVIDIA,True
815,[FEA] Implement `hostdevice_2dspan`,"Similar to `hostdevice_span`, which can support viewing of a `hostdevice_vector`, we also need to have `hostdevice_2dspan` to view a `hostdevice_2dvector`. Having this will offer more flexibility especially in cuIO where 2d vectors are used a lot.

Note that some most important features that `hostdevice_2dspan` must have is the ability to transfer data between hot/device.",2024-03-31T20:07:56Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
816,"[BUG] RMM_LOGGING_LEVEL set to static string ""LIBCUDF_LOGGING_LEVEL"" ","Looking at the CXX flags passed in cmake @jlowe noticed that RMM_LOGGING_LEVEL is not being set correctly.

We see this:

```
-DRMM_LOGGING_LEVEL=LIBCUDF_LOGGING_LEVEL
```

The bug is in this line: https://github.com/rapidsai/cudf/blob/branch-24.06/cpp/CMakeLists.txt#L773:
```
target_compile_definitions(cudf PRIVATE ""RMM_LOGGING_LEVEL=LIBCUDF_LOGGING_LEVEL"")
```

where what was probably meant was:
```
target_compile_definitions(cudf PRIVATE ""RMM_LOGGING_LEVEL=${LIBCUDF_LOGGING_LEVEL}"")
```

Unless we are missing something. 

Also, do we need this define if we are [setting](https://github.com/rapidsai/cudf/blob/branch-24.06/cpp/CMakeLists.txt#L776) the SPDLOG flag already?

```
target_compile_definitions(cudf PUBLIC ""SPDLOG_ACTIVE_LEVEL=SPDLOG_LEVEL_${LIBCUDF_LOGGING_LEVEL}"")
```
",2024-04-01T14:45:29Z,0,0,Alessandro Bellina,NVIDIA,True
817,[BUG] cuDF JNI does not set RMM_LOGGING_LEVEL,"In cuDF we build with object files that reference RMM headers to create memory resource, such as the pool_memory_resource.

There is a flag that is not being set in cuDF JNI `RMM_LOGGING_LEVEL` that appears to be causing extra logs when the [new pinned memory pool](https://github.com/rapidsai/cudf/pull/15255) is exhausted (we run out of pinned memory). In this case, RMM logs at `error` level: `maximum pool size was exceeded`.

We'd like to find a solution for 24.04. We are looking at other ways of setting the flag from spark-rapids-jni as well in the mean time.",2024-04-01T14:50:38Z,0,0,Alessandro Bellina,NVIDIA,True
818,[BUG] Enable running `tests/io/parser/common/test_read_errors.py` in `cudf.pandas` tests,"**Describe the bug**
Investigate hangs in `tests/io/parser/common/test_read_errors.py` and enable running it in `cudf.pandas` tests.

This set of tests is being ignored here: https://github.com/rapidsai/cudf/blob/aab6137c80c50eccc5007120f7140cfe6646b5e0/python/cudf/cudf/pandas/scripts/run-pandas-tests.sh#L36C10-L36C52
",2024-04-01T22:02:48Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
819,[QST]Custom class with cuDF,"```
numba.core.errors.TypingError: Failed in cuda mode pipeline (step: nopython frontend)
Untyped global name 'Master10DH': Cannot determine Numba type of <class 'abc.ABCMeta'>

File ""ik_GPU.py"", line 9:
def ik(row):
    robot = Master10DH()
```
Hello, I encountered the following error, how can I fix it?",2024-04-03T11:26:31Z,0,0,,,False
820,[BUG] Unpickling objects with `pd.read_pickle()` doesn't work with cudf.pandas enabled,"**Describe the bug**
When `cudf.pandas` is enabled, we can pickle and unpickle objects using `pickle.dump/load` or `pickle.dumps/loads`. But if we choose to unpickle with `pd.read_pickle`, things go awry. Here's a minimal reproducer:

```python
import pandas as pd
from io import BytesIO
import pickle

pdf = pd.DataFrame({'a': [1.0, 2.0, None, 3.0]})

with open(""pickled_pdf.pkl"", ""wb"") as f:
    pickle.dump(pdf, f)

with open(""pickled_pdf.pkl"", ""rb"") as f:
    df = pd.read_pickle(f)

print(df)
```

<details>

```
In [1]: %load_ext cudf.pandas

In [2]: import pandas as pd

In [3]: from io import BytesIO
   ...: import pickle
   ...: 
   ...: pdf = pd.DataFrame({'a': [1.0, 2.0, None, 3.0]})
   ...: 
   ...: with open(""pickled_pdf.pkl"", ""wb"") as f:
   ...:     pickle.dump(pdf, f)
   ...: 
   ...: with open(""pickled_pdf.pkl"", ""rb"") as f:
   ...:     df = pd.read_pickle(f)
   ...: 
   ...: print(df)
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
File ~/mroeschke-cudf/python/cudf/cudf/pandas/fast_slow_proxy.py:888, in _fast_slow_function_call(func, *args, **kwargs)
    883 with nvtx.annotate(
    884     ""EXECUTE_FAST"",
    885     color=_CUDF_PANDAS_NVTX_COLORS[""EXECUTE_FAST""],
    886     domain=""cudf_pandas"",
    887 ):
--> 888     fast_args, fast_kwargs = _fast_arg(args), _fast_arg(kwargs)
    889     result = func(*fast_args, **fast_kwargs)

File ~/mroeschke-cudf/python/cudf/cudf/pandas/fast_slow_proxy.py:1007, in _fast_arg(arg)
   1006 seen: Set[int] = set()
-> 1007 return _transform_arg(arg, ""_fsproxy_fast"", seen)

File ~/mroeschke-cudf/python/cudf/cudf/pandas/fast_slow_proxy.py:934, in _transform_arg(arg, attribute_name, seen)
    932 if type(arg) is tuple:
    933     # Must come first to avoid infinite recursion
--> 934     return tuple(_transform_arg(a, attribute_name, seen) for a in arg)
    935 elif hasattr(arg, ""__getnewargs_ex__""):
    936     # Partial implementation of to reconstruct with
    937     # transformed pieces
    938     # This handles scipy._lib._bunch._make_tuple_bunch

File ~/mroeschke-cudf/python/cudf/cudf/pandas/fast_slow_proxy.py:934, in <genexpr>(.0)
    932 if type(arg) is tuple:
    933     # Must come first to avoid infinite recursion
--> 934     return tuple(_transform_arg(a, attribute_name, seen) for a in arg)
    935 elif hasattr(arg, ""__getnewargs_ex__""):
    936     # Partial implementation of to reconstruct with
    937     # transformed pieces
    938     # This handles scipy._lib._bunch._make_tuple_bunch

File ~/mroeschke-cudf/python/cudf/cudf/pandas/fast_slow_proxy.py:917, in _transform_arg(arg, attribute_name, seen)
    916 if isinstance(arg, (_FastSlowProxy, _FastSlowProxyMeta, _FunctionProxy)):
--> 917     typ = getattr(arg, attribute_name)
    918     if typ is _Unusable:

File ~/mroeschke-cudf/python/cudf/cudf/pandas/fast_slow_proxy.py:553, in _FastSlowProxy.__getattr__(self, name)
    550 if name.startswith(""_fsproxy""):
    551     # an AttributeError was raised when trying to evaluate
    552     # an internal attribute, we just need to propagate this
--> 553     _raise_attribute_error(self.__class__.__name__, name)
    554 if name in {
    555     ""_ipython_canary_method_should_not_exist_"",
    556     ""_ipython_display_"",
   (...)
    568     # This is somewhat delicate to the order in which IPython
    569     # implements special display fallbacks.

File ~/mroeschke-cudf/python/cudf/cudf/pandas/fast_slow_proxy.py:392, in _raise_attribute_error(obj, name)
    387 """"""
    388 Raise an AttributeError with a message that is consistent with
    389 the error raised by Python for a non-existent attribute on a
    390 proxy object.
    391 """"""
--> 392 raise AttributeError(f""'{obj}' object has no attribute '{name}'"")

AttributeError: 'function' object has no attribute '_fsproxy_fast'

During handling of the above exception, another exception occurred:

AttributeError                            Traceback (most recent call last)
<ipython-input-3-deda8b8b446c> in ?()
      8 
      9 with open(""pickled_pdf.pkl"", ""rb"") as f:
     10     df = pd.read_pickle(f)
     11 
---> 12 print(df)

~/mroeschke-cudf/python/cudf/cudf/pandas/fast_slow_proxy.py in ?(self, *args, **kwargs)
    836     def __call__(self, *args, **kwargs) -> Any:
--> 837         result, _ = _fast_slow_function_call(
    838             # We cannot directly call self here because we need it to be
    839             # converted into either the fast or slow object (by
    840             # _fast_slow_function_call) to avoid infinite recursion.

~/mroeschke-cudf/python/cudf/cudf/pandas/fast_slow_proxy.py in ?(func, *args, **kwargs)
    898             domain=""cudf_pandas"",
    899         ):
    900             slow_args, slow_kwargs = _slow_arg(args), _slow_arg(kwargs)
    901             with disable_module_accelerator():
--> 902                 result = func(*slow_args, **slow_kwargs)
    903     return _maybe_wrap_result(result, func, *args, **kwargs), fast

~/mroeschke-cudf/python/cudf/cudf/pandas/fast_slow_proxy.py in ?(fn, args, kwargs)
     29 def call_operator(fn, args, kwargs):
---> 30     return fn(*args, **kwargs)

~/miniforge3/envs/cudf-dev/lib/python3.11/site-packages/pandas/core/frame.py in ?(self)
   1199             self.info(buf=buf)
   1200             return buf.getvalue()
   1201 
   1202         repr_params = fmt.get_dataframe_repr_params()
-> 1203         return self.to_string(**repr_params)

~/miniforge3/envs/cudf-dev/lib/python3.11/site-packages/pandas/util/_decorators.py in ?(*args, **kwargs)
    329                     msg.format(arguments=_format_argument_list(allow_args)),
    330                     FutureWarning,
    331                     stacklevel=find_stack_level(),
    332                 )
--> 333             return func(*args, **kwargs)

~/miniforge3/envs/cudf-dev/lib/python3.11/site-packages/pandas/core/frame.py in ?(self, buf, columns, col_space, header, index, na_rep, formatters, float_format, sparsify, index_names, justify, max_rows, max_cols, show_dimensions, decimal, line_width, min_rows, max_colwidth, encoding)
   1361         """"""
   1362         from pandas import option_context
   1363 
   1364         with option_context(""display.max_colwidth"", max_colwidth):
-> 1365             formatter = fmt.DataFrameFormatter(
   1366                 self,
   1367                 columns=columns,
   1368                 col_space=col_space,

~/miniforge3/envs/cudf-dev/lib/python3.11/site-packages/pandas/io/formats/format.py in ?(self, frame, columns, col_space, header, index, na_rep, formatters, justify, float_format, sparsify, index_names, max_rows, min_rows, max_cols, show_dimensions, decimal, bold_rows, escape)
    443         bold_rows: bool = False,
    444         escape: bool = True,
    445     ) -> None:
    446         self.frame = frame
--> 447         self.columns = self._initialize_columns(columns)
    448         self.col_space = self._initialize_colspace(col_space)
    449         self.header = header
    450         self.index = index

~/miniforge3/envs/cudf-dev/lib/python3.11/site-packages/pandas/io/formats/format.py in ?(self, columns)
    552             cols = ensure_index(columns)
    553             self.frame = self.frame[cols]
    554             return cols
    555         else:
--> 556             return self.frame.columns

~/miniforge3/envs/cudf-dev/lib/python3.11/site-packages/pandas/core/generic.py in ?(self, name)
   6292             and name not in self._accessors
   6293             and self._info_axis._can_hold_identifiers_and_holds_name(name)
   6294         ):
   6295             return self[name]
-> 6296         return object.__getattribute__(self, name)

properties.pyx in ?()
---> 65 'Could not get source, probably due dynamically evaluated source code.'

~/miniforge3/envs/cudf-dev/lib/python3.11/site-packages/pandas/core/generic.py in ?(self, name)
   6292             and name not in self._accessors
   6293             and self._info_axis._can_hold_identifiers_and_holds_name(name)
   6294         ):
   6295             return self[name]
-> 6296         return object.__getattribute__(self, name)

AttributeError: 'DataFrame' object has no attribute '_mgr'
```
</details>
 
We can (and do) control what happens when objects are pickled and unpickled via the pickle protocol (`pickle.dump` and `pickle.load`) [here](https://github.com/rapidsai/cudf/blob/5192b608eeed4bda9317c657253c3a5630aa4c5d/python/cudf/cudf/pandas/fast_slow_proxy.py#L722-L741). 

And pandas' `read_pickle` does call the ""regular"" [`pickle.load` function](https://github.com/pandas-dev/pandas/blob/05ab1af783f6590b8a2d9fbea6d39793e88dfb04/pandas/io/pickle.py#L203). 

So what's going on?

When we call `pd.read_pickle` in `cudf.pandas` mode, that will first call `cudf.read_pickle` (doesn't exist) and then fall back to the real `pandas.read_pickle`. Importantly, during fallback, we [disable ourselves](https://github.com/rapidsai/cudf/blob/5192b608eeed4bda9317c657253c3a5630aa4c5d/python/cudf/cudf/pandas/fast_slow_proxy.py#L901). Which means that our special pickle protocol handling doesn't kick in and that messes everything up. 

### Solutions

The only solution I could think of is we vendor `pandas.read_pickle`, so we can keep ourselves enabled when it is called.",2024-04-03T20:24:39Z,0,0,Ashwin Srinath,Voltron Data,False
821,[BUG] Incorrect proxying of functions with no matching fast counterpart in cudf.pandas,"**Describe the bug**

Functions in the pandas source tree which do not have a matching counterpart in the cudf source tree are proxied with a `FunctionProxy` object whose `_fsproxy_fast` attribute is an `_Unusable` object.

Unfortunately, although accessing an `_Unusuable` object in a fast-slow chained method call fails, it does so too late and already provokes slow-to-fast and fast-to-slow copies. This ends up breaking the link between the fast and slow types inside a proxied object.

This raises its head particularly in the pandas test suite where there are functions that are used to parameterise over (for example) `iloc` vs `loc` indexing, like `pandas._testing.iloc`.

To see the problem consider the following:

```python
import cudf.pandas
cudf.pandas.install()

import pandas as pd

s = pd.Series(range(10))
s._fsproxy_state # => FAST
# pd._testing.iloc has no matching fast counterpart, so this function-call will provoke
# a fast to slow copy
indexer = pd._testing.iloc(s)
s._fsproxy_state # => SLOW
# We want setitem to keep the object as  slow,
# but this is a `_FastSlowAttribute` so it provokes (if it can) a slow-to-fast copy
getattr(indexer, ""__setitem__"")
s._fsproxy_state # => FAST
# Now we are in an inconsistent state.
```

In `_transform_arg` we have a carveout early exit if the fast or slow attribute we're asking for is  `_Unusable`, but not if it is an instance of `_Unusable`.

This patch helps a bit:
```patch
diff --git a/python/cudf/cudf/pandas/fast_slow_proxy.py b/python/cudf/cudf/pandas/fast_slow_proxy.py
index e811ba1351..9d07d236bb 100644
--- a/python/cudf/cudf/pandas/fast_slow_proxy.py
+++ b/python/cudf/cudf/pandas/fast_slow_proxy.py
@@ -915,7 +915,7 @@ def _transform_arg(
 
     if isinstance(arg, (_FastSlowProxy, _FastSlowProxyMeta, _FunctionProxy)):
         typ = getattr(arg, attribute_name)
-        if typ is _Unusable:
+        if typ is _Unusable or isinstance(typ, _Unusable):
             raise Exception(""Cannot transform _Unusable"")
         return typ
     elif isinstance(arg, types.ModuleType) and attribute_name in arg.__dict__:
```

But is observed to cause the pandas test suite run to take significantly longer (indicating, probably, more fast-to-slow transfers than necessary).

Note that this change works for `pd._testing.iloc` but _not_ `pd._testing.setitem` which is just the identity function, since wrapping the identity function produces a new function which is _not_ the identity.",2024-04-08T09:44:10Z,0,0,Lawrence Mitchell,,False
822,[BUG] cudf.read_parquet takes too much time(due to cudaMallocHost overhead etc.) to load the zstd compressed parquet files with few thousands to millions of rows,"**Describe the bug**
Performance improvement proposal for cudf parquet file reading efficiency.

**Steps/Code to reproduce bug**

```python
import pandas as pd

df = pd.DataFrame({'jnac': [None] * 1000})
df.to_parquet('/dev/shm/jnac.parquet', compression='ZSTD')

# cd to /dev/shm now

import cudf
import pandas
import pyarrow.parquet

import time

# not accurate timing, while the diff is so obvious which do not require more accurate timing temporrally

ts = time.time(); tb = cudf.read_parquet('/dev/shm/jnac.parquet'); te = time.time()
time.sleep(1)
ts = time.time(); tb = cudf.read_parquet('/dev/shm/jnac.parquet'); te = time.time()
print(te - ts)

ts = time.time(); tb = pandas.read_parquet('/dev/shm/jnac.parquet'); te = time.time()
time.sleep(1)
ts = time.time(); tb = pandas.read_parquet('/dev/shm/jnac.parquet'); te = time.time()
print(te - ts)

ts = time.time(); tb = pyarrow.parquet.read_table('/dev/shm/jnac.parquet'); te = time.time()
time.sleep(1)
ts = time.time(); tb = pyarrow.parquet.read_table('/dev/shm/jnac.parquet'); te = time.time()
print(te - ts)
```

**Expected behavior**

```python
>>> ts = time.time(); tb = cudf.read_parquet('jnac.parquet'); te = time.time()
>>> print(te - ts)
0.006829023361206055
>>>
>>> ts = time.time(); tb = pandas.read_parquet('jnac.parquet'); te = time.time()
>>> time.sleep(1)

>>> ts = time.time(); tb = pandas.read_parquet('jnac.parquet'); te = time.time()
>>> print(te - ts)
0.003950357437133789
>>>
>>> ts = time.time(); tb = pyarrow.parquet.read_table('jnac.parquet'); te = time.time()
>>> time.sleep(1)
>>> ts = time.time(); tb = pyarrow.parquet.read_table('jnac.parquet'); te = time.time()
>>> print(te - ts)
0.0013420581817626953
>>>

```

**Environment overview (please complete the following information)**
internal T4 node, py3.9, cudf 24.02.02


**Additional context**

It just takes too much time to process <NA> entries, especially for cudf when num rows is just 1K(similar latency cost for 10M rows NA though).
",2024-04-08T10:05:14Z,0,0,黄(Huáng)瓒(Zàn),Georgia Institute of Technology,False
823,"[FEA] cudf.pandas profiler should show time taken by other, non-pandas functions to run.","This is a bit different from https://github.com/rapidsai/cudf/issues/14499.

The `cudf.pandas` profiler only shows the time it takes for pandas functions and methods to run; but it doesn't report the time it takes for other functions and methods. It would be useful if the total time reported by the profiler matched up roughly with the actual total wall clock time of the program.

For example:

```
In [1]: %load_ext cudf.pandas

In [2]: import pandas as pd

In [3]: import time

In [4]: def fun1():
   ...:     time.sleep(5)
   ...:

In [5]: def fun2():
   ...:     s = pd.Series([1, 2, 3])
   ...:     s.max()
   ...:     s.min()
   ...:

In [6]: %%cudf.pandas.profile
   ...: fun1()
   ...: fun2()
   ...:
   ...:

                                   Total time elapsed: 6.345 seconds
                                 3 GPU function calls in 1.090 seconds
                                 0 CPU function calls in 0.000 seconds

                                                 Stats

┏━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓
┃ Function   ┃ GPU ncalls ┃ GPU cumtime ┃ GPU percall ┃ CPU ncalls ┃ CPU cumtime ┃ CPU percall ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩
│ Series     │ 1          │ 1.088       │ 1.088       │ 0          │ 0.000       │ 0.000       │
│ Series.max │ 1          │ 0.001       │ 0.001       │ 0          │ 0.000       │ 0.000       │
│ Series.min │ 1          │ 0.001       │ 0.001       │ 0          │ 0.000       │ 0.000       │
└────────────┴────────────┴─────────────┴─────────────┴────────────┴─────────────┴─────────────┘
``` 

It would be great if the result was something like:

```
                                   Total time elapsed: 6.345 seconds
                                 3 GPU function calls in 1.090 seconds
                                 0 CPU function calls in 0.000 seconds

                                                 Stats

┏━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓
┃ Function   ┃ GPU ncalls ┃ GPU cumtime ┃ GPU percall ┃ CPU ncalls ┃ CPU cumtime ┃ CPU percall ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩
│ Series     │ 1          │ 1.088       │ 1.088       │ 0          │ 0.000       │ 0.000       │
│ Series.max │ 1          │ 0.001       │ 0.001       │ 0          │ 0.000       │ 0.000       │
│ Series.min │ 1          │ 0.001       │ 0.001       │ 0          │ 0.000       │ 0.000       │
│ Others     │ -          │ -           │ -           │ -          │ 5.000       │ 5.000       │
└────────────┴────────────┴─────────────┴─────────────┴────────────┴─────────────┴─────────────┘
```",2024-04-08T13:17:03Z,0,0,Ashwin Srinath,Voltron Data,False
824,[FEA] Use nested type-checking in dictionary concatenate,"Filing this issue as a piece of follow-up work on #14531. There is a piece of code where we are checking types directly against a key type. I want to refactor this so that the columns can be correctly compared to the key type if either is a nested type. This requires using `cudf::types_equal` as implemented in #14531, but the refactoring needed was too large and thus I left it out of scope for that PR.

https://github.com/rapidsai/cudf/blob/c91df106f55b68068e9f1b4cf977fbe5257ea9b3/cpp/src/dictionary/detail/concatenate.cu#L223-L224

https://github.com/rapidsai/cudf/pull/14531/files#diff-e57bb56f60427be2c2bab0e098d9c965996d0516867693372f72982ecb319cfeR223",2024-04-09T18:55:37Z,0,0,Bradley Dice,@NVIDIA @rapidsai,True
825,[FEA] Improve occupancy during hash table build,"**Is your feature request related to a problem? Please describe.**
cuco insert kernel has poor occupancy due to high register usage during hash table build operation executed by cuDF. If I disable some of the code paths for complex types(commenting out dict, string, list, struct, decimal) in https://github.com/rapidsai/cudf/blob/434df44d9fe1c94e8047bcc37266ae663eae8a8d/cpp/include/cudf/utilities/type_dispatcher.hpp#L456 the type dispatcher, then the register usage per thread drops from 75 -> 46 and leads to a significant occupancy bump. It seems that the insert kernel has to pay the cost of high register usage even for simpler types since the compiler has to account for all code paths.

I did some experiments by disabling different subsets of types, list has types I disable -> register count for insert kernel
- decimal -> 72
- struct -> 73
- list -> 73
- string -> 73
- dict -> 68
- struct, list -> 64
- list, decimal, struct -> 63
- dict, string, list, struct -> 58
- string, dict, struct, list, decimal -> 46

Here is the speedup I see on mixed semi join kernel by improving occupancy for int32 keys obtained by disabling complex types
![image](https://github.com/rapidsai/cudf/assets/23545205/553e66bc-0fce-4954-868b-cd8a7163eedf)

**Describe the solution you'd like**
Improve occupancy by disabling codepaths for complex types.

**Describe alternatives you've considered**
1. Add more template params to the hasher/comparator which allow us to separate codepaths for complex types and simpler types, or 
2. Add JIT compilation to only consider the types necessary for hasher/comparator for a row

**Additional context**
Add any other context, code examples, or references to existing implementations about the feature request here.
",2024-04-10T15:57:18Z,0,0,Tanmay Gujar,,False
826,[FEA] Default scalar arguments for pylibcudf APIs,"Some `libcudf` APIs have default scalar arguments, such as [cudf::strings::capitalize]. (https://docs.rapids.ai/api/libcudf/stable/group__strings__case#ga92d98733aa0f694f12ef8aa4ada2ca02). `pylibcudf` does its best to remain faithful to the `libcudf` API where technically possible, meaning we should aspire to a cython API like

```python
pylibcudf.strings.capitalize(input: Column, delimiters=Scalar(""""))
```

Where `Scalar` refers to a `pylibcudf.scalar.Scalar` object. However as of right now there's not an explicit mechanism for constructing `Scalar` objects from anything other than `unique_ptr[scalar]` results that are [produced as the result of calling libcudf APIs](https://github.com/rapidsai/cudf/blob/branch-24.06/python/cudf/cudf/_lib/pylibcudf/scalar.pyx#L50). In fact there's a [note](https://github.com/rapidsai/cudf/blob/branch-24.06/python/cudf/cudf/_lib/pylibcudf/scalar.pyx#L33) discouraging constructing `Scalar`s from the python side using the `Scalar` constructor at least.  

One way of implementing this is to wrap the `libcudf` [scalar_factories](https://docs.rapids.ai/api/libcudf/stable/group__scalar__factories#ga910858190bf158fce6adfba4cd0cfb43) API and expose one  that can produce `Scalar` we can use as a cython default, either through the `Scalar` constructor or by some other means. However I'm not sure this is the right approach and wanted to gather feedback. 

There's some issues in general that I can forsee could be important with any approach, such as making sure that we don't cause a HTOD copy when we import `pylibcudf` for the purposes of making a default argument.

cc @vyasr  ",2024-04-10T19:11:04Z,0,0,,NVIDIA,True
827,[FEA] Allow groupby scan aggregations to return listified results,"**Is your feature request related to a problem? Please describe.**

To match the way scan aggregation results in groupby operations are returned in pandas, libcudf returns scan-based aggregations in the same shape as the input table (these are then optionally reordered to mimic pandas order).

For the cudf-polars executor, it would be useful to also have a mode where the result of a scan aggregation is collected, group-wise, into a list column. This would mean that both scan-like and reduction-like groupby aggregations always produce an output table with a number of rows equal to the number of unique group keys.

**Describe the solution you'd like**

For scan-only aggregations, this is relatively easy to achieve by taking the sorted grouped result and calling `make_lists_column` with the group offsets. When mixing scan and hash-based aggregations it is tricker (since those would spit things out in a different order and would then need a join). Ideally one the scan aggs have a ""collect as list"" option, then one would be able to do scan and reduce- aggs in the same call on the sorted table.

**Describe alternatives you've considered**

I can post-process the result (and then do a join if I have any hash-based aggs in addition).

**Additional context**

Right now, polars guarantees that although the order of groups in the result is implementation dependent, within a group, the rows show up in original dataframe order. I think this is also guaranteed by libcudf, since the sort-by-key before the aggregations is stable.
",2024-04-16T11:18:38Z,0,0,Lawrence Mitchell,,False
828,[FEA] Dev container support for building and testing cuDF JNI,"**Is your feature request related to a problem? Please describe.**

More developers are using dev containers for RAPIDS development. We can build most of RAPIDS from the provided dev containers. But when changes need to be tested for impacts on the Java bindings, we have to use a completely separate process.

This would also enable experimenting with cuDF / RAPIDS changes while running Spark workloads.

**Describe the solution you'd like**

Add the proper dependencies and scripts for building and testing JNI.  

**Describe alternatives you've considered**

- Use the documented process for building libcudf for Spark and then install JDK and maven and build them.
- Use Spark-RAPIDS containers for testing.

Both of these require me to commit my changes from the dev container, and then checkout the branch inside the Spark environment in order to build and test. An integrated environment will be more productive.

**Additional context**

I have successfully installed JDK and Maven in a dev container with cuDF, but was unable to build cuDF because of a CMake error. 

```
[exec] CMake Error at /home/coder/cudf/java/target/cmake-build/_deps/rapids-cmake-src/rapids-cmake/find/package.cmake:125 (find_package):
     [exec]   By not providing ""Findcudf.cmake"" in CMAKE_MODULE_PATH this project has
     [exec]   asked CMake to find a package configuration file provided by ""cudf"", but
     [exec]   CMake did not find one.
```
",2024-04-17T04:38:11Z,0,0,Mark Harris,@NVIDIA,True
829,[FEA] Allow cudf::thread_pool to restrict the number of threads available.,"I have a benchmarking use case where it would be nice to be able to use a single thread pool across multiple benchmarks for ease of viewing in nsys.  Imagine a benchmark where one of your testing axes is the number of threads used to split up the work. Say, 2, 4 and 8 threads.  The way you would do this today is you would create a new `thread_pool` in each instance of the benchmark with the appropriate number of threads.  The problem with this is that each thread gets it's own line of data in nsys.  So you end up with 14 total threads that you have to expand and hunt down.  This gets worse if you have other axes.  You can very quickly get up into 64 or more threads, which is a bit of a headache to sort through.

Instead, it would be nice if we could create a thread pool and temporarily restrict the number of threads it would use for newly submitted jobs.   So what your benchmark could do is create a single global thread pool (say, 8 threads above). And then just set the thread count restriction in each benchmark.   So you would have a nice clean timeline with a tractable number of threads in nsys.

Alternately, a way to sub-allocate  out of an existing pool (temporarily funding one thread_pool with the threads from another)  would work as well.",2024-04-19T19:35:34Z,0,0,,,False
830,Update calls to make_strings_children to support large strings,"This issue to help keep track of the work needed to move existing calls to `cudf::strings::detail::make_strings_children` to the new `cudf::strings::detail::experimental::make_strings_children` and then ultimately replacing the non-experimental one.
Available once #15363 is merged.

The changes involve updating the functor to used by the utility to replace the `size_type* d_offsets` member with a `size_type* d_sizes` since currently the functors set the output row sizes there on the first pass call. And then a new `input_offsetalalor d_offsets` member is added to now address the output row's data in the existing `char* d_chars` device memory. This allows the `d_chars` data to point to larger than 2GB of device memory and for the offsets to be either INT32 or INT64. This effort should be minimal on each functor since the actual output size and memory writes only need to use the new members correctly and so no significant logic changes should be needed.

Right now, no additional updates will be required including benchmarks or gtests though this may change for individual APIs in the future.

APIs that use `make_strings_children` and need to be reworked to use the experimental `make_strings_children`

Convert non-string to string - #15629 
- [x] `from_booleans`
- [x] `from_timestamps`
- [x] `from_durations`
- [x] `from_fixed_point`
- [x] `from_floats`
- [x] `integers_to_hex`
- [x] `from_integers`
- [x] `integers_to_ipv4`

Other conversions -- PR #15598 
- [x] `format_list_column`
- [x] `url_encode`
- [x] `join_strings`
- [x] `join_list_elements`
- [x] `slice_strings`

Replace/Filter -- PR #15586
- [x] `replace` (string parallel)
- [x] `replace` (multiple targets)
- [x] `replace_slice`
- [x] `filter_characters_of_type`
- [x] `filter_characters`
- [x] `translate`

Others - PR #15587
- [x] `pad`
- [x] `zfill`
- [x] `to_lower/to_upper/swapcase`
- [x] `capitalize`

I/O - PR #15599
- [x] JSON writer (`get_escaped_strings`)
- [x] JSON benchmark (`build_json_string_column`)
- [x] CSV writer (escaping characters)

nvtext - PR #15595 
- [x] `filter_tokens`
- [x] `replace_tokens`
- [x] `normalize_characters`
- [x] `normalize_spaces`
- [x] `generate_character_ngrams`
- [x] `generate_ngrams`
- [x] `detokenize`",2024-04-22T19:08:37Z,0,0,David Wendt,NVIDIA,True
831,[FEA][JNI] Consider defaulting parquet dictionary encoding policy to ALWAYS,"This PR is going to set the cuDF dictionary encoding policy for parquet to ADAPTIVE (https://github.com/rapidsai/cudf/pull/15570)

This is to get around an issue in nvcomp zstd https://github.com/rapidsai/cudf/issues/15501, where too large pages are getting created and is causing zstd to not compress larger dictionary pages.

For now we can pick ALWAYS in order to retain the current behavior for Spark. We should consider the impact of this setting for different compression, especially zstd (reading and writing).",2024-04-22T21:17:23Z,0,0,Alessandro Bellina,NVIDIA,True
832,[FEA] Add support for `Frame.attrs`,"**Is your feature request related to a problem? Please describe.**
We will need to add support for `Frame.attrs`: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.attrs.html, and be able to properly round-trip the values to and from pandas.

",2024-04-26T23:46:20Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
833,"Improve ""print_env.sh"" experience for reporting issues","In #15589, @betatim had difficulty finding the [`print_env.sh` script](https://github.com/rapidsai/cudf/blob/branch-24.06/print_env.sh) that is requested as a part of filing an issue.

> Is there a way to provide the same information via something like `import cudf; cudf.print_env()`?

This is probably a viable alternative to the shell script we currently recommend. I think we could re-evaluate the contents of that script. The downside would be that users having installation troubles wouldn't be able to import the package to call such a function -- but the environment information might still be helpful for diagnosis.

Also, it seems to me that many users skip the step of reporting environment information, so either the `print_env.sh` script is hard to find or issue reporters don't think it is important.

xref: https://github.com/rapidsai/cudf/issues/15589#issuecomment-2083061696",2024-04-29T15:41:40Z,0,0,Bradley Dice,@NVIDIA @rapidsai,True
834,[FEA] Improve performance of strings matching in libcudf,"**Is your feature request related to a problem? Please describe.**
The issue documents a few performance ideas for the libcudf regular expression engine ([code pointer](https://github.com/rapidsai/cudf/tree/branch-24.06/cpp/src/strings/regex)) and strings APIs. In particular, these performance ideas came from investigation of multi-string pattern matching commonly used for IP addresses in DPUs. The DPU use case involves checking dozens (?) of string patterns against millions (?) of input strings, and most matches are negative.

| Idea | API scope | Initial scoping |
|---|---|---|
| Avoid regex and instead replace with strings contains or strings startswith/endswith whenever possible. | regex utilities | For now we encourage libcudf applications to add pattern inspection and avoid calling the regex engine if that is an important optimization in their use case. We may consider upstreaming a tool similar to the [regex parsing approach in Spark-RAPIDS](https://github.com/NVIDIA/spark-rapids/pull/10715) at some point. |
| Add a non-regex multi-string match function to the strings API, as a way to fused multiple string matches into a single kernel  |  strings | We have an investigation of this idea in #15536. Performance analysis is in progress |
| Use a shared memory Shift-Or approach to speed up strings contains.  | strings | Initial scoping suggests this method could deliver 3x throughput (~1000 GB/s on A100). However this optimization will have a larger memory footprint (256 bytes/thread) that could create other issues when integrated with libcudf. ([link to algorithm demonstration](https://www.educative.io/answers/shift-or-string-matching-algorithm)) |
| Fuse sequences of regex pattern characters into a single ""regex literal"" token | regex | After initial scoping, multi-character pattern tokens are unlikely to be compatible with the existing regex engine. Significant refactoring would be required and the benefits are uncertain. |
| ASCII-only strings `contains` | strings | There may be benefit to an ASCII-only implementation of string matching for some use cases. The potential performance benefit has not yet been evaluated. | 
| ASCII-only `match_re` | strings | There may be benefit to an ASCII-only implementation of regex pattern matching for some use cases. The potential performance benefit has not yet been evaluated. |
| [Sitaridi et al 2016](https://dl.acm.org/doi/pdf/10.1007/s00778-015-0409-y) suggests to use Knuth–Morris–Pratt (KMP) for string pattern matching | strings | Stores a partial match table that improves GPU L2 cache utilization |
| add aligned strings for vector loading | strings | add padding in the byte array, add sizes child column. always use aligned strings by default?  |
| prefix strings | strings | see Arrow (TBD) |  


**Describe the solution you'd like**
TBD

**Describe alternatives you've considered**
TBD

**Additional context**
Regex performance ideas have come out of collaboration between SM-based and DPU-based regular expression processing. For more information about DPU-based regex, please see the [NVIDIA Bluefield-2](https://docs.nvidia.com/networking/display/bluefielddpuosv385/regex+acceleration) docs.
",2024-04-29T18:29:30Z,0,0,Gregory Kimball,,False
835,[FEA] Have a global pinned memory pool by default,"Users outside of Spark-RAPIDS still use the default, non-pooled, host memory resource and thus have the overhead of pinned memory allocations in `hostdevice_vector`, and any other places where pinned memory is used for faster data transfer.

Proposal: Default to a memory resource with a small pinned pool. When the pool is full, the resource should fall back to new pinned allocations to keep consistent with the old behavior when too much pinned memory is used.

To ensure we don't impact CPU performance, the default size of the pool can be a set percentage of the total system memory. Pinning a small minority of system memory (~5%) should not have a negative impact.

Initially, only `hostdevice_vector` would use this resource but we can expand the pinned memory use in libcudf once a default pool resource is in place.

Details to consider:
Pool should probably be created on first use - avoids duplicated pool is users set the resource before the first use.
Switching the host resource should work at any point, even if we must have two pools at the same time.
Can the default pool be safely destroyed? streams can't be destroyed on exit, not sure about `cudaFreeHost`
",2024-04-29T22:19:02Z,0,0,Vukasin Milovanovic,NVIDIA,True
836,[FEA] Expand pinned memory use in libcudf,"Once https://github.com/rapidsai/cudf/issues/15612 is implemented, libcudf can make further use of the pool to optimize data transfers.

Types like `device_scalar` can use a small pinned buffer, since pool makes the allocation cost rivial. Also, use of `std::vector`s as the host mirror of device data (e.g. `make_device_uvector_async`, `make_std_vector_async`) can be replaced with a type backed with pinned memory, at least for buffers that are not in the same order of magnitude with the data size.",2024-04-30T02:10:04Z,1,0,Vukasin Milovanovic,NVIDIA,True
837,[FEA] Enable `cp.asarray(cudf.RangeIndex)`,"**Is your feature request related to a problem? Please describe.**
Currently if one attempts to explicitly materialize a cupy array via cudf.RangeIndex, this error is thrown:
```
(Pdb) import cudf
(Pdb) cp.asarray(cudf.RangeIndex(0, 100))
*** TypeError: Implicit conversion to a host NumPy array via __array__ is not allowed, To explicitly construct a GPU matrix, consider using .to_cupy()
To explicitly construct a host matrix, consider using .to_numpy().
```

Offline discussion with @vyasr and @pentschev suggests that we should have this usage working transparently. The benefit of this is that `cp.asarray(obj)` would work for all cudf objects.

**Describe the solution you'd like**
The most straight forward way is to enable `RangeIndex.__array__`, which is currently disabled. The rationale is that when `__array__` is invoked, the intention of converting to numpy array is clear. However, additional care should be taken when it's being invoked within a cuDF API. According to @vyasr , we should leverage the frame tracking tooling to check if the `__array__` interface is invoked internally in cuDF, or externally. If the former, we should raise an error and suggest that `to_cupy` method should be used. If the latter, the API should work, but maybe a warning can be thrown suggesting this is not as efficient as `to_cupy`.
",2024-04-30T04:04:00Z,0,0,Michael Wang,Nvidia Rapids,True
838,[FEA] Use SMs to submit small copies to prevent serialization on a busy copy engine,"We have seen patterns where small `cudaMemcpyAsync` collide with large `cudaMemcpyAsync` being handled by the copy engine. Importantly, the small copy is in a different stream than the large copy.  In the example below, we can see a H2D pinned copy of 51KB that was scheduled with a latency of _12ms_ because there is another pinned H2D copy happening at the same time (the larger copy is ~200MB).

![2024-04-30_10-35](https://github.com/rapidsai/cudf/assets/1901059/58e6a601-8ffb-490b-931f-1e7deee82dcd)

The big issue behind this pattern is that Stream 30 in this case is serializing because nothing else will run in the stream until this small copy is done. Usually when we invoke kernels in cuDF there is a pattern of: small H2Ds, followed by kernel invocation, then small D2Hs. Any of the pre/post copies done around a kernel is a candidate to get stuck, serializing all the work in that stream.

We have a PoC that uses `thrust::copy_n` to copy from pinned to device memory and viceversa using SMs instead of the copy engine, as kernels can directly touch pinned memory. When such an approach is followed, the small copy is able to run with much less latency and subsequent work in the stream is unblocked. This leads to kernels running at the same time as large copies, which is a desirable pattern. 

This issue was created in order to track this work and to figure out how to bring these changes to cuDF in a configurable way so we don't affect serial workloads, as the effect is really prominent for parallel workloads such as Spark.

This is NDS q9 at 3TB, looking at the CUDA HW row, we can see how the compute (blue) overlaps more often than not with pinned H2Ds (green).

Before:
![2024-04-30_10-43](https://github.com/rapidsai/cudf/assets/1901059/783b32bb-9750-4bdb-a164-daaea6ebab84)

After:
![2024-04-30_10-44](https://github.com/rapidsai/cudf/assets/1901059/54fce8ee-68db-4bb6-9985-782419e19337)
",2024-04-30T17:45:40Z,0,0,Alessandro Bellina,NVIDIA,True
839,[BUG] Enabling cudf.pandas leads to exception when using a Numpy array,"**Describe the bug**
When `cudf.pandas` is enabled then passing a Numpy array to `ExponentialSmoothing` from `statsmodels.tsa.holtwinters` involves the pandas accelerator (odd no?) and leads to an exception.

**Steps/Code to reproduce bug**

```python
import cudf.pandas
cudf.pandas.install()
import numpy as np
from statsmodels.tsa.holtwinters import ExponentialSmoothing
airpassengers = [
    112,
    118,
    132,
    129,
    121,
    135,
    148,
    148,
    136,
    119,
    104,
    118,
    115,
    126,
    141,
    135,
    125,
    149,
    170,
    170,
    158,
    133,
    114,
    140,
    145,
    150,
    178,
    163,
    172,
    178,
    199,
    199,
    184,
    162,
    146,
    166,
    171,
    180,
    193,
    181,
    183,
    218,
    230,
    242,
    209,
    191,
    172,
    194,
    196,
    196,
    236,
    235,
    229,
    243,
    264,
    272,
    237,
    211,
    180,
    201,
    204,
    188,
    235,
    227,
    234,
    264,
    302,
    293,
    259,
    229,
    203,
    229,
    242,
    233,
    267,
    269,
    270,
    315,
    364,
    347,
    312,
    274,
    237,
    278,
    284,
    277,
    317,
    313,
    318,
    374,
    413,
    405,
    355,
    306,
    271,
    306,
    315,
    301,
    356,
    348,
    355,
    422,
    465,
    467,
    404,
    347,
    305,
    336,
    340,
    318,
    362,
    348,
    363,
    435,
    491,
    505,
    404,
    359,
    310,
    337,
]
airpassengers = np.asarray(airpassengers, dtype=np.float64)

# this line leads to the traceback
ExponentialSmoothing(airpassengers, initialization_method='heuristic', seasonal='additive', seasonal_periods=12)
```

<details>
<summary>Full traceback</summary>

---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
File /nvme/1/thead/miniconda/envs/cuml-dev-24.06/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:888, in _fast_slow_function_call(func, *args, **kwargs)
    883 with nvtx.annotate(
    884     ""EXECUTE_FAST"",
    885     color=_CUDF_PANDAS_NVTX_COLORS[""EXECUTE_FAST""],
    886     domain=""cudf_pandas"",
    887 ):
--> 888     fast_args, fast_kwargs = _fast_arg(args), _fast_arg(kwargs)
    889     result = func(*fast_args, **fast_kwargs)

File /nvme/1/thead/miniconda/envs/cuml-dev-24.06/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:1007, in _fast_arg(arg)
   1006 seen: Set[int] = set()
-> 1007 return _transform_arg(arg, ""_fsproxy_fast"", seen)

File /nvme/1/thead/miniconda/envs/cuml-dev-24.06/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:934, in _transform_arg(arg, attribute_name, seen)
    932 if type(arg) is tuple:
    933     # Must come first to avoid infinite recursion
--> 934     return tuple(_transform_arg(a, attribute_name, seen) for a in arg)
    935 elif hasattr(arg, ""__getnewargs_ex__""):
    936     # Partial implementation of to reconstruct with
    937     # transformed pieces
    938     # This handles scipy._lib._bunch._make_tuple_bunch

File /nvme/1/thead/miniconda/envs/cuml-dev-24.06/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:934, in <genexpr>(.0)
    932 if type(arg) is tuple:
    933     # Must come first to avoid infinite recursion
--> 934     return tuple(_transform_arg(a, attribute_name, seen) for a in arg)
    935 elif hasattr(arg, ""__getnewargs_ex__""):
    936     # Partial implementation of to reconstruct with
    937     # transformed pieces
    938     # This handles scipy._lib._bunch._make_tuple_bunch

File /nvme/1/thead/miniconda/envs/cuml-dev-24.06/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:917, in _transform_arg(arg, attribute_name, seen)
    916 if isinstance(arg, (_FastSlowProxy, _FastSlowProxyMeta, _FunctionProxy)):
--> 917     typ = getattr(arg, attribute_name)
    918     if typ is _Unusable:

File /nvme/1/thead/miniconda/envs/cuml-dev-24.06/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:528, in _FastSlowProxy._fsproxy_fast(self)
    523 """"""
    524 Returns the wrapped object. If the wrapped object is of ""slow""
    525 type, replaces it with the corresponding ""fast"" object before
    526 returning it.
    527 """"""
--> 528 self._fsproxy_wrapped = self._fsproxy_slow_to_fast()
    529 return self._fsproxy_wrapped

File /nvme/1/thead/miniconda/envs/cuml-dev-24.06/lib/python3.11/site-packages/nvtx/nvtx.py:116, in annotate.__call__.<locals>.inner(*args, **kwargs)
    115 libnvtx_push_range(self.attributes, self.domain.handle)
--> 116 result = func(*args, **kwargs)
    117 libnvtx_pop_range(self.domain.handle)

File /nvme/1/thead/miniconda/envs/cuml-dev-24.06/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:786, in _IntermediateProxy._fsproxy_slow_to_fast(self)
    785 func, args, kwargs = self._method_chain
--> 786 args, kwargs = _fast_arg(args), _fast_arg(kwargs)
    787 return func(*args, **kwargs)

File /nvme/1/thead/miniconda/envs/cuml-dev-24.06/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:1007, in _fast_arg(arg)
   1006 seen: Set[int] = set()
-> 1007 return _transform_arg(arg, ""_fsproxy_fast"", seen)

File /nvme/1/thead/miniconda/envs/cuml-dev-24.06/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:934, in _transform_arg(arg, attribute_name, seen)
    932 if type(arg) is tuple:
    933     # Must come first to avoid infinite recursion
--> 934     return tuple(_transform_arg(a, attribute_name, seen) for a in arg)
    935 elif hasattr(arg, ""__getnewargs_ex__""):
    936     # Partial implementation of to reconstruct with
    937     # transformed pieces
    938     # This handles scipy._lib._bunch._make_tuple_bunch

File /nvme/1/thead/miniconda/envs/cuml-dev-24.06/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:934, in <genexpr>(.0)
    932 if type(arg) is tuple:
    933     # Must come first to avoid infinite recursion
--> 934     return tuple(_transform_arg(a, attribute_name, seen) for a in arg)
    935 elif hasattr(arg, ""__getnewargs_ex__""):
    936     # Partial implementation of to reconstruct with
    937     # transformed pieces
    938     # This handles scipy._lib._bunch._make_tuple_bunch

File /nvme/1/thead/miniconda/envs/cuml-dev-24.06/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:917, in _transform_arg(arg, attribute_name, seen)
    916 if isinstance(arg, (_FastSlowProxy, _FastSlowProxyMeta, _FunctionProxy)):
--> 917     typ = getattr(arg, attribute_name)
    918     if typ is _Unusable:

File /nvme/1/thead/miniconda/envs/cuml-dev-24.06/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:528, in _FastSlowProxy._fsproxy_fast(self)
    523 """"""
    524 Returns the wrapped object. If the wrapped object is of ""slow""
    525 type, replaces it with the corresponding ""fast"" object before
    526 returning it.
    527 """"""
--> 528 self._fsproxy_wrapped = self._fsproxy_slow_to_fast()
    529 return self._fsproxy_wrapped

File /nvme/1/thead/miniconda/envs/cuml-dev-24.06/lib/python3.11/site-packages/nvtx/nvtx.py:116, in annotate.__call__.<locals>.inner(*args, **kwargs)
    115 libnvtx_push_range(self.attributes, self.domain.handle)
--> 116 result = func(*args, **kwargs)
    117 libnvtx_pop_range(self.domain.handle)

File /nvme/1/thead/miniconda/envs/cuml-dev-24.06/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:787, in _IntermediateProxy._fsproxy_slow_to_fast(self)
    786 args, kwargs = _fast_arg(args), _fast_arg(kwargs)
--> 787 return func(*args, **kwargs)

File /nvme/1/thead/miniconda/envs/cuml-dev-24.06/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:30, in call_operator(fn, args, kwargs)
     29 def call_operator(fn, args, kwargs):
---> 30     return fn(*args, **kwargs)

File /nvme/1/thead/miniconda/envs/cuml-dev-24.06/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:76, in _Unusable.__call__(self, *args, **kwds)
     75 def __call__(self, *args: Any, **kwds: Any) -> Any:
---> 76     raise NotImplementedError(
     77         ""Fast implementation not available. ""
     78         ""Falling back to the slow implementation""
     79     )

NotImplementedError: Fast implementation not available. Falling back to the slow implementation

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
Cell In[1], line 130
    127 airpassengers = np.asarray(airpassengers, dtype=np.float64)
    129 # this line leads to the traceback
--> 130 ExponentialSmoothing(airpassengers, initialization_method='heuristic', seasonal='additive', seasonal_periods=12)

File /nvme/1/thead/miniconda/envs/cuml-dev-24.06/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:837, in _CallableProxyMixin.__call__(self, *args, **kwargs)
    836 def __call__(self, *args, **kwargs) -> Any:
--> 837     result, _ = _fast_slow_function_call(
    838         # We cannot directly call self here because we need it to be
    839         # converted into either the fast or slow object (by
    840         # _fast_slow_function_call) to avoid infinite recursion.
    841         # TODO: When Python 3.11 is the minimum supported Python version
    842         # this can use operator.call
    843         call_operator,
    844         self,
    845         args,
    846         kwargs,
    847     )
    848     return result

File /nvme/1/thead/miniconda/envs/cuml-dev-24.06/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:902, in _fast_slow_function_call(func, *args, **kwargs)
    900         slow_args, slow_kwargs = _slow_arg(args), _slow_arg(kwargs)
    901         with disable_module_accelerator():
--> 902             result = func(*slow_args, **slow_kwargs)
    903 return _maybe_wrap_result(result, func, *args, **kwargs), fast

File /nvme/1/thead/miniconda/envs/cuml-dev-24.06/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:30, in call_operator(fn, args, kwargs)
     29 def call_operator(fn, args, kwargs):
---> 30     return fn(*args, **kwargs)

File /nvme/1/thead/miniconda/envs/cuml-dev-24.06/lib/python3.11/site-packages/pandas/util/_decorators.py:213, in deprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper(*args, **kwargs)
    211         raise TypeError(msg)
    212     kwargs[new_arg_name] = new_arg_value
--> 213 return func(*args, **kwargs)

TypeError: ExponentialSmoothing.__init__() missing 1 required positional argument: 'endog'

</details>

**Expected behavior**
No error

**Environment overview (please complete the following information)**
Setup cuml dev environment using a conda env

**Environment details**
Please run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details

<details><summary>Click here to see environment details</summary><pre>

     **git***
     Not inside a git repository

     ***OS Information***
     DISTRIB_ID=Ubuntu
     DISTRIB_RELEASE=22.04
     DISTRIB_CODENAME=jammy
     DISTRIB_DESCRIPTION=""Ubuntu 22.04.2 LTS""
     PRETTY_NAME=""Ubuntu 22.04.2 LTS""
     NAME=""Ubuntu""
     VERSION_ID=""22.04""
     VERSION=""22.04.2 LTS (Jammy Jellyfish)""
     VERSION_CODENAME=jammy
     ID=ubuntu
     ID_LIKE=debian
     HOME_URL=""https://www.ubuntu.com/""
     SUPPORT_URL=""https://help.ubuntu.com/""
     BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
     PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
     UBUNTU_CODENAME=jammy
     Linux dt05 5.15.0-94-generic #104-Ubuntu SMP Tue Jan 9 15:25:40 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

     ***GPU Information***
     Thu May  2 08:41:30 2024
     +---------------------------------------------------------------------------------------+
     | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
     |-----------------------------------------+----------------------+----------------------+
     | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
     | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
     |                                         |                      |               MIG M. |
     |=========================================+======================+======================|
     |   0  Tesla T4                       On  | 00000000:3B:00.0 Off |                    0 |
     | N/A   36C    P8              15W /  70W |      2MiB / 15360MiB |      0%      Default |
     |                                         |                      |                  N/A |
     +-----------------------------------------+----------------------+----------------------+
     |   1  Tesla T4                       On  | 00000000:5E:00.0 Off |                    0 |
     | N/A   35C    P8              10W /  70W |      2MiB / 15360MiB |      0%      Default |
     |                                         |                      |                  N/A |
     +-----------------------------------------+----------------------+----------------------+
     |   2  Tesla T4                       On  | 00000000:AF:00.0 Off |                    0 |
     | N/A   29C    P8               9W /  70W |      2MiB / 15360MiB |      0%      Default |
     |                                         |                      |                  N/A |
     +-----------------------------------------+----------------------+----------------------+
     |   3  Tesla T4                       On  | 00000000:D8:00.0 Off |                    0 |
     | N/A   29C    P8               9W /  70W |      2MiB / 15360MiB |      0%      Default |
     |                                         |                      |                  N/A |
     +-----------------------------------------+----------------------+----------------------+

     +---------------------------------------------------------------------------------------+
     | Processes:                                                                            |
     |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
     |        ID   ID                                                             Usage      |
     |=======================================================================================|
     |  No running processes found                                                           |
     +---------------------------------------------------------------------------------------+

     ***CPU***
     Architecture:                       x86_64
     CPU op-mode(s):                     32-bit, 64-bit
     Address sizes:                      46 bits physical, 48 bits virtual
     Byte Order:                         Little Endian
     CPU(s):                             64
     On-line CPU(s) list:                0-63
     Vendor ID:                          GenuineIntel
     Model name:                         Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz
     CPU family:                         6
     Model:                              85
     Thread(s) per core:                 2
     Core(s) per socket:                 16
     Socket(s):                          2
     Stepping:                           4
     CPU max MHz:                        3700.0000
     CPU min MHz:                        1000.0000
     BogoMIPS:                           4200.00
     Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts pku ospke md_clear flush_l1d arch_capabilities
     Virtualization:                     VT-x
     L1d cache:                          1 MiB (32 instances)
     L1i cache:                          1 MiB (32 instances)
     L2 cache:                           32 MiB (32 instances)
     L3 cache:                           44 MiB (2 instances)
     NUMA node(s):                       2
     NUMA node0 CPU(s):                  0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62
     NUMA node1 CPU(s):                  1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63
     Vulnerability Gather data sampling: Mitigation; Microcode
     Vulnerability Itlb multihit:        KVM: Mitigation: VMX disabled
     Vulnerability L1tf:                 Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable
     Vulnerability Mds:                  Mitigation; Clear CPU buffers; SMT vulnerable
     Vulnerability Meltdown:             Mitigation; PTI
     Vulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable
     Vulnerability Retbleed:             Mitigation; IBRS
     Vulnerability Spec rstack overflow: Not affected
     Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp
     Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
     Vulnerability Spectre v2:           Mitigation; IBRS, IBPB conditional, STIBP conditional, RSB filling, PBRSB-eIBRS Not affected
     Vulnerability Srbds:                Not affected
     Vulnerability Tsx async abort:      Mitigation; Clear CPU buffers; SMT vulnerable

     ***CMake***
     /nvme/1/thead/miniconda/envs/cuml-dev-24.06/bin/cmake
     cmake version 3.29.2

     CMake suite maintained and supported by Kitware (kitware.com/cmake).

     ***g++***
     /nvme/1/thead/miniconda/envs/cuml-dev-24.06/bin/g++
     g++ (conda-forge gcc 11.4.0-6) 11.4.0
     Copyright (C) 2021 Free Software Foundation, Inc.
     This is free software; see the source for copying conditions.  There is NO
     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


     ***nvcc***
     /nvme/1/thead/miniconda/envs/cuml-dev-24.06/bin/nvcc
     nvcc: NVIDIA (R) Cuda compiler driver
     Copyright (c) 2005-2023 NVIDIA Corporation
     Built on Tue_Aug_15_22:02:13_PDT_2023
     Cuda compilation tools, release 12.2, V12.2.140
     Build cuda_12.2.r12.2/compiler.33191640_0

     ***Python***
     /nvme/1/thead/miniconda/envs/cuml-dev-24.06/bin/python
     Python 3.11.9

     ***Environment Variables***
     PATH                            : /home/nfs/thead/.local/bin:/home/nfs/thead/.local/bin:/nvme/1/thead/miniconda/envs/cuml-dev-24.06/bin:/nvme/1/thead/miniconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
     LD_LIBRARY_PATH                 :
     NUMBAPRO_NVVM                   :
     NUMBAPRO_LIBDEVICE              :
     CONDA_PREFIX                    : /nvme/1/thead/miniconda/envs/cuml-dev-24.06
     PYTHON_PATH                     :

     ***conda packages***
     /nvme/1/thead/miniconda/condabin/conda
     # packages in environment at /nvme/1/thead/miniconda/envs/cuml-dev-24.06:
     #
     # Name                    Version                   Build  Channel
     _libgcc_mutex             0.1                 conda_forge    conda-forge
     _openmp_mutex             4.5                       2_gnu    conda-forge
     _sysroot_linux-64_curr_repodata_hack 3                   h69a702a_14    conda-forge
     accessible-pygments       0.0.4              pyhd8ed1ab_0    conda-forge
     alabaster                 0.7.16             pyhd8ed1ab_0    conda-forge
     asttokens                 2.4.1              pyhd8ed1ab_0    conda-forge
     atk-1.0                   2.38.0               h04ea711_2    conda-forge
     attrs                     23.2.0             pyh71513ae_0    conda-forge
     aws-c-auth                0.7.18               he0b1f16_0    conda-forge
     aws-c-cal                 0.6.11               heb1d5e4_0    conda-forge
     aws-c-common              0.9.15               hd590300_0    conda-forge
     aws-c-compression         0.2.18               hce8ee76_3    conda-forge
     aws-c-event-stream        0.4.2                h01f5eca_8    conda-forge
     aws-c-http                0.8.1               hdb68c23_10    conda-forge
     aws-c-io                  0.14.7               hbfbeace_6    conda-forge
     aws-c-mqtt                0.10.4               h50844eb_0    conda-forge
     aws-c-s3                  0.5.7                h6be9164_2    conda-forge
     aws-c-sdkutils            0.1.15               hce8ee76_3    conda-forge
     aws-checksums             0.1.18               hce8ee76_3    conda-forge
     aws-crt-cpp               0.26.8               h2150271_2    conda-forge
     aws-sdk-cpp               1.11.267             hddb5a97_7    conda-forge
     babel                     2.14.0             pyhd8ed1ab_0    conda-forge
     backports.zoneinfo        0.2.1           py311h38be061_8    conda-forge
     beautifulsoup4            4.12.3             pyha770c72_0    conda-forge
     binutils                  2.40                 h4852527_0    conda-forge
     binutils_impl_linux-64    2.40                 ha885e6a_0    conda-forge
     binutils_linux-64         2.40                 hdade7a5_3    conda-forge
     bleach                    6.1.0              pyhd8ed1ab_0    conda-forge
     bokeh                     3.4.1              pyhd8ed1ab_0    conda-forge
     brotli                    1.1.0                hd590300_1    conda-forge
     brotli-bin                1.1.0                hd590300_1    conda-forge
     brotli-python             1.1.0           py311hb755f60_1    conda-forge
     bzip2                     1.0.8                hd590300_5    conda-forge
     c-ares                    1.28.1               hd590300_0    conda-forge
     c-compiler                1.5.2                h0b41bf4_0    conda-forge
     ca-certificates           2024.2.2             hbcca054_0    conda-forge
     cachetools                5.3.3              pyhd8ed1ab_0    conda-forge
     cairo                     1.18.0               h3faef2a_0    conda-forge
     certifi                   2024.2.2           pyhd8ed1ab_0    conda-forge
     charset-normalizer        3.3.2              pyhd8ed1ab_0    conda-forge
     click                     8.1.7           unix_pyh707e725_0    conda-forge
     cloudpickle               3.0.0              pyhd8ed1ab_0    conda-forge
     cmake                     3.29.2               hcfe8598_0    conda-forge
     colorama                  0.4.6              pyhd8ed1ab_0    conda-forge
     comm                      0.2.2              pyhd8ed1ab_0    conda-forge
     commonmark                0.9.1                      py_0    conda-forge
     contourpy                 1.2.1           py311h9547e67_0    conda-forge
     coverage                  7.5.0           py311h331c9d8_0    conda-forge
     cuda-cccl_linux-64        12.2.140             ha770c72_0    conda-forge
     cuda-crt-dev_linux-64     12.2.140             ha770c72_1    conda-forge
     cuda-crt-tools            12.2.140             ha770c72_1    conda-forge
     cuda-cudart               12.2.140             hd3aeb46_0    conda-forge
     cuda-cudart-dev           12.2.140             hd3aeb46_0    conda-forge
     cuda-cudart-dev_linux-64  12.2.140             h59595ed_0    conda-forge
     cuda-cudart-static        12.2.140             hd3aeb46_0    conda-forge
     cuda-cudart-static_linux-64 12.2.140             h59595ed_0    conda-forge
     cuda-cudart_linux-64      12.2.140             h59595ed_0    conda-forge
     cuda-driver-dev_linux-64  12.2.140             h59595ed_0    conda-forge
     cuda-nvcc                 12.2.140             hcdd1206_0    conda-forge
     cuda-nvcc-dev_linux-64    12.2.140             ha770c72_1    conda-forge
     cuda-nvcc-impl            12.2.140             hd3aeb46_1    conda-forge
     cuda-nvcc-tools           12.2.140             hd3aeb46_1    conda-forge
     cuda-nvcc_linux-64        12.2.140             h8a487aa_0    conda-forge
     cuda-nvrtc                12.2.140             hd3aeb46_0    conda-forge
     cuda-nvvm-dev_linux-64    12.2.140             ha770c72_1    conda-forge
     cuda-nvvm-impl            12.2.140             h59595ed_1    conda-forge
     cuda-nvvm-tools           12.2.140             h59595ed_1    conda-forge
     cuda-profiler-api         12.2.140             ha770c72_0    conda-forge
     cuda-python               12.4.0          py311h7f239a6_1    conda-forge
     cuda-version              12.2                 he2b69de_3    conda-forge
     cudf                      24.06.00a164    cuda12_py311_240430_gab5e3f3bc8_164    rapidsai-nightly
     cuml                      24.6.0                   pypi_0    pypi
     cupy                      13.1.0          py311hf829483_4    conda-forge
     cupy-core                 13.1.0          py311he1e6e68_4    conda-forge
     cxx-compiler              1.5.2                hf52228f_0    conda-forge
     cycler                    0.12.1             pyhd8ed1ab_0    conda-forge
     cython                    3.0.10          py311hb755f60_0    conda-forge
     cytoolz                   0.12.3          py311h459d7ec_0    conda-forge
     dask                      2024.4.3a240423  py_g5a588aee_1    dask/label/dev
     dask-core                 2024.4.3a240429 py_gb958ce2dc_9    dask/label/dev
     dask-cuda                 24.06.00a12     py311_240430_g85cbd00_12    rapidsai-nightly
     dask-cudf                 24.06.00a164    cuda12_py311_240430_gab5e3f3bc8_164    rapidsai-nightly
     dask-expr                 1.0.13a240425     py_g301c1a6_5    dask/label/dev
     dask-glm                  0.3.0                    pypi_0    pypi
     dask-ml                   2024.3.20          pyhd8ed1ab_0    conda-forge
     debugpy                   1.8.1           py311hb755f60_0    conda-forge
     decopatch                 1.4.10             pyhd8ed1ab_0    conda-forge
     decorator                 5.1.1              pyhd8ed1ab_0    conda-forge
     defusedxml                0.7.1              pyhd8ed1ab_0    conda-forge
     distributed               2024.4.3a240423  py_g5a588aee_1    dask/label/dev
     dlpack                    0.8                  h59595ed_3    conda-forge
     docutils                  0.19            py311h38be061_1    conda-forge
     doxygen                   1.9.1                hb166930_1    conda-forge
     entrypoints               0.4                pyhd8ed1ab_0    conda-forge
     exceptiongroup            1.2.0              pyhd8ed1ab_2    conda-forge
     execnet                   2.1.1              pyhd8ed1ab_0    conda-forge
     executing                 2.0.1              pyhd8ed1ab_0    conda-forge
     expat                     2.6.2                h59595ed_0    conda-forge
     fastrlock                 0.8.2           py311hb755f60_2    conda-forge
     fmt                       10.2.1               h00ab1b0_0    conda-forge
     font-ttf-dejavu-sans-mono 2.37                 hab24e00_0    conda-forge
     font-ttf-inconsolata      3.000                h77eed37_0    conda-forge
     font-ttf-source-code-pro  2.038                h77eed37_0    conda-forge
     font-ttf-ubuntu           0.83                 h77eed37_1    conda-forge
     fontconfig                2.14.2               h14ed4e7_0    conda-forge
     fonts-conda-ecosystem     1                             0    conda-forge
     fonts-conda-forge         1                             0    conda-forge
     fonttools                 4.51.0          py311h459d7ec_0    conda-forge
     freetype                  2.12.1               h267a509_2    conda-forge
     fribidi                   1.0.10               h36c2ea0_0    conda-forge
     fsspec                    2024.3.1           pyhca7485f_0    conda-forge
     future                    1.0.0              pyhd8ed1ab_0    conda-forge
     gcc                       11.4.0               h602e360_6    conda-forge
     gcc_impl_linux-64         11.4.0               h7abf839_6    conda-forge
     gcc_linux-64              11.4.0               h0f0c6b6_3    conda-forge
     gdk-pixbuf                2.42.11              hb9ae30d_0    conda-forge
     gflags                    2.2.2             he1b5a44_1004    conda-forge
     giflib                    5.2.2                hd590300_0    conda-forge
     glog                      0.7.0                hed5481d_0    conda-forge
     graphite2                 1.3.13            h59595ed_1003    conda-forge
     graphviz                  9.0.0                h78e8752_1    conda-forge
     gtk2                      2.24.33              h280cfa0_4    conda-forge
     gts                       0.7.6                h977cf35_4    conda-forge
     gxx                       11.4.0               h602e360_6    conda-forge
     gxx_impl_linux-64         11.4.0               h7abf839_6    conda-forge
     gxx_linux-64              11.4.0               h2730b16_3    conda-forge
     harfbuzz                  8.4.0                h3d44ed6_0    conda-forge
     hdbscan                   0.8.30          py311h1f0f07a_0    conda-forge
     hypothesis                6.100.2            pyha770c72_0    conda-forge
     icu                       73.2                 h59595ed_0    conda-forge
     idna                      3.7                pyhd8ed1ab_0    conda-forge
     imagesize                 1.4.1              pyhd8ed1ab_0    conda-forge
     importlib-metadata        7.1.0              pyha770c72_0    conda-forge
     importlib-resources       6.4.0              pyhd8ed1ab_0    conda-forge
     importlib_metadata        7.1.0                hd8ed1ab_0    conda-forge
     importlib_resources       6.4.0              pyhd8ed1ab_0    conda-forge
     iniconfig                 2.0.0              pyhd8ed1ab_0    conda-forge
     ipykernel                 6.29.3             pyhd33586a_0    conda-forge
     ipython                   8.22.2             pyh707e725_0    conda-forge
     jedi                      0.19.1             pyhd8ed1ab_0    conda-forge
     jinja2                    3.1.3              pyhd8ed1ab_0    conda-forge
     joblib                    1.4.0              pyhd8ed1ab_0    conda-forge
     jsonschema                4.21.1             pyhd8ed1ab_0    conda-forge
     jsonschema-specifications 2023.12.1          pyhd8ed1ab_0    conda-forge
     jupyter_client            8.6.1              pyhd8ed1ab_0    conda-forge
     jupyter_core              5.7.2           py311h38be061_0    conda-forge
     jupyterlab_pygments       0.3.0              pyhd8ed1ab_1    conda-forge
     kernel-headers_linux-64   3.10.0              h4a8ded7_14    conda-forge
     keyutils                  1.6.1                h166bdaf_0    conda-forge
     kiwisolver                1.4.5           py311h9547e67_1    conda-forge
     krb5                      1.21.2               h659d440_0    conda-forge
     lcms2                     2.16                 hb7c19ff_0    conda-forge
     ld_impl_linux-64          2.40                 h55db66e_0    conda-forge
     lerc                      4.0.0                h27087fc_0    conda-forge
     libabseil                 20240116.2      cxx17_h59595ed_0    conda-forge
     libarrow                  14.0.2          hefa796f_19_cpu    conda-forge
     libarrow-acero            14.0.2          hbabe93e_19_cpu    conda-forge
     libarrow-dataset          14.0.2          hbabe93e_19_cpu    conda-forge
     libarrow-flight           14.0.2          hc4f8a93_19_cpu    conda-forge
     libarrow-flight-sql       14.0.2          he4f5ca8_19_cpu    conda-forge
     libarrow-gandiva          14.0.2          hc1954e9_19_cpu    conda-forge
     libarrow-substrait        14.0.2          he4f5ca8_19_cpu    conda-forge
     libblas                   3.9.0           22_linux64_openblas    conda-forge
     libbrotlicommon           1.1.0                hd590300_1    conda-forge
     libbrotlidec              1.1.0                hd590300_1    conda-forge
     libbrotlienc              1.1.0                hd590300_1    conda-forge
     libcblas                  3.9.0           22_linux64_openblas    conda-forge
     libcrc32c                 1.1.2                h9c3ff4c_0    conda-forge
     libcublas                 12.2.5.6             hd3aeb46_0    conda-forge
     libcublas-dev             12.2.5.6             hd3aeb46_0    conda-forge
     libcudf                   24.06.00a164    cuda12_240430_gab5e3f3bc8_164    rapidsai-nightly
     libcufft                  11.0.8.103           hd3aeb46_0    conda-forge
     libcufft-dev              11.0.8.103           hd3aeb46_0    conda-forge
     libcufile                 1.7.2.10             hd3aeb46_0    conda-forge
     libcufile-dev             1.7.2.10             hd3aeb46_0    conda-forge
     libcumlprims              24.06.00a       cuda12_240429_g98a3699_7    rapidsai-nightly
     libcurand                 10.3.3.141           hd3aeb46_0    conda-forge
     libcurand-dev             10.3.3.141           hd3aeb46_0    conda-forge
     libcurl                   8.7.1                hca28451_0    conda-forge
     libcusolver               11.5.2.141           hd3aeb46_0    conda-forge
     libcusolver-dev           11.5.2.141           hd3aeb46_0    conda-forge
     libcusparse               12.1.2.141           hd3aeb46_0    conda-forge
     libcusparse-dev           12.1.2.141           hd3aeb46_0    conda-forge
     libdeflate                1.20                 hd590300_0    conda-forge
     libedit                   3.1.20191231         he28a2e2_2    conda-forge
     libev                     4.33                 hd590300_2    conda-forge
     libevent                  2.1.12               hf998b51_1    conda-forge
     libexpat                  2.6.2                h59595ed_0    conda-forge
     libffi                    3.4.2                h7f98852_5    conda-forge
     libgcc-devel_linux-64     11.4.0             hc2b0fca_106    conda-forge
     libgcc-ng                 13.2.0               hc881cc4_6    conda-forge
     libgd                     2.3.3                h119a65a_9    conda-forge
     libgfortran-ng            13.2.0               h69a702a_6    conda-forge
     libgfortran5              13.2.0               h43f5ff8_6    conda-forge
     libglib                   2.80.0               hf2295e7_6    conda-forge
     libgomp                   13.2.0               hc881cc4_6    conda-forge
     libgoogle-cloud           2.23.0               h9be4e54_1    conda-forge
     libgoogle-cloud-storage   2.23.0               hc7a4891_1    conda-forge
     libgrpc                   1.62.2               h15f2491_0    conda-forge
     libhwloc                  2.10.0          default_h2fb2949_1000    conda-forge
     libiconv                  1.17                 hd590300_2    conda-forge
     libjpeg-turbo             3.0.0                hd590300_1    conda-forge
     libkvikio                 24.06.00a       cuda12_240430_g7b0231c_11    rapidsai-nightly
     liblapack                 3.9.0           22_linux64_openblas    conda-forge
     libllvm14                 14.0.6               hcd5def8_4    conda-forge
     libllvm15                 15.0.7               hb3ce162_4    conda-forge
     libnghttp2                1.58.0               h47da74e_1    conda-forge
     libnl                     3.9.0                hd590300_0    conda-forge
     libnsl                    2.0.1                hd590300_0    conda-forge
     libnvjitlink              12.2.140             hd3aeb46_0    conda-forge
     libopenblas               0.3.27          pthreads_h413a1c8_0    conda-forge
     libparquet                14.0.2          hacf5a1f_19_cpu    conda-forge
     libpng                    1.6.43               h2797004_0    conda-forge
     libprotobuf               4.25.3               h08a7969_0    conda-forge
     libraft                   24.06.00a42     cuda12_240429_gd4d92ce9_42    rapidsai-nightly
     libraft-headers           24.06.00a42     cuda12_240429_gd4d92ce9_42    rapidsai-nightly
     libraft-headers-only      24.06.00a42     cuda12_240429_gd4d92ce9_42    rapidsai-nightly
     libre2-11                 2023.09.01           h5a48ba9_2    conda-forge
     librmm                    24.06.00a14     cuda12_240430_g9e6db746_14    rapidsai-nightly
     librsvg                   2.58.0               hadf69e7_1    conda-forge
     libsanitizer              11.4.0               hc2b0fca_6    conda-forge
     libsodium                 1.0.18               h36c2ea0_1    conda-forge
     libsqlite                 3.45.3               h2797004_0    conda-forge
     libssh2                   1.11.0               h0841786_0    conda-forge
     libstdcxx-devel_linux-64  11.4.0             hc2b0fca_106    conda-forge
     libstdcxx-ng              13.2.0               h95c4c6d_6    conda-forge
     libthrift                 0.19.0               hb90f79a_1    conda-forge
     libtiff                   4.6.0                h1dd3fc0_3    conda-forge
     libutf8proc               2.8.0                h166bdaf_0    conda-forge
     libuuid                   2.38.1               h0b41bf4_0    conda-forge
     libuv                     1.48.0               hd590300_0    conda-forge
     libwebp                   1.3.2                h658648e_1    conda-forge
     libwebp-base              1.3.2                hd590300_1    conda-forge
     libxcb                    1.15                 h0b41bf4_0    conda-forge
     libxcrypt                 4.4.36               hd590300_1    conda-forge
     libxml2                   2.12.6               h232c23b_2    conda-forge
     libzlib                   1.2.13               hd590300_5    conda-forge
     llvmlite                  0.42.0          py311ha6695c7_1    conda-forge
     locket                    1.0.0              pyhd8ed1ab_0    conda-forge
     lz4                       4.3.3           py311h38e4bf4_0    conda-forge
     lz4-c                     1.9.4                hcb278e6_0    conda-forge
     makefun                   1.15.2             pyhd8ed1ab_0    conda-forge
     markdown                  3.6                pyhd8ed1ab_0    conda-forge
     markdown-it-py            3.0.0              pyhd8ed1ab_0    conda-forge
     markupsafe                2.1.5           py311h459d7ec_0    conda-forge
     matplotlib-base           3.8.4           py311h54ef318_0    conda-forge
     matplotlib-inline         0.1.7              pyhd8ed1ab_0    conda-forge
     mdurl                     0.1.2              pyhd8ed1ab_0    conda-forge
     mistune                   3.0.2              pyhd8ed1ab_0    conda-forge
     msgpack-python            1.0.7           py311h9547e67_0    conda-forge
     multipledispatch          0.6.0                      py_0    conda-forge
     munkres                   1.1.4              pyh9f0ad1d_0    conda-forge
     nbclient                  0.10.0             pyhd8ed1ab_0    conda-forge
     nbconvert                 7.16.3               hd8ed1ab_1    conda-forge
     nbconvert-core            7.16.3             pyhd8ed1ab_1    conda-forge
     nbconvert-pandoc          7.16.3               hd8ed1ab_1    conda-forge
     nbformat                  5.10.4             pyhd8ed1ab_0    conda-forge
     nbsphinx                  0.9.3              pyhd8ed1ab_0    conda-forge
     nccl                      2.21.5.1             h3a97aeb_0    conda-forge
     ncurses                   6.4.20240210         h59595ed_0    conda-forge
     nest-asyncio              1.6.0              pyhd8ed1ab_0    conda-forge
     ninja                     1.12.0               h00ab1b0_0    conda-forge
     nltk                      3.8.1              pyhd8ed1ab_0    conda-forge
     numba                     0.59.1          py311h96b013e_0    conda-forge
     numpy                     1.26.4          py311h64a7726_0    conda-forge
     numpydoc                  1.7.0              pyhd8ed1ab_0    conda-forge
     nvcomp                    3.0.6                h10b603f_0    conda-forge
     nvtx                      0.2.10          py311h459d7ec_0    conda-forge
     openjpeg                  2.5.2                h488ebb8_0    conda-forge
     openssl                   3.2.1                hd590300_1    conda-forge
     orc                       2.0.0                h17fec99_1    conda-forge
     packaging                 24.0               pyhd8ed1ab_0    conda-forge
     pandas                    2.2.2           py311h320fe9a_0    conda-forge
     pandoc                    3.1.13               ha770c72_0    conda-forge
     pandocfilters             1.5.0              pyhd8ed1ab_0    conda-forge
     pango                     1.52.2               ha41ecd1_0    conda-forge
     parso                     0.8.4              pyhd8ed1ab_0    conda-forge
     partd                     1.4.1              pyhd8ed1ab_0    conda-forge
     pathspec                  0.12.1             pyhd8ed1ab_0    conda-forge
     patsy                     0.5.6              pyhd8ed1ab_0    conda-forge
     pcre2                     10.43                hcad00b1_0    conda-forge
     pexpect                   4.9.0              pyhd8ed1ab_0    conda-forge
     pickleshare               0.7.5                   py_1003    conda-forge
     pillow                    10.3.0          py311h18e6fac_0    conda-forge
     pip                       24.0               pyhd8ed1ab_0    conda-forge
     pixman                    0.43.2               h59595ed_0    conda-forge
     pkgutil-resolve-name      1.3.10             pyhd8ed1ab_1    conda-forge
     platformdirs              4.2.1              pyhd8ed1ab_0    conda-forge
     pluggy                    1.5.0              pyhd8ed1ab_0    conda-forge
     prompt-toolkit            3.0.42             pyha770c72_0    conda-forge
     psutil                    5.9.8           py311h459d7ec_0    conda-forge
     pthread-stubs             0.4               h36c2ea0_1001    conda-forge
     ptyprocess                0.7.0              pyhd3deb0d_0    conda-forge
     pure_eval                 0.2.2              pyhd8ed1ab_0    conda-forge
     py-cpuinfo                9.0.0              pyhd8ed1ab_0    conda-forge
     pyarrow                   14.0.2          py311hd5e4297_19_cpu    conda-forge
     pydata-sphinx-theme       0.15.2             pyhd8ed1ab_0    conda-forge
     pygments                  2.17.2             pyhd8ed1ab_0    conda-forge
     pylibraft                 24.06.00a42     cuda12_py311_240429_gd4d92ce9_42    rapidsai-nightly
     pynndescent               0.5.8              pyh1a96a4e_0    conda-forge
     pynvjitlink               0.2.2           py311hdaa3023_0    rapidsai
     pynvml                    11.4.1             pyhd8ed1ab_0    conda-forge
     pyparsing                 3.1.2              pyhd8ed1ab_0    conda-forge
     pysocks                   1.7.1              pyha2e5f31_6    conda-forge
     pytest                    7.4.4              pyhd8ed1ab_0    conda-forge
     pytest-benchmark          4.0.0              pyhd8ed1ab_0    conda-forge
     pytest-cases              3.8.5              pyhd8ed1ab_0    conda-forge
     pytest-cov                5.0.0              pyhd8ed1ab_0    conda-forge
     pytest-xdist              3.5.0              pyhd8ed1ab_0    conda-forge
     python                    3.11.9          hb806964_0_cpython    conda-forge
     python-dateutil           2.9.0              pyhd8ed1ab_0    conda-forge
     python-fastjsonschema     2.19.1             pyhd8ed1ab_0    conda-forge
     python-tzdata             2024.1             pyhd8ed1ab_0    conda-forge
     python_abi                3.11                    4_cp311    conda-forge
     pytz                      2024.1             pyhd8ed1ab_0    conda-forge
     pyyaml                    6.0.1           py311h459d7ec_1    conda-forge
     pyzmq                     26.0.2          py311h08a0b41_0    conda-forge
     raft-dask                 24.06.00a42     cuda12_py311_240429_gd4d92ce9_42    rapidsai-nightly
     rapids-dask-dependency    24.06.00a20                py_0    rapidsai-nightly
     rdma-core                 51.0                 hd3aeb46_0    conda-forge
     re2                       2023.09.01           h7f4b329_2    conda-forge
     readline                  8.2                  h8228510_1    conda-forge
     recommonmark              0.7.1              pyhd8ed1ab_0    conda-forge
     referencing               0.35.0             pyhd8ed1ab_0    conda-forge
     regex                     2024.4.28       py311h331c9d8_0    conda-forge
     requests                  2.31.0             pyhd8ed1ab_0    conda-forge
     rhash                     1.4.4                hd590300_0    conda-forge
     rich                      13.7.1             pyhd8ed1ab_0    conda-forge
     rmm                       24.06.00a14     cuda12_py311_240430_g9e6db746_14    rapidsai-nightly
     rpds-py                   0.18.0          py311h46250e7_0    conda-forge
     s2n                       1.4.12               h06160fa_0    conda-forge
     scikit-build-core         0.9.2              pyh4af843d_0    conda-forge
     scikit-learn              1.2.0           py311h67c5ca5_0    conda-forge
     scipy                     1.13.0          py311h64a7726_0    conda-forge
     seaborn                   0.13.2               hd8ed1ab_0    conda-forge
     seaborn-base              0.13.2             pyhd8ed1ab_0    conda-forge
     setuptools                69.5.1             pyhd8ed1ab_0    conda-forge
     six                       1.16.0             pyh6c4a22f_0    conda-forge
     snappy                    1.2.0                hdb0a2a9_1    conda-forge
     snowballstemmer           2.2.0              pyhd8ed1ab_0    conda-forge
     sortedcontainers          2.4.0              pyhd8ed1ab_0    conda-forge
     soupsieve                 2.5                pyhd8ed1ab_1    conda-forge
     sparse                    0.15.1             pyhd8ed1ab_1    conda-forge
     spdlog                    1.12.0               hd2e6256_2    conda-forge
     sphinx                    5.3.0              pyhd8ed1ab_0    conda-forge
     sphinx-copybutton         0.5.2              pyhd8ed1ab_0    conda-forge
     sphinx-markdown-tables    0.0.17             pyh6c4a22f_0    conda-forge
     sphinxcontrib-applehelp   1.0.8              pyhd8ed1ab_0    conda-forge
     sphinxcontrib-devhelp     1.0.6              pyhd8ed1ab_0    conda-forge
     sphinxcontrib-htmlhelp    2.0.5              pyhd8ed1ab_0    conda-forge
     sphinxcontrib-jsmath      1.0.1              pyhd8ed1ab_0    conda-forge
     sphinxcontrib-qthelp      1.0.7              pyhd8ed1ab_0    conda-forge
     sphinxcontrib-serializinghtml 1.1.10             pyhd8ed1ab_0    conda-forge
     stack_data                0.6.2              pyhd8ed1ab_0    conda-forge
     statsmodels               0.14.1          py311h1f0f07a_0    conda-forge
     sysroot_linux-64          2.17                h4a8ded7_14    conda-forge
     tabulate                  0.9.0              pyhd8ed1ab_1    conda-forge
     tbb                       2021.12.0            h00ab1b0_0    conda-forge
     tblib                     3.0.0              pyhd8ed1ab_0    conda-forge
     threadpoolctl             3.5.0              pyhc1e730c_0    conda-forge
     tinycss2                  1.3.0              pyhd8ed1ab_0    conda-forge
     tk                        8.6.13          noxft_h4845f30_101    conda-forge
     toml                      0.10.2             pyhd8ed1ab_0    conda-forge
     tomli                     2.0.1              pyhd8ed1ab_0    conda-forge
     toolz                     0.12.1             pyhd8ed1ab_0    conda-forge
     tornado                   6.4             py311h459d7ec_0    conda-forge
     tqdm                      4.66.2             pyhd8ed1ab_0    conda-forge
     traitlets                 5.14.3             pyhd8ed1ab_0    conda-forge
     treelite                  4.1.2           py311he8f9275_1    conda-forge
     typing-extensions         4.11.0               hd8ed1ab_0    conda-forge
     typing_extensions         4.11.0             pyha770c72_0    conda-forge
     tzdata                    2024a                h0c530f3_0    conda-forge
     ucx                       1.15.0               hda83522_8    conda-forge
     ucx-proc                  1.0.0                       gpu    rapidsai
     ucx-py                    0.38.00a4       py311_240430_g03c864b_4    rapidsai-nightly
     umap-learn                0.5.3           py311h38be061_1    conda-forge
     urllib3                   2.2.1              pyhd8ed1ab_0    conda-forge
     wcwidth                   0.2.13             pyhd8ed1ab_0    conda-forge
     webencodings              0.5.1              pyhd8ed1ab_2    conda-forge
     wheel                     0.43.0             pyhd8ed1ab_1    conda-forge
     xorg-kbproto              1.0.7             h7f98852_1002    conda-forge
     xorg-libice               1.1.1                hd590300_0    conda-forge
     xorg-libsm                1.2.4                h7391055_0    conda-forge
     xorg-libx11               1.8.9                h8ee46fc_0    conda-forge
     xorg-libxau               1.0.11               hd590300_0    conda-forge
     xorg-libxdmcp             1.1.3                h7f98852_0    conda-forge
     xorg-libxext              1.3.4                h0b41bf4_2    conda-forge
     xorg-libxrender           0.9.11               hd590300_0    conda-forge
     xorg-renderproto          0.11.1            h7f98852_1002    conda-forge
     xorg-xextproto            7.3.0             h0b41bf4_1003    conda-forge
     xorg-xproto               7.0.31            h7f98852_1007    conda-forge
     xyzservices               2024.4.0           pyhd8ed1ab_0    conda-forge
     xz                        5.2.6                h166bdaf_0    conda-forge
     yaml                      0.2.5                h7f98852_2    conda-forge
     zeromq                    4.3.5                h59595ed_1    conda-forge
     zict                      3.0.0              pyhd8ed1ab_0    conda-forge
     zipp                      3.17.0             pyhd8ed1ab_0    conda-forge
     zlib                      1.2.13               hd590300_5    conda-forge
     zstd                      1.5.5                hfc55251_0    conda-forge

</pre></details>

",2024-05-02T08:45:18Z,0,0,Tim Head,Nvidia ,False
840,[FEA] Concatenate dictionary of objects along axis=0,"Following up from https://github.com/rapidsai/cudf/issues/15115 and the implementation for `axis=1`.

We need to implement concatenation of dictionary objects along `axis=0`.

See important context from @shwina here https://github.com/rapidsai/cudf/issues/15115#issuecomment-1961179887",2024-05-04T00:56:15Z,0,0,,,False
841,[BUG] Concat `Index` behavior diverts from `pandas`,"As @wence- points out here https://github.com/rapidsai/cudf/pull/15623#discussion_r1586054947

We allow concatenating indexes whilst `pandas` does not.

Do we like this difference in behavior? Do we want parity? Would users be upset if we changed this behavior?",2024-05-04T04:31:14Z,0,0,,,False
842,[BUG] `test_concat` file instantiates GPU objects in the parametrize arguments,"As @bdice points out here: https://github.com/rapidsai/cudf/pull/15623#discussion_r1589362754

GPU object instantiation within the parametrize arguments results in the test suite being slower to launch. We should refactor tests as such: https://github.com/rapidsai/cudf/pull/15623/commits/b5b91166a7cb8e865e4c989e0a67f930fe643d63

Ideally, this refactor would occur across all test files with this associated issue.",2024-05-04T04:52:35Z,0,0,,,False
843,[QST] Recusively Generating AST Expressions (C++ libcudf),"I would like to implement a function that can generate AST expressions, for example, by recursively converting a different expression class, such as an Apache Arrow `arrow::compute::expression` [object](https://github.com/apache/arrow/blob/main/cpp/src/arrow/compute/expression.h), to a `cudf::ast::expression` [object](https://github.com/rapidsai/cudf/blob/branch-24.06/cpp/include/cudf/ast/expressions.hpp). 

Is this even possible? libcudf's AST expression classes only accept references that are owned by the caller function; this is a problem since we can not recurse anymore. Is there any way to do this, to implement functions that can generate AST expressions instead of hardcoding expressions in a caller?

",2024-05-04T16:10:19Z,0,0,Shriram Chandran,ETH Zürich,False
844,[FEA] Make `cudf.pandas` not perform redundant CPU<->GPU transfers if there is no in-place write operations,"**Is your feature request related to a problem? Please describe.**
In `cudf.pandas` we currently move dataframes from CPU to GPU or vice-versa for every step entirely. We can avoid performing transfers all the time by storing the dataframe in both memories and spending time in CPU<->GPU transfers if there are no in-place operations on the frames.

```python

In [1]: %load_ext cudf.pandas

In [2]: import pandas as pd

In [3]: df = pd.read_parquet(
   ...:     ""nyc_parking_violations_2022.parquet"",
   ...:     columns=[""Registration State"", ""Violation Description"", ""Vehicle Body Type"", ""Issue Date"", ""Summons Number""]
   ...: )

In [4]: %time df.count(axis=0)
CPU times: user 1.41 ms, sys: 4.35 ms, total: 5.75 ms
Wall time: 5.15 ms
Out[4]: 
Registration State       15435607
Violation Description    15435607
Vehicle Body Type        15435607
Issue Date               15435607
Summons Number           15435607
dtype: int64

In [5]: %time df.count(axis=1)
CPU times: user 15.7 s, sys: 1.85 s, total: 17.5 s
Wall time: 16.8 s
Out[5]: 
0           5
1           5
2           5
3           5
4           5
           ..
15435602    5
15435603    5
15435604    5
15435605    5
15435606    5
Length: 15435607, dtype: int64

In [6]: %time df.count(axis=0)
CPU times: user 24 s, sys: 2.43 s, total: 26.4 s
Wall time: 25.3 s
Out[6]: 
Registration State       15435607
Violation Description    15435607
Vehicle Body Type        15435607
Issue Date               15435607
Summons Number           15435607
dtype: int64

In [7]: %time df.count(axis=0)
CPU times: user 0 ns, sys: 3.08 ms, total: 3.08 ms
Wall time: 2.75 ms
Out[7]: 
Registration State       15435607
Violation Description    15435607
Vehicle Body Type        15435607
Issue Date               15435607
Summons Number           15435607
dtype: int64
```

Notice the `df.count(axis=0)` in cell `6` taking quite a bit of time to move from CPU to GPU, we can avoid this.

**Describe the solution you'd like**
Maintain two identical copies of dataframe - one in GPU, another in CPU.

",2024-05-06T18:50:01Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
845,[BUG] cudf.pandas wrapped numpy arrays not compatible with numba,"**Describe the bug**
When I try to use cudf.pandas with [datashader](https://github.com/holoviz/datashader), I get an error `Cannot determine Numba type of <class 'cudf.pandas._wrappers.numpy.ndarray'>`, full repro below. Datashader actually works directly with cudf, and a cudf.DataFrame is an exceptable data format. But using cudf as a no-code-change accelerator for pandas, this seems to fail.


**Steps/Code to reproduce bug**

```python
import cudf.pandas
cudf.pandas.install()

import pandas as pd
import numpy as np
import datashader as ds
import datashader.transfer_functions as tf
from datashader.colors import inferno

# Create a small dataset
np.random.seed(0)
n = 1000
df = pd.DataFrame({
    'x': np.random.normal(0, 1, n),
    'y': np.random.normal(0, 1, n)
})

# Create a canvas to render the plot
cvs = ds.Canvas(plot_width=400, plot_height=400)

# Aggregate the points in the canvas
agg = cvs.points(df, 'x', 'y')

# Render the plot using a transfer function
img = tf.shade(agg, cmap=inferno, how='eq_hist')

# Display the plot
img
```
Output
```bash
TypingError: Failed in nopython mode pipeline (step: nopython frontend)
non-precise type pyobject
During: typing of argument at [/home/ajay/miniconda3/envs/rapids-24.06/lib/python3.11/site-packages/datashader/glyphs/glyph.py](http://localhost:8888/lab/tree/dev/miniconda3/envs/rapids-24.06/lib/python3.11/site-packages/datashader/glyphs/glyph.py) (66)

File "".[./miniconda3/envs/rapids-24.06/lib/python3.11/site-packages/datashader/glyphs/glyph.py"", line 66](http://localhost:8888/lab/tree/dev/miniconda3/envs/rapids-24.06/lib/python3.11/site-packages/datashader/glyphs/glyph.py#line=65):
    def _compute_bounds(s):
        <source elided>

    @staticmethod
    ^ 

This error may have been caused by the following argument(s):
- argument 0: Cannot determine Numba type of <class 'cudf.pandas._wrappers.numpy.ndarray'>
```

**Expected behavior**
Ideally same output as a cudf or a pandas dataframe.

**Environment overview (please complete the following information)**
 - Environment location: Ubuntu
 - Method of cuDF install: Conda
",2024-05-07T19:56:09Z,0,0,Ajay Thorve,Nvidia,True
846,[BUG] Series/Single Column DataFrame Groupby value_counts fails (DataFrame Groupby value_counts succeeds),"Groupby value_counts fails on when selecting individual columns from a DataFrame, but succeeds when running on the entire DataFrame.

```python
import pandas as pd
import cudf

gdf = cudf.datasets.randomdata(dtypes={""id"": int, ""x"": int})
pdf = gdf.to_pandas()

print(pdf.groupby(""id"").x.value_counts().head())
print(gdf.groupby(""id"").x.value_counts())
id   x   
942  988     1
961  1026    1
965  1062    1
984  981     1
993  999     1
Name: count, dtype: int64
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
File [/nvme/0/nicholasb/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/cudf/core/groupby/groupby.py:2783](http://10.136.7.109:8881/lab/tree/nvme/0/nicholasb/benchmarks/nvme/0/nicholasb/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/cudf/core/groupby/groupby.py#line=2782), in _Grouping._handle_by_or_level(self, by, level)
   2782 try:
-> 2783     self._handle_label(by)
   2784 except (KeyError, TypeError):

File [/nvme/0/nicholasb/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/cudf/core/groupby/groupby.py:2845](http://10.136.7.109:8881/lab/tree/nvme/0/nicholasb/benchmarks/nvme/0/nicholasb/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/cudf/core/groupby/groupby.py#line=2844), in _Grouping._handle_label(self, by)
   2844     else:
-> 2845         raise e
   2846 self.names.append(by)

File [/nvme/0/nicholasb/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/cudf/core/groupby/groupby.py:2839](http://10.136.7.109:8881/lab/tree/nvme/0/nicholasb/benchmarks/nvme/0/nicholasb/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/cudf/core/groupby/groupby.py#line=2838), in _Grouping._handle_label(self, by)
   2838 try:
-> 2839     self._key_columns.append(self._obj._data[by])
   2840 except KeyError as e:
   2841     # `by` can be index name(label) too.

File [/nvme/0/nicholasb/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/cudf/core/column_accessor.py:155](http://10.136.7.109:8881/lab/tree/nvme/0/nicholasb/benchmarks/nvme/0/nicholasb/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/cudf/core/column_accessor.py#line=154), in ColumnAccessor.__getitem__(self, key)
    154 def __getitem__(self, key: Any) -> ColumnBase:
--> 155     return self._data[key]

KeyError: 'id'

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
Cell In[31], line 8
      5 pdf = gdf.to_pandas()
      7 print(pdf.groupby(""id"").x.value_counts().head())
----> 8 print(gdf.groupby(""id"").x.value_counts())

File [/nvme/0/nicholasb/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/cudf/core/groupby/groupby.py:2598](http://10.136.7.109:8881/lab/tree/nvme/0/nicholasb/benchmarks/nvme/0/nicholasb/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/cudf/core/groupby/groupby.py#line=2597), in GroupBy.value_counts(self, subset, normalize, sort, ascending, dropna)
   2591     raise ValueError(
   2592         f""Keys {set(subset) & set(groupings)} in subset ""
   2593         ""cannot be in the groupby column keys.""
   2594     )
   2596 df[""__placeholder""] = 1
   2597 result = (
-> 2598     df.groupby(groupings + list(subset), dropna=dropna)[
   2599         ""__placeholder""
   2600     ]
   2601     .count()
   2602     .sort_index()
   2603     .astype(np.int64)
   2604 )
   2606 if normalize:
   2607     levels = list(range(len(groupings), result.index.nlevels))

File [/nvme/0/nicholasb/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/nvtx/nvtx.py:116](http://10.136.7.109:8881/lab/tree/nvme/0/nicholasb/benchmarks/nvme/0/nicholasb/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/nvtx/nvtx.py#line=115), in annotate.__call__.<locals>.inner(*args, **kwargs)
    113 @wraps(func)
    114 def inner(*args, **kwargs):
    115     libnvtx_push_range(self.attributes, self.domain.handle)
--> 116     result = func(*args, **kwargs)
    117     libnvtx_pop_range(self.domain.handle)
    118     return result

File [/nvme/0/nicholasb/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/cudf/core/series.py:3426](http://10.136.7.109:8881/lab/tree/nvme/0/nicholasb/benchmarks/nvme/0/nicholasb/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/cudf/core/series.py#line=3425), in Series.groupby(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)
   3400 @_cudf_nvtx_annotate
   3401 @docutils.doc_apply(
   3402     groupby_doc_template.format(
   (...)
   3424     dropna=True,
   3425 ):
-> 3426     return super().groupby(
   3427         by,
   3428         axis,
   3429         level,
   3430         as_index,
   3431         sort,
   3432         group_keys,
   3433         squeeze,
   3434         observed,
   3435         dropna,
   3436     )

File [/nvme/0/nicholasb/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/nvtx/nvtx.py:116](http://10.136.7.109:8881/lab/tree/nvme/0/nicholasb/benchmarks/nvme/0/nicholasb/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/nvtx/nvtx.py#line=115), in annotate.__call__.<locals>.inner(*args, **kwargs)
    113 @wraps(func)
    114 def inner(*args, **kwargs):
    115     libnvtx_push_range(self.attributes, self.domain.handle)
--> 116     result = func(*args, **kwargs)
    117     libnvtx_pop_range(self.domain.handle)
    118     return result

File [/nvme/0/nicholasb/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/cudf/core/indexed_frame.py:5337](http://10.136.7.109:8881/lab/tree/nvme/0/nicholasb/benchmarks/nvme/0/nicholasb/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/cudf/core/indexed_frame.py#line=5336), in IndexedFrame.groupby(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)
   5331 if group_keys is None:
   5332     group_keys = False
   5334 return (
   5335     self.__class__._resampler(self, by=by)
   5336     if isinstance(by, cudf.Grouper) and by.freq
-> 5337     else self.__class__._groupby(
   5338         self,
   5339         by=by,
   5340         level=level,
   5341         as_index=as_index,
   5342         dropna=dropna,
   5343         sort=sort,
   5344         group_keys=group_keys,
   5345     )
   5346 )

File [/nvme/0/nicholasb/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/cudf/core/groupby/groupby.py:283](http://10.136.7.109:8881/lab/tree/nvme/0/nicholasb/benchmarks/nvme/0/nicholasb/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/cudf/core/groupby/groupby.py#line=282), in GroupBy.__init__(self, obj, by, level, sort, as_index, dropna, group_keys)
    281     self.grouping = self._by
    282 else:
--> 283     self.grouping = _Grouping(obj, self._by, level)

File [/nvme/0/nicholasb/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/cudf/core/groupby/groupby.py:2751](http://10.136.7.109:8881/lab/tree/nvme/0/nicholasb/benchmarks/nvme/0/nicholasb/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/cudf/core/groupby/groupby.py#line=2750), in _Grouping.__init__(self, obj, by, level)
   2748 # Need to keep track of named key columns
   2749 # to support `as_index=False` correctly
   2750 self._named_columns = []
-> 2751 self._handle_by_or_level(by, level)
   2753 if len(obj) and not len(self._key_columns):
   2754     raise ValueError(""No group keys passed"")

File [/nvme/0/nicholasb/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/cudf/core/groupby/groupby.py:2785](http://10.136.7.109:8881/lab/tree/nvme/0/nicholasb/benchmarks/nvme/0/nicholasb/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/cudf/core/groupby/groupby.py#line=2784), in _Grouping._handle_by_or_level(self, by, level)
   2783     self._handle_label(by)
   2784 except (KeyError, TypeError):
-> 2785     self._handle_misc(by)

File [/nvme/0/nicholasb/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/cudf/core/groupby/groupby.py:2868](http://10.136.7.109:8881/lab/tree/nvme/0/nicholasb/benchmarks/nvme/0/nicholasb/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/cudf/core/groupby/groupby.py#line=2867), in _Grouping._handle_misc(self, by)
   2866 by = cudf.core.column.as_column(by)
   2867 if len(by) != len(self._obj):
-> 2868     raise ValueError(""Grouper and object must have same length"")
   2869 self._key_columns.append(by)
   2870 self.names.append(None)

ValueError: Grouper and object must have same length
```

```python
print(gdf.groupby(""id"").value_counts()) # succeeds
# print(gdf.groupby(""id"")[[""x""]].value_counts()) # same error as above
```
",2024-05-07T20:52:37Z,0,0,Nick Becker,@NVIDIA,True
847,[FEA] Explicitly guarantee row group ordering in the parquet reader.,"From @devavret , the question came up as to whether we guarantee the relative ordering of row groups across multiple input files in the parquet reader.  That is, if you have two files `[f1, f2]` and the row groups within the files (in one column) are specified as `[[r0,r3], [r0,r1]]`, do we guarantee the output ordering would be  `[f1r0, f1r3, f2r0, f2r1]`

The code does in fact do this for both the explicitly specified case and the unspecified (empty user input / all row groups), but we don't make any guarantees about it.   Seems like a safe and easy thing to add.

https://github.com/rapidsai/cudf/blob/5d244dfc13f4db0b1e41ded3029942fec50c98f6/cpp/src/io/parquet/reader_impl_helpers.cpp#L663

",2024-05-07T21:27:02Z,0,0,,,False
848,[FEA] Migrate left join and conditional join benchmarks to use nvbench,"**Is your feature request related to a problem? Please describe.**
The current [left join](https://github.com/rapidsai/cudf/blob/580ee40bf5fe1a66eaba914cdddb718a09193bab/cpp/benchmarks/join/left_join.cu) and [conditional join](https://github.com/rapidsai/cudf/blob/580ee40bf5fe1a66eaba914cdddb718a09193bab/cpp/benchmarks/join/conditional_join.cu) benchmarks are still using gbench. We should migrate them to use nvbench for simplicity and consistency.

**Describe the solution you'd like**


TODO:

- [ ] Migrate from gbench to nvbench
- [ ] Similar to #15644, use `JOIN_KEY_TYPE_RANGE`, `JOIN_NULLABLE_RANGE` and `JOIN_SIZE_RANGE` to reduce the number of test cases and simplify the implementation
- [ ] Get rid of the dispatching between gbench and nvbench in [join_common.hpp](https://github.com/rapidsai/cudf/blob/580ee40bf5fe1a66eaba914cdddb718a09193bab/cpp/benchmarks/join/join_common.hpp) 
",2024-05-07T22:21:57Z,0,0,Yunsong Wang,@NVIDIA @rapidsai,True
849,[FEA] Implement new test organization in cuDF,"**Is your feature request related to a problem? Please describe.**
https://github.com/rapidsai/cudf/pull/12288/ created a new organization for Python tests to handle the request in https://github.com/rapidsai/cudf/issues/4730. The intent was to migrate tests to the new directories over time. However, soon after #12288 merged our focus shifted substantially towards the cudf.pandas effort, and now the cudf.polars and pylibcudf projects. As a result, the transition to a new testing organization remains largely incomplete. The combined work around the pylibcudf backend and the polars/pandas frontends already requires a fairly thorough revamping of tests. We are writing new pylibcudf tests with each PR now, and we are adding cudf tests as we go. Some of the old tests, particularly of cudf's column layer, are likely to become obsolete as we rearchitect the internals around pylibcudf. As this process proceeds, we may lose track of how we want the cudf tests reorganized.

**Describe the solution you'd like**
The next time that we have a tech debt-focused release of cudf, we should consider doing some wholesale migrations and cleanup of tests to get rid of unnecessarily duplicated tests and move tests into the final destinations that we want.",2024-05-10T22:52:59Z,0,0,Vyas Ramasubramani,@rapidsai,True
850,[QST] aggregate function that operates on vector(array of numeric) data,"**What is your question?**
I am wondering if `cudf` has native or built-in support for aggregate function that run against vector data. Namley, text/image embeddings are stored in the column of csv/parquet file. And I'd like to run various aggregate functions such as `mean`, `max` and so on. All these operations are element-wise, namely, it returns the mean of all the values in same index and return an array with same lenght. What's more, I'd like to run K-Nearest-Neighbor search as well.

If not natively supported, how to achieve these operations with performance efficient?

example code:
```
import cudf
import numpy as np
import pandas as pd

# Sample DataFrame with Pandas to cuDF conversion
data = {
    'category': ['A', 'A', 'B', 'B'],
    'values': [np.array([1, 2, 3]), np.array([4, 5, 6]), np.array([7, 8, 9]), np.array([10, 11, 12])]
}
pdf = pd.DataFrame(data)
df = cudf.DataFrame.from_pandas(pdf)

result = df.groupby('category').agg({'values': ['sum', 'mean']})

print(result)

# Expected output
'''
category
A     [2.5, 3.5, 4.5]
B    [8.5, 9.5, 10.5]
Name: values, dtype: object
'''
```",2024-05-14T03:35:37Z,0,0,Rhett Ying,@aws,False
851,[FEA] Make line terminator sequence handling in regular expression engine a configurable option,"**Is your feature request related to a problem? Please describe.**
Some notes from #11979 here: The `$` matches at the position right before a line terminator in regular expressions. In cuDF (and in Python), this is right before a  newline`\n`. However, in Spark (or rather the JDK), the line terminator can be any one of the following sequences: `\r`, `\n`, `\r\n`, `\^E`, `\u2028`, or `\u2029` (unless UNIX_LINES mode is activated) (see https://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html#lt). 


**Describe the solution you'd like**
It would be useful if we could configure the concept of line terminator sequences in cuDF. Ideally, this could be an optional parameter that would support a simple array of strings for line terminator sequences.  But this also be a flag that enables a `JDK_MODE` which would enabling the more complex handling that can be enabled when calling the corresponding methods from the CUDF Java library.

**Describe alternatives you've considered**
Currently, [spark-rapids](https://github.com/NVIDIA/spark-rapids) handles `$` by doing a heavy translation from a JDK regular expression to another regular expression supported by cuDF that handles the multiple possible line terminator sequences that the JDK uses.  With this translation, we are limited to only using the `$` in simple scenarios at the end of the regular expression, we cannot use them in choice `|` right now among other constructions because of the complexity (see https://github.com/NVIDIA/spark-rapids/issues/10764)

",2024-05-14T19:17:48Z,0,0,Navin Kumar,@NVIDIA,True
852,[BUG] cudf.pandas dataframe.__repr__ slow in jupyterlab for large datasets,"**Describe the bug**
Calling a dataframe.__repr__ in a notebook cell either takes very long or results in a kernel failure for large datasets. 
**Steps/Code to reproduce bug**
In a jupyterlab environment, run this in a cell:

```python

# [cell 1]
%load_ext cudf.pandas

# [cell 2]
import pandas as pd
import numpy as np

# Define the number of rows and columns
num_rows = 25_000_000
num_columns = 12

# Create a DataFrame with random data
df = pd.DataFrame(np.random.randint(0, 100, size=(num_rows, num_columns)),
                  columns=[f'Column_{i}' for i in range(1, num_columns + 1)])


# [cell 3]
df
```
![image](https://github.com/rapidsai/cudf/assets/20476096/f496473d-369a-4fe4-a91c-950944b44c05)


**Expected behavior**
dataframe should render quickly, as is the case when working directly with cudf, or pandas

**Note**
This works as expected in a python interactive shell, or when calling `print(df)` in a notebook.",2024-05-14T19:23:17Z,0,0,Ajay Thorve,Nvidia,True
853,[BUG] double free or memory corruption when parsing some JSON,"**Describe the bug**

We recently saw a SIGSEGV when trying to parse some customer data using JSON. As I stripped down and obfuscated the data it switched to the process aborting with the error.

```
double free or corruption (!prev)
Aborted (core dumped)
```

The code to reproduce this is

```
TEST_F(JsonReaderTest, JsonLinesCrash)
{
  std::string const fname = ""from_json_1_10_simplified.json"";

  cudf::io::json_reader_options options =
    cudf::io::json_reader_options::builder(cudf::io::source_info{fname})
    .lines(true)
    .recovery_mode(cudf::io::json_recovery_mode_t::RECOVER_WITH_NULL)
    .normalize_single_quotes(true)
    .normalize_whitespace(true)
    .mixed_types_as_string(true)
    .keep_quotes(true);

  auto result = cudf::io::read_json(options);
}
```

and the file is 
[from_json_1_10_simplified.json](https://github.com/rapidsai/cudf/files/15313863/from_json_1_10_simplified.json)
",2024-05-14T21:17:00Z,0,0,Robert (Bobby) Evans,Nvidia,True
854,"[BUG] Data corruption and strange CUDA memory address errors at the same row index, despite manipulating data, when using `.stack()` on large, wide dataset","**Describe the bug**
Whenever I'm trying to use cudf,stack() on this large wide dataframe, at around the same index location, the data gets corrupted as you stack past that index until it fails to run, or just fails to run.  It happens at index 1159550.   go one index before 1159550, everything is fine. One or two after, you start to see issues or it fails.  Even if you change around the data a bit, it still fails. eventually.  When it fails, it returns `RuntimeError: parallel_for: failed to synchronize: cudaErrorIllegalAddress: an illegal memory access was encountered`. 

Happens on both an A100 80GB and H100 running 24.04.  Completes successfully on pandas. Falls back to pandas and successfully completes on cudf.pandas.

**Steps/Code to reproduce bug**
This requires a dataset download, handled in the min repro, and a 32GB GPU or larger to test.

You can actually see the data getting corrupted at the incrementing runs at the end of the min repro, before it finally fails

```
!if [ ! -f ""job_skills.csv"" ]; then curl https://storage.googleapis.com/rapidsai/colab-data/job_skills.csv.gz -o job_skills.csv.gz; gunzip job_skills.csv.gz; else echo ""unzipped job data found""; fi
import cudf
skills = cudf.read_csv(""job_skills.csv"")

b = skills[""job_skills""].str.split("","", expand=True)
#print(b.iloc[1159550]) # incase you wanted to see what was on that index
print(b.iloc[1159550])
b2 = b[:1159549]
# b2 = b[:1159550] # Uncommenting this, it will fail
stacked_skills = b2.stack()
print(stacked_skills.head())

# this will also fail
# stacked_skills = b.stack().dropna()

# even if you change the dataframe a bit by moving up the indexes incrementally, it will not really change where it fails, as you can start to see the data start glitch
print(skills.count())
skills = skills.dropna()
print(skills.count())
b = skills[""job_skills""].str.split("","", expand=True)
print(b.iloc[1159550]) # in case you wanted to see what was on that index
b2 = b[:1159549]
stacked_skills = b2.stack()
print(1159549)
print(stacked_skills.head())
b2 = b[:1159550]
stacked_skills = b2.stack()
print(1159550)
print(stacked_skills.head()) # you can start to see data corruption or it just fails
b2 = b[:1159551]
stacked_skills = b2.stack()
print(1159551)
print(stacked_skills.head())
b2 = b[:1159552]
stacked_skills = b2.stack()
print(1159552)
print(stacked_skills.head())
b2 = b[:1159553]
stacked_skills = b2.stack()
print(1159553)
print(stacked_skills.head())
b2 = b[:1159554]
stacked_skills = b2.stack()
print(1159554)
print(stacked_skills.head()) # by here it should fail
```
Outputs:
```
0                         Anesthesiology
1                        Medical license
2                      BLS certification
3                       DEA registration
4       Controlled Substance Certificate
                     ...                
458                                 <NA>
459                                 <NA>
460                                 <NA>
461                                 <NA>
462                                 <NA>
Name: 1159550, Length: 463, dtype: object
0  0    Building Custodial Services
   1                       Cleaning
   2            Janitorial Services
   3             Materials Handling
   4                   Housekeeping
dtype: object
job_link      1296381
job_skills    1294346
dtype: int64
job_link      1294346
job_skills    1294346
dtype: int64
0      Project Management
1           Communication
2           Collaboration
3              Leadership
4          ProblemSolving
              ...        
458                  <NA>
459                  <NA>
460                  <NA>
461                  <NA>
462                  <NA>
Name: 1161237, Length: 463, dtype: object
1159549
0  0    Building Custodial Services
   1                       Cleaning
   2            Janitorial Services
   3             Materials Handling
   4                   Housekeeping
dtype: object
1159550
0  0     PCUeel Nurseendek Services
   1                       Cleaning
   2            Janitorial Services
   3             Materials Handling
   4                   Housekeeping
dtype: object
1159551
0  0     PCUeel Nursenndek Services
   1                       Cleaning
   2            Janitorial Services
   3             Materials Handling
   4                   Housekeeping
dtype: object
1159552
0  0     FoUd Safetyeg certificatio
   1                      nCleaning
   2            Janitorial Services
   3             Materials Handling
   4                   Housekeeping
dtype: object
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In[1], line 40
     38 print(stacked_skills.head())
     39 b2 = b[:1159553]
---> 40 stacked_skills = b2.stack()
     41 print(1159553)
     42 print(stacked_skills.head())

File /opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py:116, in annotate.__call__.<locals>.inner(*args, **kwargs)
    113 @wraps(func)
    114 def inner(*args, **kwargs):
    115     libnvtx_push_range(self.attributes, self.domain.handle)
--> 116     result = func(*args, **kwargs)
    117     libnvtx_pop_range(self.domain.handle)
    118     return result

File /opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py:7079, in DataFrame.stack(self, level, dropna, future_stack)
   7073     # homogenize the dtypes of the columns
   7074     homogenized = [
   7075         col.astype(common_type) if col is not None else all_nulls()
   7076         for col in columns
   7077     ]
-> 7079     stacked.append(libcudf.reshape.interleave_columns(homogenized))
   7081 # Construct the resulting dataframe / series
   7082 if not has_unnamed_levels:

File /opt/conda/lib/python3.10/contextlib.py:79, in ContextDecorator.__call__.<locals>.inner(*args, **kwds)
     76 @wraps(func)
     77 def inner(*args, **kwds):
     78     with self._recreate_cm():
---> 79         return func(*args, **kwds)

File reshape.pyx:26, in cudf._lib.reshape.interleave_columns()

RuntimeError: parallel_for: failed to synchronize: cudaErrorIllegalAddress: an illegal memory access was encountered
```

**Expected behavior**
This should just work, as it does in pandas, without ay data corruption
```
!if [ ! -f ""job_skills.csv"" ]; then curl https://storage.googleapis.com/rapidsai/colab-data/job_skills.csv.gz -o job_skills.csv.gz; gunzip job_skills.csv.gz; else echo ""unzipped job data found""; fi
import pandas as pd
skills = pd.read_csv(""job_skills.csv"")

b = skills[""job_skills""].str.split("","", expand=True)
print(b.iloc[1159550])
b2 = b # just to keep the copying similar.  it doesn't matter.
stacked_skills = b2.stack()
print(stacked_skills.head())
```
Outputs:
```
0                         Anesthesiology
1                        Medical license
2                      BLS certification
3                       DEA registration
4       Controlled Substance Certificate
                     ...                
458                                 None
459                                 None
460                                 None
461                                 None
462                                 None
Name: 1159550, Length: 463, dtype: object
0  0    Building Custodial Services
   1                       Cleaning
   2            Janitorial Services
   3             Materials Handling
   4                   Housekeeping
dtype: object
```

**Environment overview (please complete the following information)**
 - Environment location: Docker
 - Method of cuDF install: Docker
   - If method of install is [Docker], docker run --user root --gpus all --rm -it --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -p 9888:8888 -p 9787:8787 -p 9786:8786 -p 9999:9999 rapidsai/notebooks:24.04-cuda11.8-py3.10 jupyter-lab --notebook-dir=/home/rapids/notebooks --ip=0.0.0.0 --no-browser --NotebookApp.token='' --NotebookApp.allow_origin='*' --allow-root


**Environment details**
RAPIDS 24.04 cuda 11.8, py 3.9 and 3.10 Docker on ARM SBSA machines

**Additional context**
When running cudf.pandas, this will succeed, but at the costs of taking nearly 30-40% longer than pandas alone.  If and when it succeeds (by reducing it to the last row where it succeeds, it would be 50x+ faster.  I have not done a data integrity test just yet, to see if the corruption happens earlier.
@vyasr fyi.",2024-05-15T10:22:32Z,1,0,Taurean Dyer,,False
855,[FEA] explore using KMP for string matching like operations,"**Is your feature request related to a problem? Please describe.**
https://dl.acm.org/doi/pdf/10.1007/s00778-015-0409-y shows some really great performance numbers for doing string matching on GPUs. It would be great if we could look into using it to speed up some string operations, like contains, or more generically the LIKE command for a literal pattern.",2024-05-15T14:30:35Z,2,0,Robert (Bobby) Evans,Nvidia,True
856,[DOC] cudf.Series.to_list is empty,"## Report incorrect documentation

**Location of incorrect documentation**
[API](https://docs.rapids.ai/api/cudf/stable/user_guide/api_docs/api/cudf.series.to_list/#cudf.Series.to_list)

**Describe the problems or issues found in the documentation**
This method raises a `TypeError`, however the API docs for this method are blank. The documentation should mirror the exception message of:
```
cuDF does not support conversion to host memory via the `tolist()` method. Consider using `.to_arrow().to_pylist()` to construct a Python list.
```

**Steps taken to verify documentation is incorrect**
List any steps you have taken:

**Suggested fix for documentation**
Detail proposed changes to fix the documentation if you have any.

---

## Report needed documentation

**Report needed documentation**
A clear and concise description of what documentation you believe it is needed and why.

**Describe the documentation you'd like**
A clear and concise description of what you want to happen.

**Steps taken to search for needed documentation**
List any steps you have taken:
",2024-05-16T16:30:54Z,0,0,David Gardner,@Nvidia,True
857,[FEA] Avoid materializing temporary table in ORC chunked reader,"In ORC chunked reader, the decoded stripes are materialized into a cudf table before it is splitt into multiple output chunks through slicing. Materializing such table is memory consuming. We can optimize memory usage in the chunked reader by avoiding that step altogether. By doing so, only part of the decoded stripes is materialized which is enough for one output chunk.

This requires some amount of work to rewrite `cudf::io::detail::column_buffer`.",2024-05-16T19:11:26Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
858,[FEA] Potential optimization:  Batched memset.,Under some situations in the Parquet reader (particularly the case with tables containing many columns or deeply nested column) we burn a decent amount of time doing `cudaMemset()` operations on output buffers. A good amount of this overhead seems to stem from the fact that we're simply launching many tiny kernels.  It might be useful to have a batched/multi memset kernel that takes a list of address/sizes/values as a single input and does all the work under a single kernel launch.  Similar to the Cub multi-buffer memcpy or `contiguous_split`.,2024-05-17T15:43:03Z,0,0,,,False
859,[FEA] Support binary operations between timezone-aware datetime columns,"**Is your feature request related to a problem? Please describe.**
Support for timezone-aware datetime columns was largely added in #12813. However, one item remains incomplete: support for binary operations between such columns, or between these columns and timedelta scalars.

**Describe the solution you'd like**
We should work on adding that support.",2024-05-17T18:35:53Z,0,0,Vyas Ramasubramani,@rapidsai,True
860,[BUG] `Index.is_monotonic_*` methods not factoring `nan` values,"**Describe the bug**
When `nan` values are present, `Index.is_montonic_*` methods seem to be returning incorrect results:

**Steps/Code to reproduce bug**
```python

In [1]: import pandas as pd

In [2]: import numpy as np

In [3]: idx = pd.Index([1.0, 2.0, np.nan])

In [4]: idx
Out[4]: Index([1.0, 2.0, nan], dtype='float64')

In [5]: idx.is_monotonic_increasing
Out[5]: False

In [6]: import cudf

In [7]: gidx = cudf.from_pandas(idx, nan_as_null=False)

In [8]: gidx
Out[8]: Index([1.0, 2.0, nan], dtype='float64')

In [9]: gidx.is_monotonic_increasing
Out[9]: True
```

**Expected behavior**
Match pandas.

",2024-05-17T20:15:13Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
861,[BUG] `uses_custom_row_groups` should not be hardcoded to true in `chunked_parquet_reader`,"**Describe the bug**

Related to #15764

Changing `uses_custom_row_groups` from hardcoded `true` to the below (correct) logic for `chunked_parquet_reader` tests from `ParquetChunkedReaderTest.TestChunkedReadWithListsNoNulls` start failing (see the log below). This happens if `uses_custom_row_groups = false` and eventually [ComputePageSizes](https://github.com/rapidsai/cudf/blob/branch-24.06/cpp/src/io/parquet/reader_impl_preprocess.cu#L1473) is not computed.

```c++
bool const uses_custom_row_bounds = options.get_num_rows().has_value() or options.get_skip_rows() != 0;
```

This has been done to avoid a couple other bugs as described by @nvdbaranec and @ttnghia:

> We are calling `read_chunk_internal()` in a few places where we're only passing`read_mode`.  But if chunked_read_size is > 0 we need to force that to be true.   If we don't, various important things don't happen for all the remaining chunks. Basically if this code doesn't execute, it'll be bad

https://github.com/rapidsai/cudf/blob/bd7b5101cfb77b5b0210cc1d7b78a587821b9bf6/cpp/src/io/parquet/reader_preprocess.cu#L499

Test fail log:
```bash
/home/coder/cudf/cpp/tests/utilities/column_utilities.cu:258: Failure
31: Expected equality of these values:
31:   lhs_size
31:     Which is: 150000
31:   rhs_size
31:     Which is: 750000
31: Google Test trace:
31: /home/coder/cudf/cpp/tests/io/parquet_chunked_reader_test.cu:624:  <--  line of failure
31: 
31: unknown file: Failure
31: C++ exception with description ""CUDF failure at: /home/coder/cudf/cpp/include/cudf/detail/null_mask.cuh:380: End index cannot be smaller than the starting index."" thrown in the test body.
31: [  FAILED  ] ParquetChunkedReaderTest.TestChunkedReadWithListsNoNulls (49 ms)
31: [ RUN      ] ParquetChunkedReaderTest.TestChunkedReadWithListsHavingNulls
31: unknown file: Failure
31: C++ exception with description ""CUDF failure at:/home/coder/cudf/cpp/src/io/parquet/reader_impl.cpp:328: Parquet data decode failed with code(s) 0x77777777"" thrown in the test body.
31: [  FAILED  ] ParquetChunkedReaderTest.TestChunkedReadWithListsHavingNulls (16 ms)
31: [ RUN      ] ParquetChunkedReaderTest.TestChunkedReadWithStructsOfLists
31: /home/coder/cudf/cpp/tests/utilities/column_utilities.cu:258: Failure
31: Expected equality of these values:
31:   lhs_size
31:     Which is: 150000
31:   rhs_size
31:     Which is: 1500000
31: Google Test trace:
31: /home/coder/cudf/cpp/tests/io/parquet_chunked_reader_test.cu:804:  <--  line of failure
31: 
31: /home/coder/cudf/cpp/tests/utilities/column_utilities.cu:258: Failure
31: Expected equality of these values:
31:   lhs_size
31:     Which is: 100000
31:   rhs_size
31:     Which is: 500000
31: Google Test trace:
31: /home/coder/cudf/cpp/tests/io/parquet_chunked_reader_test.cu:809:  <--  line of failure
31: 
31: /home/coder/cudf/cpp/tests/utilities/column_utilities.cu:562: Failure
31: Failed
31: depth 2
31: first difference: lhs[0] = -1145324613, rhs[0] = 0
31: Google Test trace:
31: /home/coder/cudf/cpp/tests/io/parquet_chunked_reader_test.cu:821:  <--  line of failure
31: 
31: /home/coder/cudf/cpp/tests/utilities/column_utilities.cu:258: Failure
31: Expected equality of these values:
31:   lhs_size
31:     Which is: 150000
31:   rhs_size
31:     Which is: 1050000
31: Google Test trace:
31: /home/coder/cudf/cpp/tests/io/parquet_chunked_reader_test.cu:834:  <--  line of failure
31: 
31: /home/coder/cudf/cpp/tests/utilities/column_utilities.cu:258: Failure
31: Expected equality of these values:
31:   lhs_size
31:     Which is: 150000
31:   rhs_size
31:     Which is: 600000
31: Google Test trace:
31: /home/coder/cudf/cpp/tests/io/parquet_chunked_reader_test.cu:840:  <--  line of failure
31: 
31: /home/coder/cudf/cpp/tests/utilities/column_utilities.cu:258: Failure
31: Expected equality of these values:
31:   lhs_size
31:     Which is: 150000
31:   rhs_size
31:     Which is: 600000
31: Google Test trace:
31: /home/coder/cudf/cpp/tests/io/parquet_chunked_reader_test.cu:846:  <--  line of failure
31: 
31: /home/coder/cudf/cpp/tests/utilities/column_utilities.cu:258: Failure
31: Expected equality of these values:
31:   lhs_size
31:     Which is: 150000
31:   rhs_size
31:     Which is: 300000
31: Google Test trace:
31: /home/coder/cudf/cpp/tests/io/parquet_chunked_reader_test.cu:852:  <--  line of failure
31: 
31: /home/coder/cudf/cpp/tests/utilities/column_utilities.cu:258: Failure
31: Expected equality of these values:
31:   lhs_size
31:     Which is: 100000
31:   rhs_size
31:     Which is: 500000
31: Google Test trace:
31: /home/coder/cudf/cpp/tests/io/parquet_chunked_reader_test.cu:858:  <--  line of failure
31: 
31: /home/coder/cudf/cpp/tests/utilities/column_utilities.cu:258: Failure
31: Expected equality of these values:
31:   lhs_size
31:     Which is: 100000
31:   rhs_size
31:     Which is: 500000
31: Google Test trace:
31: /home/coder/cudf/cpp/tests/io/parquet_chunked_reader_test.cu:864:  <--  line of failure
31: 
31: /home/coder/cudf/cpp/tests/utilities/column_utilities.cu:258: Failure
31: Expected equality of these values:
31:   lhs_size
31:     Which is: 100000
31:   rhs_size
31:     Which is: 300000
31: Google Test trace:
31: /home/coder/cudf/cpp/tests/io/parquet_chunked_reader_test.cu:870:  <--  line of failure
31: 
31: /home/coder/cudf/cpp/tests/utilities/column_utilities.cu:562: Failure
31: Failed
31: depth 1
31: first difference: lhs[9612] = 9612++��\0��\0��\0��\0��\0��, rhs[9612] = 9612++++++++++++++++++++9612
31: Google Test trace:
31: /home/coder/cudf/cpp/tests/io/parquet_chunked_reader_test.cu:876:  <--  line of failure
31: 
31: [  FAILED  ] ParquetChunkedReaderTest.TestChunkedReadWithStructsOfLists (609 ms)
31: [ RUN      ] ParquetChunkedReaderTest.TestChunkedReadWithListsOfStructs
31: /home/coder/cudf/cpp/tests/utilities/column_utilities.cu:562: Failure
31: Failed
31: first difference: lhs[0] = 0, rhs[0] = 94272
31: Google Test trace:
31: /home/coder/cudf/cpp/tests/io/parquet_chunked_reader_test.cu:945:  <--  line of failure
31: 
31: /home/coder/cudf/cpp/tests/utilities/column_utilities.cu:258: Failure
31: Expected equality of these values:
31:   lhs_size
31:     Which is: 150000
31:   rhs_size
31:     Which is: 1500000
31: Google Test trace:
31: /home/coder/cudf/cpp/tests/io/parquet_chunked_reader_test.cu:945:  <--  line of failure
31: 
31: /home/coder/cudf/cpp/tests/utilities/column_utilities.cu:262: Failure
31: Expected equality of these values:
31:   lhs.null_count()
31:     Which is: 25000
31:   rhs.null_count()
31:     Which is: 20427
31: Google Test trace:
31: /home/coder/cudf/cpp/tests/io/parquet_chunked_reader_test.cu:950:  <--  line of failure
31: 
31: /home/coder/cudf/cpp/tests/utilities/column_utilities.cu:562: Failure
31: Failed
31: depth 2
31: first difference: lhs[9696] = 30000, rhs[9696] = 9696
31: Google Test trace:
31: /home/coder/cudf/cpp/tests/io/parquet_chunked_reader_test.cu:957:  <--  line of failure
31: 
31: /home/coder/cudf/cpp/tests/utilities/column_utilities.cu:562: Failure
31: Failed
31: depth 2
31: first difference: lhs[479] = 7`�\0\0`�\0\0, rhs[479] = 719719719
31: Google Test trace:
31: /home/coder/cudf/cpp/tests/io/parquet_chunked_reader_test.cu:962:  <--  line of failure
31: 
31: CMake Error at /home/coder/cudf/cpp/build/latest/rapids-cmake/run_gpu_test.cmake:34 (execute_process):
31:   execute_process failed command indexes:
31: 
31:     1: ""Abnormal exit with child return code: Segmentation fault""
```

**Steps/Code to reproduce bug**
Modify in [reader_impl.hpp](https://github.com/rapidsai/cudf/blob/branch-24.06/cpp/src/io/parquet/reader_impl.hpp)
```c++
  [[nodiscard]] bool uses_custom_row_bounds(read_mode mode) const
  {
    return (mode == read_mode::READ_ALL)
             ? (_options.num_rows.has_value() or _options.skip_rows != 0)
             : true;
  }
```
to
```c++
  [[nodiscard]] bool uses_custom_row_bounds(read_mode mode) const
  {
    return _options.num_rows.has_value() or _options.skip_rows != 0;
  }
```


**Expected behavior**
The gtests should not fail.

**Environment overview**
 - Environment location: Bare-metal (dgx-05)
 - Method of cuDF install: cuDF devcontainer (conda, cuda 12.2)

",2024-05-17T22:25:41Z,0,0,Muhammad Haseeb,@NVIDIA,True
862,[BUG] Issues with `codecov` on `cudf` CI,"**Describe the bug**
There are various issues with `codecov` I'm trying to summarize into one master issue.
1. The `codecov/patch` custom github action doesn't always seem to be posted on the PR. Last PR that had this posted on was: https://github.com/rapidsai/cudf/pull/15719
<img width=""851"" alt=""Screenshot 2024-05-20 at 6 53 12 AM"" src=""https://github.com/rapidsai/cudf/assets/11664259/6173fc79-3b1f-434e-9d94-d33a12aa79fb"">

2. `codecov` bot seems to have stopped commenting on PRs the diff coverage as it used to:
<img width=""1197"" alt=""Screenshot 2024-05-20 at 6 54 30 AM"" src=""https://github.com/rapidsai/cudf/assets/11664259/978a924a-96be-42a4-b214-bd0088dcf9af"">

3. The base branch coverage is off by a few lines always because we generate the coverage by running the tests nightly are there any best practices and recommendations from `codecov` team on how to have a very upto-date base branch coverage?

",2024-05-20T11:59:36Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
863,[BUG] orc reader returning an incorrect timestamp for `rockylinux8`,"**Describe the bug**
This bug is OS specific and happens only in `rocklylinux8`. We already have a pytest that is failing. This issue surfaced after arrow-16.1 upgrade.

**Steps/Code to reproduce bug**
```python
FAILED tests/test_orc.py::test_orc_reader_apache_negative_timestamp - AssertionError: DataFrame.iloc[:, 0] (column name=""v"") are different
DataFrame.iloc[:, 0] (column name=""v"") values are different (100.0 %)
[index]: [0]
[left]:  [8265956376710189616]
[right]: [8265956798710189616]
```

**Expected behavior**
Match pandas.

**Environment overview (please complete the following information)**
 - Environment location: [Bare-metal]
 - Method of cuDF install: [from source]


**Environment details**
Please run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details

",2024-05-21T16:42:07Z,0,0,GALI PREM SAGAR,NVIDIA @rapidsai ,True
864,[BUG] `cudf.read_json` does not raise an exception with invalid data when `lines=True` and `engine='cudf'`,"**Describe the bug**
`cudf.read_json` doesn't raise an exception when parsing invalid json when `lines=True` and `engine='cudf'`. Instead it returns a single row DF with an empty string value.

Setting `lines=False` raises a `RuntimeError` (should be a `ValueError`).
Alternately setting `engine='pandas'` raises a `ValueError`.

**Steps/Code to reproduce bug**
```python
from io import StringIO

import cudf

print(cudf.__version__)
invalid_payload = '{""not_valid"":""json'

# Produces a single row DF
print(""Testing lines=True, engine=cudf"")
print(cudf.read_json(StringIO(invalid_payload), lines=True, engine='cudf'))


# Works as expected
print(""Testing lines=False, engine=cudf"")
try: 
    cudf.read_json(StringIO(invalid_payload), lines=False, engine='cudf')
except Exception as e:
    print(e)

# Works as expected
print(""Testing lines=True, engine=pandas"")
try:
    cudf.read_json(StringIO(invalid_payload), lines=True, engine='pandas')
except Exception as e:
    print(e)
```

**Expected behavior**
A raised `ValueError`, although any exception is better than 

**Environment overview (please complete the following information)**
 - Environment location: [Bare-metal]
 - Method of cuDF install: [conda]

Observed in versions 24.04.01 and 24.02.02",2024-05-22T19:04:58Z,0,0,David Gardner,@Nvidia,True
865,Share struct member definition for parse_options and parse_options_view,"*question*: It seems a shame that one must repeat most of the fields between `parse_options` and `parse_options_view`. Shall we open an issue to discuss whether these should be shared with something like:

```c++
struct _parse_options {
   char delimiter;
   ...;
   ...;
}

struct parse_options_view {
    struct _parse_options opts;
    cudf::detail::trie_view trie_true;
    ...;
}

struct parse_options {
   struct _parse_options opts;
   cudf::detail::optional_tree trie_true;
   ...
   
}
```
WDYT?

_Originally posted by @wence- in https://github.com/rapidsai/cudf/pull/15727#discussion_r1609612576_
            ",2024-05-22T20:20:15Z,0,0,Vyas Ramasubramani,@rapidsai,True
866,[ENH] Use `strict=True` argument to `zip` once py39 support is dropped,"In many places in the cudf code we zip two (or more) iterables together with the assumption/precondition that they are all of equal length. The pattern is (approximately):

```python
names: list[str]
columns: list[Column]
data: dict[str, Column] = dict(zip(names, columns))
```

This has, by design of zip, a potential problem lurking in that if the two inputs are _not_ of equal length, we only keep the first `min(len(names), len(columns))` objects.

To avoid check this at user-input boundaries we need to be careful to check (and then raise if not) that the inputs we are zipping _do_ have equal length. This is easy to forget.

[Python 3.10](https://docs.python.org/3/library/functions.html#zip) introduces a new keyword-argument to `zip`, `strict`, which can be used to assert that the inputs have the same length. We should consider using this across the code-base when no longer supporting Python 3.9.
",2024-05-23T09:59:08Z,0,0,Lawrence Mitchell,,False
867,[FEA] Support `arrow:Schema` in Parquet writer for faithful roundtrip with Arrow via Parquet,"Support writing `arrow:Schema` to parquet file footer to allow faithful interop with Arrow via Parquet. The writer's side may be much more complicated and involve supporting all types, and computing and writing dictionary ids.

Related to: #15617",2024-05-23T22:04:54Z,0,0,Muhammad Haseeb,@NVIDIA,True
868,[FEA] Handle size overflow in nested columns by ORC chunked reader,"In the current chunked ORC reader, only row counts of the top-level columns are considered when splitting the loaded stripes into subsets of stripes to avoid decoding more than 2B rows, which is cudf column size limit. The nested children columns are not taken into account.

We should consider row count at all nested levels.",2024-05-23T22:11:45Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
869,[FEA] Better control over the output dtype in aggregations,"**Is your feature request related to a problem? Please describe.**

For the cudf-polars work, I'd like to match dtypes with polars where possible, preferably without casting the result of a libcudf call post-hoc if the interface in theory supports specifying an output type.

For whole-frame aggregations (`cudf::reduce`) although one is able to specify an output_dtype, this is not obeyed for a number of aggregations. Specifically:

- `MEDIAN` (always returns the datatype matching `double`)
- `NUNIQUE` (always returns the datatype matching `cudf::size_type`)
- `QUANTILE` (always returns the datatype matching `double`)


The same is true of many grouped aggregations.

**Describe the solution you'd like**

I'd like that aggregations could support output dtype as specified by the user.

**Describe alternatives you've considered**

post-hoc unary casting of the result, but this is yet another kernel launch, and produces more memory overhead.",2024-05-24T13:37:43Z,0,0,Lawrence Mitchell,,False
870,"For the overload of replace in libcudf where input/target/repl are columns, there isn't a maxrepl arg.","For the overload of replace in libcudf where input/target/repl are columns, there isn't a maxrepl arg.

We should probably support this in libcudf replace (eventually), otherwise we'll have some weirdness in pylibcudf where we'll have to raise for maxrepl despite accepting it as an argument.

_Originally posted by @lithomas1 in https://github.com/rapidsai/cudf/pull/15839#discussion_r1611981114_
            ",2024-05-24T16:46:09Z,0,0,Thomas Li,@pandas-dev,False
871,[BUG] `cudf::round` with `HALF_UP` mode produces non-deterministic output,"Reproducible code:
```
using f_wrapper = cudf::test::fixed_width_column_wrapper<double>;
auto const input = f_wrapper{1.95, 2.95, 3.95, 4.95, 5.95, 6.95, 7.95, 8.95, 9.95};

auto const result = cudf::round(input, 1, cudf::rounding_method::HALF_UP);
cudf::test::print(*result);
```
Output:
```
2,3,4,5,6,7,8,8.9000000000000004,9.9000000000000004
```

I digged into the code and see that the rounding operation is indeed doesn't do round up properly:
```
template <typename T>
struct half_up_positive {
  T n;
  template <typename U = T, std::enable_if_t<cudf::is_floating_point<U>()>* = nullptr>
  __device__ U operator()(U e)
  {
    T integer_part;
    T const fractional_part = generic_modf(e, &integer_part);
    return integer_part + generic_round(fractional_part * n) / n;
  }
```
When debugging `generic_round`, given the input  I see that:
```
input:    9.5000000000, output:   10.0000000000
input:    9.5000000000, output:   10.0000000000
input:    9.5000000000, output:   10.0000000000
input:    9.5000000000, output:   10.0000000000
input:    9.5000000000, output:   10.0000000000
input:    9.5000000000, output:   10.0000000000
input:    9.5000000000, output:   10.0000000000
input:    9.5000000000, output:    9.0000000000
input:    9.5000000000, output:    9.0000000000
```
In other word, `generic_round` can produce different outputs due to very small round-off error.",2024-05-24T20:58:21Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
872,[FEA] Add developer/private cudf.pandas API to check for proxy objects ,"**Is your feature request related to a problem? Please describe.**
We've encountered a few scenarios where we need to know whether an object is an cudf.pandas proxy object vs an actual cudf or pandas object e.g. https://github.com/rapidsai/cuml/pull/5861#discussion_r1584001820

The classic `isinstance` does not work since proxy objects ensures this always returns `True` for the case above. Like the solution above, we're relied on checking attributes but this is not a sustainable long term solution as private attributes should be subject to change.

**Describe the solution you'd like**
Exposing the proxy type in `cudf.pandas._types.`(?) that allows developers to explicitly check if they have a proxy object

**Describe alternatives you've considered**
Checking if proxy object attributes exist

**Additional context**
Add any other context, code examples, or references to existing implementations about the feature request here.
",2024-05-24T23:51:20Z,0,0,Matthew Roeschke,@rapidsai ,True
873,[BUG] Converting from floating point types to fixed point types does not consider `NaN` and `inf`,"Currently, when converting floating point types to fixed point types, the special values `NaN` and `inf` are blindly scaled and cast to integer types, resulting in garbage values. The more reasonable outcome should be to nullify the output rows corresponding to these values in the input.",2024-05-29T21:36:01Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
874,[BUG] Converting from numeric types to fixed point types does not handle underflow/overflow,"When converting floating point types to fixed point types, the casting operations are just blindly scaling the input values and then casting to integer without checking whether the scaled values are too big to be stored in the target integer types.

Probably a more proper way to do casting here is to check for underflow/overflow and nullify the output accordingly.",2024-05-29T21:44:15Z,0,0,Nghia Truong, GPU-Accelerating Apache Spark @Nvidia,True
