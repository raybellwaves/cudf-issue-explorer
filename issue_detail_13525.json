{"assignees":[],"author":{"id":"MDQ6VXNlcjEyNzI1MTEx","is_bot":false,"login":"GregoryKimball","name":"Gregory Kimball"},"body":"libcudf includes a GPU-accelerated JSON reader that uses a finite-state transducer parser combined with token-processing tree algorithms to transform character buffers into columnar data. This issue tracks the technical work leading up to the launch of libcudf's JSON reader as a default component of the Spark-RAPIDS plugin. Please also refer to the [Nested JSON reader milestone](https://github.com/rapidsai/cudf/milestone/13) and [Spark-RAPIDS JSON epic](https://github.com/NVIDIA/spark-rapids/issues/9458).\r\n\r\n\r\n### Spark compatibility issues: Blockers\r\n| Status | Impact for Spark | Change to libcudf |\r\n|---|---|---|\r\n| âœ… #13344  | #12532, Blocker: if any line has an error, libcudf throws an exception  | Rework state machine to include error states and scrub tokens from lines with error | \r\n| âœ… #14252 | #14227, Blocker: Incorrect parsing | Fix bug in error recovery state transitions |  \r\nâœ… #14279 | #14226, Blocker: requesting alternate error recovery behavior from #13344, where valid data before an error state are preserved  | Changes in JSON parser pushdown automaton for JSON_LINES_RECOVER option  | \r\n| âœ… #14936 | #14288, Blocker: libcudf does not have an efficient representation for map types in Spark |  libcudf does not support map types, and modeling the map types as structs results in poor performance due to one child column per unique key. We will return the struct data that represents map types as string and then the plugin can use [unify_json_strings](https://github.com/NVIDIA/spark-rapids-jni/blob/54ef9991f46fa873d580315212aeae345da7152a/src/main/cpp/src/map_utils.cu#L63-L112) to parse tokens |\r\n| âœ… #14572  | #14239, Blocker: fields with mixed types raise an exception | add libcudf reader option to return mixed types as strings. Also see improvements in  #15236 and  #14939 | \r\n| âœ… #14545 | #10004, Blocker: Can't parse data with single quote variant of JSON when `allowSingleQuotes` is enabled in Spark | Introduce a preprocessing function to normalize single and double quotes as double quotes | \r\n| âœ… #15324 | #15303, escaped single quotes have their escapes dropped during quote normalization | Adjust quote normalization FST |\r\n| ðŸ”„ #15419 | #15390 + #15409, Blocker: race conditions found in nested JSON reader | Solve synchronization problems in nested JSON reader  |\r\n| | #15260, Blocker: crash in mixed type support | |\r\n| ðŸ”„ | #15278, Blocker: allow list type to be coerced to string, also see #14239. Without this, Spark-RAPIDS will fallback when user requests a field as \"string\" | Support List types coercion to string | \r\n| | #15277, Blocker: we need to support multi-line JSON objects. Also see #10267 | libcudf is scoping a \"multi-object\" reader |  \r\n\r\n\r\n\r\n### Spark compatibility issues: non-blockers\r\n\r\n| Status  | Impact for Spark | Change to libcudf  | \r\n|---|---|---|\r\n| | #15222, compatibility problems with leading zeros, \"NAN\" and escape options | None for now. This feature should live in Spark-RAPIDS as a post-processing option for now, based on the approach for `get_json_object` modeled after Spark CPU code (see https://github.com/NVIDIA/spark-rapids-jni/pull/1836). Then the plugin can set to null any entries from objects that Spark would treat as invalid. Later we could provide Spark-RAPIDS access to raw tokens that they could run through a more efficient validator.  |\r\n| âœ… #15033 |  #14865, Strip whitespace from JSON inputs, otherwise Spark will have to add this in post-processing the coerced strings types | Create new normalization pre-processing tool for whitespace | \r\n| ðŸ”„ #14996 | #13473, Performance: only process columns in the schema | Skip parsing and column creation for keys not specified in the schema |\r\n| ðŸ”„ #15124 | Reader option performance is unknown | #15041, add JSON reader option benchmarking | |\r\n| | Performance: Avoid preprocessing to replace empty lines with `{}`. Also see #5712 |  libcudf provides strings column data source | \r\n|  | #15280 find a solution when whitespace normalization fixes a line that originally was invalid | We could move whitespace normalization after tokenization. Also we would like to address #15277 so that we can remove unquoted newline characters as well. |\r\n| | n/a, Spark-RAPIDS doesn't use byte range reading | #15185, reduce IO overhead in JSON byte range reading |\r\n| | n/a, Spark-RAPIDS doesn't use byte range reading | #15186, address data loss edge case for byte range reading |\r\n|  | reduce peak memory usage | add chunking to the JSON reader | \r\n| | #15222, Spark-RAPIDS must return null if any field is invalid | Provide token stream to Spark-RAPIDS for validation, including checks or leading zeros, special string numbers like `NaN`, `+INF`, `-INF`, and optional limits for which characters can be escaped |\r\n\r\n\r\n","closed":false,"closedAt":null,"comments":[{"id":"IC_kwDOBWUGps53NOlZ","author":{"login":"revans2"},"authorAssociation":"CONTRIBUTOR","body":"@GregoryKimball From the Spark perspective The following are in priority order. This is based mostly on how likely I think it is that a customer would see these problems/limitations.  And also if we have a work around that would let us enable the JSON parsing functionality by default or not without this change, even if it is limited functionality.\r\n\r\nBlocker:\r\n1. #15260\r\n2. #15278\r\n3. #15277\r\n4. #15280\r\n5. #15303\r\n6. #15222 - This is likely going to need to be broken down into smaller pieces, not all of which are going to be blockers. I also think we need to what is the best way to support this because there will be a performance impact to others that don't want validation like this.\r\n7. #15318 I don't want to mark this a blocker, but we have a customer that insists on it. We are in the process of trying to develop normalization code that would work, but a lot of the problem is how can/would we be able to integrate this with the existing JSON parsing code. \r\n\r\nNon-Blocker:\r\n1. #5712 - I think I can work around this, but it will end up being a performance hit if we don't have a better way to deal with it.\r\n2. #14951 / #13473 - performance optimization (I think these might be dupes of each other)\r\n\r\n\r\n","createdAt":"2024-03-15T15:52:21Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/13525#issuecomment-1999956313","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps53PN-X","author":{"login":"GregoryKimball"},"authorAssociation":"CONTRIBUTOR","body":"Thank you @revans2 for summarizing your investigation. We've been studying these requirements and we would like to continue the discussion with you next week.\r\n\r\nlibcudf will soon address:\r\n1, 2, 5\r\n\r\nlibcudf is doing design work on:\r\nemitting raw strings (helps with 6, 7)\r\nmoving whitespace normalization after tokenization (helps with 4)\r\n\r\nlibcudf suggests that 3 is a non-blocker","createdAt":"2024-03-15T21:36:39Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/13525#issuecomment-2000478103","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps53c1ow","author":{"login":"revans2"},"authorAssociation":"CONTRIBUTOR","body":"Like I said I can work around 3, but I don't know how to make it performant without help from CUDF, and we have seen this in actual customer data.  Perhaps I can write a custom kernel myself that looks at quotes and replaces values in quotes vs outside of quotes as needed. I'll see.","createdAt":"2024-03-18T14:19:50Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/13525#issuecomment-2004048432","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps54Ju0_","author":{"login":"GregoryKimball"},"authorAssociation":"CONTRIBUTOR","body":"We had more discussions on the JSON compatibility issues and identified \"multi-line\" support as a blocker (relates to 3 above). We don't currently have a way to process a strings column as JSON Lines when the rows contain unquoted newline characters. Also our whitespace normalization can't remove unquoted newline characters. (See #10267 and #15277 for related requests)","createdAt":"2024-03-22T20:01:18Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/13525#issuecomment-2015817023","viewerDidAuthor":false}],"createdAt":"2023-06-07T16:30:32Z","id":"I_kwDOBWUGps5oFiLp","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NjE=","name":"feature request","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwxMDEzOTg3MzUy","name":"0 - Backlog","description":"In queue waiting for assignment","color":"d4c5f9"},{"id":"MDU6TGFiZWwxMTM5NzQwNjY2","name":"libcudf","description":"Affects libcudf (C++/CUDA) code.","color":"c5def5"},{"id":"MDU6TGFiZWwxMTg1MjQ0MTQy","name":"cuIO","description":"cuIO issue","color":"fef2c0"},{"id":"MDU6TGFiZWwxNDA1MTQ2OTc1","name":"Spark","description":"Functionality that helps Spark RAPIDS","color":"7400ff"}],"milestone":{"number":13,"title":"Nested JSON reader","description":"Data-parallel reader for nested JSON text data","dueOn":null},"number":13525,"projectCards":[],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[FEA] JSON reader improvements for Spark-RAPIDS","updatedAt":"2024-04-01T19:29:22Z","url":"https://github.com/rapidsai/cudf/issues/13525"}
