{"assignees":[],"author":{"id":"MDQ6VXNlcjU2Njk1OTMw","is_bot":false,"login":"nvdbaranec","name":""},"body":"\r\nFor parquet files that contain very large schemas with strings (either large numbers of columns, or large numbers of nested columns) we pay a very heavy price postprocessing the string data after the core decode kernels runs.  \r\n\r\nEssentially, the \"decode\" process for strings is just emitting a large array of pointer/size pairs that are then passed to other cudf functions to reconstruct actual columns.    The problem is that we are doing this with no batching - each output string column results in an entire cudf function call (`make_strings_column`) with multiple internal kernel calls each.  In situations with thousands of columns, this gets very expensive.\r\n\r\n![image](https://user-images.githubusercontent.com/56695930/228312025-67ea3177-9d67-4e84-84e5-cb317c895001.png)\r\n\r\nIn the image above, the green span represents the time spent in the decode kernel and the time spent in all of the `make_strings_column` calls afterwards.  The time is totally dominated by the many many calls to `make_strings_column` (the red span).  \r\n\r\nIdeally, we would have some kind of batched interface to `make_strings_column`  (`make_strings_columns` ?) that can do the work for the thousands of output columns coalesced into fewer kernels.  \r\n\r\n\r\nOn a related note, the area under the blue line represents a similar problem involving preprocessing the file (thousands of calls to `thrust::reduce` and `thrust::exclusive_scan_by_key`).   This has been largely addressed by this PR  https://github.com/rapidsai/cudf/pull/12931  \r\n","closed":false,"closedAt":null,"comments":[{"id":"IC_kwDOBWUGps5fqp7M","author":{"login":"etseidl"},"authorAssociation":"CONTRIBUTOR","body":"@nvdbaranec I'm curious if #13302 had any impact on the file used to generate the profile above.","createdAt":"2023-06-23T21:45:35Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"EYES","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/13024#issuecomment-1605017292","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps50MxgG","author":{"login":"GregoryKimball"},"authorAssociation":"CONTRIBUTOR","body":"@vuule recently conducted some experiments using an internal stream pool to hide latencies during column buffer allocation. Perhaps evaluating string types with larger column counts would show a bigger signal.","createdAt":"2024-02-17T00:03:09Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/13024#issuecomment-1949505542","viewerDidAuthor":false}],"createdAt":"2023-03-28T17:01:42Z","id":"I_kwDOBWUGps5iArtN","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NjE=","name":"feature request","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwxMTM5NzQwNjY2","name":"libcudf","description":"Affects libcudf (C++/CUDA) code.","color":"c5def5"},{"id":"MDU6TGFiZWwxMTg1MjQ0MTQy","name":"cuIO","description":"cuIO issue","color":"fef2c0"},{"id":"MDU6TGFiZWwxMzIyMjUyNjE3","name":"Performance","description":"Performance related issue","color":"C2E0C6"},{"id":"MDU6TGFiZWwyNTQ2NTIxMDI0","name":"improvement","description":"Improvement / enhancement to an existing function","color":"bfd4f2"}],"milestone":{"number":22,"title":"Parquet continuous improvement","description":"","dueOn":null},"number":13024,"projectCards":[],"projectItems":[],"reactionGroups":[{"content":"ROCKET","users":{"totalCount":1}}],"state":"OPEN","title":"[FEA] Performance issue with the Parquet reader for very large schemas (especially when containing strings)","updatedAt":"2024-02-17T00:03:10Z","url":"https://github.com/rapidsai/cudf/issues/13024"}
