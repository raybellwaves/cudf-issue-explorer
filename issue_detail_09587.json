{"assignees":[{"id":"MDQ6VXNlcjE1MzgxNjU=","login":"vyasr","name":"Vyas Ramasubramani"}],"author":{"id":"MDQ6VXNlcjE1MzgxNjU=","is_bot":false,"login":"vyasr","name":"Vyas Ramasubramani"},"body":"**Is your feature request related to a problem? Please describe.**\r\nPython's `sys` module provides the `sys.getsizeof` function to determine the size of a Python object. The behavior of `getsizeof` when applied to a user-defined class may be customized by overriding the `__sizeof__` attribute. For the purpose of computing the size of a Python object backed by GPU memory, however, `getsizeof` has a couple of major drawbacks:\r\n1. `getsizeof` is traditionally defined as a shallow calculation, so the `sizeof` a container will not recursively traverse nested elements. The `sizeof` a list of lists is essentially equivalent to (in pseudocode) `sizeof(PyObject *) * len(list)`, with few extra bytes allocated for the overhead of the list's metadata. The internet is rife with recipes for performing a corresponding deep calculation, but they typically have sharp edges and in practice users of GPU libraries usually want the deep calculation if they are making this request. Even if a suitable override of this attribute could be defined that always returned the deep calculation, it would not be desirable to do so since it would overload the standard meaning of the operator in Python.\r\n2. GPU memory allocations are more complex than host calculations in the sense that there are multiple \"pools\" of memory from which a buffer might be allocated and the user may be interested in having those separated out. In addition to standard device allocations via cudaMalloc, a user may also have requested pinned host memory or managed memory. Any API for querying GPU memory usage must be sufficiently general to support all of these types of information.\r\n\r\nVarious higher-level Python libraries that leverage GPU libraries under the hood would benefit from a standardized approach to requesting total GPU memory allocations. For instance, Dask could leverage this calculation to determine when to spill memory to disk.\r\n\r\n**Describe the solution you'd like**\r\nIt would be nice to define a standard protocol for all Python libraries backed by GPU memory to expose the allocations underlying a Python object. The most obvious possibility for this would be a new protocol to correspond with `__cuda_array_interface__`, something like `__cuda_sizeof__` that would return a dictionary of allocated memory by type, but other implementations are also possible. It would be important to consider whether this would always be a deep calculation or if there would be any cases when a shallow calculation might be appropriate, for instance with containers (like those that might live in [cuCollections](https://github.com/NVIDIA/cuCollections/)). It would also be important to consider how it should behave for slices: for instance, would `s = cudf.Series(1000); s[::2].__cuda_sizeof__` indicate the size of the a column of size 1000 or 500?\r\n\r\nIt may also be necessary for users to have some way to account for host memory allocations, but I think it makes the most sense to have that calculation be entirely independent of this protocol. That does raise some questions about the suitable way to treat pinned memory (host memory allocated via `cudaHostAlloc`).\r\n\r\n**Describe alternatives you've considered**\r\nNone at this stage.\r\n\r\n**Additional context**\r\nThis proposal comes out of a discussion precipitated in https://github.com/rapidsai/cudf/pull/9544#issuecomment-955011270. That PR removed the `__sizeof__` overrides in cuDF, which were likely to be more confusing than helpful, and standardized the `memory_usage` method of cuDF objects. `memory_usage` is a pandas function that we seek to mimic, but our goal of making cuDF objects pandas-compatible makes this method unsuitable for adaptation into a new \"gpusizeof\" protocol.","closed":false,"closedAt":null,"comments":[{"id":"IC_kwDOBWUGps45NGw0","author":{"login":"vyasr"},"authorAssociation":"CONTRIBUTOR","body":"CC @leofang @jakirkham @gmarkall","createdAt":"2021-11-03T17:04:34Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":3}}],"url":"https://github.com/rapidsai/cudf/issues/9587#issuecomment-959736884","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps45NL51","author":{"login":"leofang"},"authorAssociation":"MEMBER","body":"cc: @kmaehashi @emcastillo @asi1024 for vis","createdAt":"2021-11-03T17:27:03Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/9587#issuecomment-959757941","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps46wOrE","author":{"login":"github-actions"},"authorAssociation":"NONE","body":"This issue has been labeled `inactive-30d` due to no recent activity in the past 30 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed. This issue will be labeled `inactive-90d` if there is no activity in the next 60 days.","createdAt":"2021-12-03T18:03:08Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/9587#issuecomment-985721540","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps4_FOJ5","author":{"login":"github-actions"},"authorAssociation":"NONE","body":"This issue has been labeled `inactive-90d` due to no recent activity in the past 90 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed.","createdAt":"2022-03-03T18:04:31Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/9587#issuecomment-1058333305","viewerDidAuthor":false}],"createdAt":"2021-11-03T17:04:08Z","id":"I_kwDOBWUGps4-N--Z","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NjE=","name":"feature request","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwxMDEzOTg3OTIx","name":"proposal","description":"Change current process or code","color":"2a2c89"},{"id":"MDU6TGFiZWwxMDE2MzIwNzI3","name":"numba","description":"Numba issue","color":"9c2cdd"},{"id":"MDU6TGFiZWwxMTM5NzQxMjEz","name":"cuDF (Python)","description":"Affects Python cuDF API.","color":"1d76db"},{"id":"MDU6TGFiZWwxMTg1MjQwODk4","name":"dask","description":"Dask issue","color":"fcc25d"}],"milestone":null,"number":9587,"projectCards":[{"project":{"name":"Feature Planning"},"column":{"name":"Needs prioritizing"}}],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[FEA] Define a standard mechanism for querying GPU memory usage","updatedAt":"2024-02-23T18:42:36Z","url":"https://github.com/rapidsai/cudf/issues/9587"}
