{"assignees":[],"author":{"id":"MDQ6VXNlcjE5MDEwNTk=","is_bot":false,"login":"abellina","name":"Alessandro Bellina"},"body":"Original title:\r\n\"[FEA] research enabling BIT_PACKED encoding for columns in parquet writer\"\r\n\r\nWe have a user report of larger sizes for parquet encoded files via the GPU as opposed to Spark CPU. With their sample data, I can get a 30% increase in the GPU file size vs the CPU. I have been able to produce a single row group and the same number of files, so I am down to column encodings. The types of columns are all INT64 nullable columns.\r\n\r\nIt looks like one of the differences between the two files is that in cuDF columns are not using the BIT_PACKED encoding:\r\n\r\nCPU (`ENC:RLE,BIT_PACKED,PLAIN_DICTIONARY`):\r\n```\r\ncol1:         INT64 SNAPPY DO:4 FPO:508942 SZ:856794/1169887/1.37 VC:822216 ENC:RLE,BIT_PACKED,PLAIN_DICTIONARY ST:[min: 1237, max: 1234559, num_nulls: 0]\r\ncol2:         INT64 SNAPPY DO:856798 FPO:1365736 SZ:856794/1169887/1.37 VC:822216 ENC:RLE,BIT_PACKED,PLAIN_DICTIONARY ST:[min: 1237, max: 1234559, num_nulls: 0]\r\n```\r\n\r\nGPU (`ENC:PLAIN_DICTIONARY,RLE`):\r\n```\r\ncol1:         INT64 SNAPPY DO:4 FPO:509620 SZ:924686/1234683/1.34 VC:822216 ENC:PLAIN_DICTIONARY,RLE ST:[min: 1237, max: 1234559, num_nulls: 0]\r\ncol2:         INT64 SNAPPY DO:924690 FPO:1434330 SZ:924742/1234683/1.34 VC:822216 ENC:PLAIN_DICTIONARY,RLE ST:[min: 1237, max: 1234559, num_nulls: 0]\r\n```\r\n\r\nDiscussing with @nvdbaranec, he suggested that BIT_PACKED could be enough of a reason for the difference. I have generated two files with my own mock data (just sequences of longs) and encoded it with the CPU and the GPU. I have placed two of the generated file in this zip file:\r\n\r\n[bit_packed_example.zip](https://github.com/rapidsai/cudf/files/12467035/bit_packed_example.zip)\r\n\r\nI would appreciate any comments. If you want me to try a small change in cuDF and rebuild/retest, I am happy to do so.\r\n\r\n","closed":false,"closedAt":null,"comments":[{"id":"IC_kwDOBWUGps5lMjYM","author":{"login":"nvdbaranec"},"authorAssociation":"CONTRIBUTOR","body":"@vuule ","createdAt":"2023-08-29T16:34:18Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/13995#issuecomment-1697789452","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5lrb0s","author":{"login":"etseidl"},"authorAssociation":"CONTRIBUTOR","body":"Looking at the files, the BIT_PACKED is a bit of a red herring.\r\n```\r\n    col1 TV=822216 RL=0 DL=1 DS:  102777 DE:PLAIN_DICTIONARY\r\n    ----------------------------------------------------------------------------\r\n    page 0:                        DLE:RLE RLE:BIT_PACKED VLE:PLAIN_DICTIONARY [more]... SZ:7509\r\n\r\n```\r\nThe file metadata claims the repetition level data is BIT_PACKED, but the max repetition level is 0, so there is no data to encode. I'm surprised to see BIT_PACKED listed since it's been deprecated for at least a decade now. ðŸ˜‰ \r\n\r\nDigging deeper, the CPU pages are 7509 bytes for the first 26 pages, and then 10009 bytes for the remaining pages of the column chunk. It seems that the encoder is using a variable bit width for the dictionary encoding. 3 bits for 20000 values would be 7500 bytes, 4 bits for 20000 values is 10000 bytes. (Actually, there are runs of 8 values in the data, so it's really where the dictionary switches from <= 16 bits to >= 17 bits that the bump in page size occurs). libcudf uses a fixed bit width for the entire column chunk based on the number of dictionary keys present. I think it would be a lot of work to use variable bit widths in cudf.\r\n\r\nYou can try limiting the rowgroup size to 400k rows. That might keep the dictionaries in the ~3~ 16 bit range.","createdAt":"2023-09-05T03:09:30Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/13995#issuecomment-1705884972","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5nghfB","author":{"login":"GregoryKimball"},"authorAssociation":"CONTRIBUTOR","body":"Thank you @abellina for sharing the file size difference you observed, and thank you @etseidl for your triage of this issue. Once we finish the work around DELTA decoding and encoding, we can consider the feasibility of variable bit width dictionary encoding.\r\n\r\nI'll close this issue for now. ","createdAt":"2023-09-27T02:33:17Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/13995#issuecomment-1736579009","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5qclw3","author":{"login":"GregoryKimball"},"authorAssociation":"CONTRIBUTOR","body":"@etseidl thank you for studying this file. \r\n\r\nWould you please help me understand your observations a bit better?\r\n\r\nI thought that each column chunk has one dictionary page, and this dictionary is used for all of the pages in the column chunk. How could the pages in a column chunk switch between 16-bit and 17-bit dictionaries? \r\n\r\n> libcudf uses a fixed bit width for the entire column chunk based on the number of dictionary keys present. I think it would be a lot of work to use variable bit widths in cudf.\r\n\r\nWould you please share a bit more about this feature idea? How would the encoder choose the bit width for each page?","createdAt":"2023-10-30T19:13:41Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/13995#issuecomment-1785879607","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5qdBSS","author":{"login":"etseidl"},"authorAssociation":"CONTRIBUTOR","body":"@GregoryKimball, it has to do with the size of the dictionary when the page is encoded.  Because parquet-mr is more stream based, it will keep the current dictionary in RAM, and add keys as it goes.  So say that the first page has just under 64k distinct entries; in this case the maximum key size will be 16 bits, and the page will be RLE encoded with that bit width.  Now while encoding the second page, the number of distinct entries exceeds 64k; the RLE encoder will now use 17 bits. cuDF, on the other hand, computes the dictionary up front, and then uses the total number of entries to determine the bit width to use for all pages.\r\n\r\ncuDF could modify the dictionary page-encoder (not dictionary-page encoder :smile:)  to first find the largest dictionary key for the given page, and use that value to determine how many bits to use when doing the RLE encoding. I'm not sure what that would do to pre-computed page sizes and how the encoded values are stuffed into the column buffer.  This could get expensive to compute, but some users might prefer the smallest file possible and be willing to make that trade.","createdAt":"2023-10-30T20:33:02Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"LAUGH","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/13995#issuecomment-1785992338","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5qdjUj","author":{"login":"GregoryKimball"},"authorAssociation":"CONTRIBUTOR","body":"> cuDF could modify the dictionary page-encoder (not dictionary-page encoder ðŸ˜„) to first find the largest dictionary key for the given page, and use that value to determine how many bits to use when doing the RLE encoding.\r\n\r\nThank you @etseidl, I appreciate the explanation. If we added this dynamic bit width, ~we might see smaller file sizes for cuDF than parquet-mr, because parquet-mr can only go up as it writes more pages, but then cuDF could go up or down as needed~ then cuDF would also be able to change the bit width of data pages based on the largest key value in that page. This also makes me wonder if we could add more tricks with dict key order to yield even smaller files.\r\n\r\n>  I'm not sure what that would do to pre-computed page sizes and how the encoded values are stuffed into the column buffer. \r\n\r\nGood points. It seems like it would take more work upfront to re-compute the page sizes depending on the max key.\r\n\r\n> This could get expensive to compute, but some users might prefer the smallest file possible and be willing to make that trade.\r\n\r\nCertainly some users put a huge premium on file size. This also comes up a lot with the nvCOMP team where there are often runtime/filesize tradeoffs.\r\n\r\n","createdAt":"2023-10-30T22:16:48Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/13995#issuecomment-1786131747","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5qd0xP","author":{"login":"etseidl"},"authorAssociation":"CONTRIBUTOR","body":"I did a quick test and found that each page winds up with a wide range of keys due to the parallel nature of the dictionary construction. This will need more thought.","createdAt":"2023-10-30T23:29:58Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/13995#issuecomment-1786203215","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5r0cRH","author":{"login":"GregoryKimball"},"authorAssociation":"CONTRIBUTOR","body":"> I did a quick test and found that each page winds up with a wide range of keys due to the parallel nature of the dictionary construction. This will need more thought.\r\n\r\nThank you @etseidl for testing this. Do you think we could do better by sorting the keys descending based on the number of occurrences in the column chunk? (and then using dynamic bit width for the pages)\r\n\r\nIf sorting on number of occurrences doesn't work well, then I suppose we would be stuck with a more difficult optimization such as filling the first 2^16 keys to cover the most number of pages.\r\n\r\nDoes this sound right to you?","createdAt":"2023-11-13T19:38:31Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/13995#issuecomment-1808909383","viewerDidAuthor":false}],"createdAt":"2023-08-29T16:32:33Z","id":"I_kwDOBWUGps5vlNck","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NjE=","name":"feature request","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwxMDEzOTg3MzUy","name":"0 - Backlog","description":"In queue waiting for assignment","color":"d4c5f9"},{"id":"MDU6TGFiZWwxMTM5NzQwNjY2","name":"libcudf","description":"Affects libcudf (C++/CUDA) code.","color":"c5def5"},{"id":"MDU6TGFiZWwxMTg1MjQ0MTQy","name":"cuIO","description":"cuIO issue","color":"fef2c0"},{"id":"MDU6TGFiZWwxNDA1MTQ2OTc1","name":"Spark","description":"Functionality that helps Spark RAPIDS","color":"7400ff"}],"milestone":{"number":22,"title":"Parquet continuous improvement","description":"","dueOn":null},"number":13995,"projectCards":[],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[FEA] Add variable bit-width keys and improved key order for Parquet dict pages","updatedAt":"2024-04-02T03:36:21Z","url":"https://github.com/rapidsai/cudf/issues/13995"}
