{"assignees":[],"author":{"id":"MDQ6VXNlcjQ4Mzc1NzE=","is_bot":false,"login":"VibhuJawa","name":"Vibhu Jawa"},"body":"**Describe the bug**\r\n\r\ndask_cudf.read_json seems to be freezing when given a path with large number of files. Providing the list of files directly works\r\n\r\nBelow seems to be freezing\r\n```python3\r\ntext_ddf = dask_cudf.read_json(f'{INPUT_PATH}/data/*',engine='cudf',lines=True)\r\n```\r\n\r\nBelow works \r\n```python3\r\nfiles = list(map(lambda x: os.path.join(data_path, x), os.listdir(data_path)))\r\n\r\ntext_ddf = dask_cudf.read_json(files,engine='cudf',lines=True)\r\n```\r\n\r\n**Additional context**\r\nCC: @ayushdg ","closed":false,"closedAt":null,"comments":[{"id":"IC_kwDOBWUGps5Xnf23","author":{"login":"wence-"},"authorAssociation":"CONTRIBUTOR","body":"How many files? What filesystem? It seems like the globbing that `fsspec` does is a bit slower than `glob.glob` or `os.listdir`, but that doesn't make that much difference for me on a local filesystem with 50000 empty files.\r\n\r\nI think this is all happening in the graph construction (since no work is being done yet) so can you run the client-side code under a profiler and post the output (e.g. `pyinstrument` for a sampling-based callgraph). Or even just interrupt the \"hung\" code and see where it is.","createdAt":"2023-03-15T13:04:03Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12951#issuecomment-1469971895","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5XpHjM","author":{"login":"VibhuJawa"},"authorAssociation":"MEMBER","body":"So the graph creation is fine the reading is now working as well as directly using cuDF. \r\n\r\n\r\n### Using dask_cudf.read_json \r\n```python3\r\nwith performance_report(filename=\"dask-report-read_json.html\"):\r\n    text_ddf = dask_cudf.read_json(f'{INPUT_PATH}/data/*',engine='cudf',lines=True)\r\n    subset_ddf = text_ddf.partitions[:5_000]\r\n    st = time.time()\r\n    len_ddf = len(subset_ddf)\r\n    et = time.time()\r\n    print(f\"Time taken in reading = {et-st} s\")\r\n    print(len_ddf)\r\n  ```\r\n  \r\n``` python3\r\nTime taken in reading = 175.02658891677856 s\r\n27206847\r\n```\r\n\r\n\r\n### Using cudf.read_json\r\n[](url)\r\n```python3\r\ndef read_json(paths):\r\n    return cudf.read_json(paths,lines=True)\r\n\r\nwith performance_report(filename=\"dask-report-read_with_glob.html\"):\r\n    paths = [entry.path for entry in os.scandir(f'{INPUT_PATH}/data')]\r\n    text_ddf = dd.from_map(read_json,paths)\r\n    subset_ddf = text_ddf.partitions[:5_000]\r\n    st = time.time()\r\n    len_ddf = len(subset_ddf)\r\n    et = time.time()\r\n    print(f\"Time taken in reading = {et-st} s\")\r\n    print(len_ddf)\r\n```\r\n```python3\r\nTime taken in reading = 58.15872025489807 s\r\n27458208\r\n```\r\n\r\n\r\n\r\nSee Profiles here: https://gist.github.com/VibhuJawa/93e88e8f326546069f8abcb5f50deda6\r\n\r\n","createdAt":"2023-03-15T16:51:28Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12951#issuecomment-1470396620","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5XpJ9t","author":{"login":"wence-"},"authorAssociation":"CONTRIBUTOR","body":"So that really does look like fsspec is taking 60 or so seconds to open a bunch of filehandles.\r\n\r\nCan you compare the time for:\r\n```\r\ntext_ddf = dask_cudf.read_json(f'{INPUT_PATH}/data/*',engine='cudf',lines=True)\r\n```\r\n\r\nand\r\n```\r\npaths = [entry.path for entry in os.scandir(f'{INPUT_PATH}/data')]\r\ntext_ddf = dd.from_map(read_json,paths)\r\n```\r\n?\r\n\r\nWould again be useful to know how many files you have.","createdAt":"2023-03-15T16:57:55Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12951#issuecomment-1470406509","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5Xpt7X","author":{"login":"VibhuJawa"},"authorAssociation":"MEMBER","body":"\r\n>Comparison:\r\n```python3\r\ntext_ddf = dask_cudf.read_json(f'{INPUT_PATH}/data/*',engine='cudf',lines=True)\r\n```\r\n```\r\nCPU times: user 7.23 s, sys: 2.67 s, total: 9.9 s\r\nWall time: 12.1 s\r\n```\r\n\r\n```python3\r\ndef read_json(paths):\r\n    return cudf.read_json(paths,lines=True)\r\n\r\npaths = [entry.path for entry in os.scandir(f'{INPUT_PATH}/data')]\r\ntext_ddf = dd.from_map(read_json,paths)\r\n```\r\n```\r\nCPU times: user 393 ms, sys: 213 ms, total: 606 ms\r\nWall time: 593 ms\r\n```\r\n\r\n\r\n>Would again be useful to know how many files you have.\r\n```\r\nprint(len(paths))\r\n60000\r\n```\r\n","createdAt":"2023-03-15T18:29:21Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12951#issuecomment-1470553815","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5eQ3LL","author":{"login":"GregoryKimball"},"authorAssociation":"CONTRIBUTOR","body":"Seems like this could be related to https://github.com/rapidsai/cudf/issues/13246. Some suggestions from there were:\r\n* setting environment variable LIBCUDF_CUFILE_POLICY=\"OFF\". \r\n* increasing the DefaultLimitNOLIMIT of systemd to one million files ","createdAt":"2023-06-07T20:41:26Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12951#issuecomment-1581478603","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5-QLFM","author":{"login":"vyasr"},"authorAssociation":"CONTRIBUTOR","body":"I still see a meaningful performance difference here, but not as drastic of one. Some of this behavior could be largely filesystem-dependent, so my numbers may not be directly comparable to @VibhuJawa's above, especially if the overhead is mostly coming from a filesystem library like fsspec, but running this script\r\n```\r\nimport cudf\r\nimport dask_cudf\r\nimport json\r\nimport os\r\nfrom tqdm import tqdm\r\nimport time\r\n\r\n# Write 10000 copies of the same JSON file to a subdirectory\r\nsubdir = \"subdir\"\r\nos.makedirs(subdir, exist_ok=True)\r\nfor i in tqdm(range(60000)):\r\n    with open(os.path.join(subdir, f\"file_{i}.json\"), \"w\") as f:\r\n        json.dump({\"key\": \"value\"}, f)\r\n\r\n\r\n\r\nstart = time.time()\r\ntext_ddf = dask_cudf.read_json(os.path.join(subdir, \"*\"), engine='cudf', lines=True)\r\nprint(\"Time to read 60000 files with dask_cudf:\", time.time() - start)\r\n\r\nstart = time.time()\r\ntext_df = cudf.read_json(os.path.join(subdir, \"*\"), lines=True)\r\nprint(\"Time to read 60000 files with cudf:\", time.time() - start)\r\n```\r\n\r\nproduces\r\n```\r\n(rapids) coder _ ~/cudf $ python test.py\r\n100%|_____________________________________________________________________________________________________________________________________| 60000/60000 [00:05<00:00, 11487.66it/s]\r\nTime to read 60000 files with dask_cudf: 20.821030139923096\r\nTime to read 60000 files with cudf: 7.378933429718018\r\n```\r\n\r\n@VibhuJawa @wence- do you think there's some action we should be taking here based on these numbers?","createdAt":"2024-05-17T18:28:14Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12951#issuecomment-2118168908","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5-RLtk","author":{"login":"VibhuJawa"},"authorAssociation":"MEMBER","body":"> I still see a meaningful performance difference here\r\n\r\n@vyasr , I think we forgot a `.compute` here, dask is lazy be default so here example is not really computing here. \r\n\r\n I changed to `6_000` from  `60_000` (10x lower) , the delta on `6_000` files is still very significant and i think at 10x the files its only going to increase. \r\n\r\n```python3\r\n\r\nimport cudf\r\nimport dask_cudf\r\nimport json\r\nimport os\r\nfrom tqdm import tqdm\r\nimport time\r\n\r\n# Directory to write the JSON files\r\nsubdir = \"/raid/vjawa/test_dir\"\r\nos.makedirs(subdir, exist_ok=True)\r\n\r\n# Write 6000 copies of the same JSON file to the subdirectory\r\nfor i in tqdm(range(6000)):\r\n    with open(os.path.join(subdir, f\"file_{i}.json\"), \"w\") as f:\r\n        json.dump({\"key\": \"value\"}, f)\r\n\r\n# Measure time to read files with dask_cudf\r\nstart = time.time()\r\ntext_ddf = dask_cudf.read_json(os.path.join(subdir, \"*\"), engine='cudf', lines=True).compute()\r\nprint(\"Time to read 6000 files with dask_cudf:\", time.time() - start)\r\n\r\n# Measure time to read files with cudf\r\nstart = time.time()\r\ntext_df = cudf.read_json(os.path.join(subdir, \"*\"), lines=True)\r\nprint(\"Time to read 6000 files with cudf:\", time.time() - start)\r\n\r\n```\r\n\r\nThis produces: \r\n\r\n```\r\nTime to read 6000 files with dask_cudf: 44.730552434921265\r\nTime to read 6000 files with cudf: 0.32662487030029297\r\n```\r\n\r\nSo we are still suffering from the problem. \r\n\r\nThe workaround we have of using `dd.from_map(read_json,paths)` for Nemo-Curator can potentially cause problems on S3 based systems, cc: @ayushdg  to add more context here. \r\n","createdAt":"2024-05-17T22:04:20Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12951#issuecomment-2118433636","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5-cTGB","author":{"login":"vyasr"},"authorAssociation":"CONTRIBUTOR","body":"I thought I had computed, but that must have been on a different issue... lost track of things a bit during all the triaging I was doing last week. Thanks for the catch. I can reproduce this now by adding the compute call to make it hang.","createdAt":"2024-05-20T22:53:51Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12951#issuecomment-2121347457","viewerDidAuthor":false}],"createdAt":"2023-03-15T03:54:13Z","id":"I_kwDOBWUGps5g1sVz","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NTk=","name":"bug","description":"Something isn't working","color":"d73a4a"},{"id":"MDU6TGFiZWwxMDEzOTg3Nzk5","name":"0 - Waiting on Author","description":"Waiting for author to respond to review","color":"ffb88c"},{"id":"MDU6TGFiZWwxMTM5NzQwNjY2","name":"libcudf","description":"Affects libcudf (C++/CUDA) code.","color":"c5def5"},{"id":"MDU6TGFiZWwxMTg1MjQwODk4","name":"dask","description":"Dask issue","color":"fcc25d"},{"id":"MDU6TGFiZWwxMTg1MjQ0MTQy","name":"cuIO","description":"cuIO issue","color":"fef2c0"}],"milestone":null,"number":12951,"projectCards":[],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[BUG]  dask_cudf.read_json seems to be freezing when given a path with large number of files","updatedAt":"2024-05-20T22:53:52Z","url":"https://github.com/rapidsai/cudf/issues/12951"}
