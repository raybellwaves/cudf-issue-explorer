{"assignees":[],"author":{"id":"MDQ6VXNlcjEzNjA3NjY=","is_bot":false,"login":"jlowe","name":"Jason Lowe"},"body":"**Describe the bug**\r\nUsing libcudf to load a Parquet file that is malformed \"succeeds\" by producing a table with some corrupted rows rather than returning an error as expected.  Spark 3.5, parquet-mr 1.13.1, and pyarrow 13 all produce unexpected EOF errors when trying to load the same file.\r\n\r\n**Steps/Code to reproduce bug**\r\nLoad https://github.com/apache/parquet-testing/blob/master/data/fixed_length_byte_array.parquet using libcudf.  Note that it will produce a table with 1000 rows with no nulls, and some of the rows have a list of bytes longer than 4 entries.  According to the [docs for the file](https://github.com/apache/parquet-testing/blob/master/data/fixed_length_byte_array.md), the data is supposed to be a single column with a fixed-length byte array of size 4, yet some rows load with more than four bytes, some with no bytes.\r\n\r\n**Expected behavior**\r\nlibcudf should return an error when trying to load the file rather than producing corrupted rows.\r\n","closed":false,"closedAt":null,"comments":[{"id":"IC_kwDOBWUGps5mhylQ","author":{"login":"etseidl"},"authorAssociation":"CONTRIBUTOR","body":"These are like puzzles :sweat_smile: So the issue with this file is that the schema says the `flba_field` is required, but the column index indicates there are nulls. Because the schema says the field is required, there is no definition level data either. I suppose this one could be detected by doing a sanity check during page header parsing (make sure the uncompressed size makes sense for how many values should be present), or even after reading the file metadata (schema says required but metadata says nulls are present). The page reader should also exit (but does not...it trusts the value counts and doesn't currently detect buffer overruns) when it reaches the end of the page data, but as with other errors that occur on the device, it's hard to communicate back to the host that some exception occurred. ","createdAt":"2023-09-14T20:49:29Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/14104#issuecomment-1720133968","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5mxZsX","author":{"login":"GregoryKimball"},"authorAssociation":"CONTRIBUTOR","body":"Thank you @etseidl for looking into this. Of your proposals I prefer:\r\n> after reading the file metadata (schema says required but metadata says nulls are present)\r\n\r\nI don't mind doing more work if we are going to crash anyways. What do you think is the simplest check to implement?\r\n(FYI @PointKernel)","createdAt":"2023-09-18T19:16:10Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/14104#issuecomment-1724226327","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5mxrEu","author":{"login":"etseidl"},"authorAssociation":"CONTRIBUTOR","body":"@GregoryKimball I think the simplest would be to walk through the schema in some fashion, find the max definition level for each column, and then check the ColumnIndex for for each column chunk for that column and see if the num_nulls field is consistent with the max definition level (i.e, if max_def == 0 and num_nulls > 0 then error). This would be doable on the host without digging into the page data.  But this requires that column indexes are present (which they are for this file).  The next option would be to do the same thing, but instead walk the page headers in the file to get the null counts, but that would require V2 data page headers.\r\n\r\nThe only surefire way is to detect the buffer overun when decoding the values (which is what parquet-mr and arrow seem to do), but as I've said, erroring out of the kernel when that is detected and communicating the error to the host is an issue.","createdAt":"2023-09-18T20:10:24Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/14104#issuecomment-1724297518","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5nAviQ","author":{"login":"vuule"},"authorAssociation":"CONTRIBUTOR","body":"The decode kernel does not detect the error, `page_state_s.error` flag stays at zero when reading the linked file.\r\nIf this flag was raised, we could use it to communicate the error to the host. I think the overhead of such solution would be acceptable (4 byte D2H copy, w/o errors; plus `atomicOr` when an error occurs).","createdAt":"2023-09-20T18:37:15Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/14104#issuecomment-1728247952","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5ngj6N","author":{"login":"GregoryKimball"},"authorAssociation":"CONTRIBUTOR","body":"#14167 is taking the first step to solving this case. We will also need to update the decode kernel to detect this error.","createdAt":"2023-09-27T02:45:55Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/14104#issuecomment-1736588941","viewerDidAuthor":false}],"createdAt":"2023-09-13T15:40:58Z","id":"I_kwDOBWUGps5w8FKW","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NTk=","name":"bug","description":"Something isn't working","color":"d73a4a"},{"id":"MDU6TGFiZWwxMDEzOTg3NDE3","name":"1 - On Deck","description":"To be worked on next","color":"bfd4f2"},{"id":"MDU6TGFiZWwxMTM5NzQwNjY2","name":"libcudf","description":"Affects libcudf (C++/CUDA) code.","color":"c5def5"},{"id":"MDU6TGFiZWwxMTg1MjQ0MTQy","name":"cuIO","description":"cuIO issue","color":"fef2c0"},{"id":"MDU6TGFiZWwxNDA1MTQ2OTc1","name":"Spark","description":"Functionality that helps Spark RAPIDS","color":"7400ff"}],"milestone":{"number":22,"title":"Parquet continuous improvement","description":"","dueOn":null},"number":14104,"projectCards":[],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[BUG] Malformed fixed length byte array Parquet file loads corrupted data instead of error","updatedAt":"2024-02-16T23:57:35Z","url":"https://github.com/rapidsai/cudf/issues/14104"}
