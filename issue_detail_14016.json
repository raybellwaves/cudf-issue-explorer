{"assignees":[],"author":{"id":"MDQ6VXNlcjE4NDUzNjA0","is_bot":false,"login":"rohanpaul14855","name":""},"body":"I'm trying to train an xgboost model on a machine with 8xA100 GPUs with 80GB memory each but I'm getting an out of memory error:\r\n`MemoryError('std::bad_alloc: out_of_memory: CUDA error at: .../include/rmm/mr/device/cuda_memory_resource.hpp')`. The error is slightly different if I use `rmm_pool_size` parameter but it is still a memory error `\"MemoryError('std::bad_alloc: out_of_memory: RMM failure at:../include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')`\r\n\r\nI'm using a `LocalCUDACluster` to distribute the workload amongst the 8 GPUs. I can tell by looking at the dask dashboard, that the data is mostly loading into a single GPU and all of the other GPUs are sitting empty and idle. \r\n\r\nI read the data using `dask_cudf.read_parquet(file_name, blocksize=int(2e4))` and it is a dataframe of size `(20459297, 213)`. Though I would like to try it with much larger datasets.  The training completes successfully with a smaller dataframe of size `(16304159, 213)` and fewer workers but it still mostly uses a single GPU. \r\n\r\nEdit: Here's a screenshot of the dashboard when the model is successfully training - note this is with only 2 GPUs and the smaller dataframe noted above\r\n\r\n<img width=\"1672\" alt=\"Screenshot\" src=\"https://github.com/rapidsai/cudf/assets/18453604/98706b71-19e3-4266-9c9c-23472994675c\">\r\n\r\nThe GPUs are running CUDA 12.0 and driver  525.60.13\r\nHere are the versions of some relevant packages\r\n\r\n```\r\nxgboost                   1.7.4           rapidsai_py310h1395376_6    rapidsai\r\nrapids                    23.08.00        cuda12_py310_230809_g2a5b6f0_0    rapidsai\r\npython                    3.10.12         hd12c33a_0_cpython    conda-forge\r\nrapids-xgboost            23.08.00        cuda12_py310_230809_g2a5b6f0_0    rapidsai\r\ncuda-version              12.0                 hffde075_2    conda-forge\r\ncudf                      23.08.00        cuda12_py310_230809_g8150d38e08_0    rapidsai\r\ndask                      2023.7.1           pyhd8ed1ab_0    conda-forge\r\ndask-core                 2023.7.1           pyhd8ed1ab_0    conda-forge\r\ndask-cuda                 23.08.00        py310_230809_gefbd6ca_0    rapidsai\r\ndask-cudf                 23.08.00        cuda12_py310_230809_g8150d38e08_0    rapidsai\r\n```\r\n\r\nAny help on the memory issue would be much appreciated\r\n\r\n","closed":false,"closedAt":null,"comments":[],"createdAt":"2023-08-31T08:34:47Z","id":"I_kwDOBWUGps5vwv1Z","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NjQ=","name":"question","description":"Further information is requested","color":"D4C5F9"},{"id":"MDU6TGFiZWwxMDEzOTg3MzUy","name":"0 - Backlog","description":"In queue waiting for assignment","color":"d4c5f9"},{"id":"MDU6TGFiZWwxMTM5NzQxMjEz","name":"cuDF (Python)","description":"Affects Python cuDF API.","color":"1d76db"},{"id":"MDU6TGFiZWwxMTg1MjQwODk4","name":"dask","description":"Dask issue","color":"fcc25d"}],"milestone":{"number":20,"title":"Stabilizing large workflows (OOM, spilling, partitioning)","description":"","dueOn":null},"number":14016,"projectCards":[],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[QST] Dask-cudf/Xgboost out of memory error","updatedAt":"2023-09-27T02:34:02Z","url":"https://github.com/rapidsai/cudf/issues/14016"}
