{"assignees":[],"author":{"id":"MDQ6VXNlcjQyNDk0NDc=","is_bot":false,"login":"lmeyerov","name":""},"body":"**Is your feature request related to a problem? Please describe.**\r\n\r\nWe are currently putting manual `await sleep(0)` points into our cudf code to enable use of cudf alongside Python async code in web server scenarios. Otherwise, cudf hangs our web server when it is used for handling requests. This gets worse as tasks get bigger, complicates having mixed CPU/GPU tasks, and unnecessarily forces architectural decisions like carefully separating processes and 2-level scheduling. \r\n\r\n**Describe the solution you'd like**\r\n\r\nSupport for Python3's async/await constructor. Part of the Python 2 -> 3 shift is native support for `async`, especially for IO (e.g., when handling web requests) and compute (e.g., compression tasks).  \r\n\r\nEx:\r\n\r\n```\r\n@route(/cluster/big/dataset/<datasetid>, method=POST)\r\nasync def cluster_big_dataset(dataset_id):\r\n    \r\n    df = await cudf.read_parquet(f'/files/{dataset_id}.parquet')\r\n    ...\r\n```\r\n\r\nThis would be great universally, but there's probably a ~top 10 list for most slow in practice: to/from I/O, groupby & merge, ... .\r\n\r\n**Describe alternatives you've considered**\r\n\r\n* Going via Dask also supports this, but with way more overhead and complexity. \r\n\r\n* We currently use multiple Python processes to help with SLAs, but it's clearly avoidable.","closed":false,"closedAt":null,"comments":[{"id":"MDEyOklzc3VlQ29tbWVudDU5OTgyMjI2Mg==","author":{"login":"lmeyerov"},"authorAssociation":"NONE","body":"Clarifying from Slack:\r\n\r\nThis is related to but not dependent on CUDA streams. CUDA streams (afaict) allow control of concurrent + parallel execution of ~heterogeneous CUDA task graphs. \r\n\r\nConsider a modern server (python, node, ruby, php, java, ...) GPU-accelerating requests of different type & size:\r\n\r\nThe relationship is:\r\n\r\n* Priority request - non-blocking of host event loops: Allow the host's event loop to not block on cudf calls, and instead proceed to handling the next server request, and get back to cudf when it's done. So the GPU crunching doesn't prevent the CPU from continuing, such as being able to tell clients  \"come back later\" and \"progress\" and \"here's some CPU calcs\". This helps prevents big tasks from making the system non-responsive and otherwise starving the system.\r\n\r\n* Maybe viable - non-blocking event loop w/ streams across requests: If each event context is really a sequence of cudf calls, CUDA streams would support better (e.g., parallel) execution of heterogenous tasks across concurrent events. This would help increase GPU utilization when tasks are of different sizes, so a bunch of tiny tasks wouldn't clog up a big box through low utilization / inversion of control.\r\n\r\n* User-level: Some better users may occasionally benefit from user-control of cuda streams:\r\n\r\n```\r\nawait asyncio.gather([ task1, task2, task3 ])\r\n```\r\n\r\n* Framework-level: Algo writers may be even more prone to writing such tasks.\r\n\r\nPriority order, IMO is:\r\n-- cudf not blocking the event loop\r\n-- a concurrency control for allowing concurrent task execution across event contexts\r\n\r\n```\r\ncudf.set_asyncio_concurrent_streams(10)\r\n```\r\n\r\nOr enabling user schedulers to map event context <> default cuda stream\r\n```\r\ncudf.set_stream_context_id( cudf.get_stream_context_id() )\r\ncudf.set_stream_policy(....)\r\n```\r\n\r\n-- DAG primitives like `gather`","createdAt":"2020-03-17T00:47:43Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/4529#issuecomment-599822262","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDc5ODk2MjY4Mw==","author":{"login":"github-actions"},"authorAssociation":"NONE","body":"This issue has been labeled `inactive-90d` due to no recent activity in the past 90 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed.","createdAt":"2021-03-14T19:14:06Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/4529#issuecomment-798962683","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDc5ODk2MjY5Mg==","author":{"login":"github-actions"},"authorAssociation":"NONE","body":"This issue has been labeled `inactive-30d` due to no recent activity in the past 30 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed. This issue will be labeled `inactive-90d` if there is no activity in the next 60 days.","createdAt":"2021-03-14T19:14:08Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/4529#issuecomment-798962692","viewerDidAuthor":false}],"createdAt":"2020-03-16T21:44:13Z","id":"MDU6SXNzdWU1ODI2MTYzNDU=","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NjE=","name":"feature request","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwxMTM5NzQxMjEz","name":"cuDF (Python)","description":"Affects Python cuDF API.","color":"1d76db"}],"milestone":null,"number":4529,"projectCards":[{"project":{"name":"Feature Planning"},"column":{"name":"Needs prioritizing"}}],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[FEA] Async, especially for heavier ops","updatedAt":"2024-02-23T18:43:32Z","url":"https://github.com/rapidsai/cudf/issues/4529"}
