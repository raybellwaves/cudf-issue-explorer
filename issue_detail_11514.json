{"assignees":[],"author":{"id":"MDQ6VXNlcjE2NzQ2NDA5","is_bot":false,"login":"trivialfis","name":"Jiaming Yuan"},"body":"A related issue: https://github.com/rapidsai/cudf/issues/11462\r\n\r\nWe would like to transfer a cuDF dataframe between a JVM process and a Python process without data copy . This is primarily used in PySpark environment where Spark can execute user-defined Python functions in JVM processes. The current solution in Spark is to serialize the dataframe into arrow format and perform an inter-process transfer on host. This is not efficient for both memory usage and computation time. Our proposed solution at the moment is to use the CUDA IPC mechanism for transferring metadata between 2 processes without actually copying data between host and device or between different processes. The message sent between 2 processes in our proposed method is a JSON document that describes some properties of the dataframe along with an encoded CUDA IPC handle.\r\n\r\n**Describe the solution you'd like**\r\nWe would like to add two roundtrip methods for generating an IPC message that describes the dataframe and reconstructs the dataframe from that message at the C++/C level along with wrappers in java/python.  \r\n\r\n``` python\r\ndf = cudf.DataFrame({\"a\": [1, 2, 3]})\r\nmsg = df.to_ipc()\r\n\r\n# in a different process but on the same CUDA device\r\ndf = cudf.DataFrame.from_ipc(msg)\r\n```\r\n\r\nAs for a quick design of the message, for the lack of a better term, we can jsonify the `__dataframe__` protocol along with the use of encoded CUDA IPC handle. The message sent between processes can be something similar to:\r\n``` json\r\n{\r\n  \"columns\": [\r\n    {\r\n      \"describe_categorical\": {\r\n        \"is_dictionary\": true,\r\n        \"is_ordered\": true,\r\n        \"mapping\": {\r\n          \"a\": 0\r\n        }\r\n      },\r\n      \"describe_null\": [\r\n        3,\r\n        null\r\n      ],\r\n      \"dtype\": [\r\n        0,\r\n        64,\r\n        \"i\",\r\n        \"=\"\r\n      ],\r\n      \"buffers\": [{\"ipc_handle\": \"aeb6df622e4d6ed84dac526c8815c52c5bb2a34855a765270d6e1a502dc6574b\"}],\r\n      \"metadata\": null,\r\n      \"null_count\": null,\r\n      \"offset\": 0,\r\n      \"size\": 128\r\n    },\r\n    {\r\n      ...\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nWe might be able to reuse some logic in the `df_protocol.py` module for implementing this feature.\r\n\r\n**Describe alternatives you've considered**\r\n- Suggested by @jakirkham , I looked into the serialize and deserialize methods. They are good starting points but not suitable for transferring ownership between 2 different language environments due to the use of pickle in type serialization.\r\n- Suggested by @shwina , I looked into the dataframe protocol, which is very close to what we want, except for its Python only interface and the use of a pointer.\r\n- `mapInArrow` method from PySpark, which still requires a full data transfer.","closed":false,"closedAt":null,"comments":[{"id":"IC_kwDOBWUGps5ITeir","author":{"login":"shwina"},"authorAssociation":"CONTRIBUTOR","body":"Thanks for writing this up, @trivialfis! Firstly, I'm not super familiar with CUDA IPC and any associated pitfalls, so I'll cc @trxcllnt and @kkraus14 here, who may be able to comment more on the suggested approach and API.\r\n\r\nRegarding the suggestion to use a 'jsonified' interchange protocol for messaging, I do have some concerns. Firstly, the dataframe interchange protocol has the narrower goal of enabling interchange between Python libraries. The semantics may not always match 1-to-1 with IPC.\r\n\r\nSecondly, and relatedly, I'm concerned that we would effectively be introducing a new IPC format, while others like [Feather](https://arrow.apache.org/docs/python/feather.html) already exist. Would supporting Feather solve this problem using a more standard approach? \r\n\r\nWe recently [removed](https://github.com/rapidsai/cudf/pull/10995/) the bit of functionality around the Arrow IPC Format from cudf. I don't believe Spark was supporting that, and on the Python side, it was largely unused. More importantly, my understanding was that for GPU-resident data, the implementation wasn't even truly zero-copy (@trxcllnt can confirm). But perhaps we could consider bringing back (and fixing) that code?\r\n","createdAt":"2022-08-12T12:34:42Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11514#issuecomment-1213065387","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5ITlKS","author":{"login":"trivialfis"},"authorAssociation":"MEMBER","body":"@shwina Thank you for the reply.\r\n\r\n> Would supporting Feather solve this problem using a more standard approach?\r\n\r\nI looked into arrow IPC, which allows writing and reading data efficiently, and reading can be optimized with memory mapped file.  But still, we will have to go through device -> host then host -> device along with read-write to socket/mmap/pipe/file for transferring data from jvm to python process. Feel free to correct me if I'm wrong, the wording of the document seems a little bit vague.\r\n\r\n> Secondly, and relatedly, I'm concerned that we would effectively be introducing a new IPC format\r\n\r\nI think it will remain proprietary and doesn't need to be stable for some period of time since we are transferring CUDA IPC mem handle. Anyone who wants to read the data must understand how to use CUDA API to obtain the pointer. For the \"jsonify df protocol\", that's because the protocol has complete information about the dataframe and I would like to avoid reinventing the wheel.\r\n\r\n> my understanding was that for GPU-resident data, the implementation wasn't even truly zero-copy\r\n\r\nIf it's close to zero-copy then it's probably good enough. The best case scenario is there's a way to let another process view the data in the current process without incurring significant overhead in data transfer using only arrow. At the moment, CUDA IPC handle (not arrow CUDA writer, but the IPC API provided by CUDA toolkit) is the only way I can think of: https://numba.pydata.org/numba-doc/latest/cuda/ipc.html","createdAt":"2022-08-12T13:08:08Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11514#issuecomment-1213092498","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5IU75K","author":{"login":"kkraus14"},"authorAssociation":"COLLABORATOR","body":"The previous Arrow IPC code that was recently removed didn't go through host memory, it was entirely in GPU. That being said it did require a lightweight serialization to pack the data of a dataframe into a single buffer that could be IPCed instead of having to IPC each underlying buffer. Here's the spec: https://arrow.apache.org/docs/format/Columnar.html#ipc-streaming-format. One issue with this is that it expects the Schema to be inline with the data and it doesn't really make sense for the Schema to be in GPU memory.\r\n\r\nThe dataframe interchange protocol referenced by @shwina currently uses pointers within a process and has a pointer per underlying buffer in the dataframe. This could be an arbitrarily large number of buffers depending on the number of columns and their types. You could imagine building something similar to this and using CUDA IPC handles instead of pointers as well as serializing all of the Python class hierarchy and information surrounding it and using some form of side channel to move that between processes. Looks like you've roughly sketched this out already.\r\n\r\nIdeally, something like this would get handled in C/C++ instead of in Python so that the implementation can be reused across languages.","createdAt":"2022-08-12T19:30:08Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11514#issuecomment-1213447754","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5IVtXD","author":{"login":"trivialfis"},"authorAssociation":"MEMBER","body":"@kkraus14 Thank you for the comment!\r\n\r\n> The previous Arrow IPC code that was recently removed didn't go through host memory, it was entirely in GPU. \r\n\r\nCould you please share some insight on how it's implemented under the hood?\r\n\r\n> something like this would get handled in C/C++ instead\r\n\r\nI agree, also might be easier to maintain due to memory management.","createdAt":"2022-08-13T04:17:44Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11514#issuecomment-1213650371","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5IVuAa","author":{"login":"kkraus14"},"authorAssociation":"COLLABORATOR","body":"> Could you please share some insight on how it's implemented under the hood?\r\n\r\nYou can see the implementation here:\r\n- https://github.com/rapidsai/cudf/blob/v22.06.01/cpp/src/comms/ipc/ipc.cpp\r\n- https://github.com/rapidsai/cudf/blob/v22.06.01/cpp/include/cudf/ipc.hpp","createdAt":"2022-08-13T04:31:21Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11514#issuecomment-1213653018","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5IV8T8","author":{"login":"trivialfis"},"authorAssociation":"MEMBER","body":"Thank you for the reference. I will dig into arrow.\r\n\r\nAlso, this issue is more a question of whether this feature is welcomed for cuDF. I can help work on it if we agree on the design.","createdAt":"2022-08-13T05:12:39Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11514#issuecomment-1213711612","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5Ib-QM","author":{"login":"trivialfis"},"authorAssociation":"MEMBER","body":"I looked into cuDF and arrow today. I think the arrow IPC should be sufficient for now. Will work on a wrapper tomorrow.\r\n\r\ncc @wbo4958 ","createdAt":"2022-08-15T16:23:44Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/11514#issuecomment-1215292428","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5Ieeqm","author":{"login":"jakirkham"},"authorAssociation":"MEMBER","body":"cc @pentschev @madsbk (in case either of you have thoughts on this process interchange use case)","createdAt":"2022-08-15T22:45:56Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11514#issuecomment-1215949478","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5Is-sG","author":{"login":"trivialfis"},"authorAssociation":"MEMBER","body":"Hey, I opened a draft PR https://github.com/rapidsai/cudf/pull/11564 .","createdAt":"2022-08-18T17:29:18Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11514#issuecomment-1219750662","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5I2tnx","author":{"login":"shwina"},"authorAssociation":"CONTRIBUTOR","body":"@trivialfis - Jake's comment [here](https://github.com/rapidsai/cudf/pull/11564#pullrequestreview-1078702940) raises an important question that I'd like for us to collectively answer: can something like UCX with serialization not be used here? \r\n\r\n> Suggested by @jakirkham , I looked into the serialize and deserialize methods. They are good starting points but not suitable for transferring ownership between 2 different language environments due to the use of pickle in type serialization.\r\n\r\n`pickle` is used only for serializing and deserializing metadata between Python processes. For this use-case, couldn't we use e.g., json for the metadata, and `serialize()` for the data? \r\n\r\nIntra-node transfers via UCX _should_ use CUDA IPC. For communication between a Python process using cuDF, and a JVM process running PySpark, I can imagine using ucx-py for sending/writing messages and UCX for receiving/reading , although admittedly I'm not clear on the implementation details. cc: @pentschev who is the expert in this area and could provide more insight.","createdAt":"2022-08-22T12:39:07Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11514#issuecomment-1222302193","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5I33rh","author":{"login":"trivialfis"},"authorAssociation":"MEMBER","body":"Thank you for the suggestions @shwina ! Would love to hear from @pentschev on the details.\r\n\r\nIf UCX can help zero-copy data transfer, we might still want to have a unified API as part of cuDF to help different language envs work together.","createdAt":"2022-08-22T16:30:47Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11514#issuecomment-1222605537","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5I4egv","author":{"login":"pentschev"},"authorAssociation":"MEMBER","body":"I've checked with UCX devs and zero-copy in the truest sense (directly accessing the remote process' memory without need for a local copy) is not currently supported by UCX. For the short-term, AFAICT the only way to achieve true zero-copy would be to manually handle CUDA IPC.\r\n\r\nWhat we can do with UCX is as Ashwin said already above, transfer intra-GPU data via CUDA IPC. On a V100, this achieves bandwidth > 350GB/s (see [`CUDA_IPC_SELF` in UCX-Py's nightly benchmark charts](https://raw.githack.com/pentschev/ucx-py-ci/test-results/assets/ucx-py-barchart.html)), but will require data duplication while transferring.\r\n\r\nBecause UCX-Py is an interface for UCX, we could do UCX-Py<->UCX transfers with no problem, we are required to first establish endpoints between UCX workers on both processes. I know Spark uses Java bindings for UCX, but I'm not familiar with either Spark nor the Java bindings internals, is there any kind of interface within PySpark for UCX? Given the \"Py\" prefix, I would assume we could just use UCX-Py on both cuDF and PySpark ends, but maybe there's already some UCX within PySpark or that it could utilize.","createdAt":"2022-08-22T18:35:11Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11514#issuecomment-1222764591","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5I4hXU","author":{"login":"trivialfis"},"authorAssociation":"MEMBER","body":"Hi @revans2 , could you please share some insights into the current status of UCX in spark-rapids? https://github.com/rapidsai/cudf/issues/11514#issuecomment-1222764591\r\n\r\nI'm under the impression that it's very difficult for us to upstream any non-standard (GPU) changes to spark.\r\n\r\n> What we can do with UCX is as Ashwin said already above, transfer intra-GPU data via CUDA IPC\r\n\r\nOut of curiosity, does that mean we are simply doing a cudaMemcpy when two nodes are using the same GPU?","createdAt":"2022-08-22T18:47:43Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11514#issuecomment-1222776276","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5I4h0k","author":{"login":"pentschev"},"authorAssociation":"MEMBER","body":"> Out of curiosity, does that mean we are simply doing a cudaMemcpy when two nodes are using the same GPU?\r\n\r\nI assume by nodes in this context you mean processes on the same system, in that case yes, that's correct.","createdAt":"2022-08-22T18:49:41Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/11514#issuecomment-1222778148","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5I4kjn","author":{"login":"trivialfis"},"authorAssociation":"MEMBER","body":">  assume by nodes in this context you mean processes on the same system, in that case yes, that's correct.\r\n\r\nThat's encouraging! Thank you for the confirmation.","createdAt":"2022-08-22T19:00:12Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11514#issuecomment-1222789351","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5I46hd","author":{"login":"trivialfis"},"authorAssociation":"MEMBER","body":"@pentschev Could you please point me to the code of ucx transferring cuDF dataframe?","createdAt":"2022-08-22T19:58:26Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11514#issuecomment-1222879325","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5I5DoH","author":{"login":"pentschev"},"authorAssociation":"MEMBER","body":"Just to be clear, the `cudaMemcpy` call is handled by UCX via its API (e.g., via [`ucp_tag_send_nbx`](https://openucx.readthedocs.io/en/master/api/file/ucp_8h.html#group___u_c_p___c_o_m_m_1ga8323878b60f426c630d4ff8996ede3cc)/[`ucp_tag_recv_nbx`](https://openucx.readthedocs.io/en/master/api/file/ucp_8h.html#group___u_c_p___c_o_m_m_1gaa842f8ca8ad1363ed857ab938285a16f)). IOW, you won't get the CUDA IPC handle to do the `cudaMemcpy` directly when using the UCP layer of UCX (which is what UCX-Py exposes). In UCX-Py, you would do that using [`send`](https://ucx-py.readthedocs.io/en/latest/api.html#ucp.Endpoint.send)/[`recv`](https://ucx-py.readthedocs.io/en/latest/api.html#ucp.Endpoint.recv) methods.\r\n\r\nWe have a [complete unit test for cuDF message transfer](https://github.com/rapidsai/ucx-py/blob/branch-0.28/tests/test_custom_send_recv.py), it shows the entire procedure to connect two processes (one being the listener and another being the client), which one process will [`write`](https://github.com/rapidsai/ucx-py/blob/f585c5084f4fa1cc1ff7f1f21dd24a8bcea3c630/tests/test_custom_send_recv.py#L42-L58) and another will [`read`](https://github.com/rapidsai/ucx-py/blob/f585c5084f4fa1cc1ff7f1f21dd24a8bcea3c630/tests/test_custom_send_recv.py#L60-L88)","createdAt":"2022-08-22T20:13:46Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11514#issuecomment-1222916615","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5I5F8u","author":{"login":"trivialfis"},"authorAssociation":"MEMBER","body":"Thank you for the references! cc @wbo4958 .","createdAt":"2022-08-22T20:17:40Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11514#issuecomment-1222926126","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5I95ME","author":{"login":"trivialfis"},"authorAssociation":"MEMBER","body":"Hi, let me make a brief summary on the topic, there are serval proposed approaches:\r\n1. Use Arrow IPC format along with its IO procedures. This is too slow and incurs data copies. Also, we need to patch arrow to handle the memory pool properly. Lastly, arrow_gpu dependency will be introduced back to cuDF along with libcuda.so.\r\n2. Define custom IPC routines in cuDF with one handle per buffer as in https://github.com/rapidsai/cudf/pull/11564 . This adds a non-trivial amount of code to the cuDF which might be a potential burden on future maintenance for cuDF developers. But the upside is that there's no data copy in C++.\r\n3. Define custom IPC routines in cuDF with one handle for a packed buffer based on `pack` and `unpack` functions. Tested by @wbo4958 and advocated by @abellina. This can reduce the amount of code compared to the per-column approach and is easier to implement and maintain. It needs a pair of methods in cuDF and some memory management for opened memory handle. Also, an extra copy of the data will be created. But overall is easier if we can live with the copy.\r\n4. Use UCX. This has already been implemented in other places and also has some integration in spark. But mentioned by @abellina , introducing UCX into dependencies might impose additional configuration onto users due to signal handling conflicts between UCX and jvm.\r\n5. Host the IPC code in https://github.com/NVIDIA/spark-rapids/issues/6391 .","createdAt":"2022-08-23T14:49:52Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11514#issuecomment-1224184580","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5I-d6b","author":{"login":"abellina"},"authorAssociation":"CONTRIBUTOR","body":"If we go with:\r\n\r\n> Define custom IPC routines in cuDF with one handle for a packed buffer based on pack and unpack functions. Tested by @wbo4958 and advocated by @abellina. This can reduce the amount of code compared to the per-column approach and is easier to implement and maintain. It needs a pair of methods in cuDF and some memory management for opened memory handle. Also, an extra copy of the data will be created. But overall is easier if we can live with the copy.\r\n\r\nDoes that make it a smaller change that builds on the pack/unpack functionality already supported in cuDF/py-cuDF and java-cuDF?\r\n\r\nIt seems that the main thing that is needed here is to add code (somewhere) that takes the contiguous device buffer and opens a cuda IPC mem handle to it. This seems like a separate concern than packing and unpacking, so I would expect there to be a function that takes a packed table reference and returns an IPC mem handle that can be sent to a peer. \r\n\r\nThe serialized mem handle is sent in addition to the opaque metadata generated in `pack`. On the other side, a function that can take this serialized IPC mem handle and the serialized metadata can be invoked to unpack a table view on this memory.","createdAt":"2022-08-23T16:48:40Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11514#issuecomment-1224335003","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5I-2nw","author":{"login":"trivialfis"},"authorAssociation":"MEMBER","body":"yes, the code should be smaller with the conversion from cudf column to IPC memory handle removed. After unpacking the dataframe, we either attach the opened memory handle to the new dataframe and keep the original one alive, or make an additional copy. This part of the code is needed by both of the per-column and per-dataframe approaches.","createdAt":"2022-08-23T17:40:08Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11514#issuecomment-1224436208","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5JBwaY","author":{"login":"trivialfis"},"authorAssociation":"MEMBER","body":"Regarding the overhead of opening memory handles, from my very simple test with dataframe generated by:\r\n``` python\r\n    from cudf.testing.dataset_generator import rand_dataframe\r\n    df = rand_dataframe([{\"dtype\": \"float32\", \"null_frequency\": 0.0, \"cardinality\": 10}] * n_columns, rows=int(1e5))\r\n    df = cudf.DataFrame.from_arrow(df)\r\n````\r\n\r\nWith 128 primitive dtype columns, `export_ipc​` takes 0.003928661346435547 seconds, `import_ipc` takes 0.801159143447876 seconds. With 1024 primitive dtype columns, `export_ipc`​ takes 0.03810715675354004 while `import_ipc​` takes 0.9540588855743408.  According to the CUDA document of [cudaIpcGetMemHandle](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g8a37f7dfafaca652391d0758b3667539) (the exporter):\r\n\r\n> This is a lightweight operation and may be called multiple times on an allocation without adverse effects\r\n\r\nI think the operation is fairly cheap, might even be cheaper than memory copies inside `pack`.  The problem is mostly about code gain. With the `pack` and `unpack` solution the code can be leaner. One additional concern raised by @abellina is memory deallocation on the producer side. We either have to export an additional RAII struct that tells the producer when it's safe to deallocate the dataframe or we make another copy on the consumer side.","createdAt":"2022-08-24T05:03:56Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11514#issuecomment-1225197208","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5JEMbc","author":{"login":"shwina"},"authorAssociation":"CONTRIBUTOR","body":"@trivialfis thank you for the excellent [summary](https://github.com/rapidsai/cudf/issues/11514#issuecomment-1224184580)! It sounds like we can discard the UCX approach, as it not only introduces extra copies, but also introduces a complex dependency that most users would not want to deal with.\r\n","createdAt":"2022-08-24T14:52:40Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11514#issuecomment-1225836252","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5JEPRh","author":{"login":"abellina"},"authorAssociation":"CONTRIBUTOR","body":">  We either have to export an additional RAII struct that tells the producer when it's safe to deallocate the dataframe or we make another copy on the consumer side.\r\n\r\nThe RAII struct won't solve it for this scenario, since you don't want to free once the stack that called \"send\" goes out of scope. What you really need is some type of callback from an RPC message being sent back by the consumer saying \"I am done with the copy/operation\". Either way, if the consumer says they are not touching that memory anymore, the producer can then free or repurpose the memory. This would be the same protocol layer that sent the metadata and serialized IPC handle to begin with, just in the opposite direction.","createdAt":"2022-08-24T15:02:15Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11514#issuecomment-1225847905","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5JESv4","author":{"login":"pentschev"},"authorAssociation":"MEMBER","body":"`cudaIpcGetMemHandle` is a fairly cheap operation, but `cudaIpcOpenMemHandle` is very expensive (IIRC, that was > 100ms), so make sure that gets cached.","createdAt":"2022-08-24T15:13:34Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/11514#issuecomment-1225862136","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5JFYz2","author":{"login":"trivialfis"},"authorAssociation":"MEMBER","body":"@shwina I agree. So the remaining options are 2) and 3). I can change the PR to using 3) if it's believed to be a better alternative. \r\n\r\nAlso, I just found out that in https://github.com/rapidsai/cudf/pull/11564 , even though there's no copy in c++, the cython code still does a copy with `Column.from_column_view` called inside `columns_from_table_view`. Is there a way to avoid that copy?\r\n\r\n@abellina emphasized the importance of making sure the producer doesn't free the dataframe until the consumer is finished. That has an implication that, if the consumer never actually import the IPC message, the producer should keep the dataframe around forever. There are two possible ways to achieve this: Add a remote callback into the IPC message as suggested by Alessandro and use it for decreasing the reference counter in Python and Java.  An alternative to this is to handle the lifetime of dataframe on the user side (say, spark) instead of putting it into cuDF. (which I prefer, but the cuDF API becomes less safe).","createdAt":"2022-08-24T19:26:17Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11514#issuecomment-1226149110","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5JGlqg","author":{"login":"shwina"},"authorAssociation":"CONTRIBUTOR","body":"> though there's no copy in c++, the cython code still does a copy with Column.from_column_view called inside columns_from_table_view. Is there a way to avoid that copy?\r\n\r\nIf you're constructing a Python `Column` from a `column_view`, you should provide an additional `owner` argument, which is a Python object that owns the memory that the `column_view` points to. IIRC, if you don't provide an owner, the best we can do is copy from the column view.\r\n\r\n\r\n\r\n\r\n","createdAt":"2022-08-24T22:01:41Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11514#issuecomment-1226463904","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5JIDkD","author":{"login":"trivialfis"},"authorAssociation":"MEMBER","body":"Thank you for the suggestion, I have provided a list of owners, but they are not `cudf.Column` objects. I think there's copying happening somewhere in the `build_column`","createdAt":"2022-08-25T06:50:17Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11514#issuecomment-1226848515","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5K6pPu","author":{"login":"github-actions"},"authorAssociation":"NONE","body":"This issue has been labeled `inactive-30d` due to no recent activity in the past 30 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed. This issue will be labeled `inactive-90d` if there is no activity in the next 60 days.","createdAt":"2022-09-24T07:04:50Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11514#issuecomment-1256887278","viewerDidAuthor":false}],"createdAt":"2022-08-11T09:52:09Z","id":"I_kwDOBWUGps5PnY-9","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NjE=","name":"feature request","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwxMTM5NzQwNjY2","name":"libcudf","description":"Affects libcudf (C++/CUDA) code.","color":"c5def5"},{"id":"MDU6TGFiZWwxMTM5NzQxMjEz","name":"cuDF (Python)","description":"Affects Python cuDF API.","color":"1d76db"},{"id":"MDU6TGFiZWwxMTg1MjQ0MTQy","name":"cuIO","description":"cuIO issue","color":"fef2c0"}],"milestone":{"number":25,"title":"Helps libcudf C++ integrations","description":"","dueOn":null},"number":11514,"projectCards":[{"project":{"name":"Feature Planning"},"column":{"name":"Needs prioritizing"}}],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[FEA] Interchange protocol between processes on the same device.","updatedAt":"2023-04-02T22:40:32Z","url":"https://github.com/rapidsai/cudf/issues/11514"}
