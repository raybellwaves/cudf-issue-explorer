{"assignees":[],"author":{"id":"MDQ6VXNlcjE1MzgxNjU=","is_bot":false,"login":"vyasr","name":"Vyas Ramasubramani"},"body":"**Is your feature request related to a problem? Please describe.**\r\n`cuco::static_map` is currently slower than cudf's internal unordered map. As part of a general move towards relying on cuCollections for data structures, however, we have made the replacement in #9666.\r\n\r\n**Describe the solution you'd like**\r\n`cuco::static_multimap` was optimized as part of its incorporation into cudf in #8934, but because of the current structure of cuCollections these optimizations have not been ported to `cuco::static_map`. While addressing https://github.com/NVIDIA/cuCollections/issues/110 we should profile and make sure that the resulting optimizations are sufficient to close the performance gap.\r\n\r\n**Additional context**\r\nThe performance loss was deemed acceptable in order to progress faster towards #9695, which will rely on features that `cuco::static_map` offers that `cudf::concurrent_unordered_map` does not.\r\n","closed":false,"closedAt":null,"comments":[{"id":"IC_kwDOBWUGps48GHUG","author":{"login":"abellina"},"authorAssociation":"CONTRIBUTOR","body":"I am not entirely sure this is the same regression, but I am seeing one after (https://github.com/rapidsai/cudf/pull/9666) was merged. I see less invocations of this kernel, but they are ~10x slower than before the PR was merged. In one of the TPCDS @ 3TB query q87 (and possibly others) I see a loss of 4 seconds (before: 14263 ms, after: 18194 ms)\r\n\r\n```\r\nvoid thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__copy_if::CopyIfAgent<thrust::counting_iterator<int, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::cuda_cub::__copy_if::no_stencil_tag_*, int*, cudf::detail::left_semi_anti_join(cudf::detail::join_kind, cudf::table_view const&, cudf::table_view const&, cudf::null_equality, rmm::cuda_stream_view, rmm::mr::device_memory_resource*)::{lambda(int)#2}, int, int*>, thrust::counting_iterator<int, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::cuda_cub::__copy_if::no_stencil_tag_*, int*, {lambda(int)#2}, int, int*, cub::ScanTileState<int, true>, unsigned long>(thrust::counting_iterator<int, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::cuda_cub::__copy_if::no_stencil_tag_*, int*, {lambda(int)#2}, int, int*, cub::ScanTileState<int, true>, unsigned long)\r\n```\r\n\r\nDoes this seem to be the same issue as in this ticket?\r\n\r\nI also tried with an early version of the mixed condition PR and this kernel seems to be back to normal. That said, I don't see us invoking the mixed condition kernels. Perhaps there are general performance improvements here: https://github.com/rapidsai/cudf/pull/9917 that help. ","createdAt":"2022-01-09T05:54:42Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/9973#issuecomment-1008235782","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps48JGJd","author":{"login":"abellina"},"authorAssociation":"CONTRIBUTOR","body":"> I also tried with an early version of the mixed condition PR and this kernel seems to be back to normal. That said, I don't see us invoking the mixed condition kernels. Perhaps there are general performance improvements here: #9917 that help.\r\n\r\nI realize now that this also means the change (https://github.com/rapidsai/cudf/pull/9666) wasn't merged into that branch. So I don't think the mixed join change will address. \r\n\r\nI would still like to confirm that the place I saw the regression is expected of this issue.","createdAt":"2022-01-10T15:53:38Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/9973#issuecomment-1009017437","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps48JnTg","author":{"login":"vyasr"},"authorAssociation":"CONTRIBUTOR","body":"The slowdowns that I observed were more in the 1-3.5x range than 10x, so I'm surprised that you're seeing something so significant, but a regression is expected because of this issue. I'm especially surprised because I observed larger slowdowns for _smaller_ data sizes, whereas for larger data sizes the relative slowdown was much smaller (although in absolute terms a 4 second slowdown is of course possible). I wonder (but have no specific evidence to suggest this) whether #9912 may also have led to some slowdown in the specific algorithms used here that is exacerbating the issue.\r\n\r\nThe mixed condition PR is independent and will not change the perf here.","createdAt":"2022-01-10T17:22:58Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/9973#issuecomment-1009153248","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps48Jo_4","author":{"login":"abellina"},"authorAssociation":"CONTRIBUTOR","body":"Thanks @vyasr I don't think that PR is related as I used: https://github.com/rapidsai/cudf/commit/b1ae789e8542e39a8b6811624ee1dba4ff1b1915 as my baseline, and that was after the upgrade to thrust. I compared against: https://github.com/rapidsai/cudf/commit/33f7f0d032728494dcb9bb69eea96bfdc889084d.","createdAt":"2022-01-10T17:30:58Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/9973#issuecomment-1009160184","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps49ollm","author":{"login":"github-actions"},"authorAssociation":"NONE","body":"This issue has been labeled `inactive-30d` due to no recent activity in the past 30 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed. This issue will be labeled `inactive-90d` if there is no activity in the next 60 days.","createdAt":"2022-02-09T18:07:40Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/9973#issuecomment-1034049894","viewerDidAuthor":false}],"createdAt":"2022-01-05T17:31:58Z","id":"I_kwDOBWUGps5BPhV3","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NjE=","name":"feature request","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwxMTM5NzQwNjY2","name":"libcudf","description":"Affects libcudf (C++/CUDA) code.","color":"c5def5"}],"milestone":{"number":18,"title":"Refactor using cuco containers","description":"","dueOn":null},"number":9973,"projectCards":[{"project":{"name":"Feature Planning"},"column":{"name":"Needs prioritizing"}}],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[FEA] Address performance regression in semi/anti joins from switching to cuco","updatedAt":"2024-02-23T18:42:24Z","url":"https://github.com/rapidsai/cudf/issues/9973"}
