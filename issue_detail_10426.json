{"assignees":[],"author":{"id":"MDQ6VXNlcjY1MTk3NTQ=","is_bot":false,"login":"titericz","name":"Gilberto Titericz Junior"},"body":"**Describe the bug**\r\nWritting large DataFrames to disk fails and gets memory error:\r\n>>>df.to_parquet('myfile.parquet')\r\n>>>RuntimeError: CUDA error at: /home/giba/anaconda3/envs/rapids-22.02/include/rmm/cuda_stream_view.hpp:81: cudaErrorIllegalAddress an illegal memory access was encountered\r\n\r\n**Steps/Code to reproduce bug**\r\nMy DataFrame have shape: (414395052, 4)\r\ndtypes: var0 int32, var1 int32, var2 int8, var3 int8\r\ndf.memory_usage().sum() returns: 4143950520  (4GB)\r\n\r\n**Expected behavior**\r\nFile write to disk in .parquet or .csv format without issues.\r\n\r\n**Environment overview (please complete the following information)**\r\nUsed conda with default RAPIDS 22.02 install.\r\n\r\n**Environment details**\r\nUsing a 32GB V100 GPU.\r\n\r\n**Additional context**\r\nSending cudf Dataframe to Pandas then calling .to_parquet() works:\r\n>>> df.to_pandas().to_parquet('myfile.parquet')\r\n\r\n","closed":false,"closedAt":null,"comments":[{"id":"IC_kwDOBWUGps5AGaVV","author":{"login":"devavret"},"authorAssociation":"CONTRIBUTOR","body":"This should've succeeded using a 32 GB device. What was the GPU memory usage just before the `to_parquet` call? There could be other items present in the device and the actual available memory could've been lower.","createdAt":"2022-03-22T17:28:41Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/10426#issuecomment-1075422549","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5ApqH5","author":{"login":"charlesbluca"},"authorAssociation":"MEMBER","body":"I'm able to reproduce these failures on 22.06 nightlies on both V100 32GB and RTX 8000 48GB (though the latter is on WSL2 so could be related to that):\r\n\r\n```python\r\nimport cudf\r\nimport cupy as cp\r\n\r\ndf = cudf.DataFrame(cp.random.randint(0, 1_000_000, size=(400_000_000, 2)), columns=[\"src\", \"dst\"], dtype=\"int32\")\r\ndf.memory_usage().sum()  # 3.2 GB\r\n\r\ndf.to_csv(\"test.csv\")\r\n# MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /workspace/.conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:70: cudaErrorMemoryAllocation out of memory\r\n\r\ndf.to_parquet(\"test.parquet\")\r\n# RuntimeError: CUDA error at: /datasets/charlesb/miniconda3/envs/cudf-test/include/rmm/cuda_stream_view.hpp:81: cudaErrorIllegalAddress an illegal memory access was encountered\r\n```\r\n\r\nMemory usage doesn't seem very high on the GPU when the failure occurs:\r\n\r\n<details><summary>Click here to see environment details</summary><pre>\r\n\r\n     ***OS Information***\r\n     DGX_NAME=\"DGX Server\"\r\n     DGX_PRETTY_NAME=\"NVIDIA DGX Server\"\r\n     DGX_SWBUILD_DATE=\"2020-03-04\"\r\n     DGX_SWBUILD_VERSION=\"4.4.0\"\r\n     DGX_COMMIT_ID=\"ee09ebc\"\r\n     DGX_PLATFORM=\"DGX Server for DGX-1\"\r\n     DGX_SERIAL_NUMBER=\"QTFCOU8220028\"\r\n     DISTRIB_ID=Ubuntu\r\n     DISTRIB_RELEASE=18.04\r\n     DISTRIB_CODENAME=bionic\r\n     DISTRIB_DESCRIPTION=\"Ubuntu 18.04.4 LTS\"\r\n     NAME=\"Ubuntu\"\r\n     VERSION=\"18.04.4 LTS (Bionic Beaver)\"\r\n     ID=ubuntu\r\n     ID_LIKE=debian\r\n     PRETTY_NAME=\"Ubuntu 18.04.4 LTS\"\r\n     VERSION_ID=\"18.04\"\r\n     HOME_URL=\"https://www.ubuntu.com/\"\r\n     SUPPORT_URL=\"https://help.ubuntu.com/\"\r\n     BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\r\n     PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\r\n     VERSION_CODENAME=bionic\r\n     UBUNTU_CODENAME=bionic\r\n     Linux dgx12 4.15.0-1083-oracle #91-Ubuntu SMP Mon Oct 25 06:45:22 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n     ***GPU Information***\r\n     Thu Mar 31 07:25:54 2022\r\n     +-----------------------------------------------------------------------------+\r\n     | NVIDIA-SMI 495.44       Driver Version: 495.44       CUDA Version: 11.5     |\r\n     |-------------------------------+----------------------+----------------------+\r\n     | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n     | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n     |                               |                      |               MIG M. |\r\n     |===============================+======================+======================|\r\n     |   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\r\n     | N/A   33C    P0    55W / 300W |    328MiB / 32510MiB |      0%      Default |\r\n     |                               |                      |                  N/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |\r\n     | N/A   30C    P0    42W / 300W |      3MiB / 32510MiB |      0%      Default |\r\n     |                               |                      |                  N/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |\r\n     | N/A   28C    P0    41W / 300W |      3MiB / 32510MiB |      0%      Default |\r\n     |                               |                      |                  N/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |\r\n     | N/A   28C    P0    41W / 300W |      3MiB / 32510MiB |      0%      Default |\r\n     |                               |                      |                  N/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |\r\n     | N/A   30C    P0    42W / 300W |      3MiB / 32510MiB |      0%      Default |\r\n     |                               |                      |                  N/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |\r\n     | N/A   30C    P0    41W / 300W |      3MiB / 32510MiB |      0%      Default |\r\n     |                               |                      |                  N/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\r\n     | N/A   32C    P0    43W / 300W |      3MiB / 32510MiB |      0%      Default |\r\n     |                               |                      |                  N/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\r\n     | N/A   28C    P0    41W / 300W |      3MiB / 32510MiB |      0%      Default |\r\n     |                               |                      |                  N/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n\r\n     +-----------------------------------------------------------------------------+\r\n     | Processes:                                                                  |\r\n     |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n     |        ID   ID                                                   Usage      |\r\n     |=============================================================================|\r\n     |    0   N/A  N/A     65167      C   ....5/envs/rapids/bin/python      303MiB |\r\n     +-----------------------------------------------------------------------------+\r\n\r\n     ***CPU***\r\n     Architecture:        x86_64\r\n     CPU op-mode(s):      32-bit, 64-bit\r\n     Byte Order:          Little Endian\r\n     CPU(s):              80\r\n     On-line CPU(s) list: 0-79\r\n     Thread(s) per core:  2\r\n     Core(s) per socket:  20\r\n     Socket(s):           2\r\n     NUMA node(s):        2\r\n     Vendor ID:           GenuineIntel\r\n     CPU family:          6\r\n     Model:               79\r\n     Model name:          Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz\r\n     Stepping:            1\r\n     CPU MHz:             3337.814\r\n     CPU max MHz:         3600.0000\r\n     CPU min MHz:         1200.0000\r\n     BogoMIPS:            4390.10\r\n     Virtualization:      VT-x\r\n     L1d cache:           32K\r\n     L1i cache:           32K\r\n     L2 cache:            256K\r\n     L3 cache:            51200K\r\n     NUMA node0 CPU(s):   0-19,40-59\r\n     NUMA node1 CPU(s):   20-39,60-79\r\n     Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap intel_pt xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts md_clear flush_l1d\r\n\r\n     ***CMake***\r\n     /usr/bin/cmake\r\n     cmake version 3.10.2\r\n\r\n     CMake suite maintained and supported by Kitware (kitware.com/cmake).\r\n\r\n     ***g++***\r\n     /usr/bin/g++\r\n     g++ (Ubuntu 9.4.0-1ubuntu1~18.04) 9.4.0\r\n     Copyright (C) 2019 Free Software Foundation, Inc.\r\n     This is free software; see the source for copying conditions.  There is NO\r\n     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n     ***nvcc***\r\n     /usr/local/cuda-11.5/bin/nvcc\r\n     nvcc: NVIDIA (R) Cuda compiler driver\r\n     Copyright (c) 2005-2021 NVIDIA Corporation\r\n     Built on Thu_Nov_18_09:45:30_PST_2021\r\n     Cuda compilation tools, release 11.5, V11.5.119\r\n     Build cuda_11.5.r11.5/compiler.30672275_0\r\n\r\n     ***Python***\r\n     /datasets/charlesb/miniconda3/envs/cudf-test/bin/python\r\n     Python 3.9.12\r\n\r\n     ***Environment Variables***\r\n     PATH                            : /datasets/charlesb/miniconda3/envs/cudf-test/bin:/datasets/charlesb/miniconda3/condabin:/home/nfs/charlesb/bin:/usr/local/cuda-11.5/bin:/usr/local/cuda/bin:/opt/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\r\n     LD_LIBRARY_PATH                 : /usr/local/cuda-11.5/lib64\r\n     NUMBAPRO_NVVM                   :\r\n     NUMBAPRO_LIBDEVICE              :\r\n     CONDA_PREFIX                    : /datasets/charlesb/miniconda3/envs/cudf-test\r\n     PYTHON_PATH                     :\r\n\r\n     ***conda packages***\r\n     conda is /datasets/charlesb/miniconda3/condabin/conda\r\n     /datasets/charlesb/miniconda3/condabin/conda\r\n     # packages in environment at /datasets/charlesb/miniconda3/envs/cudf-test:\r\n     #\r\n     # Name                    Version                   Build  Channel\r\n     _libgcc_mutex             0.1                 conda_forge    conda-forge\r\n     _openmp_mutex             4.5                       1_gnu    conda-forge\r\n     abseil-cpp                20211102.0           h27087fc_1    conda-forge\r\n     arrow-cpp                 6.0.1           py39h653df1f_13_cuda    conda-forge\r\n     arrow-cpp-proc            3.0.0                      cuda    conda-forge\r\n     asttokens                 2.0.5              pyhd8ed1ab_0    conda-forge\r\n     aws-c-cal                 0.5.11               h95a6274_0    conda-forge\r\n     aws-c-common              0.6.2                h7f98852_0    conda-forge\r\n     aws-c-event-stream        0.2.7               h3541f99_13    conda-forge\r\n     aws-c-io                  0.10.5               hfb6a706_0    conda-forge\r\n     aws-checksums             0.1.11               ha31a3da_7    conda-forge\r\n     aws-sdk-cpp               1.8.186              hb4091e7_3    conda-forge\r\n     backcall                  0.2.0              pyh9f0ad1d_0    conda-forge\r\n     backports                 1.0                        py_2    conda-forge\r\n     backports.functools_lru_cache 1.6.4              pyhd8ed1ab_0    conda-forge\r\n     bzip2                     1.0.8                h7f98852_4    conda-forge\r\n     c-ares                    1.18.1               h7f98852_0    conda-forge\r\n     ca-certificates           2021.10.8            ha878542_0    conda-forge\r\n     cachetools                5.0.0              pyhd8ed1ab_0    conda-forge\r\n     cuda-python               11.6.1           py39h3fd9d12_0    nvidia\r\n     cudatoolkit               11.5.0               h36ae40a_9    nvidia\r\n     cudf                      22.06.00a220331 cuda_11_py39_gbc8f57843d_78    rapidsai-nightly\r\n     cupy                      10.2.0           py39hc3c280e_0    conda-forge\r\n     decorator                 5.1.1              pyhd8ed1ab_0    conda-forge\r\n     dlpack                    0.5                  h9c3ff4c_0    conda-forge\r\n     executing                 0.8.3              pyhd8ed1ab_0    conda-forge\r\n     fastavro                  1.4.10           py39hb9d737c_0    conda-forge\r\n     fastrlock                 0.8              py39he80948d_1    conda-forge\r\n     fsspec                    2022.2.0           pyhd8ed1ab_0    conda-forge\r\n     gflags                    2.2.2             he1b5a44_1004    conda-forge\r\n     glog                      0.5.0                h48cff8f_0    conda-forge\r\n     grpc-cpp                  1.45.0               h3d78c48_0    conda-forge\r\n     ipython                   8.2.0            py39hf3d152e_0    conda-forge\r\n     jedi                      0.18.1           py39hf3d152e_0    conda-forge\r\n     keyutils                  1.6.1                h166bdaf_0    conda-forge\r\n     krb5                      1.19.3               h3790be6_0    conda-forge\r\n     ld_impl_linux-64          2.36.1               hea4e1c9_2    conda-forge\r\n     libblas                   3.9.0           13_linux64_openblas    conda-forge\r\n     libbrotlicommon           1.0.9                h7f98852_6    conda-forge\r\n     libbrotlidec              1.0.9                h7f98852_6    conda-forge\r\n     libbrotlienc              1.0.9                h7f98852_6    conda-forge\r\n     libcblas                  3.9.0           13_linux64_openblas    conda-forge\r\n     libcudf                   22.06.00a220331 cuda11_gbc8f57843d_78    rapidsai-nightly\r\n     libcurl                   7.82.0               h7bff187_0    conda-forge\r\n     libedit                   3.1.20191231         he28a2e2_2    conda-forge\r\n     libev                     4.33                 h516909a_1    conda-forge\r\n     libevent                  2.1.10               h9b69904_4    conda-forge\r\n     libffi                    3.4.2                h7f98852_5    conda-forge\r\n     libgcc-ng                 11.2.0              h1d223b6_14    conda-forge\r\n     libgfortran-ng            11.2.0              h69a702a_14    conda-forge\r\n     libgfortran5              11.2.0              h5c6108e_14    conda-forge\r\n     libgomp                   11.2.0              h1d223b6_14    conda-forge\r\n     liblapack                 3.9.0           13_linux64_openblas    conda-forge\r\n     libllvm11                 11.1.0               hf817b99_3    conda-forge\r\n     libnghttp2                1.47.0               h727a467_0    conda-forge\r\n     libnsl                    2.0.0                h7f98852_0    conda-forge\r\n     libopenblas               0.3.18          pthreads_h8fe5266_0    conda-forge\r\n     libprotobuf               3.19.4               h780b84a_0    conda-forge\r\n     librmm                    22.06.00a220331 cuda11_g41a2461_19    rapidsai-nightly\r\n     libssh2                   1.10.0               ha56f1ee_2    conda-forge\r\n     libstdcxx-ng              11.2.0              he4da1e4_14    conda-forge\r\n     libthrift                 0.16.0               h519c5ea_1    conda-forge\r\n     libutf8proc               2.7.0                h7f98852_0    conda-forge\r\n     libuuid                   2.32.1            h7f98852_1000    conda-forge\r\n     libzlib                   1.2.11            h166bdaf_1014    conda-forge\r\n     llvmlite                  0.38.0           py39h1bbdace_0    conda-forge\r\n     lz4-c                     1.9.3                h9c3ff4c_1    conda-forge\r\n     matplotlib-inline         0.1.3              pyhd8ed1ab_0    conda-forge\r\n     ncurses                   6.3                  h9c3ff4c_0    conda-forge\r\n     numba                     0.55.1           py39h56b8d98_0    conda-forge\r\n     numpy                     1.21.5           py39haac66dc_0    conda-forge\r\n     nvtx                      0.2.3            py39h3811e60_1    conda-forge\r\n     openssl                   1.1.1n               h166bdaf_0    conda-forge\r\n     orc                       1.7.3                h1be678f_0    conda-forge\r\n     packaging                 21.3               pyhd8ed1ab_0    conda-forge\r\n     pandas                    1.3.5            py39hde0f152_0    conda-forge\r\n     parquet-cpp               1.5.1                         2    conda-forge\r\n     parso                     0.8.3              pyhd8ed1ab_0    conda-forge\r\n     pexpect                   4.8.0              pyh9f0ad1d_2    conda-forge\r\n     pickleshare               0.7.5                   py_1003    conda-forge\r\n     pip                       22.0.4             pyhd8ed1ab_0    conda-forge\r\n     prompt-toolkit            3.0.27             pyha770c72_0    conda-forge\r\n     protobuf                  3.19.4           py39he80948d_0    conda-forge\r\n     ptxcompiler               0.3.0           cuda_11_py39_geed289a_9    rapidsai-nightly\r\n     ptyprocess                0.7.0              pyhd3deb0d_0    conda-forge\r\n     pure_eval                 0.2.2              pyhd8ed1ab_0    conda-forge\r\n     pyarrow                   6.0.1           py39h1ed2e5d_13_cuda    conda-forge\r\n     pygments                  2.11.2             pyhd8ed1ab_0    conda-forge\r\n     pyparsing                 3.0.7              pyhd8ed1ab_0    conda-forge\r\n     python                    3.9.12          h9a8a25e_1_cpython    conda-forge\r\n     python-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge\r\n     python_abi                3.9                      2_cp39    conda-forge\r\n     pytz                      2022.1             pyhd8ed1ab_0    conda-forge\r\n     re2                       2022.02.01           h9c3ff4c_0    conda-forge\r\n     readline                  8.1                  h46c0cb4_0    conda-forge\r\n     rmm                       22.06.00a220331 cuda11_py39_g41a2461_19    rapidsai-nightly\r\n     s2n                       1.0.10               h9b69904_0    conda-forge\r\n     setuptools                59.8.0           py39hf3d152e_1    conda-forge\r\n     six                       1.16.0             pyh6c4a22f_0    conda-forge\r\n     snappy                    1.1.8                he1b5a44_3    conda-forge\r\n     spdlog                    1.8.5                h4bd325d_1    conda-forge\r\n     sqlite                    3.37.1               h4ff8645_0    conda-forge\r\n     stack_data                0.2.0              pyhd8ed1ab_0    conda-forge\r\n     tk                        8.6.12               h27826a3_0    conda-forge\r\n     traitlets                 5.1.1              pyhd8ed1ab_0    conda-forge\r\n     typing_extensions         4.1.1              pyha770c72_0    conda-forge\r\n     tzdata                    2022a                h191b570_0    conda-forge\r\n     wcwidth                   0.2.5              pyh9f0ad1d_2    conda-forge\r\n     wheel                     0.37.1             pyhd8ed1ab_0    conda-forge\r\n     xz                        5.2.5                h516909a_1    conda-forge\r\n     zlib                      1.2.11            h166bdaf_1014    conda-forge\r\n     zstd                      1.5.2                ha95c52a_0    conda-forge\r\n\r\n</pre></details>\r\n","createdAt":"2022-03-31T14:27:17Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/10426#issuecomment-1084662265","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5AqcrN","author":{"login":"titericz"},"authorAssociation":"NONE","body":"> I'm able to reproduce these failures on 22.06 nightlies on both V100 32GB and RTX 8000 48GB (though the latter is on WSL2 so could be related to that):\r\n> \r\n> ```python\r\n> import cudf\r\n> import cupy as cp\r\n> \r\n> df = cudf.DataFrame(cp.random.randint(0, 1_000_000, size=(400_000_000, 2)), columns=[\"src\", \"dst\"], dtype=\"int32\")\r\n> df.memory_usage().sum()  # 3.2 GB\r\n> \r\n> df.to_csv(\"test.csv\")\r\n> # MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /workspace/.conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:70: cudaErrorMemoryAllocation out of memory\r\n> \r\n> df.to_parquet(\"test.parquet\")\r\n> # RuntimeError: CUDA error at: /datasets/charlesb/miniconda3/envs/cudf-test/include/rmm/cuda_stream_view.hpp:81: cudaErrorIllegalAddress an illegal memory access was encountered\r\n> ```\r\n> \r\n> Memory usage doesn't seem very high on the GPU when the failure occurs:\r\n> \r\n> Click here to see environment details\r\n\r\nThats similar to what I observed. I have 4GB GPU memory utilization before the .to_parquet() call.","createdAt":"2022-03-31T17:11:08Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/10426#issuecomment-1084869325","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5AuQwj","author":{"login":"devavret"},"authorAssociation":"CONTRIBUTOR","body":"I have a 12 GB titan V. Couldn't create a df of size `400_000_000, 2` so I tested with `260_000_000, 2` which was the max size that worked. Interestingly, the peak memory consumption during dataframe creation (9.69 GB) was more than the peak during parquet writing (5.82 GB)\r\n\r\nDoes csv writing always precede the parquet writing. Because that seemed to consume more temp memory and once we have a memory error, it'll stick during subsequent calls.","createdAt":"2022-04-01T13:00:04Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/10426#issuecomment-1085869091","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5AuSGN","author":{"login":"charlesbluca"},"authorAssociation":"MEMBER","body":"> Does csv writing always precede the parquet writing\r\n\r\nSorry if I wrote the reproducer out in a confusing way - I typically ran either one of these `to_*` commands alone when I encountered the failures, just consolidated them in one code block to cut down on duplication.\r\n\r\nThough I did note that the `to_csv` call had a smaller limit before running into OOM issues (in comparison to `to_parquet`):\r\n\r\n```python\r\nimport cudf\r\nimport cupy as cp\r\n\r\ndf = cudf.DataFrame(cp.random.randint(0, 1_000_000, size=(200_000_000, 2)), columns=[\"src\", \"dst\"], dtype=\"int32\")\r\ndf.memory_usage().sum()  # 1.6 GB\r\n\r\ndf.to_csv(\"test.csv\")  # also fails\r\n```","createdAt":"2022-04-01T13:04:10Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/10426#issuecomment-1085874573","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5CadYu","author":{"login":"github-actions"},"authorAssociation":"NONE","body":"This issue has been labeled `inactive-30d` due to no recent activity in the past 30 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed. This issue will be labeled `inactive-90d` if there is no activity in the next 60 days.","createdAt":"2022-05-01T13:08:39Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/10426#issuecomment-1114232366","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5K8_Ik","author":{"login":"github-actions"},"authorAssociation":"NONE","body":"This issue has been labeled `inactive-90d` due to no recent activity in the past 90 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed.","createdAt":"2022-09-26T05:30:49Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/10426#issuecomment-1257501220","viewerDidAuthor":false}],"createdAt":"2022-03-13T14:00:51Z","id":"I_kwDOBWUGps5Fl_-D","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NTk=","name":"bug","description":"Something isn't working","color":"d73a4a"},{"id":"MDU6TGFiZWwxMTM5NzQwNjY2","name":"libcudf","description":"Affects libcudf (C++/CUDA) code.","color":"c5def5"},{"id":"MDU6TGFiZWwxMTg1MjQ0MTQy","name":"cuIO","description":"cuIO issue","color":"fef2c0"},{"id":"MDU6TGFiZWwxMzIyMjUyNjE3","name":"Performance","description":"Performance related issue","color":"C2E0C6"}],"milestone":{"number":20,"title":"Stabilizing large workflows (OOM, spilling, partitioning)","description":"","dueOn":null},"number":10426,"projectCards":[{"project":{"name":"Bug Squashing"},"column":{"name":"Needs prioritizing"}}],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[BUG] .to_parquet() and .to_csv() fails and get OOM with large DataFrames.","updatedAt":"2024-02-23T18:42:52Z","url":"https://github.com/rapidsai/cudf/issues/10426"}
