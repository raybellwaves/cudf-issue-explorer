{"assignees":[],"author":{"id":"MDQ6VXNlcjEyNzI1MTEx","is_bot":false,"login":"GregoryKimball","name":"Gregory Kimball"},"body":"In libcudf, strings columns have child columns containing character data and offsets, and the offsets child column uses a 32-bit signed size type. This limits strings columns to containing ~2.1 billion characters. For LLM training, documents have up to 1M characters, and a median around 3K characters. Due to the size type limit, LLM training pipelines have to carefully batch the data down to a few thousand rows to stay comfortably within the size type limit. We have a general issue open to explore a 64-bit size type in libcudf (#13159). For size issues with LLM training pipelines, we should consider a targeted change to only address the size limit for strings columns.\r\n\r\n### Requirements\r\n* We must maintain or improve throughput for functions processing strings columns with <2.1 billion characters. This requirement prevents us from using 64-bit offsets for all strings columns. It does not prevent us from using 64-bit offsets for strings columns with >2.1 billion characters.\r\n* We must not introduce a new data type or otherwise increase compile times significantly. This requirement prevents us from dispatching between \"strings\" types and \"large strings\" types. \r\n\r\n### Proposed solution\r\nOne idea that satisfies these requirements would be to represent the character data as an `int64` typed column instead of an `int8` typed column. This would allow us to store 8x more bytes of character data. To access the character bytes, we would use an offset-normalizing iterator (inspired by [\"indexalator\"](https://github.com/rapidsai/cudf/blob/branch-23.08/cpp/include/cudf/detail/indexalator.cuh)) to identify byte positions using an `int64` iterator output. Please note that the row count 32-bit size type would still apply to the proposed \"large strings\" columns.\r\n\r\nWe should also consider an \"unbounded\" character data allocation that is not typed, but rather a single buffer up to 2^64 bytes in size. The 64-bit offset type would be able to index into much larger allocations.\r\n\r\nPlease note that this solution will not impact the offsets for list columns. We believe that the best design to allow for more than 2.1B elements in lists will be to use 64-bit size type in libcudf as discussed in #13159.\r\n\r\n### Creating strings columns\r\nStrings columns factories would choose child column types at the time of column creation, based on the size of the character data. This change would impact strings column factories, as well as algorithms that use strings column utilities or generate their own offsets buffers. At column creation time, the constructor will choose between `int32` offsets with `int8` character data and `int64` offsets with `int64` character data, based on the size of the character data. Any function that calls [make_offsets_child_column](https://github.com/rapidsai/cudf/blob/9e099cef25b11821c6307bb9c231656a2bae700f/cpp/include/cudf/detail/sizes_to_offsets_iterator.cuh#L298-L302) will need to be aware of the alternate child column types for large strings.\r\n\r\n### Accessing strings data\r\nThe offset-normalizing iterator would always return `int64` type so that strings column consumers would not need to support both `int32` and `int64` offset types. See [cudf::detail::sizes_to_offsets_iterator](https://github.com/rapidsai/cudf/pull/12180) for an example of how an iterator operating on `int32` data can output `int64` data.\r\n\r\n### Interoperability with Arrow\r\nThe new strings column variant with `int64` offsets with `int64` character data may already be Arrow-compatible. This requires more testing and some changes to our Arrow interop utilities.\r\n\r\n### Part 1: libcudf changes to support large strings columns\r\n\r\nDefinitions:\r\n\"strings column\": `int8` character data and `int32` offset data (2.1B characters) \r\n\"large strings column\": `int8` character data up to 2^64 bytes and `int64` offset data (18400T characters)\r\n\r\n| Step | PR | Notes | \r\n|---|---|---|\r\n| Replace `offset_type` references with `size_type` | ‚úÖ #13788 | offsets generated by the offset-normalizing iterator will have type `int64_t` | \r\n| <s> Add new data-size member to `cudf::column_view`, `cudf::mutable_column_view` and `cudf::column_device_view` </s> | ‚ùå #14031 | solution for character counts greater than `int32` | \r\n| Create an offset-normalizing iterator over character data that always outputs 64-bit offsets| ‚úÖ #14206 <br> ‚úÖ #14234 | First step in #14043 |\r\n| * Add the character data buffer to the parent strings column, rather than as a child column <br> * Also refactor algorithms such as concat, contiguous split and gather which access character data <br> * Update code in cuDF-python that interact with character child columns <br> * Update code in cudf-java that interact with character child columns | ‚úÖ #14202 | See performance blocker resolved in ‚úÖ #14540 |\r\n| Deprecate unneeded factories and use strings column factories consistently | ‚úÖ | #14461, #14771, #14695, #14612, +one more | \r\n| Introduce an environment variable to control the threshold for converting to 64-bit indices, to enable testing on smaller strings columns | ‚úÖ `LIBCUDF_LARGE_STRINGS_THRESHOLD` added | part of #14612 | \r\n| Transition strings APIs to use the offset-normalizing iterator (\"offsetalator\") | ‚úÖ | See #14611, #14700, #14744, #14745, #14757, #14783, #14824  | \r\n| Remove references to `strings_column_view::offsets_begin()` in libcudf since it hardcodes the return type as int32. | ‚úÖ | See #15112 #15077  | \r\n| Remove references to `create_chars_child_column` in libcudf since it wraps a column around chars data. | ‚úÖ | #15241 | \r\n| Change the current `make_strings_children` to return a uvector for chars instead of a column | ‚úÖ | See #15171  | \r\n| Introduce an environment variable `LIBCUDF_LARGE_STRINGS_ENABLED` to let users force libcudf to throw rather than start using 64-bit offsets, to allow try-catch-repartitioning instead | ‚úÖ |  #15195  | \r\n| Introduce an environment variable `LIBCUDF_LARGE_STRINGS_THRESHOLD` | ‚úÖ |  #14612 | \r\n| Rework `concatenate` to produce large strings when `LIBCUDF_LARGE_STRINGS_ENABLED` and character count is above the `LIBCUDF_LARGE_STRINGS_THRESHOLD` | ‚úÖ |  See #15195  |\r\n| cuDF-python testing. use concat to create a large string column. We should be able to operate on this column, as long as we aren't creating a large string. Can we: (1) returns int/bool, like, contains, (2) slice (3) returns smaller strings. | üîÑ | |\r\n| Add an `experimental` version of `make_strings_children` that generates 64-bit offsets when the total character length exceeds the threshold | ‚úÖ |#15363 | \r\n| Add a large strings test fixture that stores large columns between unit tests and controls the environment variables | ‚úÖ | #15513  | \r\n| Check appropriate cudf tests pass with `LIBCUDF_LARGE_STRINGS_THRESHOLD` at zero  | | |\r\n| benchmark regressions analyzed and approved | | |\r\n| Spark-RAPIDS tests pass | | |\r\n| Remove `experimental` namespace. Replace `make_strings_children` with implementation with the `experimental` namespace version. | ‚úÖ | #15702  | \r\n| Live session with cuDF-python expert to start producing and operating on large strings | | |\r\n| Ensure that we can interop strings columns with 64-bit offsets to arrow as LARGE_STRING type | | Also see #15093 about `large_strings` compatibility for pandas-2.2 |\r\n\r\n### Part 2: cuIO changes to read and write large strings columns\r\n\r\n| Step | PR | Notes |\r\n|---|---|---| \r\n| Add functionality to JSON reader to construct large string columns | | Could require building a chunked JSON reader |\r\n| Add functionality to Parquet reader to construct large string columns | | | \r\n| to be continued... | | | ","closed":false,"closedAt":null,"comments":[{"id":"IC_kwDOBWUGps5m-HnJ","author":{"login":"harrism"},"authorAssociation":"MEMBER","body":"**üëè praise:** ‚Äè Great descriptive issue!\r\n**üí° suggestion:** ‚Äè I think the table in the description could be a GH task list so each \"step\" can be made into an issue.\r\n**‚ùì question:** ‚Äè \r\n> \"large strings column\": int64 character data and int64 offset data\r\n\r\nint64 data with int64 offsets sounds larger than:\r\n\r\n> \"unbounded strings column\": int8 character data up to 2^64 bytes and int64 offset data\r\n\r\nAnd this sounds bounded, but large. Not boundless.\r\n\r\nCan you clarify or give an example of the structure?\r\n","createdAt":"2023-09-20T11:43:15Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"LAUGH","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/13733#issuecomment-1727560137","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5n6n0c","author":{"login":"gaohao95"},"authorAssociation":"CONTRIBUTOR","body":"This feature would be a great help for us. We use `cudf::table` as a CPU-memory storage to enable zero-copy. In theory, CPU memory has more capacity to hold larger tables, but we constantly run into the character limits.","createdAt":"2023-10-02T17:06:09Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/13733#issuecomment-1743420700","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5n-9Wv","author":{"login":"LutzCle"},"authorAssociation":"NONE","body":"Following up on @gaohao95's comment, our use case is storing TPC-H data in a `cudf::table`. At scale factor 100, the `l_comment` and `ps_comment` string columns overflow the 32-bit offset.","createdAt":"2023-10-03T09:12:50Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/13733#issuecomment-1744557487","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps56zYq7","author":{"login":"GregoryKimball"},"authorAssociation":"CONTRIBUTOR","body":"Now that #15195 is merged, I did some testing via cuDF-python\r\n```\r\n>>> df2['char'].str.slice(0,2)  \r\nOK\r\n\r\n>>> df2['char'].str.contains(0,2)  \r\nOK\r\n\r\n>>> df2['char'].str.contains('p', regex=True)\r\nOK\r\n\r\n>>> df2['char'].nunique()\r\n57\r\n\r\n# add a column, df2['c'] = 1\r\n>>> df2.groupby('char').sum()['c'].reset_index()\r\nOK\r\n\r\n>>> df2['char'] + 'b'\r\nOverflowError: CUDF failure at: /nfs/repo/cudf24.06/cpp/include/cudf/strings/\r\ndetail/strings_children.cuh:82: Size of output exceeds the column size limit\r\n\r\n>>> df2['char'].str.upper()\r\nRuntimeError: THRUST_INDEX_TYPE_DISPATCH 64-bit count is unsupported in libcudf\r\n```\r\n\r\nSo far so good! Many APIs can successfully consume large strings, and only concat can produce them for now. üéâ ","createdAt":"2024-04-17T03:45:41Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/13733#issuecomment-2060290747","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps579Ql6","author":{"login":"beckernick"},"authorAssociation":"MEMBER","body":"This is incredibly exciting! More than any individual string operation, one of the most common pain points I see in workflows is the inability to bring strings along as a payload during joins (now that concat works):\r\n\r\n```python\r\n%env LIBCUDF_LARGE_STRINGS_ENABLED=1\r\n\r\nimport cudf\r\nimport numpy as np\r\n\r\nN = 6000\r\n\r\ndf1 = cudf.DataFrame({\r\n    \"val\": [\"this is a fairly short string\", \"this one is a bit longer, but not much\"]*N,\r\n    \"key\": [0, 1]*N\r\n})\r\n\r\nres = df1.merge(df1, on=\"key\")\r\nprint(f\"{res.val_x.str.len().sum():,} characters in string column\")\r\n---------------------------------------------------------------------------\r\nOverflowError                             Traceback (most recent call last)\r\nCell In[11], line 13\r\n      6 N = 6000\r\n      8 df1 = cudf.DataFrame({\r\n      9     \"val\": [\"this is a fairly short string\", \"this one is a bit longer, but not much\"]*N,\r\n     10     \"key\": [0, 1]*N\r\n     11 })\r\n---> 13 res = df1.merge(df1, on=\"key\")\r\n     14 print(f\"{res.val_x.str.len().sum():,} characters in string column\")\r\n\r\nFile [/nvme/0/nicholasb/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/nvtx/nvtx.py:116]\r\n...\r\nFile copying.pyx:151, in cudf._lib.copying.gather()\r\n\r\nFile copying.pyx:34, in cudf._lib.pylibcudf.copying.gather()\r\n\r\nFile copying.pyx:66, in cudf._lib.pylibcudf.copying.gather()\r\n\r\nOverflowError: CUDF failure at: /opt/conda/conda-bld/work/cpp/include/cudf/detail/sizes_to_offsets_iterator.cuh:323: Size of output exceeds the column size limit\r\n```\r\n\r\nIf I only have numeric data, this works smoothly as the output dataframe is only 72M rows.\r\n\r\n```python\r\n%env LIBCUDF_LARGE_STRINGS_ENABLED=1\r\n\r\nimport cudf\r\nimport numpy as np\r\n\r\nN = 6000\r\n\r\ndf1 = cudf.DataFrame({\r\n    \"val\": [10, 100]*N,\r\n    \"key\": [0, 1]*N\r\n})\r\n\r\nres = df1.merge(df1, on=\"key\")\r\nprint(f\"{len(res):,} rows in dataframe\")\r\n72,000,000 rows in dataframe\r\n```\r\n\r\nI'd love to be able to complete this (contrived) example, because I think it's representative of something we see often: this limit causing failures in workflows where users _expect_ things to work smoothly.\r\n\r\nAs a reference, the self-join works with `N=5000` as it only ends up with 1.7B total characters.\r\n\r\n```python\r\n%env LIBCUDF_LARGE_STRINGS_ENABLED=1\r\n\r\nimport cudf\r\nimport numpy as np\r\n\r\nN = 5000\r\n\r\ndf1 = cudf.DataFrame({\r\n    \"val\": [\"this is a fairly short string\", \"this one is a bit longer, but not much\"]*N,\r\n    \"key\": [0, 1]*N\r\n})\r\n\r\nres = df1.merge(df1, on=\"key\")\r\nprint(f\"{res.val_x.str.len().sum():,} characters in string column\")\r\nenv: LIBCUDF_LARGE_STRINGS_ENABLED=1\r\n1,675,000,000 characters in string column\r\n```","createdAt":"2024-04-26T15:51:01Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/13733#issuecomment-2079656314","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps59Q8-W","author":{"login":"davidwendt"},"authorAssociation":"CONTRIBUTOR","body":"The issue described here https://github.com/rapidsai/cudf/issues/13733#issuecomment-2079656314 should be fixed with https://github.com/rapidsai/cudf/pull/15621","createdAt":"2024-05-08T22:19:43Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/13733#issuecomment-2101596054","viewerDidAuthor":false}],"createdAt":"2023-07-22T20:58:01Z","id":"I_kwDOBWUGps5sS5RD","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NjE=","name":"feature request","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwxMDEzOTg3MzUy","name":"0 - Backlog","description":"In queue waiting for assignment","color":"d4c5f9"},{"id":"MDU6TGFiZWwxMTM5NzQwNjY2","name":"libcudf","description":"Affects libcudf (C++/CUDA) code.","color":"c5def5"},{"id":"MDU6TGFiZWwxNTE1NjE2MjUz","name":"strings","description":"strings issues (C++ and Python)","color":"0e8a16"}],"milestone":{"number":30,"title":"Language model acceleration","description":"","dueOn":null},"number":13733,"projectCards":[],"projectItems":[],"reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":2}}],"state":"OPEN","title":"[FEA] Increase maximum characters in strings columns","updatedAt":"2024-05-13T12:44:56Z","url":"https://github.com/rapidsai/cudf/issues/13733"}
