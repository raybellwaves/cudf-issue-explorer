{"assignees":[{"id":"MDQ6VXNlcjU2MDczMzA=","login":"mythrocks","name":"MithunR"}],"author":{"id":"MDQ6VXNlcjEzNjA3NjY=","is_bot":false,"login":"jlowe","name":"Jason Lowe"},"body":"**Is your feature request related to a problem? Please describe.**\r\nA customer has a query that performs many string find/contains operations, often on long strings.  Nsight traces show most of the GPU time is being spent in finder_warp_parallel_fn or contains_warp_parallel_fn, significantly more than Parquet decompress and decode which are typically the top GPU kernels.\r\n\r\n**Describe the solution you'd like**\r\nImproved performance for these kernels.\r\n","closed":false,"closedAt":null,"comments":[{"id":"IC_kwDOBWUGps54pSwB","author":{"login":"mythrocks"},"authorAssociation":"CONTRIBUTOR","body":"This is definitely more in @davidwendt's wheelhouse than in mine. I'm trying to familiarize myself with `finder_warp_parallell_fn<>`.","createdAt":"2024-03-27T22:24:27Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/15405#issuecomment-2024090625","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps55BAnD","author":{"login":"GregoryKimball"},"authorAssociation":"CONTRIBUTOR","body":"Perhaps we need to continue the long strings improvements for `find` that were started in #13226 (also see story issue #13048). Would you please share any information you might have on the distribution of string lengths?","createdAt":"2024-04-01T18:30:13Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/15405#issuecomment-2030307779","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps55BJmk","author":{"login":"mythrocks"},"authorAssociation":"CONTRIBUTOR","body":"I'll explore the data and post more info here.\r\n\r\nI'm still looking at the call stack, etc.  There are certainly wins to be had by switching the query from using `strings::find()` to using `strings::contains()`.\r\n\r\nThe user query is of the form:\r\n```sql\r\nCASE WHEN instr(lower(my_str), 'my_sub_str') > 0 THEN ...\r\n```\r\n\r\nI'd like to check the feasibility of translating `instr(..., ...) > 0` to use `strings::contains()` in the Spark Plugin.\r\n","createdAt":"2024-04-01T18:47:39Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/15405#issuecomment-2030344612","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps55Dn9i","author":{"login":"mythrocks"},"authorAssociation":"CONTRIBUTOR","body":"@jlowe mentioned this:  @revans2 already has a fix/workaround in place in Spark-RAPIDS, to detect queries of the from `instr(...,...) > 0`, to convert them into a call to `strings::contains()`.  It appears that this isn't performing much faster than `strings::find()` does. :/","createdAt":"2024-04-02T03:07:46Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/15405#issuecomment-2030993250","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps55EDbn","author":{"login":"mythrocks"},"authorAssociation":"CONTRIBUTOR","body":"I have done some exploration of the data in question, and the query.\r\n\r\n1. The query has about 25 invocations of `instr`, which amounts to 25 calls to `strings::contains()` per Spark input-row batch.\r\n2. Exploring the input column for `strings::contains()`: Out of a sampling of 102M rows, 99.98% of them were `\"_\"`, i.e. nominally blank.\r\n3. The input column had only a handful of unique values (in my sample). Here's a breakdown of their counts and lengths. (I have redacted the actual values.)\r\n```\r\n+--------------------+-----------------------+---------+\r\n|          input_str | char_length(input_str)|   count |\r\n+--------------------+-----------------------+---------+\r\n|                  _ |                      1|102542310|\r\n| AAAAAAAAAAAAAAAAAA |                     54|    13611|\r\n| BBBBBBBBBBBBBBBBBB |                     22|     1404|\r\n| XXXXXXXXXXXXXXXXXX |                     17|      702|\r\n| YYYYYYYYYYYYYYYYYY |                    149|      390|\r\n| ZZZZZZZZZZZZZZZZZZ |                    104|       39|\r\n+--------------------+-----------------------+---------+\r\n```\r\n4. The column seems to have only ASCII characters.\r\n5. The average string length is `1.008`, vastly skewed by the 99% \"_\" value.  Ignoring that row, the average length is `52.02`.\r\n\r\nI'm not sure why/how `finder_warp_parallel_fn` or `contains_warp_parallel_fn` kernels landed up in the slow path, given that the average string length seems not to exceed the warp-parallelism threshold (`64`).\r\n\r\nI *think* @revans2's changes have seen to it that `strings::contains` is called correctly.  Thereafter, it *should* call the string-per-thread version.\r\n\r\nI haven't run a profile on the sample yet. I'll try get that going tomorrow.","createdAt":"2024-04-02T05:23:57Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/15405#issuecomment-2031105767","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps55N5kH","author":{"login":"mythrocks"},"authorAssociation":"CONTRIBUTOR","body":"I'm working on a block-parallel version of `contains()` that looks a lot like the warp-parallel one. \r\nTesting it out now.","createdAt":"2024-04-03T06:49:32Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/15405#issuecomment-2033686791","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps55N6Bg","author":{"login":"mythrocks"},"authorAssociation":"CONTRIBUTOR","body":"As an aside, I should mention that the data distributions I mentioned above can be ignored, for the moment.  The sample is not representative of the user's data. ","createdAt":"2024-04-03T06:50:57Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/15405#issuecomment-2033688672","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps55V4Jo","author":{"login":"mythrocks"},"authorAssociation":"CONTRIBUTOR","body":"I have a naive block-parallel implementation [here](https://github.com/mythrocks/cudf/blob/block-parallel-contains/cpp/src/strings/search/find.cu#L378).\r\n\r\nThis change switches to block-parallel if the average string length reaches 256 or 512. (I've tried both.)\r\n\r\nHere are some results from running `STRING_BENCH` with and without block-parallel\r\n[STRINGS_BENCH.block-parallel.256.log](https://github.com/rapidsai/cudf/files/14859363/STRINGS_BENCH.block-parallel.256.log)\r\n[STRINGS_BENCH.warp-parallel.log](https://github.com/rapidsai/cudf/files/14859364/STRINGS_BENCH.warp-parallel.log)\r\n\r\nIt appears that benefits aren't apparent unless the average string sizes reach around 4K-8K. And that gets slightly worse at higher row counts:\r\n| Row Count | String Size | Warp Parallel Time (ms) | Block Parallel Time (ms) |\r\n|-----------|-------------|-------------------------|--------------------------|\r\n| 4096 |  256 |  0.034  |  0.035 |\r\n| 4096 |  512 |  0.039  |  0.040 |  \r\n| 4096 | 1024 |  0.048  |  0.056 |\r\n| 4096 | 2048 |  0.070  |  0.069 |\r\n| 4096 | 4096 |  0.110  |  0.094 |\r\n| 4096 | 8192 |  0.183  |  0.137 |\r\n| 262144 |  256 |  0.313  |  0.325 |\r\n| 262144 |  512 |  0.503  |  0.515 |  \r\n| 262144 | 1024 |  0.902  |  1.6 |\r\n| 262144 | 2048 |  1.74  |  2.25 |\r\n| 262144 | 4096 |  3.47  |  3.53 |\r\n","createdAt":"2024-04-03T23:07:27Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/15405#issuecomment-2035778152","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps55V9lE","author":{"login":"mythrocks"},"authorAssociation":"CONTRIBUTOR","body":"From exploring the customer's data, it appears that the majority of the search input strings are under 256 bytes long, although there are outliers (some being 64K long).  The average length amounts to 145.  I don't think going wider than 256 threads per block is reasonable.\r\n\r\nI've gotten some `nsys` traces for the block-parallel implementation, and compared them to the warp-parallel versions.  It appears that block-parallel lags warp-per-string, even for short search strings.  My test searched 4 Million input strings with an average length of 256. \r\n\r\nI've also run `ncu` (NSight Compute CLI). I will attach the findings here, but it appears that the block-parallel kernel takes longer than warp-per-string (~1 ms vs 2.76 ms), but has higher `Compute (SM) Throughput` (52.8% vs 39%).\r\n\r\nFor block-parallel:\r\n```\r\n    Section: GPU Speed Of Light Throughput\r\n    ----------------------- ------------- ------------\r\n    Metric Name               Metric Unit Metric Value\r\n    ----------------------- ------------- ------------\r\n    DRAM Frequency          cycle/nsecond         6.48\r\n    SM Frequency            cycle/nsecond         1.45\r\n    Elapsed Cycles                  cycle    3,991,213\r\n    Memory Throughput                   %        50.92\r\n    DRAM Throughput                     %         7.40\r\n    Duration                      msecond         2.75\r\n    L1/TEX Cache Throughput             %        54.65\r\n    L2 Cache Throughput                 %         2.78\r\n    SM Active Cycles                cycle 3,978,530.51\r\n    Compute (SM) Throughput             %        52.83\r\n    ----------------------- ------------- ------------\r\n\r\n    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance\r\n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate\r\n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.\r\n\r\n    Section: Launch Statistics\r\n    -------------------------------- --------------- ---------------\r\n    Metric Name                          Metric Unit    Metric Value\r\n    -------------------------------- --------------- ---------------\r\n    Block Size                                                   256\r\n    Function Cache Configuration                     CachePreferNone\r\n    Grid Size                                                485,000\r\n    Registers Per Thread             register/thread              18\r\n    Shared Memory Configuration Size           Kbyte           32.77\r\n    Driver Shared Memory Per Block        byte/block               0\r\n    Dynamic Shared Memory Per Block       byte/block               0\r\n    Static Shared Memory Per Block        byte/block              17\r\n    Threads                                   thread     124,160,000\r\n    Waves Per SM                                            1,684.03\r\n    -------------------------------- --------------- ---------------\r\n\r\n    Section: Occupancy\r\n    ------------------------------- ----------- ------------\r\n    Metric Name                     Metric Unit Metric Value\r\n    ------------------------------- ----------- ------------\r\n    Block Limit SM                        block           16\r\n    Block Limit Registers                 block           10\r\n    Block Limit Shared Mem                block          128\r\n    Block Limit Warps                     block            4\r\n    Theoretical Active Warps per SM        warp           32\r\n    Theoretical Occupancy                     %          100\r\n    Achieved Occupancy                        %        85.35\r\n    Achieved Active Warps Per SM           warp        27.31\r\n    ------------------------------- ----------- ------------\r\n\r\n\r\n    OPT   Estimated Speedup: 14.65%\r\n          This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated\r\n          theoretical (100.0%) and measured achieved occupancy (85.4%) can be the result of warp scheduling overheads\r\n          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block\r\n          as well as across blocks of the same kernel. See the CUDA Best Practices Guide\r\n          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on\r\n          optimizing occupancy.\r\n```\r\n\r\nFor warp-per-string:\r\n```\r\n    Section: GPU Speed Of Light Throughput\r\n    ----------------------- ------------- ------------\r\n    Metric Name               Metric Unit Metric Value\r\n    ----------------------- ------------- ------------\r\n    DRAM Frequency          cycle/nsecond         6.46\r\n    SM Frequency            cycle/nsecond         1.44\r\n    Elapsed Cycles                  cycle    1,445,593\r\n    Memory Throughput                   %        35.02\r\n    DRAM Throughput                     %        20.44\r\n    Duration                      usecond       998.78\r\n    L1/TEX Cache Throughput             %        35.99\r\n    L2 Cache Throughput                 %         7.33\r\n    SM Active Cycles                cycle 1,438,799.26\r\n    Compute (SM) Throughput             %        39.00\r\n    ----------------------- ------------- ------------\r\n\r\n    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance\r\n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate\r\n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.\r\n\r\n    Section: Launch Statistics\r\n    -------------------------------- --------------- ---------------\r\n    Metric Name                          Metric Unit    Metric Value\r\n    -------------------------------- --------------- ---------------\r\n    Block Size                                                   256\r\n    Function Cache Configuration                     CachePreferNone\r\n    Grid Size                                                 60,625\r\n    Registers Per Thread             register/thread              18\r\n    Shared Memory Configuration Size           Kbyte           32.77\r\n    Driver Shared Memory Per Block        byte/block               0\r\n    Dynamic Shared Memory Per Block       byte/block               0\r\n    Static Shared Memory Per Block        byte/block               0\r\n    Threads                                   thread      15,520,000\r\n    Waves Per SM                                              210.50\r\n    -------------------------------- --------------- ---------------\r\n\r\n    Section: Occupancy\r\n    ------------------------------- ----------- ------------\r\n    Metric Name                     Metric Unit Metric Value\r\n    ------------------------------- ----------- ------------\r\n    Block Limit SM                        block           16\r\n    Block Limit Registers                 block           10\r\n    Block Limit Shared Mem                block           16\r\n    Block Limit Warps                     block            4\r\n    Theoretical Active Warps per SM        warp           32\r\n    Theoretical Occupancy                     %          100\r\n    Achieved Occupancy                        %        91.36\r\n    Achieved Active Warps Per SM           warp        29.24\r\n    ------------------------------- ----------- ------------\r\n\r\n    INF   This kernel's theoretical occupancy is not impacted by any block limit.\r\n```\r\n\r\nI wonder if I might be missing a trick here, with the block-parallel implementation.\r\n\r\ncc @davidwendt, @nvdbaranec, @hyperbolic2346, whom I've been consulting on this.","createdAt":"2024-04-03T23:24:39Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/15405#issuecomment-2035800388","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps55owH9","author":{"login":"mythrocks"},"authorAssociation":"CONTRIBUTOR","body":"I've generated a local dataset with the search key distributions in much the same way as that of the reported slow case. (This includes the order of if-else clauses, with a similar match rate.)\r\n\r\nAt 4M rows, with an average string length of 256, with the search keys of 12-char average lengths, the total runtimes are a near match.  It's not looking like the kernel runtimes have an appreciable effect on the total runtime.  If there's anything afflicting the `block-per-string` approach, it might also be affecting the existing `warp-per-string`.  \r\n\r\nNSight Compute analysis did seem to indicate the following warning regarding `Long Scoreboard Stalls`:\r\n>```On average, each warp of this kernel spends 11.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 63.5% of the total average of 18.4 cycles between issuing two instructions```\r\n\r\nI'm trying to understand what can be changed here, but I'm wondering if we should be considering an algorithmic fix:\r\n1. We might consider what @jlowe has mentioned vis-a-vis employing short-circuit evaluations of `CASE WHEN`.\r\n2. A longer term change might involve pre-processing the input strings for searches.","createdAt":"2024-04-05T22:45:36Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/15405#issuecomment-2040726013","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps55pCDf","author":{"login":"mythrocks"},"authorAssociation":"CONTRIBUTOR","body":"P.S.  I think I left the impression that the `string::contains` kernel is \"slow\", in absolute terms.  That isn't accurate.\r\n\r\nMore correctly, the profiles of the Spark tasks indicate that the `contains` kernel is being called 30K times per task (as per how the user query is written), presumably around 25 times per input batch (once per `CASE WHEN` clause).  Each call runs in milliseconds, but the number of calls pushes the total time spent in said kernel to several seconds.\r\n\r\nEven a small improvement to the kernel is likely to amplify, at that scale.","createdAt":"2024-04-06T00:01:39Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/15405#issuecomment-2040799455","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps57b-uG","author":{"login":"mythrocks"},"authorAssociation":"CONTRIBUTOR","body":"The first approach (processing 1 string/threadblock instead of 1 string/warp) was a bust.  At the user's average string size of 144 characters, it appeared that too many threads in the block had too little to do.\r\n\r\nThe second approach (processing N strings in the same kernel, instead of running the single \"contains_warp_parallel_fn\" kernel N times) should have reduced the processing time (by amortizing the costs of kernel launch).  This seemed like a bust.  Tests at the user site indicated that this was taking slightly longer as well.\r\n\r\nThe current thought (hat tip to @nvdbaranec) is that this might have something to do with null string inputs.  It's possible that GPU occupancy reduces when there are more null input rows than non-null.  The null-threads exit early, and wait for the completion of non-null threads.  \r\nIt might be possible to increase occupancy by reducing the threads-per-block for the kernel launch.  \r\nI'm awaiting clarification on the user's actual null counts. I'm trying to test the behaviour with nulls separately.","createdAt":"2024-04-22T20:52:19Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/15405#issuecomment-2070932358","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps57cXfn","author":{"login":"mythrocks"},"authorAssociation":"CONTRIBUTOR","body":"Null distribution |   | 0% nulls | 30% nulls | 50% nulls | 3 nulls out of 4 | 7 nulls out of 8 | 90% nulls\r\n-- | -- | -- | -- | -- | -- | -- | --\r\n  |   | (ms) | (ms) | (ms) | (ms) | (ms) | (ms)\r\nmulti_contains_warp_parallel | 256 threads/block | 11.13 | 10.04 | 8.27 | 4.38 | 3.4 | 3.18\r\n5 x contains_warp_parallel | 256 threads/block | 10.64 | 9.6 | 7.87 | 4.18 | ??? | ???\r\nmulti_contains_warp_parallel | 32 threads/block | 18.68 | 13.76 | 10.51 | 6.74 | 4.69 | 4.29\r\n5 x contains_warp_parallel | 32 threads/block | 18.08 | 13.24 | 9.04 | 6.3 | ??? | ???\r\n\r\nThe fastest execution time to find 5 sub-strings across 1M input rows for a variety of null distributions seems to be to call the `contains_warp_parallel_fn` 5 times.\r\n\r\nIt appears that the null-row theory isn't completely accurate. :/","createdAt":"2024-04-22T22:08:28Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/15405#issuecomment-2071033831","viewerDidAuthor":false}],"createdAt":"2024-03-27T19:11:15Z","id":"I_kwDOBWUGps6D0_v6","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NjE=","name":"feature request","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwxMTM5NzQwNjY2","name":"libcudf","description":"Affects libcudf (C++/CUDA) code.","color":"c5def5"},{"id":"MDU6TGFiZWwxNDA1MTQ2OTc1","name":"Spark","description":"Functionality that helps Spark RAPIDS","color":"7400ff"},{"id":"MDU6TGFiZWwxNTE1NjE2MjUz","name":"strings","description":"strings issues (C++ and Python)","color":"0e8a16"}],"milestone":null,"number":15405,"projectCards":[],"projectItems":[],"reactionGroups":[{"content":"EYES","users":{"totalCount":2}}],"state":"OPEN","title":"[FEA] Improved performance for strings finder_warp_parallel_fn / contains_warp_parallel_fn kernels","updatedAt":"2024-04-22T22:08:29Z","url":"https://github.com/rapidsai/cudf/issues/15405"}
