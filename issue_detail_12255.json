{"assignees":[],"author":{"id":"MDQ6VXNlcjE2OTI5MTQ=","is_bot":false,"login":"randerzander","name":"Randy Gelhausen"},"body":"I have a lot of gzip compressed CSV files. When I use cudf to read them, the host handles decompression before copying decompressed data to device.\r\n\r\nFor cudf, that's not a problem, since it'll at worst be no slower than CPU.\r\n\r\nBut when I read w/ dask_cudf, compared to CPU dask.dataframe, I will usually have <=8 workers in a LocalCUDACluster. If I'm reading a large number of compressed files, those 8 workers will be highly bottlenecked by decompression.\r\n\r\n**Describe the solution you'd like**\r\nIdeally, we could have fast device side decompression for gzip compressed CSVs.\r\n\r\n**Describe alternatives you've considered**\r\nAnother solution for dask_cudf could be some logic to make more parallel use of host CPUs for decompression, which should increase throughput t device.\r\n\r\n**Additional context**\r\nPer file compression level can be high, such that doing device side decompression, even if faster than CPU, could easily lead to OOM scenarios.\r\n\r\nAn illustrative dataset for use in exploring this problem is NOAA's daily weather observations:\r\n```\r\nimport urllib, os\r\n\r\ndata_dir = '/raid/weather/csv/'\r\n\r\n# download weather observations\r\nbase_url = 'ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/'\r\nyears = list(range(1763, 2020))\r\nfor year in years:\r\n    fn = str(year) + '.csv.gz'\r\n    if not os.path.isfile(data_dir+fn):\r\n        print(f'Downloading {base_url+fn} to {data_dir+fn}')\r\n        urllib.request.urlretrieve(base_url+fn, data_dir+fn) \r\n```\r\n\r\ncc @GregoryKimball ","closed":false,"closedAt":null,"comments":[{"id":"IC_kwDOBWUGps5R-D-b","author":{"login":"vuule"},"authorAssociation":"CONTRIBUTOR","body":"This is very messy for `read_csv`, because we do need the data on the host in the current implementation. So this would lead to additional H2D and D2H copies.\r\nOther than that, it should be possible to add a parameter to request device-side decompression. Not a trivial change in `read_csv`, though.","createdAt":"2023-01-09T07:57:28Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12255#issuecomment-1375223707","viewerDidAuthor":false}],"createdAt":"2022-11-29T17:34:28Z","id":"I_kwDOBWUGps5Xhsxn","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NjE=","name":"feature request","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwxMDEzOTg3MzUy","name":"0 - Backlog","description":"In queue waiting for assignment","color":"d4c5f9"},{"id":"MDU6TGFiZWwxMTM5NzQwNjY2","name":"libcudf","description":"Affects libcudf (C++/CUDA) code.","color":"c5def5"},{"id":"MDU6TGFiZWwxMTg1MjQ0MTQy","name":"cuIO","description":"cuIO issue","color":"fef2c0"}],"milestone":{"number":12,"title":"CSV continuous improvement","description":"","dueOn":null},"number":12255,"projectCards":[{"project":{"name":"Feature Planning"},"column":{"name":"Needs prioritizing"}}],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[FEA] Support device-side de/compression of CSV files","updatedAt":"2023-04-02T22:38:52Z","url":"https://github.com/rapidsai/cudf/issues/12255"}
