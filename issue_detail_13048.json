{"assignees":[{"id":"MDQ6VXNlcjQ1Nzk1OTkx","login":"davidwendt","name":"David Wendt"}],"author":{"id":"MDQ6VXNlcjEyNzI1MTEx","is_bot":false,"login":"GregoryKimball","name":"Gregory Kimball"},"body":"Many [strings APIs in libcudf](https://docs.rapids.ai/api/libcudf/stable/group__strings__apis.html) use thread-per-string parallelism in their implementation. This approach works great for processing smaller strings of relatively consistent length. However, for long strings (roughly 256 bytes and above) the performance of thread-per-string algorithms begins to degrade. Some strings APIs are compatible with data-parallel algorithms and can be refactored to improve performance for long strings, while other strings APIs are difficult to refactor with data-parallel algorithms. \r\n\r\nLet's use this issue to track the progression: \r\nâœ… - this API works well with long strings\r\nðŸŸ¢ - we think this API will be straightforward to refactor\r\nðŸŸ¡ - we have some ideas on how to refactor this API, and we'll need to experiment\r\nðŸ”´ - we think this will be very difficult to refactor!\r\nâšª - long string support is not a priority for this API\r\n\r\n|Module|Function|Status|Notes|\r\n|---|---|---|---|\r\n| [Case](https://docs.rapids.ai/api/libcudf/nightly/group__strings__case.html) | capitalize <br> title <br> is_title <br> to_lower <br> to_upper <br> swapcase | ðŸŸ¡ <br> ðŸŸ¡ <br> ðŸŸ¡ <br> âœ…#13142 <br> âœ…#13142 <br>âœ…#13142  | |\r\n| [Character Types](https://docs.rapids.ai/api/libcudf/nightly/group__strings__types.html) | all_characters_of_type <br> filter_characters_of_type | âœ…#13259 <br> ðŸ”´ | |\r\n| [Combining](https://docs.rapids.ai/api/libcudf/nightly/group__strings__combine.html) | join_strings <br> concatenate <br> join_list_elements | âœ…#13283 <br> ðŸŸ¡ <br> ðŸ”´ | |\r\n| [Searching](https://docs.rapids.ai/api/libcudf/nightly/group__strings__contains.html) | contains_re <br> matches_re <br> count_re <br> like <br> find_all | ðŸŸ¡ <br> âšª <br> ðŸ”´ <br> ðŸŸ¢#13594 <br> ðŸ”´ |  |\r\n| [Converting](https://docs.rapids.ai/api/libcudf/nightly/group__strings__convert.html) | to_XXXX <br> from_XXXX  | âšª <br> âšª | these are rarely long strings|\r\n| [Copying](https://docs.rapids.ai/api/libcudf/nightly/group__strings__copy.html) | repeat_string <br> repeat_strings | âœ… <br> âœ… | One [overload](https://docs.rapids.ai/api/libcudf/nightly/group__strings__copy.html#ga160c075327cb4fb081db19884dba294c) is an exception  |\r\n| [Slicing](https://docs.rapids.ai/api/libcudf/nightly/group__strings__slice.html) | slice_strings | âœ…#13057 | One [overload](https://docs.rapids.ai/api/libcudf/nightly/group__strings__slice.html#ga2bc738cebebcf6d1331d6e9d13d4cd28) allows for skipping characters. <br>Long string support is not a priority for <br> `step > 1 or step < 0` |\r\n| [Finding](https://docs.rapids.ai/api/libcudf/nightly/group__strings__find.html) | find <br> rfind <br> contains <br> starts_with <br> ends_with <br> find_multiple | âœ…#13226 <br> âœ…#13226 <br> âœ…#10739 <br> âšª <br> âšª <br> ðŸŸ¢  | |\r\n| [Modifying](https://docs.rapids.ai/api/libcudf/nightly/group__strings__modify.html) | pad <br> zfill <br> reverse <br> strip <br> translate <br> filter_characters <br> wrap | ðŸŸ¡ <br> âšª <br> ðŸŸ¡ <br> âœ… <br> ðŸŸ¡ <br> ðŸ”´ <br> ðŸ”´ |  |\r\n| [Replacing](https://docs.rapids.ai/api/libcudf/nightly/group__strings__replace.html) | replace  <br> replace_slice <br> replace_re  <br> replace_with_backrefs | âœ…#12858 <br> ðŸŸ¡ <br> ðŸ”´ <br> ðŸ”´ | |\r\n| [Splitting](https://docs.rapids.ai/api/libcudf/nightly/group__strings__split.html) | partition <br> split <br> split_record <br> split_re <br> split_record_re | ðŸŸ¡ <br> âœ…#4922 #13680 <br> âœ…#12729 <br> ðŸ”´ <br> ðŸ”´ | |\r\n| other | count_characters  <br> count_bytes| âœ…#12779 <br> ðŸŸ¢ | | \r\n\r\nLibcudf also includes [NVText](https://docs.rapids.ai/api/libcudf/stable/group__nvtext__apis.html) APIs that will benefit from improvements in performance when processing long strings. Generally long string performance is even more important for our text APIs, where each row could represent a sentence, paragraph or document.\r\n\r\n|Module|Function|Status|Notes|\r\n|---|---|---|---|\r\n| [NGrams](https://docs.rapids.ai/api/libcudf/stable/group__nvtext__ngrams.html) | generate_ngrams <br> generate_character_ngrams <br> ngrams_tokenize|  âšª  <br> ðŸŸ¢ <br> ðŸŸ¢#13480 | these are generally not long strings |\r\n| [Normalizing](https://docs.rapids.ai/api/libcudf/stable/group__nvtext__normalize.html) | normalize_characters <br> normalize_spaces  |  ðŸŸ¢ <br> ðŸŸ¢#13480  |  |\r\n| [Stemming](https://docs.rapids.ai/api/libcudf/stable/group__nvtext__stemmer.html) | is_letter <br> porter_stemmer_measure  |   ðŸŸ¢ <br>ðŸŸ¢  | |\r\n| [Edit Distance](https://docs.rapids.ai/api/libcudf/stable/group__nvtext__edit__distance.html) | edit_distance <br> edit_distance_matrix | âšª <br> âšª  | these are generally not long strings |\r\n| [Tokenizing](https://docs.rapids.ai/api/libcudf/stable/group__nvtext__tokenize.html) | byte_pair_encoding <br> subword_tokenize <br> tokenize <br> count_tokens <br> character_tokenize <br> detokenize  | ðŸŸ¡ <br>ðŸŸ¡ <br>ðŸŸ¢#13480 <br>ðŸŸ¢#13480 <br>ðŸŸ¡ <br>ðŸŸ¡  |  |\r\n| [Replacing](https://docs.rapids.ai/api/libcudf/stable/group__nvtext__replace.html) | replace_tokens <br> filter_tokens | ðŸŸ¢#13480 <br>ðŸŸ¢#13480 |  |\r\n| [MinHashing](https://docs.rapids.ai/api/libcudf/nightly/group__nvtext__minhash.html) | minhash  | âœ…#13333 |  |\r\n","closed":false,"closedAt":null,"comments":[{"id":"IC_kwDOBWUGps5ZGE7K","author":{"login":"bdice"},"authorAssociation":"CONTRIBUTOR","body":"This is a great use of a \"Story\" issue for documenting this work. I have some very high level comments / ideas:\r\n\r\n- Some of these algorithms might benefit from using [Cooperative Groups](https://developer.nvidia.com/blog/cooperative-groups/) and/or studying implementations of related algorithms in CUB.\r\n- For some of these cases, I suspect that \"prefix strings\" as described in the [Velox paper](https://vldb.org/pvldb/vol15/p3372-pedreira.pdf) could be useful. That would use a (non-Arrow) layout with a fixed-width prefix that could be computed as a temporary column used to quickly eliminate possible matches. Filtering and ordering are specifically called out in that paper.","createdAt":"2023-04-03T18:13:04Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":2}}],"url":"https://github.com/rapidsai/cudf/issues/13048#issuecomment-1494765258","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5Z4FDg","author":{"login":"GregoryKimball"},"authorAssociation":"CONTRIBUTOR","body":"After studying the results of the strings microbenchmarks in libcudf, I'd like to share some results and early thoughts on the performance we should target with long strings. \r\n\r\nFirst, here is data throughput versus data size for the `split_record` API. It shows about 15-35 GB/s throughput at the ~100 MB data size for string lengths between 32 and 8192 bytes.\r\n<img width=\"899\" alt=\"image\" src=\"https://user-images.githubusercontent.com/12725111/231931798-1fef4e13-00f0-43ad-8963-904df29e73c7.png\">\r\n\r\nNext, here is the same plot for `filter_characters_of_type`. It shows 20 GB/s throughput for ~100 MB data size and strings lengths <100 bytes. For longer strings the throughput falls to the 1-6 GB/s range.\r\n<img width=\"906\" alt=\"image\" src=\"https://user-images.githubusercontent.com/12725111/231932289-9f85f3cd-8c89-42ea-a7e9-88368da538d5.png\">\r\n\r\nSo perhaps we should target data throughputs for long strings to be better than 80% of data throughput for short strings.","createdAt":"2023-04-14T03:18:53Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/13048#issuecomment-1507872992","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5k3fXx","author":{"login":"GregoryKimball"},"authorAssociation":"CONTRIBUTOR","body":"Hello @davidwendt, I was discussing this issue with @elstehle. Do you think the \"combining\" or \"copying\" strings algorithms could benefit from [DeviceBatchMemcpy](https://github.com/NVIDIA/cub/pull/359)?","createdAt":"2023-08-24T19:12:38Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/13048#issuecomment-1692268017","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5lM4UO","author":{"login":"davidwendt"},"authorAssociation":"CONTRIBUTOR","body":"> Hello @davidwendt, I was discussing this issue with @elstehle. Do you think the \"combining\" or \"copying\" strings algorithms could benefit from [DeviceBatchMemcpy](https://github.com/NVIDIA/cub/pull/359)?\r\n\r\nThere are several parts of the strings (and nvtext) API implementations that basically resolve into a fragmented set of (pointer,size) pairs which are passed to a factory that performs an optimized gather function to build a contiguous byte array for an output strings column. So a significant impact here may be to improve that gather function by taking advantage of `DeviceBatchMemcpy` perhaps.\r\n\r\nThe factory function that uses the gather is illustrated in my blog post which use the custom kernel here: https://github.com/rapidsai/cudf/blob/cd56cc2b4cc47a1d0c63e56fac945a66905c28df/cpp/examples/strings/custom_prealloc.cu#L118\r\n\r\nI'll locate other places that libcudf uses this factory function that also include benchmarks.","createdAt":"2023-08-29T17:39:50Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"HEART","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/13048#issuecomment-1697875214","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps55BC_v","author":{"login":"GregoryKimball"},"authorAssociation":"CONTRIBUTOR","body":"Spark-RAPIDS identified long strings performance issues in `find` (#15405) as well as `upper`/`lower` (#15406)","createdAt":"2024-04-01T18:33:59Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/13048#issuecomment-2030317551","viewerDidAuthor":false}],"createdAt":"2023-04-03T17:58:26Z","id":"I_kwDOBWUGps5if168","labels":[{"id":"MDU6TGFiZWwxMDEzOTg3NTAz","name":"2 - In Progress","description":"Currently a work in progress","color":"fef2c0"},{"id":"MDU6TGFiZWwxMTM5NzQwNjY2","name":"libcudf","description":"Affects libcudf (C++/CUDA) code.","color":"c5def5"},{"id":"MDU6TGFiZWwxMzIyMjUyNjE3","name":"Performance","description":"Performance related issue","color":"C2E0C6"},{"id":"MDU6TGFiZWwxNTE1NjE2MjUz","name":"strings","description":"strings issues (C++ and Python)","color":"0e8a16"}],"milestone":{"number":30,"title":"Language model acceleration","description":"","dueOn":null},"number":13048,"projectCards":[],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[FEA] Story - Improve performance with long strings","updatedAt":"2024-04-01T18:34:01Z","url":"https://github.com/rapidsai/cudf/issues/13048"}
