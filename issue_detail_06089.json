{"assignees":[{"id":"MDQ6VXNlcjQ1Nzk1OTkx","login":"davidwendt","name":"David Wendt"}],"author":{"id":"MDQ6VXNlcjQ4Mzc1NzE=","is_bot":false,"login":"VibhuJawa","name":"Vibhu Jawa"},"body":"**Is your feature request related to a problem? Please describe.**\r\n\r\nCurrently, the tokenized string is shorter than max_length, output is be padded with 0s.  So if `max( tokenized string lengths)` < `max_length`, it  leads to performance penalties as the compute time for `Transformer` models is often proportional to the sequence length of the input . \r\n\r\nHuggingFace's tokenizer defaults to padding to max input sequence length if `max_length` and `pad_to_max_length` are not provided . We should try to follow that, this is especially beneficial for streaming cases that feature https://github.com/rapidsai/cudf/issues/5868 will help. \r\n\r\n\r\n##### See below example: \r\n\r\n#### Padding to max sequence length.(Proposed Default Behaviour)  \r\n```python\r\nfrom transformers import BertTokenizerFast\r\n\r\n\r\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', do_lower_case=True)\r\ndata = ['a', 'a b', 'a b c d']\r\noutput = tokenizer.batch_encode_plus(data,padding=True,add_special_tokens=False, return_tensors = 'pt')\r\noutput['input_ids']\r\n\r\ntensor([[1037,    0,    0,    0],\r\n        [1037, 1038,    0,    0],\r\n        [1037, 1038, 1039, 1040]])\r\n```\r\n\r\n####  Padding to max_length (Current Default Behavior) \r\n```python\r\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', do_lower_case=True)\r\noutput = tokenizer.batch_encode_plus(\r\n        data, truncation=True, max_length=64, pad_to_max_length=True,\r\n        add_special_tokens=False, return_tensors = 'pt'\r\n    )\r\noutput['input_ids']\r\n```\r\n```\r\ntensor([[1037,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\r\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\r\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\r\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\r\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\r\n            0,    0,    0,    0],\r\n        [1037, 1038,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\r\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\r\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\r\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\r\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\r\n            0,    0,    0,    0],\r\n        [1037, 1038, 1039, 1040,    0,    0,    0,    0,    0,    0,    0,    0,\r\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\r\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\r\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\r\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\r\n            0,    0,    0,    0]])\r\n```\r\n\r\n\r\n\r\n**Related Implications:**\r\n\r\na. We might have to switch from returning one-dimensional cupy arrays to 2-dimensional arrays for token-ids and attention masks which we allready do for most workflow cases so should not have performance penalties.   \r\n\r\n\r\n**Describe alternatives you've considered**\r\n\r\nCurrently, a user can do the tokenization twice. \r\n\r\n1. First time to get maximum sequence length, do this without a `to_dlpack` call. \r\n2. Inputting that sequence length to tokenizer again and then convert to tensors using `dlpack` \r\n\r\nI do above for gpu-bdb q27 HF. \r\n), As most of the time is spent doing `to_dlpack` so this workaround should not have big performance implications. \r\n\r\n\r\n\r\nCC: @raykallen , @randerzander , @davidwendt \r\n","closed":false,"closedAt":null,"comments":[{"id":"MDEyOklzc3VlQ29tbWVudDc5ODk2MjM4Nw==","author":{"login":"github-actions"},"authorAssociation":"NONE","body":"This issue has been labeled `inactive-30d` due to no recent activity in the past 30 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed. This issue will be labeled `inactive-90d` if there is no activity in the next 60 days.","createdAt":"2021-03-14T19:12:50Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/6089#issuecomment-798962387","viewerDidAuthor":false}],"createdAt":"2020-08-25T17:48:07Z","id":"MDU6SXNzdWU2ODU2NzQ5Njc=","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NjE=","name":"feature request","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwxMTM5NzQwNjY2","name":"libcudf","description":"Affects libcudf (C++/CUDA) code.","color":"c5def5"},{"id":"MDU6TGFiZWwxMTM5NzQxMjEz","name":"cuDF (Python)","description":"Affects Python cuDF API.","color":"1d76db"},{"id":"MDU6TGFiZWwxNTE1NjE2MjUz","name":"strings","description":"strings issues (C++ and Python)","color":"0e8a16"}],"milestone":null,"number":6089,"projectCards":[{"project":{"name":"Feature Planning"},"column":{"name":"Needs prioritizing"}}],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[FEA] Support packing to a max input sequence length with cudf-subword tokenizer","updatedAt":"2024-02-23T18:43:26Z","url":"https://github.com/rapidsai/cudf/issues/6089"}
