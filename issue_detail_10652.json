{"assignees":[{"id":"MDQ6VXNlcjM5NDM3NjE=","login":"bdice","name":"Bradley Dice"},{"id":"MDQ6VXNlcjc0MTY5MzU=","login":"ttnghia","name":"Nghia Truong"}],"author":{"id":"MDQ6VXNlcjkzNDA4NA==","is_bot":false,"login":"andygrove","name":"Andy Grove"},"body":"**Is your feature request related to a problem? Please describe.**\r\nI would like to be able to implement a GPU version of Spark's `approx_count_distinct` function, which uses the [HyperLogLog++](https://en.wikipedia.org/wiki/HyperLogLog) cardinality estimation algorithm. \r\n\r\ncuDF does not appear to provide any features today that would allow me to do this.\r\n\r\n**Describe the solution you'd like**\r\nI would like cuDF to implement this capability and expose an API that is likely similar to `approx_percentile` in that there would be methods both for computing and merging the underlying data structure, whether that is based on HyperLogLog++ or some other algorithm.\r\n\r\n**Describe alternatives you've considered**\r\nNone\r\n\r\n**Additional context**\r\nNone\r\n","closed":false,"closedAt":null,"comments":[{"id":"IC_kwDOBWUGps5BeDhd","author":{"login":"jrhemstad"},"authorAssociation":"CONTRIBUTOR","body":"I found this article to be an excellent description of the algorithm: https://towardsdatascience.com/hyperloglog-a-simple-but-powerful-algorithm-for-data-scientists-aed50fe47869\r\n\r\nSeems fairly straightforward to implement in parallel too. Compute the hash value for each row, then compute a histogram using the upper `p` bits for `m` buckets (`m == 2^p`) and using [`__clzll`](https://docs.nvidia.com/cuda/cuda-math-api/group__CUDA__MATH__INTRINSIC__INT.html#group__CUDA__MATH__INTRINSIC__INT_1g88ff76484edc65cafbef80e71a1daa53) to get the count of consecutive 0s. \r\n\r\nThen a merge step to compute the average of the values in the histogram taking into account discarding outliers. ~The histogram is small enough it probably makes sense to just copy it to the CPU and do this step serially.~ I changed my mind, I think it probably makes more sense to just do the merging on a single block.","createdAt":"2022-04-13T19:18:56Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/10652#issuecomment-1098397789","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5BhV_A","author":{"login":"revans2"},"authorAssociation":"CONTRIBUTOR","body":"> The histogram is small enough it probably makes sense to just copy it to the CPU and do this step serially.\r\n\r\nThat is true if we are doing a reduction. Spark supports this for groupby aggregations and technically for window operations too, but I don't think we need it there just yet. Although Spark does not support distict window functions so it might be used just because it is the only option. For a groupby we could be doing a lot of smaller distinct counts so having the ability to do that on the GPU would be good.\r\n\r\n@andygrove could you please feel in more details of what we are going to need here?\r\n\r\nIs this just for a reduction or is it for a group by aggregation? Do we need to include windowing support? Because we do distributed group by and reductions, with two phases to build the aggregation, what operations/aggregations do we need? What are the inputs and the outputs to each aggregation/operation? It looks like Spark supports setting a scalar `relative standard deviation` parameter that directly impacts `p`. It also looks like Spark expects to shuffle the intermediate results (a histogram) as a struct of longs, and the number of entries in that struct is `p^2/6` or something odd like that.\r\n\r\nWe need a lot more details so we can work with CUDF to hammer out exactly what this is going to look like.","createdAt":"2022-04-14T14:43:12Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/10652#issuecomment-1099259840","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5BhcfT","author":{"login":"jrhemstad"},"authorAssociation":"CONTRIBUTOR","body":"One note about the implementation. From what I've read, most implementations use some small set of bits per histogram bucket (like 5 bits to store the per-bucket count). The only way I'd be comfortable with returning something like that to a user is in an opaque type. Otherwise it should just be a standard integer type per bucket (I'm guessing using a 4B int per bucket will be better for performance since this is going to require atomics to merge histograms).\r\n\r\n> Because we do distributed group by and reductions, with two phases to build the aggregation, what operations/aggregations do we need?\r\n\r\nI spent a little time thinking about this, but indeed we'd need more concrete requirements to say definitively what it should look like.\r\n\r\nFor the \"reduction\"/column level API I envision a two phase API where the first returns an opaque-ish object holding the histogram and the second takes `n` histograms and merges them to return the approximate cardinality. Spitballing...\r\n\r\n```\r\nnamespace cudf::hll { // namespace for hyperloglog functionality? \r\n\r\n// Does this need to work on tables? or just columns?\r\n// This could return an opaque `hyperloglog_histogram` object, or it could just return a regular `column`?\r\ncompute_histogram(table_view or column_view?, size_type num_buckets, cudf::hash_function hash)\r\n\r\n// Depending on what `compute_histogram` returns influences what the input to this function would be, but essentially is just\r\n// a list of histograms of the same size. Should probably be some strongly typed thing that enforces requirements\r\nsize_t approx_distinct_count( table_view histograms, float outlier_threshold = 0.7, float bias_correction =  0.77351)\r\n\r\n```\r\n\r\nFor the groupby case we can do something like with tdigest where there's a `hll_histogram` aggregation and `merge_hll_histogram` aggregation. \r\n\r\n> Spark expects to shuffle the intermediate results (a histogram) as a struct of longs\r\n\r\nI definitely don't think we should do it this way. Storing the histogram in a literal `struct<int, int, int, ...., int>` column would be a bad idea to have 1 child per element in the histogram. For the single column case I imagine the histogram could just be a regular `int32` column. For the groupby case, I'd think a `LIST<int32>` column would make the most sense. ","createdAt":"2022-04-14T15:11:06Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/10652#issuecomment-1099286483","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5BkofM","author":{"login":"revans2"},"authorAssociation":"CONTRIBUTOR","body":"From what I saw in the code Spark is using a long as the word size, but appears to have 6 bits per histogram bucket.\r\n\r\nhttps://github.com/apache/spark/blob/4835946de2ef71b176da5106e9b6c2706e182722/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/HyperLogLogPlusPlusHelper.scala#L271\r\n\r\nhttps://github.com/apache/spark/blob/4835946de2ef71b176da5106e9b6c2706e182722/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/HyperLogLogPlusPlusHelper.scala#L78-L85\r\n\r\nI think what we want is a `compute_hyperloglogplusplus_histogram` aggregation and a `merge_hyperloglogplusplus_histogram` aggregation.  I am not tied to the names and if merging histograms is generic enough we can drop the hyperloglog... part. We would want this because it would tie into all of the existing code that we already have for doing these aggregations. If it is different then we will have to create an entirely new special cased code path. For reductions this is not a big deal, but for group by aggregations it would mean we would have to do a join afterwards to line up the different groups with each other again, and I really don't want to do that. \r\n\r\nSo that means the histogram would be computed on a single input column because that is how reductions and group by aggregations tend to work. The output would need to be a cudf::column for a group by or a cudf::scalar for a reduction just like the existing APIs. I would want the type returned to be the same for both the reduction and the group_by because it makes life simpler to have common code to handle moving it around/etc. a LIST<INT32> would be fine. I would also be fine with a LIST<INT8> because Spark uses an int6? I think? for the bucket.\r\n\r\nBut I am no expert on this. Ideally we would like to be able to match the format that Spark is using as the result between the `compute_histogram` aggregate and the `merge_histogram` aggregate because it lets us not worry about making sure that the first aggregation and the merge aggregation are both of the GPU. But looking at how complex this is I have my doubts we should even try to do that. And we have code to try to make sure that they both are on the GPU or none of them are, but it is a bit brittle.","createdAt":"2022-04-15T13:50:55Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/10652#issuecomment-1100122060","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5Bk14X","author":{"login":"jrhemstad"},"authorAssociation":"CONTRIBUTOR","body":"@revans2 I don't think anything you said is controversial or incompatible with what I was thinking. The only piece I don't like is about returning the histogram in an `int64` where it's really 6 bits per histogram bucket. That would mean the result isn't _actually_ a `INT64` column, but some special column with special meaning that isn't reflected anywhere in it's type or metadata. If it's going to be a non-opaque object, it has to at least be an `INT8` per bucket. \r\n\r\nTo summarize:\r\n- New aggregations\r\n   - `hll::compute_histogram`\r\n      - `cudf::reduce`\r\n         - Input: `column_view` of any(?) type\r\n         - Output: `cudf::scalar` with `LIST<int8>` (i.e., a single list of 8 bit integers that is the histogram)\r\n      - `cudf::groupby` \r\n         - Input: `column_view` of any(?) type \r\n         - Output: `column` with `LIST<int8>`, histogram/list per group\r\n   - `hll::merge_histogram`\r\n      - `cudf::reduce`\r\n         - Input: `column` with `LIST<INT8>`, i.e., all the histograms to merge\r\n            -  Each histogram must be the same size\r\n         - Output: `scalar` with `LIST<INT8>`\r\n       - `cudf::groupby`\r\n          - Input: `column` with `LIST<INT8>`\r\n             -  Each histogram must be the same size\r\n          - Output: `coumn` with `LIST<INT8>`\r\n- New `hll::histogram_column_view`\r\n   - Similar to `tdigest_column_view` provides facade over `LIST<INT8>` column\r\n   - Each histogram must be the same size\r\n   - This should really be a fixed-size list column, but we don't have that\r\n   - Open question if this ctor should enforce each list being the same size (requires a kernel)\r\n- New function \r\n   - `hll::approx_distinct_count`\r\n      - Input: `hll::histogram_column_view`\r\n      - Output: `column` with `INT64` of the approximate cardinality per histogram in the input column\r\n","createdAt":"2022-04-15T15:28:45Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/10652#issuecomment-1100176919","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5BlD35","author":{"login":"revans2"},"authorAssociation":"CONTRIBUTOR","body":"That sounds great.\r\n\r\n> Open question if this ctor should enforce each list being the same size (requires a kernel)\r\n\r\nI thought in general if it made the code slower you wouldn't do the check. That it would be a separate method to do the check. I am fine with the separate method, but honestly it is not hard to do that check with existing code.","createdAt":"2022-04-15T17:06:50Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/10652#issuecomment-1100234233","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5CnLou","author":{"login":"teju85"},"authorAssociation":"MEMBER","body":"Also tagging @vinaydes, since we too have been looking into using hll for implementing a fast FP-growth like algo for rule mining.","createdAt":"2022-05-04T16:37:28Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/10652#issuecomment-1117567534","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5F1lKA","author":{"login":"github-actions"},"authorAssociation":"NONE","body":"This issue has been labeled `inactive-30d` due to no recent activity in the past 30 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed. This issue will be labeled `inactive-90d` if there is no activity in the next 60 days.","createdAt":"2022-06-30T21:03:19Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/10652#issuecomment-1171673728","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5F5hRU","author":{"login":"revans2"},"authorAssociation":"CONTRIBUTOR","body":"This is still relevant ","createdAt":"2022-07-01T20:55:37Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/10652#issuecomment-1172706388","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5F5hle","author":{"login":"ttnghia"},"authorAssociation":"CONTRIBUTOR","body":"I'm going to start working on it from next week, targeting for 22.10 release.","createdAt":"2022-07-01T20:58:29Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/10652#issuecomment-1172707678","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5HjlCE","author":{"login":"github-actions"},"authorAssociation":"NONE","body":"This issue has been labeled `inactive-30d` due to no recent activity in the past 30 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed. This issue will be labeled `inactive-90d` if there is no activity in the next 60 days.","createdAt":"2022-07-31T22:03:24Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/10652#issuecomment-1200509060","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5HxITY","author":{"login":"revans2"},"authorAssociation":"CONTRIBUTOR","body":"This is still relevant","createdAt":"2022-08-03T14:57:13Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/10652#issuecomment-1204061400","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5hAtS_","author":{"login":"sleeepyjack"},"authorAssociation":"NONE","body":"Has there been any progress on this issue? I am planning to implement an HLL-backed version of `cuco::util::approx_distinct_count`, which can potentially be used in cudf.","createdAt":"2023-07-09T02:16:15Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/10652#issuecomment-1627575487","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5hCtN5","author":{"login":"ttnghia"},"authorAssociation":"CONTRIBUTOR","body":"Hi! We deprioritized this for something else thus it was paused for a while. Having `cuco::util::approx_distinct_count` would be a big foundation for it. Thanks in advance!","createdAt":"2023-07-10T04:05:30Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/10652#issuecomment-1628099449","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5hFnO-","author":{"login":"bdice"},"authorAssociation":"CONTRIBUTOR","body":"Yes, having an implementation in cuco would be fantastic. @sleeepyjack if you want to discuss design / implementation, I would be more than happy to offer input from our past conversations about this topic and/or PR review.","createdAt":"2023-07-10T12:29:08Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/10652#issuecomment-1628861374","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5hQhYy","author":{"login":"sleeepyjack"},"authorAssociation":"NONE","body":"@bdice Good idea. Let me read through the relevant papers first. I guess the crucial part I have to wrap my head around is how to represent the counters and merge them in parallel. I'll ping you on Slack once I'm prepared so we can schedule a 1:1.","createdAt":"2023-07-12T01:35:17Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/10652#issuecomment-1631721010","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps55WYEM","author":{"login":"sleeepyjack"},"authorAssociation":"NONE","body":"FYI `cuco::distinct_count_estimator` (NVIDIA/cuCollections#429) has been merged so we can start implementing and exposing this feature in cudf.","createdAt":"2024-04-04T01:16:19Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/10652#issuecomment-2035908876","viewerDidAuthor":false}],"createdAt":"2022-04-13T18:16:08Z","id":"I_kwDOBWUGps5HvfXw","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NjE=","name":"feature request","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwxMTM5NzQwNjY2","name":"libcudf","description":"Affects libcudf (C++/CUDA) code.","color":"c5def5"},{"id":"MDU6TGFiZWwxNDA1MTQ2OTc1","name":"Spark","description":"Functionality that helps Spark RAPIDS","color":"7400ff"}],"milestone":null,"number":10652,"projectCards":[{"project":{"name":"Feature Planning"},"column":{"name":"Needs prioritizing"}}],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[FEA] Support for approx_count_distinct","updatedAt":"2024-04-04T01:16:20Z","url":"https://github.com/rapidsai/cudf/issues/10652"}
