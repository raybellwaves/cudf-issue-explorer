{"assignees":[],"author":{"id":"MDQ6VXNlcjEyNzI1MTEx","is_bot":false,"login":"GregoryKimball","name":"Gregory Kimball"},"body":"**Is your feature request related to a problem? Please describe.**\r\nlibcudf aggregations show lower throughput when data cardinality is less than ~1000 distinct values. This is due to serializing atomic operations over a small range of global memory. We received some projections that use hash maps that begin in shared memory and then spill to global if they exceed a certain size. The projections indicate 2-10x speedup for cardinalities below 100.\r\n\r\n![image](https://github.com/rapidsai/cudf/assets/12725111/f28d02c5-f107-4c4a-b44d-687094a0a7a8)\r\n(Aggregation throughput data was collected for groupby max over 20M rows of int64 key and int64 payload, based on benchmarks introduced in https://github.com/rapidsai/cudf/pull/15134 and using A100 hardware. Projections were provided as speedup versus cardinality data and were applied to the A100 measured throughput to yield projected throughput.)\r\n\r\n**Describe the solution you'd like**\r\nWe could provide an implementation that uses shared memory hash maps when cardinality is low. [Shared memory as storage](https://github.com/NVIDIA/cuCollections/blob/dev/examples/static_set/shared_memory_example.cu) is supported in [cuCollections](https://github.com/NVIDIA/cuCollections), so we could leverage this option to offer a higher throughput code path when cardinality is low.\r\n\r\nAs far as the API design, we could add an optional `cardinality` parameter to the `aggregate` API. When [hyperloglog](https://github.com/NVIDIA/cuCollections/pull/429) cardinality estimates are available in cuCollections, we may want to support cardinality estimates as well. Some open questions include:\r\n* What is the throughput difference between hyperloglog and count distinct? We expect the memory footprint of hyperloglog to be much lower, but I don't believe throughput has had controlled measurements.\r\n* If we accept cardinality estimates, what happens if the cardinality is underestimated and the shared memory hash map fails? \r\n* Does it make sense for column objects to track cardinality, or should the application layer track cardinality?\r\n\r\n\r\n**Describe alternatives you've considered**\r\nWe aren't sure how common low cardinality aggregation keys are in customer workloads. Are there cases where cardinality will be known ahead of time, or will it always need to be computed or estimated before triggering the aggregation? Could we instrument NDS to log cardinality and row count before each aggregation node?\r\n\r\n**Additional context**\r\nWe could also consider using shared memory hash maps for low-cardinality distinct-key joins. This optimization is mentioned in https://github.com/rapidsai/cudf/issues/14948.\r\n","closed":false,"closedAt":null,"comments":[{"id":"IC_kwDOBWUGps52akOE","author":{"login":"sameerz"},"authorAssociation":"CONTRIBUTOR","body":"Related issue https://github.com/NVIDIA/spark-rapids/issues/7529","createdAt":"2024-03-09T01:22:34Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/15262#issuecomment-1986675588","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps52f0ln","author":{"login":"wence-"},"authorAssociation":"CONTRIBUTOR","body":"Is the low throughput specifically only for low cardinality data, or also for skewed cardinality data? That is, suppose I have many distinct group keys, but most of the weight of the data is only in a few. In that scenario, as you describe the implementation, I think we will still run slowly. However, dispatching based on absolute cardinality will not be that useful since it is not a true representation of the data distribution.","createdAt":"2024-03-11T10:14:10Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/15262#issuecomment-1988053351","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps53MaTR","author":{"login":"sleeepyjack"},"authorAssociation":"NONE","body":"> What is the throughput difference between hyperloglog and count distinct? We expect the memory footprint of hyperloglog to be much lower, but I don't believe throughput has had controlled measurements.\r\n\r\nHere are the newest benchmark numbers for HLL: https://github.com/NVIDIA/cuCollections/pull/429#issuecomment-1999730719\r\ntl;dr Depending on the parameter, we achieve between 72-89% of the SOL memory bandwidth of an H100.\r\n\r\nThe memory footprint is only a few KB or 1MB max and depends on the precision value. More precisely it is `4 * 2^precision` bytes, where the precision is typically in range [10, 18]. So yes, it should be much smaller compared to a fully fledged exact distinct count algorithm.","createdAt":"2024-03-15T14:05:10Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/15262#issuecomment-1999742161","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5_dP7G","author":{"login":"PointKernel"},"authorAssociation":"MEMBER","body":"Using this issue to continue the discussions in https://github.com/NVIDIA/spark-rapids/issues/7529.\r\n\r\nLow cardinality join and low cardinality groupby share some commons but the performance bottlenecks are not the same:\r\n\r\n- Groupby uses a single map so the probing sequence (number of steps for a query key to find a match in hash table) is short (thus few hash collisions). The performance killer for low cardinality cases is the contention with atomic aggregations. Shared memory hash table can help since it splits the atomic aggregation into two levels: a block-level aggregation first and then only one aggregation per block with the final output.\r\n- Hash join uses a multimap so duplicate keys will result in a longer probing sequence thus more hash collisions. The culprit should be high multiplicity as opposed to low cardinality. I need to think more about this but a shared memory hash table probably won't help here.\r\n\r\nWith that, to address this issue:\r\nFor groupby:\r\n\r\n- [ ] implement shared-memory hash table with two-level aggregation\r\n- [ ] evaluate the performance impact of using hyperloglog\r\n\r\nfor high-multiplicity hash join:\r\n\r\n- [ ] verify the impact of CG size tuning (https://github.com/NVIDIA/spark-rapids/issues/7529#issuecomment-2138286065). Expose CG size tuning in public APIs if needed","createdAt":"2024-05-29T22:42:17Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/15262#issuecomment-2138373830","viewerDidAuthor":false}],"createdAt":"2024-03-08T22:50:19Z","id":"I_kwDOBWUGps6BwLiA","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NjE=","name":"feature request","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwxMTM5NzQwNjY2","name":"libcudf","description":"Affects libcudf (C++/CUDA) code.","color":"c5def5"},{"id":"MDU6TGFiZWwxMzIyMjUyNjE3","name":"Performance","description":"Performance related issue","color":"C2E0C6"}],"milestone":{"number":28,"title":"Aggregations continuous improvement","description":"","dueOn":null},"number":15262,"projectCards":[],"projectItems":[],"reactionGroups":[{"content":"HEART","users":{"totalCount":1}}],"state":"OPEN","title":"[FEA] Add shared memory hash map for low-cardinality aggregations","updatedAt":"2024-05-29T22:42:18Z","url":"https://github.com/rapidsai/cudf/issues/15262"}
