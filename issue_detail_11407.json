{"assignees":[],"author":{"id":"MDQ6VXNlcjM5NDM3NjE=","is_bot":false,"login":"bdice","name":"Bradley Dice"},"body":"**Is your feature request related to a problem? Please describe.**\r\nA user with existing Feather data files asked about GPU-accelerated Feather support. cuDF currently supports CPU-based Feather reading but does not use GPU acceleration.\r\n\r\n**Describe the solution you'd like**\r\n`cudf.read_feather` should be GPU accelerated.\r\n\r\n**Describe alternatives you've considered**\r\nConverting files to another format like Parquet is probably ideal for performance and can leverage existing GPU accelerated I/O. This conversion can be done with pandas.\r\n\r\n**Additional context**\r\nFeather format description: https://github.com/wesm/feather/blob/master/doc/FORMAT.md","closed":false,"closedAt":null,"comments":[{"id":"IC_kwDOBWUGps5Hj4TX","author":{"login":"kkraus14"},"authorAssociation":"COLLABORATOR","body":"FYI -- https://issues.apache.org/jira/browse/ARROW-17092\r\n\r\nI believe the link above is for Feather V1 and the IPC File Format is Feather V2.","createdAt":"2022-08-01T01:46:31Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11407#issuecomment-1200587991","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5JdOOk","author":{"login":"github-actions"},"authorAssociation":"NONE","body":"This issue has been labeled `inactive-30d` due to no recent activity in the past 30 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed. This issue will be labeled `inactive-90d` if there is no activity in the next 60 days.","createdAt":"2022-08-31T02:59:44Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11407#issuecomment-1232397220","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5RhrY_","author":{"login":"mjshare"},"authorAssociation":"NONE","body":"It's a valuable need. Don't the community have a plan?","createdAt":"2022-12-30T08:07:33Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11407#issuecomment-1367782975","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5Rp0JP","author":{"login":"beckernick"},"authorAssociation":"MEMBER","body":"> Problem description:\r\nWhen there are a large number of string columns, data in feather format is much smaller than that in arrow format. When data is transmitted to the GPU in feather format for decompression, the PICE bandwidth pressure is much lower. This requirement is valuable. Can the GPU support the feather format for decompression in GPU like parquet?\r\n\r\n_Originally posted by @mjshare in https://github.com/rapidsai/cudf/issues/12453#issue-1514306164_\r\n\r\n> Similar requirements, Does the community have a solution plan? This reduces the bandwidth consumption of the GPU.\r\n@davidwendt\r\n\r\n_Originally posted by @mjshare in https://github.com/rapidsai/cudf/issues/12453#issuecomment-1369787656_\r\n\r\n@mjshare , would you be able to provide an example in which Feather vs Arrow makes a significant difference in a workload? Are you working in C++ or Python?","createdAt":"2023-01-03T15:40:44Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11407#issuecomment-1369915983","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5Rrs5P","author":{"login":"infzo"},"authorAssociation":"NONE","body":"> > Problem description:\r\n> > When there are a large number of string columns, data in feather format is much smaller than that in arrow format. When data is transmitted to the GPU in feather format for decompression, the PICE bandwidth pressure is much lower. This requirement is valuable. Can the GPU support the feather format for decompression in GPU like parquet?\r\n> \r\n> _Originally posted by @mjshare in [#12453 (comment)](https://github.com/rapidsai/cudf/issues/12453#issue-1514306164)_\r\n> \r\n> > Similar requirements, Does the community have a solution plan? This reduces the bandwidth consumption of the GPU.\r\n> > @davidwendt\r\n> \r\n> _Originally posted by @mjshare in [#12453 (comment)](https://github.com/rapidsai/cudf/issues/12453#issuecomment-1369787656)_\r\n> \r\n> @mjshare , would you be able to provide an example in which Feather vs Arrow makes a significant difference in a workload? Are you working in C++ or Python?\r\n\r\n```\r\n>>> import pyarrow\r\n>>> import numpy as np\r\n>>>\r\n>>> def create_table(n_rows, n_cols):\r\n...     table = pyarrow.Table.from_pydict({f'col_{c}': np.random.randint(0, 10000, size=[n_rows]) for c in range(n_cols)})\r\n...     return table\r\n...\r\n>>>\r\n>>> tbl = create_table(8000 * 10000, 6)\r\n>>> print(f'arrow table size {tbl.nbytes // 1024 // 1024} MB')\r\narrow table size 3662 MB\r\n>>>\r\n>>> import pyarrow.feather as feather\r\n>>> feather.write_feather(tbl, '/tmp/data.feather')\r\n>>>\r\n>>> import os\r\n>>> feather_size = os.path.getsize('/tmp/data.feather')\r\n>>> print(f'feather file size {feather_size // 1024 // 1024} MB')\r\nfeather file size 1755 MB\r\n```\r\n\r\n```\r\n>>> import pyarrow\r\n>>> import numpy as np\r\n>>>\r\n>>> def create_table(n_rows, n_cols):\r\n...     table = pyarrow.Table.from_pydict({f'col_{c}': np.random.randint(0, 10000, size=[n_rows]) for c in range(n_cols)})\r\n...     return table\r\n...\r\n>>>\r\n>>> def create_table_with_str(n_rows, n_cols, n_strs, n_strs_cols):\r\n...     start = time.time()\r\n...     prefix = 'xxxx_' * ((n_strs - 10) // 5)\r\n...     cdf = create_table(n_rows, n_cols).to_pandas()\r\n...     for i in range(n_strs_cols):\r\n...         cdf[f'col_{i}'] = cdf[f'col_{i}'].apply(lambda x: f'{prefix}{x:010}')\r\n...     return pyarrow.Table.from_pandas(cdf)\r\n...\r\n>>>\r\n>>> tbl = create_table_with_str(2000 * 10000, 2, 40, 2)\r\n>>> print(tbl.to_pandas())\r\n                                             col_0                                     col_1\r\n0         xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000008295  xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000007592\r\n1         xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000004599  xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000002469\r\n2         xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000004553  xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000008704\r\n3         xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000004059  xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000003143\r\n4         xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000006622  xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000000062\r\n...                                            ...                                       ...\r\n19999995  xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000009539  xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000002897\r\n19999996  xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000000073  xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000009834\r\n19999997  xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000002616  xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000008283\r\n19999998  xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000001085  xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000005927\r\n19999999  xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000008378  xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000002684\r\n\r\n[20000000 rows x 2 columns]\r\n>>>\r\n>>> print(f'arrow table size {tbl.nbytes // 1024 // 1024} MB')\r\narrow table size 1678 MB\r\n>>>\r\n>>> import pyarrow.feather as feather\r\n>>> feather.write_feather(tbl, '/tmp/data.feather')\r\n>>>\r\n>>> import os\r\n>>> feather_size = os.path.getsize('/tmp/data.feather')\r\n>>> print(f'feather file size {feather_size // 1024 // 1024} MB')\r\nfeather file size 352 MB\r\n>>>\r\n\r\n```","createdAt":"2023-01-04T02:09:47Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11407#issuecomment-1370410575","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5Rru5b","author":{"login":"mjshare"},"authorAssociation":"NONE","body":"> > > Problem description:\r\n> > > When there are a large number of string columns, data in feather format is much smaller than that in arrow format. When data is transmitted to the GPU in feather format for decompression, the PICE bandwidth pressure is much lower. This requirement is valuable. Can the GPU support the feather format for decompression in GPU like parquet?\r\n> > \r\n> > \r\n> > _Originally posted by @mjshare in [#12453 (comment)](https://github.com/rapidsai/cudf/issues/12453#issue-1514306164)_\r\n> > > Similar requirements, Does the community have a solution plan? This reduces the bandwidth consumption of the GPU.\r\n> > > @davidwendt\r\n> > \r\n> > \r\n> > _Originally posted by @mjshare in [#12453 (comment)](https://github.com/rapidsai/cudf/issues/12453#issuecomment-1369787656)_\r\n> > @mjshare , would you be able to provide an example in which Feather vs Arrow makes a significant difference in a workload? Are you working in C++ or Python?\r\n> \r\n> ```\r\n> >>> import pyarrow\r\n> >>> import numpy as np\r\n> >>>\r\n> >>> def create_table(n_rows, n_cols):\r\n> ...     table = pyarrow.Table.from_pydict({f'col_{c}': np.random.randint(0, 10000, size=[n_rows]) for c in range(n_cols)})\r\n> ...     return table\r\n> ...\r\n> >>>\r\n> >>> tbl = create_table(8000 * 10000, 6)\r\n> >>> print(f'arrow table size {tbl.nbytes // 1024 // 1024} MB')\r\n> arrow table size 3662 MB\r\n> >>>\r\n> >>> import pyarrow.feather as feather\r\n> >>> feather.write_feather(tbl, '/tmp/data.feather')\r\n> >>>\r\n> >>> import os\r\n> >>> feather_size = os.path.getsize('/tmp/data.feather')\r\n> >>> print(f'feather file size {feather_size // 1024 // 1024} MB')\r\n> feather file size 1755 MB\r\n> ```\r\n> \r\n> ```\r\n> >>> import pyarrow\r\n> >>> import numpy as np\r\n> >>>\r\n> >>> def create_table(n_rows, n_cols):\r\n> ...     table = pyarrow.Table.from_pydict({f'col_{c}': np.random.randint(0, 10000, size=[n_rows]) for c in range(n_cols)})\r\n> ...     return table\r\n> ...\r\n> >>>\r\n> >>> def create_table_with_str(n_rows, n_cols, n_strs, n_strs_cols):\r\n> ...     start = time.time()\r\n> ...     prefix = 'xxxx_' * ((n_strs - 10) // 5)\r\n> ...     cdf = create_table(n_rows, n_cols).to_pandas()\r\n> ...     for i in range(n_strs_cols):\r\n> ...         cdf[f'col_{i}'] = cdf[f'col_{i}'].apply(lambda x: f'{prefix}{x:010}')\r\n> ...     return pyarrow.Table.from_pandas(cdf)\r\n> ...\r\n> >>>\r\n> >>> tbl = create_table_with_str(2000 * 10000, 2, 40, 2)\r\n> >>> print(tbl.to_pandas())\r\n>                                              col_0                                     col_1\r\n> 0         xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000008295  xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000007592\r\n> 1         xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000004599  xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000002469\r\n> 2         xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000004553  xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000008704\r\n> 3         xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000004059  xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000003143\r\n> 4         xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000006622  xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000000062\r\n> ...                                            ...                                       ...\r\n> 19999995  xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000009539  xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000002897\r\n> 19999996  xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000000073  xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000009834\r\n> 19999997  xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000002616  xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000008283\r\n> 19999998  xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000001085  xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000005927\r\n> 19999999  xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000008378  xxxx_xxxx_xxxx_xxxx_xxxx_xxxx_0000002684\r\n> \r\n> [20000000 rows x 2 columns]\r\n> >>>\r\n> >>> print(f'arrow table size {tbl.nbytes // 1024 // 1024} MB')\r\n> arrow table size 1678 MB\r\n> >>>\r\n> >>> import pyarrow.feather as feather\r\n> >>> feather.write_feather(tbl, '/tmp/data.feather')\r\n> >>>\r\n> >>> import os\r\n> >>> feather_size = os.path.getsize('/tmp/data.feather')\r\n> >>> print(f'feather file size {feather_size // 1024 // 1024} MB')\r\n> feather file size 352 MB\r\n> >>>\r\n> ```\r\n\r\nIn typical scenarios, an integer can be compressed to 1/2, and a character string can be compressed to 1/4.","createdAt":"2023-01-04T02:28:00Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11407#issuecomment-1370418779","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5Rr9lM","author":{"login":"beckernick"},"authorAssociation":"MEMBER","body":"Writing the file out to Feather will compress the data using LZ4 by default. Writing out to Parquet will compress the data using snappy by default, but you can select LZ4 if it matters for your dataset / use case.\r\n\r\ncuDF providers accelerated readers for Parquet. Could you use that for your use case?","createdAt":"2023-01-04T04:35:16Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11407#issuecomment-1370478924","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5RsdID","author":{"login":"infzo"},"authorAssociation":"NONE","body":"> Writing the file out to Feather will compress the data using LZ4 by default. Writing out to Parquet will compress the data using snappy by default, but you can select LZ4 if it matters for your dataset / use case.\r\n> \r\n> cuDF providers accelerated readers for Parquet. Could you use that for your use case?\r\n\r\n```\r\n>>> def create_table(n_rows, n_cols):\r\n...     table = pyarrow.Table.from_pydict({f'col_{c}': np.random.randint(0, 10000, size=[n_rows]) for c in range(n_cols)})\r\n...     return table\r\n...\r\n>>> def create_table_with_str(n_rows, n_cols, n_strs, n_strs_cols):\r\n...     prefix = 'xxxx_' * ((n_strs - 10) // 5)\r\n...     cdf = create_table(n_rows, n_cols).to_pandas()\r\n...     for i in range(n_strs_cols):\r\n...         cdf[f'col_{i}'] = cdf[f'col_{i}'].apply(lambda x: f'{prefix}{x:010}')\r\n...     return pyarrow.Table.from_pandas(cdf)\r\n...\r\n>>> tbl = create_table_with_str(2000 * 10000, 2, 40, 2)\r\n>>> print(f'arrow table size {tbl.nbytes // 1024 // 1024} MB')\r\narrow table size 1678 MB\r\n>>>\r\n>>> def arrow_cudf():\r\n...     start = time.time()\r\n...     df = cudf.DataFrame.from_arrow(tbl)\r\n...     print(f'arrow_cudf cost {time.time() - start} s')\r\n...\r\n>>> arrow_cudf()\r\narrow_cudf cost 0.3969125747680664 s\r\n>>>\r\n>>> arrow_cudf()\r\narrow_cudf cost 0.39836621284484863 s\r\n>>>\r\n>>> def feather():\r\n...     import pyarrow.feather as feather\r\n...     start = time.time()\r\n...     feather.write_feather(tbl, '/tmp/data.feather')\r\n...     print(f'write_feather cost {time.time() - start} s')\r\n...     feather_size = os.path.getsize('/tmp/data.feather')\r\n...     print(f'feather file size {feather_size // 1024 // 1024} MB')\r\n...     start = time.time()\r\n...     df = cudf.read_feather('/tmp/data.feather')\r\n...     print(f'read_feather cost {time.time() - start} s')\r\n...\r\n>>> feather()\r\nwrite_feather cost 0.8628420829772949 s\r\nfeather file size 352 MB\r\nread_feather cost 1.010714054107666 s\r\n>>>\r\n>>> feather()\r\nwrite_feather cost 0.8496232032775879 s\r\nfeather file size 352 MB\r\nread_feather cost 1.0259618759155273 s\r\n>>>\r\n>>> def parquet():\r\n...     start = time.time()\r\n...     parquet.write_table(tbl, '/tmp/data.parquet', compression='LZ4')\r\n...     print(f'write_parquet cost {time.time() - start} s')\r\n...     parquet_size = os.path.getsize('/tmp/data.parquet')\r\n...     print(f'parquet file size {parquet_size // 1024 // 1024} MB')\r\n...     start = time.time()\r\n...     df = cudf.read_parquet('/tmp/data.parquet')\r\n...     print(f'read_parquet cost {time.time() - start} s')\r\n...     start = time.time()\r\n...     df.to_parquet('/tmp/data_second.parquet')\r\n...     print(f'to_parquet cost {time.time() - start} s')\r\n...\r\n>>> parquet()\r\nwrite_parquet cost 2.4992175102233887 s\r\nparquet file size 67 MB\r\nread_parquet cost 0.03415870666503906 s\r\nto_parquet cost 0.05026721954345703 s\r\n>>>\r\n>>> parquet()\r\nwrite_parquet cost 2.4414985179901123 s\r\nparquet file size 67 MB\r\nread_parquet cost 0.034529924392700195 s\r\nto_parquet cost 0.05629706382751465 s\r\n>>>\r\n\r\n#################################\r\narrow table size 1678 MB\r\n\r\narrow   -> cudf         0.39    s\r\n\r\narrow   -> feather      0.84    s\r\nfeather -> cudf         1.02    s\r\n\r\narrow   -> parquet      2.4     s\r\nparquet -> cudf         0.034   s  \r\ncudf    -> parquet      0.05    s\r\n```\r\n\r\n1. It takes too long to output the arrow as parquet. If feather supports gpu read, can we combine the read and write advantages to achieve better conversion performance?\r\n2. Can the arrow-to-cudf conversion be pre-compressed and then transmitted to the GPU to reduce bandwidth pressure and improve transmission efficiency?\r\n\r\n","createdAt":"2023-01-04T08:21:19Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11407#issuecomment-1370608131","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5RvNuN","author":{"login":"vyasr"},"authorAssociation":"CONTRIBUTOR","body":"What about doing arrow->cudf->parquet? Based on your timings that looks to be nearly optimal in terms of both timing and compression.","createdAt":"2023-01-04T19:28:14Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11407#issuecomment-1371331469","viewerDidAuthor":false}],"createdAt":"2022-07-29T20:32:10Z","id":"I_kwDOBWUGps5O1hm5","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NjE=","name":"feature request","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwxMDEzOTg3MzUy","name":"0 - Backlog","description":"In queue waiting for assignment","color":"d4c5f9"},{"id":"MDU6TGFiZWwxMTM5NzQwNjY2","name":"libcudf","description":"Affects libcudf (C++/CUDA) code.","color":"c5def5"},{"id":"MDU6TGFiZWwxMTg1MjQ0MTQy","name":"cuIO","description":"cuIO issue","color":"fef2c0"}],"milestone":null,"number":11407,"projectCards":[{"project":{"name":"Feature Planning"},"column":{"name":"Needs prioritizing"}}],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[FEA] GPU-accelerated Feather support.","updatedAt":"2023-06-07T20:39:24Z","url":"https://github.com/rapidsai/cudf/issues/11407"}
