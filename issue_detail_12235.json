{"assignees":[],"author":{"id":"MDQ6VXNlcjMxOTA0MDU=","is_bot":false,"login":"shwina","name":"Ashwin Srinath"},"body":"We should move to using an RMM managed memory pool by default.\r\n\r\nThis was brought up before in https://github.com/rapidsai/cudf/issues/2676. In response to that issue, we implemented `set_allocator`, https://github.com/rapidsai/cudf/pull/2682, but we chose not to enable the RMM pool by default (likely because we didn't want to monopolize GPU memory away from other libraries). \r\n\r\nSince then, CuPy, Numba (and soon [PyTorch](https://github.com/pytorch/pytorch/pull/86786)) all can be configured to use RMM, and therefore share the same memory pool as cuDF.\r\n\r\n## Proposal\r\n\r\nConcretely, the proposal is that `import cudf` will:\r\n\r\n* Set RMM's default memory resource to a [pool memory resource](https://docs.rapids.ai/api/rmm/stable/api.html#rmm.mr.PoolMemoryResource)\r\n* Configure CuPy, Numba, (and PyTorch?) to all use RMM's default memory resource\r\n\r\n## What should the initial and maximum pool size be? \r\n\r\nAn RMM pool can be configured with an initial and maximum pool size. The pool grows according to an implementation-defined strategy (see [here](https://github.com/rapidsai/rmm/blob/d132e5236b444d2dcdae25e846c4fe4d5651ee79/include/rmm/mr/device/pool_memory_resource.hpp#L246-L249) for the current strategy).\r\n\r\n- As we cannot assume that all (or any) GPU memory is available when cuDF is imported, the initial pool size should be 0 bytes.\r\n- The only reasonable maximum pool size I can think of is the maximum available GPU memory. If the pool cannot expand to this size because of allocations made outside of RMM, so be it: we will OOM. \r\n\r\n## What happens if `import cudf` appears in the middle of the program?\r\n\r\nAll this works well if `import cudf` appears at the beginning of the program, i.e., before any device memory is actually allocated by any library). However, if it appears _after_ some device objects have already been allocated, it can lead to early out-of-memory errors. As an example, consider some code that uses both PyTorch and cuDF in the following way:\r\n\r\n```python\r\nimport torch\r\n \r\n# this part of the code uses PyTorch\r\n\r\nimport cudf\r\n\r\n# this part of the code uses cudf\r\n```\r\n\r\nBecause PyTorch [uses a caching allocator](https://pytorch.org/docs/stable/notes/cuda.html#memory-management), a memory pool already exists by the time we import cuDF. Importing cuDF initializes a second pool that all libraries (including PyTorch) will use going forward. The first pool essentially becomes a kind of dead space: no new device objects are ever allocated within the pool, and no device memory is ever freed from it. \r\n\r\nThere's no perfect solution I can think of to this particular problem, but it's probably a good idea to call [`empty_cache()`](https://pytorch.org/docs/stable/generated/torch.cuda.empty_cache.html#torch.cuda.empty_cache) before resetting the PyTorch allocator to minimize the amount of wasted memory.\r\n\r\n---\r\n\r\nThat's just one example of the kind of issues that can arise if `import cudf` appears later. I think it's fine to assume this will be less common than importing it at the beginning.","closed":false,"closedAt":null,"comments":[{"id":"IC_kwDOBWUGps5PWENy","author":{"login":"vyasr"},"authorAssociation":"CONTRIBUTOR","body":"Some thoughts:\r\n- cuda-python/pynvml have APIs for checking total device memory usage (e.g. `cudaMemGetInfo`). Should we inject some checks at import time to ensure that the entire GPU is available, or maybe print some initial stats on the max pool size based on available GPU memory?\r\n- Can we wrap the allocator in an adapter that checks (on OOM) whether memory has been allocated outside of the pool (perhaps just using a naive metric like `total_gpu_mem - pool_size`) and provides the user a more useful error?\r\n- I'm always wary of making implicit and far-reaching changes like this silently. Should we consider adding some notification on `import cudf` that indicates that such a change is being made? I _also_ don't like libraries that print information (especially warnings) unconditionally on import, so I don't particularly like this suggestion, just raising the possibility. Alternatively it could be set up via logging of some sort. I think we can safely assume that no matter how we broadcast this some users will not read the docs/notification/warning/etc and be surprised at the behavior, but hopefully (and I think it's likely that) more people are happy about the pool usage improving default performance.\r\n- Similar to the PyTorch example where cudf is imported later, could this change potentially break scripts that are using rmm to allocate memory prior to the cudf import? I believe that the answer is no since we made the necessary changes to save an allocator used for any allocation such that `set_allocator` won't make accessing allocations using the prior allocator UB, just want to be sure.","createdAt":"2022-11-29T19:26:25Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12235#issuecomment-1331184498","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5PWPIB","author":{"login":"shwina"},"authorAssociation":"CONTRIBUTOR","body":"> cuda-python/pynvml have APIs for checking total device memory usage (e.g. cudaMemGetInfo). Should we inject some checks at import time to ensure that the entire GPU is available, or maybe print some initial stats on the max pool size based on available GPU memory?\r\n\r\nI don't know what (if anything) this does for us, and perhaps my concerns about the `max_pool_size` were unfounded to begin with. I think the best option is simply not to set a maximum pool size and let the pool grow to whatever size it can. This is what happens today if you do `cudf.set_allocator(pool=True)`. Setting the `max_pool_size` to the amount of memory available at import time may be a bad idea because additional memory might be freed up later. \r\n\r\n> Can we wrap the allocator in an adapter that checks (on OOM) whether memory has been allocated outside of the pool (perhaps just using a naive metric like total_gpu_mem - pool_size) and provides the user a more useful error?\r\n\r\nWe can. I'm not opposed to it, but what can the user do with that information? When the GPU is e.g., being shared by multiple processes, it may even be misleading to say that their code is allocating memory outside of RMM's knowledge.\r\n\r\n> Should we consider adding some notification on import cudf that indicates that such a change is being made?\r\n\r\nGiven that other libraries like CuPy and PyTorch also use a pool by default, I don't know if this is very controversial. I want to say that 99% of users won't ever know or care. \r\n\r\n> could this change potentially break scripts that are using rmm to allocate memory prior to the cudf import? \r\n\r\nI don't believe so (especially if we choose an initial pool size of 0). You are right that we keep a reference to the previous memory resource around so there's no danger/UB.","createdAt":"2022-11-29T20:02:54Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/12235#issuecomment-1331229185","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5YoGN_","author":{"login":"vyasr"},"authorAssociation":"CONTRIBUTOR","body":"This question came up again today. Personally I tend to be against this sort of implicit behavior and prefer that it always be opt-in, but it seems like many feel that the benefit to users outweighs potential risks (but not all, CC @wence- and @pentschev). That said, increasing cudf import times is a concrete cost that I probably wouldn't be willing to stomach for pytorch, whereas for cupy and numba we are importing them already so the question is more of a philosophical one of whether or not we should have side effects to importing cudf.","createdAt":"2023-03-28T13:37:53Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12235#issuecomment-1486906239","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5YoMrm","author":{"login":"wence-"},"authorAssociation":"CONTRIBUTOR","body":"In 23.04, importing RMM will no longer hook up cupy, numba, and pytorch to use the RMM allocators (that was https://github.com/rapidsai/rmm/pull/1221).\r\n\r\nThat was mostly done for import time reduction, but it also has the consequence that importing rmm doesn't modify the stateful allocation environment.\r\n\r\nWithin cudf, since we _use_ cupy and numba allocated data in some places, it makes sense to configure those libraries (when imported) to use the same allocator that cudf uses. For now at least, pytorch is not imported by cudf, so it's not as critical that we configure things.\r\n\r\nThat said, if we do go with pool-based allocation by default, then the likelihood of early (not actually) OOM does go up. My feeling is that we should solve this with a \"best practices for interop\" guide rather than trying to pin down all the possible libraries that might interoperate on device memory with cudf.\r\n\r\nIn general I tend not to like libraries that configure themselves into too many third-party systems (it always feels more \"framework\" than \"library\").","createdAt":"2023-03-28T13:52:11Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":3}}],"url":"https://github.com/rapidsai/cudf/issues/12235#issuecomment-1486932710","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5YpvM1","author":{"login":"VibhuJawa"},"authorAssociation":"MEMBER","body":"There are two things here we are discussing here. \r\n\r\n1. Using RMM Pool by default \r\n\r\nThis comes down to choosing perf over potential memory contention from other libraries .  \r\n\r\nIn all my interactions with customers when they are just using `cuDF` without `dask` i have never seen them use `rmm pool`. I agree with the technical challenges of doing this but the actual impact of this is 99% of customers will never use POOL and we will have to be fine with it.     \r\n\r\n\r\n2. Configuring Pytorch to use rmm torch allocator. \r\n\r\nRegarding 2,  My fear is that most users wont know if this is configurable  and I think there is also no  clean way for us to inform user when they run into OOMs .  \r\n\r\nThat said , we cant probably use the RMM Pool cleanly right now with Pytorch as that can lead to fragmentation. We will have to probably use the `async ` memory allocator. \r\n\r\nExample of what a user will have to configure currently is below: \r\n```\r\n    import cudf\r\n    import cupy\r\n    import rmm\r\n\r\n    mr = rmm.mr.CudaAsyncMemoryResource()\r\n    rmm.mr.set_current_device_resource(mr)\r\n    torch.cuda.memory.change_current_allocator(rmm.rmm_torch_allocator)\r\n    cupy.cuda.set_allocator(rmm.allocators.cupy.rmm_cupy_allocator)\r\n```    \r\n\r\n","createdAt":"2023-03-28T17:33:12Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12235#issuecomment-1487336245","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5Ypy8f","author":{"login":"jakirkham"},"authorAssociation":"MEMBER","body":"On 2, there's at least [a blogpost]( https://medium.com/rapids-ai/pytorch-rapids-rmm-maximize-the-memory-efficiency-of-your-workflows-f475107ba4d4 ) now. Maybe we can do more to promote that?","createdAt":"2023-03-28T17:44:43Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12235#issuecomment-1487351583","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5Yp1VS","author":{"login":"VibhuJawa"},"authorAssociation":"MEMBER","body":"> On 2, there's at least [a blogpost](https://medium.com/rapids-ai/pytorch-rapids-rmm-maximize-the-memory-efficiency-of-your-workflows-f475107ba4d4) now. Maybe we can do more to promote that?\r\n\r\nI agree that blog post is what I link so that is very useful indeed. \r\n\r\nI think having a dedicated place in the  cuDF docs for it will be great as a lot of people who use cuDF  would probably not know what RMM Pool is (similar to people using Pytorch not knowing about memory [management](https://pytorch.org/docs/stable/cuda.html#memory-management) ) so we will need a way to link it from cuDF docs too.  ","createdAt":"2023-03-28T17:52:26Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12235#issuecomment-1487361362","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5m6bj3","author":{"login":"harrism"},"authorAssociation":"MEMBER","body":"The first line of the PR description says:\r\n\r\n> using an RMM managed memory pool \r\n\r\nBe careful. \"managed memory pool\" implies a `pool_memory_resource` with a `managed_memory_resource` (aka Unified Memory, `cudaMallocManaged`) as upstream. Are you saying that should be the default? Or do you mean a `pool_memory_resource` with `cuda_memory_resource` (`cudaMalloc`) as upstream?","createdAt":"2023-09-19T21:54:06Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12235#issuecomment-1726593271","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5718gK","author":{"login":"GregoryKimball"},"authorAssociation":"CONTRIBUTOR","body":"There is a great discussion in this issue so far, and it (implicitly) has mostly referred to a default _unmanaged_ (device-only) pool. The issue of a default memory resource is coming up again in the context of preventing OOM errors for the single workstation, beginner cuDF user, where a default _managed_ memory resource could be a good solution. \r\n\r\nI'm interested to learn more about the default pool behavior in cupy and pytorch as mentioned [above](https://github.com/rapidsai/cudf/issues/12235#issuecomment-1331229185). I'm also interested to learn more about how experienced users and other libraries could automatically be opted-out of a default managed resource, while still providing this feature automatically to the beginner user.\r\n\r\nPerhaps we could have a hook on the first column allocation that checks if RMM has ever had `reinitialize` called, and if not, create a managed pool with initial size zero. Then when the pool grows we can add a hook to check the available system memory and throw an OOM exception well before completely locking up the system.","createdAt":"2024-04-25T16:52:17Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12235#issuecomment-2077739018","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps574wd1","author":{"login":"harrism"},"authorAssociation":"MEMBER","body":"Another concern I have in this issue is the recommendation to start with a pool of 0 bytes. With the current pool implementation this will create a lot of fragmentation.","createdAt":"2024-04-26T01:36:00Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12235#issuecomment-2078476149","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps579v35","author":{"login":"vyasr"},"authorAssociation":"CONTRIBUTOR","body":"See also #14498","createdAt":"2024-04-26T17:18:15Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12235#issuecomment-2079784441","viewerDidAuthor":false}],"createdAt":"2022-11-23T16:49:48Z","id":"I_kwDOBWUGps5XJbKQ","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NjE=","name":"feature request","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwxMTM5NzQxMjEz","name":"cuDF (Python)","description":"Affects Python cuDF API.","color":"1d76db"}],"milestone":null,"number":12235,"projectCards":[{"project":{"name":"Feature Planning"},"column":{"name":"Needs prioritizing"}}],"projectItems":[],"reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":2}},{"content":"HEART","users":{"totalCount":2}},{"content":"ROCKET","users":{"totalCount":1}},{"content":"EYES","users":{"totalCount":1}}],"state":"OPEN","title":"[FEA] Use RMM memory pool by default","updatedAt":"2024-04-26T17:18:16Z","url":"https://github.com/rapidsai/cudf/issues/12235"}
