{"assignees":[{"id":"MDQ6VXNlcjQ1Nzk1OTkx","login":"davidwendt","name":"David Wendt"}],"author":{"id":"MDQ6VXNlcjQ4Mzc1NzE=","is_bot":false,"login":"VibhuJawa","name":"Vibhu Jawa"},"body":"**Is your feature request related to a problem? Please describe.**\r\n\r\nWe should add byte pair encoding tokenizer to cuDF.  Like  our [subword-tokenizer](https://docs.rapids.ai/api/cudf/21.12/api_docs/api/cudf.core.subword_tokenizer.SubwordTokenizer.__call__.html) adds  a bridge to Bert link models.  Byte Pair EncodingTokenizer is used by `roberta`, `gpt-2` , `gpt-3` and will give us a bridge to a lot of DL models. \r\n\r\nWe should focus porting a pre-trained tokenizer first. \r\n\r\n\r\n**Describe the solution you'd like**\r\n\r\nThe implimentation should follow [GPT-2 tokenizer ](https://huggingface.co/transformers/_modules/transformers/models/gpt2/tokenization_gpt2.html#GPT2Tokenizer) but should be extendable to the robert-a , `gpt-3`, `megatron` etc.  We should follow the HuggingFace API for this. \r\n\r\n**Algorithim:**\r\n\r\n\r\n1. Add an identifier `(</w>)` at the end of each word to identify the end of a word and then calculate the word frequency in the text.\r\n2. Split the word into characters and then calculate the character frequency.\r\n3. From the character tokens, for a predefined number of iterations, count the frequency of the consecutive byte pairs and merge the most frequently occurring byte pairing.\r\n4. Keep iterating until you have reached the iteration limit (set by you) or until you have reached the token limit.\r\n\r\nRef: [Link](https://www.freecodecamp.org/news/evolution-of-tokenization/)\r\n\r\n\r\n**Additional context**\r\n\r\nBest Explanation of Algorithm: https://leimao.github.io/blog/Byte-Pair-Encoding/\r\n\r\n\r\nCC: @randerzander , @beckernick \r\n\r\n","closed":false,"closedAt":null,"comments":[{"id":"IC_kwDOBWUGps46B_YC","author":{"login":"VibhuJawa"},"authorAssociation":"MEMBER","body":"We will probably need a libcudf  implementation of the following function BPE function .  (See HF [reference](https://github.com/huggingface/transformers/blob/b24ead87e1be6bce17e4ec5c953b6d028e4b3af7/src/transformers/models/gpt2/tokenization_gpt2.py#L202-L242) implementation)  . \r\n\r\nHere given the rank of each bigram we combine the most occuring bigram based on the rank provided in merges file.  Once we have that we then convert it into  token id using the vocabulary provided. \r\n\r\n\r\n### Actual Algorithim: \r\n```python\r\n\r\n\r\ndef bpe(token, bpe_ranks):\r\n    # if token in self.cache:\r\n    #     return self.cache[token]\r\n    word = tuple(token)\r\n    pairs = get_pairs(word)\r\n\r\n    if not pairs:\r\n        return token\r\n\r\n    while True:\r\n        bigram = min(pairs, key=lambda pair: bpe_ranks.get(pair, float(\"inf\")))\r\n        #print(bigram)\r\n        \r\n        if bigram not in bpe_ranks:\r\n            break\r\n        first, second = bigram\r\n        new_word = []\r\n        i = 0\r\n        while i < len(word):\r\n            try:\r\n                j = word.index(first, i)\r\n            except ValueError:\r\n                new_word.extend(word[i:])\r\n                break\r\n            else:\r\n                new_word.extend(word[i:j])\r\n                i = j\r\n\r\n            if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\r\n                new_word.append(first + second)\r\n                i += 2\r\n            else:\r\n                new_word.append(word[i])\r\n                i += 1\r\n        new_word = tuple(new_word)\r\n        word = new_word\r\n        if len(word) == 1:\r\n            break\r\n        else:\r\n            pairs = get_pairs(word)\r\n            print(pairs)\r\n    word = \" \".join(word)\r\n    #self.cache[token] = word\r\n    return word\r\n\r\n   def get_pairs(word):\r\n    \"\"\"\r\n    Return set of symbol pairs in a word.\r\n\r\n    Word is represented as tuple of symbols (symbols being variable-length strings).\r\n    \"\"\"\r\n    pairs = set()\r\n    prev_char = word[0]\r\n    for char in word[1:]:\r\n        pairs.add((prev_char, char))\r\n        prev_char = char\r\n    return pairs\r\n\r\n```\r\n### Example Call\r\n```python\r\n# wget https://huggingface.co/gpt2/raw/main/merges.txt \r\n# to get this file\r\n\r\nmerges_file = 'gpt_2_tokenizer/merges.txt'\r\nwith open(merges_file, encoding=\"utf-8\") as merges_handle:\r\n    bpe_merges = merges_handle.read().split(\"\\n\")[1:-1]\r\nbpe_merges = [tuple(merge.split()) for merge in bpe_merges]\r\nbpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\r\n\r\nbpe(\"Thisisit\", bpe_ranks)\r\n```\r\n\r\n```\r\n'This is it'\r\n```\r\n","createdAt":"2021-11-19T00:47:37Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/9657#issuecomment-973600258","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps46B_dx","author":{"login":"VibhuJawa"},"authorAssociation":"MEMBER","body":"CC: @davidwendt  for awareness. ","createdAt":"2021-11-19T00:47:48Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/9657#issuecomment-973600625","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps46hxhJ","author":{"login":"meghmak13"},"authorAssociation":"NONE","body":"There is a need for the aforementioned feature request as we currently only support tokenization for BERT. As especially considering newer architectures like RoBERTa, GPT, T5 are getting adopted.  ","createdAt":"2021-11-29T19:10:19Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/9657#issuecomment-981932105","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps46qXAT","author":{"login":"VibhuJawa"},"authorAssociation":"MEMBER","body":"\r\n**Basic Algo:**\r\n1.  Basic pre-processing like space cleanup, utf-8 decoding. \r\n2.  Tokenize  each sentence based on a delimiter \r\n3.   Call BPE on each token to further tokenize it \r\n4.   Find the numeric representation of each token in the provided vocabulary \r\n5.   Pad according to the provided padding and return the `input_ids` which are essentially the key look up from  the vocabulary table  \r\n6.    Also return the `attention_masks` which are a binary tensor indicating the position of the padded indices so that the model does not attend to them.\r\n\r\n** Extra Notes **\r\nWe will have to add stuff like padding and strides similar to what we have for the Subword tokenizer.  \r\n\r\n\r\n**Python code to show this in action**\r\n```python\r\nfrom transformers import GPT2Tokenizer\r\nimport pandas as pd\r\nimport json\r\n\r\n# !wget https://huggingface.co/gpt2/raw/main/vocab.json\r\n# !wget https://huggingface.co/gpt2/raw/main/merges.txt\r\nwith open('vocab.json') as f:\r\n    token_to_id = json.load(f)\r\n    id_to_token = {v: k for k, v in token_to_id.items()}\r\n    \r\ntext_ser = [\"This is test-sentence-1\", \"This is test sentence-2\", \"This-is test sentence 3\"]\r\ntokenizer = GPT2Tokenizer(vocab_file = 'vocab.json', merges_file='merges.txt')\r\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\r\nencoded_batch = tokenizer.batch_encode_plus(text_ser,\r\n                                            return_tensors='np',\r\n                                            truncation=True, \r\n                                            padding='max_length',\r\n                                            max_length=12)\r\n\r\n\r\n\r\n\r\nprint(\"BPE output\", [tokenizer.bpe(token) for token in text_ser[0].split(' ')])\r\n\r\nprint(\"tokenizer-output-with-not=cleaned-up-special-token \", [id_to_token.get(i, '[PAD]') for i in encoded_batch['input_ids'][0]])\r\nprint(\"tokenizer-output-cleaned-up\", [tokenizer.decode(i) for i in encoded_batch['input_ids'][0]])\r\nprint(\"Final Output of tokenizer: \", encoded_batch['input_ids'][0])\r\n\r\nprint(\"\\n\"+\"*\"*50+\"\\n\")\r\nprint(\"Batched Output\")\r\nprint(\"Final Output of tokenizer:\\n\", encoded_batch['input_ids'])\r\n```\r\n\r\n```python\r\nBPE output ['This', 'is', 'test - sent ence - 1']\r\ntokenizer-output-with-not=cleaned-up-special-token  ['This', 'Ġis', 'Ġtest', '-', 'sent', 'ence', '-', '1', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\r\ntokenizer-output-cleaned-up ['This', ' is', ' test', '-', 'sent', 'ence', '-', '1', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\r\nFinal Output of tokenizer:  [ 1212   318  1332    12 34086   594    12    16 50257 50257 50257 50257]\r\n\r\n**************************************************\r\n\r\nBatched Output\r\nFinal Output of tokenizer:\r\n [[ 1212   318  1332    12 34086   594    12    16 50257 50257 50257 50257]\r\n [ 1212   318  1332  6827    12    17 50257 50257 50257 50257 50257 50257]\r\n [ 1212    12   271  1332  6827   513 50257 50257 50257 50257 50257 50257]]\r\n\r\n```\r\n\r\n\r\nCC: @davidwendt ","createdAt":"2021-12-02T00:32:33Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/9657#issuecomment-984182803","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps47z-eR","author":{"login":"github-actions"},"authorAssociation":"NONE","body":"This issue has been labeled `inactive-30d` due to no recent activity in the past 30 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed. This issue will be labeled `inactive-90d` if there is no activity in the next 60 days.","createdAt":"2022-01-01T01:26:57Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/9657#issuecomment-1003480977","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps48pLP4","author":{"login":"teju85"},"authorAssociation":"MEMBER","body":"Has anyone been working on this? Or has this been prioritized for anytime soon? In the past week I got/saw requests for this at a couple of places.","createdAt":"2022-01-20T12:01:15Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/9657#issuecomment-1017426936","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps48paXE","author":{"login":"davidwendt"},"authorAssociation":"CONTRIBUTOR","body":"> Has anyone been working on this? Or has this been prioritized for anytime soon? In the past week I got/saw requests for this at a couple of places.\r\n\r\nI've not worked on it yet but I hope to start on it in 22.04.","createdAt":"2022-01-20T13:08:20Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"HEART","users":{"totalCount":3}}],"url":"https://github.com/rapidsai/cudf/issues/9657#issuecomment-1017488836","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps49VR6W","author":{"login":"davidwendt"},"authorAssociation":"CONTRIBUTOR","body":"@VibhuJawa Some questions based on the examples given here. You want a BPE function that takes a host string (and the merge/rank table) and returns the BPE as a host string?\r\n\r\nThis shows passing in a word (substring of a string) and returning the BPE and then the Python code builds an array of BPE strings from each token.\r\n```\r\ntext_ser = [\"This is test-sentence-1\", \"This is test sentence-2\", \"This-is test sentence 3\"]\r\n...\r\nprint(\"BPE output\", [tokenizer.bpe(token) for token in text_ser[0].split(' ')])\r\n```\r\n\r\nThe [`Thisisit`](https://github.com/rapidsai/cudf/issues/9657#issuecomment-973600258) example showed the same thing -- single host string returns a single host string.\r\n\r\nI'm trying to understand the inputs and outputs from a cudf usecase. Are you expecting give the libcudf BPE API a strings column of words and return the encoding of each as a strings column? \r\n\r\nOr do I have this all wrong and you are expecting a libcudf API that does everything `GPT2Tokenizer` is doing in the last example above?","createdAt":"2022-02-03T13:25:18Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/9657#issuecomment-1028988566","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps49XRCw","author":{"login":"davidwendt"},"authorAssociation":"CONTRIBUTOR","body":"For reference: https://gist.github.com/VibhuJawa/8df50cd638d3d98f36109d8316dfa4ad","createdAt":"2022-02-03T23:52:08Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/9657#issuecomment-1029509296","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps49XX1c","author":{"login":"VibhuJawa"},"authorAssociation":"MEMBER","body":"**On the vocab front**\r\n\r\n I tried to verify if we can indeed treat the `vocab.json` files similar to how we treat `vocab` in Subword tokenizer and  i think we can but there are three main discrepancies i found.  \r\n\r\n\r\n**Similarity:** The vocab dict is a continuous range of ints mapping to a tokens. \r\n\r\nVerified that across the commonly used models the `token->id dict` can be treated as a list as there are no missing ids (Its a continuous range ) like `subword` tokenizer vocabulary. \r\n\r\nBelow for the verification reference: \r\nhttps://gist.github.com/VibhuJawa/1670178d07d9659a084a8fbe7d160d23\r\n\r\n**Discrepancy:** \r\n\r\n1.  **Special Tokens:** \r\nMost BPE models have these special tokens \r\n```\r\n'<s>', '</s>', '<unk>', '<pad>', '<mask>'\r\n```\r\nbut can also include something like \r\n```\r\n'<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>',\r\n```\r\n\r\nwhile  subword one mostly has [these](https://github.com/rapidsai/cudf/blob/85109e6f1ed3e3066d8bd765ed87786b3f460bd7/cpp/src/text/subword/wordpiece_tokenizer.cu#L151):\r\n```\r\n[BOS],[EOS],[UNK],[SEP],[PAD],[CLS],[MASK];\r\n```\r\n\r\nI think it might make sense to make this configurable from the python API that we will initialize with the right defaults. \r\n\r\n**2. Padding Token:**\r\n\r\nPadding token's id is dependent on dictionary (id of `<pad>`) so its value can change. We should ensure we handle that correctly.\r\n\r\nI think (unsure) but we just treat it as `0` currently in Subword. \r\n\r\n3.  Treating space characters.\r\n\r\nBPE seems to treat space characters differently . That is ` Hello world` and `Hello world` get mapped [differently](https://github.com/huggingface/transformers/blob/c962c2adbff678ae6d2e98378bed5b8d1a9831d9/src/transformers/models/roberta/tokenization_roberta.py#L69-L73).  \r\n\r\nWhen there is space before the word it gets mapped to ` ĠHello` and if no space  to `Hello` . \r\n\r\n```python\r\nfrom transformers import AutoTokenizer\r\ntokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\r\n\r\nid_to_token = {v: k for k, v in tokenizer.vocab.items()}\r\n\r\nno_space_hello = \"Hello world\"\r\nno_space_input_ids = tokenizer(no_space_hello ,add_special_tokens=False)['input_ids']\r\nprint(no_space_input_ids)\r\nprint([id_to_token[i] for i in no_space_input_ids])\r\nprint(\"----\"*10)\r\nspace_hello = \" Hello world\"\r\nspace_input_ids = tokenizer(space_hello ,add_special_tokens=False)['input_ids']\r\nprint(space_input_ids)\r\nprint([id_to_token[i] for i in space_input_ids])\r\n \r\n```\r\n\r\n```\r\n[31414, 232]\r\n['Hello', 'Ġworld']\r\n----------------------------------------\r\n[20920, 232]\r\n['ĠHello', 'Ġworld']\r\n```\r\n\r\n**On getting a testable example to you.** \r\n\r\nSorry on  getting a meaningful end to end  python example that works across models.  It turns out to be tougher than I anticipated but will update here once i have it working. \r\n","createdAt":"2022-02-04T00:48:35Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/9657#issuecomment-1029537116","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps4_LFbw","author":{"login":"github-actions"},"authorAssociation":"NONE","body":"This issue has been labeled `inactive-30d` due to no recent activity in the past 30 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed. This issue will be labeled `inactive-90d` if there is no activity in the next 60 days.","createdAt":"2022-03-06T01:28:46Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/9657#issuecomment-1059870448","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5EVg1m","author":{"login":"github-actions"},"authorAssociation":"NONE","body":"This issue has been labeled `inactive-90d` due to no recent activity in the past 90 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed.","createdAt":"2022-06-04T01:31:07Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/9657#issuecomment-1146490214","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5MK4t-","author":{"login":"BartleyR"},"authorAssociation":"MEMBER","body":"We have a potential Morpheus customer who wants to use the phishing detection pipeline but in a non-English language. So we'd have to replace the BERT model with something else, and it would need a BPE tokenizer. We can do a POC using a CPU-based tokenizer, but would be good to scope this if we can for an upcoming release. @GregoryKimball for viz","createdAt":"2022-10-13T17:05:23Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/9657#issuecomment-1277922174","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5eAi2q","author":{"login":"GregoryKimball"},"authorAssociation":"CONTRIBUTOR","body":"This request is still relevant. After discussing with @VibhuJawa, the next step is benchmarking a GPT-3 style training workflow, and measuring the percentage of time spent in tokenization. If tokenization is 15-30% of the total time (as we see in `bert`), then this is worth prioritizing. Otherwise we should recommend tokenization with HuggingFace.","createdAt":"2023-06-05T17:34:27Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/9657#issuecomment-1577201066","viewerDidAuthor":false}],"createdAt":"2021-11-11T00:33:11Z","id":"I_kwDOBWUGps4-nKah","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NjE=","name":"feature request","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwxMTM5NzQwNjY2","name":"libcudf","description":"Affects libcudf (C++/CUDA) code.","color":"c5def5"},{"id":"MDU6TGFiZWwxNTE1NjE2MjUz","name":"strings","description":"strings issues (C++ and Python)","color":"0e8a16"}],"milestone":{"number":30,"title":"Language model acceleration","description":"","dueOn":null},"number":9657,"projectCards":[{"project":{"name":"Feature Planning"},"column":{"name":"Needs prioritizing"}}],"projectItems":[],"reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"state":"OPEN","title":"[FEA] Byte Pair Encoding Tokenizer","updatedAt":"2023-08-14T17:50:04Z","url":"https://github.com/rapidsai/cudf/issues/9657"}
