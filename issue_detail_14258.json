{"assignees":[],"author":{"id":"MDQ6VXNlcjU2MDczMzA=","is_bot":false,"login":"mythrocks","name":"MithunR"},"body":"**Description**\r\n\r\nThis was uncovered in [Spark tests](https://github.com/NVIDIA/spark-rapids/pull/9366) that compare Parquet read/write compatibility with [`fastparquet`](https://fastparquet.readthedocs.io/en/latest/index.html).\r\n\r\nThe last row of a String column written with `fastparquet` seems to be interpreted by CUDF as having more null characters at the end than expected.\r\n\r\n**Repro**\r\n\r\nI'll spare the Scala/Spark details in this bug. [Here](https://github.com/NVIDIA/spark-rapids/files/12812419/fastparquet_string.zip) is a zipped Parquet file that seems to be read differently in CUDF.\r\n\r\nFrom https://github.com/NVIDIA/spark-rapids/issues/9387:\r\n```\r\nGPU COLUMN LENGTH - NC: 0 DATA: DeviceMemoryBufferView{address=0x30a003400, length=20, id=-1} VAL: DeviceMemoryBufferView{address=0x30a001e00, length=64, id=-1}\r\nCOLUMN LENGTH - STRING\r\n0 \"all\" 616c6c\r\n1 \"the\" 746865\r\n2 \"leaves\" 6c65617665730000000000000000\r\n```\r\n\r\nIt would be good to check with the CUDF native Parquet reader, and compare against the results from  `parquet-mr`.","closed":false,"closedAt":null,"comments":[{"id":"IC_kwDOBWUGps5oaovO","author":{"login":"etseidl"},"authorAssociation":"CONTRIBUTOR","body":"Looking at a hex dump of the file, it seems the page data is padded with an extra 8 bytes of zero valued bytes. The string length calculation is likely just using the data length minus `4*num_values` to calculate the string buffer length, so the pad bytes are being included in the final string. This is unfortunate because it means the quick short cut to getting the string lengths can't be used and we'll instead always have to do an expensive traversal of the plain encoded string data. :( \r\n\r\nTBH I'd consider this a bug on the write side...I'll have to check the spec to see if these padding bytes are forbidden or this is just a gray area.\r\n\r\nLooking at fastparquet, it seems the padding was added to get some tests to pass.  See [this](https://github.com/dask/fastparquet/commit/8f62b0efa195099228b6201afd64cc8981df0f52) commit (writer.py, lines 452 and 484). I wonder if it's worth bringing up with the fastparquet devs, adding garbage padding to byte array columns should not be necessary.\r\n\r\nThe relevant line in the current fastparquet is [here](https://github.com/dask/fastparquet/blame/58cdab6adc72c377e2460f90b05e90b9140ddf29/fastparquet/writer.py#L622)","createdAt":"2023-10-07T20:37:35Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}},{"content":"EYES","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/14258#issuecomment-1751813070","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5ohS9M","author":{"login":"etseidl"},"authorAssociation":"CONTRIBUTOR","body":"Just FYI, here's a profile showing the impact of having to do the string size calculation the hard way. The profile shows reading 50M lines from a large parquet file containing plain encoded strings. The top profile is traversing the encoded string data, summing string lengths as it goes. Due to the structure of the data, this cannot be parallelized so a single thread per page is doing this operation. The bottom profile uses the page data size from the header to calculate string sizes. The call to `gpuComputeStringSizes` in the former takes 848ms, nearly doubling the read time. Overall read time for the entire file (200M rows, 8.6GB) goes from 3.5s to 6.9s.\r\n\r\n![Screenshot from 2023-10-09 11-53-07](https://github.com/rapidsai/cudf/assets/25541553/7e5e02b1-9ee2-4fce-8861-c1fae7701b54)\r\n\r\n","createdAt":"2023-10-09T19:20:44Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/14258#issuecomment-1753558860","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5oki9J","author":{"login":"mythrocks"},"authorAssociation":"CONTRIBUTOR","body":"> I'll have to check the spec to see if these padding bytes are forbidden or this is just a gray area.\r\n\r\nThank you for the analysis, @etseidl. This has me curious.\r\n\r\nThe main reason I considered this might be something we should address is that the Spark Parquet reader, and the parquet tools seem to read the file correctly. ","createdAt":"2023-10-10T05:22:59Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/14258#issuecomment-1754410825","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5okoIC","author":{"login":"etseidl"},"authorAssociation":"CONTRIBUTOR","body":"> The main reason I considered this might be something we should address is that the Spark Parquet reader, and the parquet tools seem to read the file correctly.\r\n\r\nYep, because they're reading a page at a time in batches, so they don't need to worry about exact total sizes, they just read the length of each string as they consume it. Even libcudf as of last year would have read that file ok because the string reads were done in two passes.  Now that we have the single pass read, we need to rely on accurate metadata to get the string data copied into the correct places in the column buffer.\r\n\r\nAs usual, the Parquet spec is silent on this. The closest I could find to an answer is in the section on data pages, where it is stated that the `uncompressed_page_size` field is the sum of rep level data, def level data, and encoded values. I personally think adding the 8 bytes of padding is kind of hacky, and it's just a happy accident that other readers are ok with that.\r\n\r\nEdit: Actually, I missed [this](https://github.com/apache/parquet-format#data-pages) \"For data pages, the 3 pieces of information are encoded back to back, after the page header. No padding is allowed in the data page.\" \r\n\r\nAnyway, so as to not kill performance for all, would it be acceptable to add an option to do the more expensive string size calculation when necessary?","createdAt":"2023-10-10T05:47:39Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/14258#issuecomment-1754432002","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5o6bIO","author":{"login":"GregoryKimball"},"authorAssociation":"CONTRIBUTOR","body":"Thank you @etseidl and @mythrocks for studying this anomaly. If we did have a reader option to pre-compute sizes, would Spark-RAPIDS have to always set this option to make sure we are correctly avoiding this behavior in the fastparquet writer? \r\n\r\nDo you think there could be any sensible postprocessing options to trim the null characters? TBH I didn't even know null characters could exist.","createdAt":"2023-10-12T18:24:25Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/14258#issuecomment-1760145934","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5o6kBr","author":{"login":"etseidl"},"authorAssociation":"CONTRIBUTOR","body":"> If we did have a reader option to pre-compute sizes, would Spark-RAPIDS have to always set this option to make sure we are correctly avoiding this behavior in the fastparquet writer?\r\n\r\nYes...or we could turn it around and make the slow pre-compute the default, and enable the faster version on demand. But if fastparquet fixes their writer (I don't see why they wouldn't, they did not do the padding for V2 pages, for instance), then the cudf reader would be slower by default for no reason. A brittle option would be to check the `created_by` tag in the metadata and enable the slow reader if fastparquet is detected there.\r\n\r\n> Do you think there could be any sensible postprocessing options to trim the null characters?\r\n\r\nWell, you'd wind up with a column buffer with holes in it.  `CCCC00CCCCCCC00CCCCCCC`. You would have to shift chunks of char data left and update the offsets array to match.\r\n\r\n> TBH I didn't even know null characters could exist.\r\n\r\nThey're just byte arrays, with an annotation to interpret as UTF8 strings, so null chars are just fine.  The trouble here is fastparquet is adding this padding and counting it in the data size, even though it's not strictly data.\r\n","createdAt":"2023-10-12T18:39:21Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}},{"content":"LAUGH","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/14258#issuecomment-1760182379","viewerDidAuthor":false}],"createdAt":"2023-10-05T23:20:44Z","id":"I_kwDOBWUGps5y_WaC","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NTk=","name":"bug","description":"Something isn't working","color":"d73a4a"},{"id":"MDU6TGFiZWwxMDEzOTg3MzUy","name":"0 - Backlog","description":"In queue waiting for assignment","color":"d4c5f9"},{"id":"MDU6TGFiZWwxMTM5NzQwNjY2","name":"libcudf","description":"Affects libcudf (C++/CUDA) code.","color":"c5def5"},{"id":"MDU6TGFiZWwxMTg1MjQ0MTQy","name":"cuIO","description":"cuIO issue","color":"fef2c0"}],"milestone":{"number":22,"title":"Parquet continuous improvement","description":"","dueOn":null},"number":14258,"projectCards":[],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[BUG] String columns written with `fastparquet` seem to be read incorrectly via CUDF's Parquet reader","updatedAt":"2024-02-16T23:56:57Z","url":"https://github.com/rapidsai/cudf/issues/14258"}
