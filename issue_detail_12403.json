{"assignees":[],"author":{"id":"MDQ6VXNlcjE1MzgxNjU=","is_bot":false,"login":"vyasr","name":"Vyas Ramasubramani"},"body":"**Describe the bug**\r\nThe hash vocab test in cudf currently warns about an overflow occurring. This can be easily observed by running the pytest with warnings set to raise errors.\r\n\r\n**Steps/Code to reproduce bug**\r\nExecute `pytest -W error python/cudf/cudf/tests/test_hash_vocab.py::test_correct_bert_base_vocab_hash` from the root of the repository.\r\n\r\nThe output should include a traceback like this:\r\n```\r\ntest_correct_bert_base_vocab_hash ____________________________________________________________________________________\r\n\r\ndatadir = '/home/vyasr/local/rapids/cudf/python/cudf/cudf/tests/data/subword_tokenizer_data/bert_base_cased_sampled', tmpdir = local('/tmp/pytest-of-rapids/pytest-2/test_correct_bert_base_vocab_h0')\r\n\r\n    def test_correct_bert_base_vocab_hash(datadir, tmpdir):\r\n        # The vocabulary is drawn from bert-base-cased\r\n        vocab_path = os.path.join(datadir, \"vocab.txt\")\r\n    \r\n        groundtruth_path = os.path.join(datadir, \"vocab-hash.txt\")\r\n        output_path = tmpdir.join(\"cudf-vocab-hash.txt\")\r\n>       hash_vocab(vocab_path, output_path)\r\n\r\npython/cudf/cudf/tests/test_hash_vocab.py:23: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\npython/cudf/cudf/utils/hash_vocab_utils.py:269: in hash_vocab\r\n    ) = _perfect_hash(keys, 10)\r\npython/cudf/cudf/utils/hash_vocab_utils.py:129: in _perfect_hash\r\n    internal_table, coeff_a, coeff_b = _find_hash_for_internal(b)\r\npython/cudf/cudf/utils/hash_vocab_utils.py:102: in _find_hash_for_internal\r\n    bins = _make_bins(hash_bin, new_length, a, b)\r\npython/cudf/cudf/utils/hash_vocab_utils.py:60: in _make_bins\r\n    bins[_hash_func(item, a, b, num_bins)].append(item)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\nk = 233297689050786, a = 21608458564245, b = 116, size = 6\r\n\r\n    def _hash_func(k, a, b, size):\r\n        k = np.uint64(k)\r\n        a = np.uint64(a)\r\n        b = np.uint64(b)\r\n        size = np.uint64(size)\r\n>       return ((a * k + b) % PRIME) % size\r\nE       RuntimeWarning: overflow encountered in ulong_scalars\r\n\r\npython/cudf/cudf/utils/hash_vocab_utils.py:49: RuntimeWarning\r\n------------------------------------------------------------------------------------------ Captured stdout call -------------------------------------------------------------------------------------------\r\nAttempting to build table using 1.500000n space\r\nLongest bin was 11\r\nProcessing bin 0 / 875 of size = 6\r\n```\r\n\r\n**Expected behavior**\r\nWe should not have overflows occurring. The reason for the overflow is that all the inputs to `_hash_func` are being converted to `np.uint64` (limited to 64 bits) rather than primitive Python ints (which have unlimited precision). I attempted the naive modification of just removing the conversions to `np.uint64` here (which also requires rewriting some of the call sites to do conversions since they involve indexing into numpy arrays or adding numpy ints to Python ints), but my quick conversion led to the test failing outright. I didn't check my work all that thoroughly so it's possible I made an error, but we should make sure that we understand whether the numpy integer overflow here is some property that we are depending on implicitly, if it is a bug that users could actually hit and we need to fix, or if it's just the expected behavior and the warning can be silenced.\r\n","closed":false,"closedAt":null,"comments":[{"id":"IC_kwDOBWUGps5-R180","author":{"login":"VibhuJawa"},"authorAssociation":"MEMBER","body":"I think long term we want to move away from `hash_vocab` functionality and make `subword` tokenizer work with `vocab` files directly. \r\n\r\nSimilar to what we do in BPE ","createdAt":"2024-05-18T02:35:22Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12403#issuecomment-2118606644","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5-cQN3","author":{"login":"vyasr"},"authorAssociation":"CONTRIBUTOR","body":"Hmm OK so you think we'll end up removing this functionality altogether at some point, then?","createdAt":"2024-05-20T22:41:40Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12403#issuecomment-2121335671","viewerDidAuthor":false}],"createdAt":"2022-12-16T17:48:11Z","id":"I_kwDOBWUGps5ZccA3","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NTk=","name":"bug","description":"Something isn't working","color":"d73a4a"},{"id":"MDU6TGFiZWwxMDEzOTg3MzUy","name":"0 - Backlog","description":"In queue waiting for assignment","color":"d4c5f9"},{"id":"MDU6TGFiZWwxMDE2MzQyNTY3","name":"tests","description":"Unit testing for project","color":"204ea3"},{"id":"MDU6TGFiZWwxMTM5NzQxMjEz","name":"cuDF (Python)","description":"Affects Python cuDF API.","color":"1d76db"},{"id":"MDU6TGFiZWwxNTE1NjE2MjUz","name":"strings","description":"strings issues (C++ and Python)","color":"0e8a16"}],"milestone":{"number":30,"title":"Language model acceleration","description":"","dueOn":null},"number":12403,"projectCards":[{"project":{"name":"Bug Squashing"},"column":{"name":"Needs prioritizing"}}],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[BUG] Overflow potentially corrupting hashes in hash_vocab implementation","updatedAt":"2024-05-20T22:41:41Z","url":"https://github.com/rapidsai/cudf/issues/12403"}
