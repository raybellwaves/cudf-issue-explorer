{"assignees":[],"author":{"id":"MDQ6VXNlcjQzNTMyMDU1","is_bot":false,"login":"infzo","name":"Liu"},"body":"**Is your feature request related to a problem? Please describe.**\r\n\r\nCurrently, we use cuDF to implement distributed group aggregation operations. That is, GROUP BY + DISTINCT is performed on the local node, and GROUP BY + MIN/MAX/COUNT DISTINCT are performed on the merge node. The LIST is transferred as an intermediate format, but the aggregation operation cannot be implemented.\r\n\r\n**Describe the solution you'd like**\r\n\r\nGroup +  Aggregation supports the list format.\r\n\r\n![捕获2](https://user-images.githubusercontent.com/43532055/205540928-0359139e-03ea-4c71-bd8d-0daaf0d5f875.PNG)\r\n\r\n**Describe alternatives you've considered**\r\n\r\nAggregation operators are supported in the list format. For example, the current list supports only `len`, and aggregation operators such as `count`, `min`, `max`, and `avg` are expected to be added.\r\n\r\n![捕获](https://user-images.githubusercontent.com/43532055/205540951-3d8b4f09-7227-4cb1-836e-49e79c428e31.PNG)\r\n","closed":false,"closedAt":null,"comments":[{"id":"IC_kwDOBWUGps5PrcQw","author":{"login":"ttnghia"},"authorAssociation":"CONTRIBUTOR","body":"What do you mean groupby + aggregation? If I guessed correctly, we already have `MERGE_LIST` aggregation that does the merging for lists.","createdAt":"2022-12-05T06:00:33Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12301#issuecomment-1336788016","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5Prcu9","author":{"login":"ttnghia"},"authorAssociation":"CONTRIBUTOR","body":"If you want to do groupby min/max etc. on lists as the input values then we recently have enough tools to support them---just a matter of time to enable them.","createdAt":"2022-12-05T06:02:41Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/12301#issuecomment-1336789949","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5PrkuE","author":{"login":"infzo"},"authorAssociation":"NONE","body":"> If you want to do groupby min/max etc. on lists as the input values then we recently have enough tools to support them---just a matter of time to enable them.\r\n\r\n@ttnghia Yes, something like this; there are a few questions:\r\n\r\n1. How long will this feature be supported in the release version?\r\n2. Our requirements are urgent. Is there a faster way to implement this function in the current version?\r\n\r\nSQL case:\r\n\r\n![捕获3](https://user-images.githubusercontent.com/43532055/205566477-dc2cced0-d664-4c74-a7d0-495cc3dff7d0.PNG)\r\n\r\n","createdAt":"2022-12-05T06:45:12Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12301#issuecomment-1336822660","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5PrrDb","author":{"login":"mjshare"},"authorAssociation":"NONE","body":"> @ttnghia\r\n\r\nHow do I perform operations such as count and sum after deduplication in the list?","createdAt":"2022-12-05T07:14:50Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12301#issuecomment-1336848603","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5PuVcR","author":{"login":"ttnghia"},"authorAssociation":"CONTRIBUTOR","body":"Oh sorry, what you've shown in the picture is per-list aggregation on the list elements (I thought that you want aggregation just at the lists level). This needs to implement new aggregations from scratch. I believe that @GregoryKimball can answer more questions about prioritizing for this FEA.","createdAt":"2022-12-05T15:11:00Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12301#issuecomment-1337546513","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5PvB7_","author":{"login":"ttnghia"},"authorAssociation":"CONTRIBUTOR","body":"After thinking shortly, I realized that this doesn't require any new cudf aggregations. You can have it by:\r\n1. Merge your lists corresponding to the same key using MERGE_LIST aggregation, then\r\n2. Call segmented min/max/sum on the list elements using list offsets.\r\n\r\nHowever, I'm not sure how you can leverage these existing (internal) segmented APIs to do that.","createdAt":"2022-12-05T16:57:24Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/12301#issuecomment-1337728767","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5PvLyE","author":{"login":"shwina"},"authorAssociation":"CONTRIBUTOR","body":"@infzo perhaps I'm misunderstanding, but can you not just do this?\r\n\r\n```python\r\nIn [67]: df1 = cudf.DataFrame({'a': [1, 1, 2], 'b': [1, cudf.NA, 3], 'c': ['x', 'x', 'z']})\r\n\r\nIn [68]: df2 = cudf.DataFrame({'a': [1, 3, 3], 'b': [1, cudf.NA, 5], 'c': ['x', 'z', cudf.NA]})\r\n\r\nIn [69]: cudf.concat([df1, df2]).groupby('a').agg({'b': 'max', 'c': 'nunique'})\r\nOut[69]: \r\n   b  c\r\na      \r\n1  1  1\r\n2  3  1\r\n3  5  1\r\n\r\n```","createdAt":"2022-12-05T17:17:03Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12301#issuecomment-1337769092","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5PyXA2","author":{"login":"mjshare"},"authorAssociation":"NONE","body":"\r\n> After thinking shortly, I realized that this doesn't require any new cudf aggregations. You can have it by:\r\n> \r\n> 1. Merge your lists corresponding to the same key using MERGE_LIST aggregation, then\r\n> 2. Call segmented min/max/sum on the list elements using list offsets.\r\n> \r\n> However, I'm not sure how you can leverage these existing (internal) segmented APIs to do that.\r\n@ttnghia ，The problem is that the list does not support count, max, and sum with groupby\r\nDoes the community have a plan to support it? We have scenarios to support it？\r\n","createdAt":"2022-12-06T01:36:44Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12301#issuecomment-1338601526","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5PyZLY","author":{"login":"infzo"},"authorAssociation":"NONE","body":"> After thinking shortly, I realized that this doesn't require any new cudf aggregations. You can have it by:\r\n> \r\n> 1. Merge your lists corresponding to the same key using MERGE_LIST aggregation, then\r\n> 2. Call segmented min/max/sum on the list elements using list offsets.\r\n> \r\n> However, I'm not sure how you can leverage these existing (internal) segmented APIs to do that.\r\n\r\n@ttnghia Yes, this is our current solution. Segmented min/max/sum is a requirement for the list method. For details, see Describe alternatives you've considered. The current cudf version does not have these operators.\r\n","createdAt":"2022-12-06T01:43:35Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12301#issuecomment-1338610392","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5PyZ6j","author":{"login":"infzo"},"authorAssociation":"NONE","body":"> @infzo perhaps I'm misunderstanding, but can you not just do this?\r\n> \r\n> ```python\r\n> In [67]: df1 = cudf.DataFrame({'a': [1, 1, 2], 'b': [1, cudf.NA, 3], 'c': ['x', 'x', 'z']})\r\n> \r\n> In [68]: df2 = cudf.DataFrame({'a': [1, 3, 3], 'b': [1, cudf.NA, 5], 'c': ['x', 'z', cudf.NA]})\r\n> \r\n> In [69]: cudf.concat([df1, df2]).groupby('a').agg({'b': 'max', 'c': 'nunique'})\r\n> Out[69]: \r\n>    b  c\r\n> a      \r\n> 1  1  1\r\n> 2  3  1\r\n> 3  5  1\r\n> ```\r\n\r\n@shwina A use case given above. In actual scenarios, more than one billion rows of data are queried in the table. As a result, the following problems occur:\r\n1. The GPU of a single machine cannot accommodate such a large amount of data, and the cuDF DataFrame has an upper limit on the number of rows.\r\n2. A large amount of data consumes a long time for network transmission, which reduces processing efficiency.","createdAt":"2022-12-06T01:48:58Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12301#issuecomment-1338613411","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5P09M1","author":{"login":"shwina"},"authorAssociation":"CONTRIBUTOR","body":"Thanks for the context, @infzo. I think I understand. How about doing it this way then?\r\n\r\nThe following happens on the local nodes:\r\n\r\n```python\r\nIn [79]: df1 = cudf.DataFrame({'a': [1, 1, 2], 'b': [1, cudf.NA, 3], 'c': ['x', 'x', 'z']})\r\n\r\nIn [80]: df2 = cudf.DataFrame({'a': [1, 3, 3], 'b': [1, cudf.NA, 5], 'c': ['x', 'z', cudf.NA]})\r\n\r\nIn [81]: df1\r\nOut[81]: \r\n   a     b  c\r\n0  1     1  x\r\n1  1  <NA>  x\r\n2  2     3  z\r\n\r\nIn [82]: df2\r\nOut[82]: \r\n   a     b     c\r\n0  1     1     x\r\n1  3  <NA>     z\r\n2  3     5  <NA>\r\n\r\nIn [83]: df1_unique = df1.groupby('a', as_index=False).unique()\r\n\r\nIn [84]: df2_unique = df2.groupby('a', as_index=False).unique()\r\n```\r\n\r\nThe following happens on the merge node:\r\n\r\n```python\r\nIn [85]: df_merged = cudf.concat([df1_unique, df2_unique], ignore_index=True)\r\n\r\nIn [86]: b_max = df_merged[['a', 'b']].explode('b').groupby('a').max()\r\n\r\nIn [87]: c_nunique = df_merged[['a', 'c']].explode('c').groupby('a').nunique()\r\n\r\nIn [88]: result = cudf.concat([b_max, c_nunique], axis=1)\r\n\r\nIn [89]: result\r\nOut[89]: \r\n   b  c\r\na      \r\n1  1  1\r\n2  3  1\r\n3  5  1\r\n```","createdAt":"2022-12-06T12:57:28Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/12301#issuecomment-1339282229","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5P1VRx","author":{"login":"mjshare"},"authorAssociation":"NONE","body":"> Thanks for the context, @infzo. I think I understand. How about doing it this way then?\r\n> \r\n> The following happens on the local nodes:\r\n> \r\n> ```python\r\n> In [79]: df1 = cudf.DataFrame({'a': [1, 1, 2], 'b': [1, cudf.NA, 3], 'c': ['x', 'x', 'z']})\r\n> \r\n> In [80]: df2 = cudf.DataFrame({'a': [1, 3, 3], 'b': [1, cudf.NA, 5], 'c': ['x', 'z', cudf.NA]})\r\n> \r\n> In [81]: df1\r\n> Out[81]: \r\n>    a     b  c\r\n> 0  1     1  x\r\n> 1  1  <NA>  x\r\n> 2  2     3  z\r\n> \r\n> In [82]: df2\r\n> Out[82]: \r\n>    a     b     c\r\n> 0  1     1     x\r\n> 1  3  <NA>     z\r\n> 2  3     5  <NA>\r\n> \r\n> In [83]: df1_unique = df1.groupby('a', as_index=False).unique()\r\n> \r\n> In [84]: df2_unique = df2.groupby('a', as_index=False).unique()\r\n> ```\r\n> \r\n> The following happens on the merge node:\r\n> \r\n> ```python\r\n> In [85]: df_merged = cudf.concat([df1_unique, df2_unique], ignore_index=True)\r\n> \r\n> In [86]: b_max = df_merged[['a', 'b']].explode('b').groupby('a').max()\r\n> \r\n> In [87]: c_nunique = df_merged[['a', 'c']].explode('c').groupby('a').nunique()\r\n> \r\n> In [88]: result = cudf.concat([b_max, c_nunique], axis=1)\r\n> \r\n> In [89]: result\r\n> Out[89]: \r\n>    b  c\r\n> a      \r\n> 1  1  1\r\n> 2  3  1\r\n> 3  5  1\r\n> ```\r\n@shwina \r\nThis method may have a risk: If there are multi-column aggregation and multi-column groupby, explode expansion may cause insufficient GPU memory. Is there any solution to this problem?","createdAt":"2022-12-06T13:20:52Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12301#issuecomment-1339380849","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5P1hNf","author":{"login":"shwina"},"authorAssociation":"CONTRIBUTOR","body":"> If there are multi-column aggregation and multi-column groupby, explode expansion may cause insufficient GPU memory\r\n\r\nYou're right that the call to `explode()` does incur additional memory cost although that cost should be linear with the number of elements in the list column (`'b'` or `'c'` in this case).\r\n\r\nWhat I'm proposing here is a workaround that is compatible with the existing Pandas/cuDF API. Segmented reduction would be the appropriate solution to the problem, but those are not currently available via the Python API.","createdAt":"2022-12-06T13:57:40Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":2}}],"url":"https://github.com/rapidsai/cudf/issues/12301#issuecomment-1339429727","viewerDidAuthor":false}],"createdAt":"2022-12-05T03:11:31Z","id":"I_kwDOBWUGps5X8tkY","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NjE=","name":"feature request","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwxMDEzOTg3MzUy","name":"0 - Backlog","description":"In queue waiting for assignment","color":"d4c5f9"},{"id":"MDU6TGFiZWwxMTM5NzQxMjEz","name":"cuDF (Python)","description":"Affects Python cuDF API.","color":"1d76db"}],"milestone":{"number":2,"title":"List and Struct data types and operations","description":"","dueOn":null},"number":12301,"projectCards":[{"project":{"name":"Feature Planning"},"column":{"name":"Needs prioritizing"}}],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[FEA] Support segmented reductions (MIN/MAX/COUNT DISTINCT) in cuDF `list` accessor","updatedAt":"2022-12-22T05:52:38Z","url":"https://github.com/rapidsai/cudf/issues/12301"}
