{"assignees":[],"author":{"id":"MDQ6VXNlcjM0NDEzMjE=","is_bot":false,"login":"revans2","name":"Robert (Bobby) Evans"},"body":"**Is your feature request related to a problem? Please describe.**\r\nThe high level goal is to be able to reduce the amount of data that we read from disk on parquet files with a ColumnIndex/PageIndex.\r\n\r\nhttps://github.com/apache/parquet-format/blob/master/PageIndex.md\r\n\r\n**Describe the solution you'd like**\r\n\r\nIn Spark we currently do some hacked up things when reading parquet files. We use [parquet-mr](https://github.com/apache/parquet-mr) to read the metadata about the file(s). We then let it do a predicate push down to find the row groups that fit the requested predicate. Finally we read the pages for the desired columns in those row groups and put it all back together as a new in-memory parquet file that we send down to CUDF. It is ugly, but it let us do add a lot of features in Spark before CUDF could support them. It still lets us read the data using the existing Hadoop File System interface, which because it is a \"standard\" that our customers can and do replace. We might be able to move to the Arrow FileSystem API, but I'll talk about that in the alternatives.\r\n\r\nIdeally I would like an API where we can send compressed pages and metadata to CUDF for decoding.  The metadata would include things like the file and row group that the pages came from and what range of column indicies within those pages we would like to be read.\r\n\r\nThe hard part with filtering using a ColumnIndex is that the pages within a row group are not split on the same row boundaries, like a row group is.  An example might help here. Lets say we have two columns A and B in a row group. The predicate to push down is `A > 100`, which corresponds to page 5 in the row group. That page is for rows 500-599.  Column B requires us to load 2 pages to cover that same range. In this case lets say pages 10 and 11 which cover 450 - 549 and 550 to 700 respectively.  So we would have to hand CUDF the pages 5, 10, and 11 along with the metadata about the row group and file so CUDF can know how to decode the data, and information to say only decode the rows 500 to 599 and throw away anything else that is outside of that. In a real situation it is probably going to be a lot more complicated.\r\n\r\nIdeally this would let us pass down row groups from multiple different files too. I am not 100% sure how the row group filtering works on a multi-file source_info.\r\n\r\n**Describe alternatives you've considered**\r\nThe other alternative is for us to start using the Arrow FileSystem API and also have cudf implement row number filtering (need to check, but I think the row numbers are relative to the start of the row group and not total within the file) similar to the `set_row_groups` API that currently exists.\r\n\r\nThis is kind of hard for us to do.\r\n\r\n1. One of the main performance features that we have is overlapping I/O with computation. Spark likes to use lots of threads, more than we want to allow on the GPU at any point in time for memory reasons. So we let some onto the GPU, but we let the others read data to CPU memory. This lets us overlap the slow reading from a remote file system with computation on the GPU. We would need some kind of callback, or multiple APIs so that we could have CUDF read all of the needed data into host memory, and then we can wait until it is time to run at which point we can finish processing the data.\r\n2. JNI is very slow for moving data. We use a number of tricks/alternative APIs to get around this. This is especially true when calling back from native code into Java. So having an API that goes from java to C back to java so it can read data over a socket (through C again, but a slightly more optimized interface than JNI) is far from ideal especially if it is going to involve small reads.\r\n\r\n**Additional context**\r\nThis is related to #9268, but the read side compared to the write side of it.","closed":false,"closedAt":null,"comments":[{"id":"IC_kwDOBWUGps43KZDQ","author":{"login":"devavret"},"authorAssociation":"CONTRIBUTOR","body":"I'm trying to understand this. So there's broadly two steps where parquet reader spends time:\r\n1.  Reading rowgroup data from the file.\r\n2. Decompressing and decoding said data.\r\n\r\nIn case of rowgroup based filtering, you can ask cudf for the rowgroups to read which means rest of the rowgroups are neither loaded from disk into host/device memory nor decompressed/decoded.\r\nYou can filter based on exact row numbers but that would mean the entire rowgroup is still read from disk but only the necessary pages are decompressed/decoded. So only partial time savings.\r\n\r\nWith this new page level filtering, you're trying to avoid reading unnecessary pages from _within_ the rowgroups, from disk. Is that correct?","createdAt":"2021-09-23T02:57:01Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/9269#issuecomment-925470928","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps45xp-r","author":{"login":"github-actions"},"authorAssociation":"NONE","body":"This issue has been labeled `inactive-30d` due to no recent activity in the past 30 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed. This issue will be labeled `inactive-90d` if there is no activity in the next 60 days.","createdAt":"2021-11-15T21:03:25Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/9269#issuecomment-969318315","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps495X0V","author":{"login":"github-actions"},"authorAssociation":"NONE","body":"This issue has been labeled `inactive-90d` due to no recent activity in the past 90 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed.","createdAt":"2022-02-13T22:03:11Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/9269#issuecomment-1038449941","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5JX0-M","author":{"login":"GregoryKimball"},"authorAssociation":"CONTRIBUTOR","body":">  API where we can send compressed pages and metadata to CUDF for decoding. The metadata would include things like the file and row group that the pages came from and what range of column indicies within those pages we would like to be read.\r\n\r\n@revans2 would you please share more about the API you have in mind? Would the inputs be a new metadata class, or are there existing ones that would suffice? Would the output be a table or an iterable of column fragments?\r\n","createdAt":"2022-08-29T23:49:40Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/9269#issuecomment-1230983052","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5Kk9Hh","author":{"login":"revans2"},"authorAssociation":"CONTRIBUTOR","body":"Sorry about the long delay for a response. I see two possible APIs.\r\n\r\nThe first would be that in Spark we add in a glue layer that would let the CUDF reader call back into java and the Hadoop File System API so we can properly support security and all of the various pluggable APIs that Spark/Java can support, but CUDF does not yet.\r\n\r\nCUDF would then need to add in APIs that would allow us to partition the data the same way that Spark does, not hard I can show you how to do it. APIs to do predicate push down so row groups can be skipped if needed (and in this case possibly pages within a row group being skipped too). APIs to do column pruning so we can tell you exactly which columns we want read. A callback API so we know just before data is moved to the GPU so we can overlap I/O as much as possible, and not overload the GPU's memory. And an API, if it does not exist already, to be able to group multiple smaller files together so that the GPU can be more efficient when reading the data. Also we can use a thread pool to read data from some file systems, because the bandwidth for a lot of CSP blob stores is limited by each connection or single threaded CPU performance in decompressing/decrypting the data, so having more connections improves the total throughput.\r\n\r\nI like this approach because it allows for more improvement in the future, including things like GPU direct storage and it bypasses having to read/write the footer multiple times.  But we would have to do a bunch of profiling to be sure that the java callback is efficient and not slowing things down, and it would be a lot of large changes to CUDF to add in all of the features that we have been doing directly in scala today.\r\n\r\nThe second approach would be something where we would pass down structured data to CUDF. So instead of passing down a buffer that holds a complete parquet file.  We would parse the footer and the ColumnIndex ourselves. Read the data that we know we need to read, and then pass the information to CUDF as almost a tree, where we would give you the some form of a footer that would point to the columns, row groups, pages, and row ranges that we care about and want read.\r\n\r\nThis approach is a lot more complicated to do, in my option, and is prone to also being slow because sending structured data through JNI is also rather problematic and slow. So I would prefer the first option, but...","createdAt":"2022-09-19T15:44:51Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/9269#issuecomment-1251201505","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5aWGqh","author":{"login":"GregoryKimball"},"authorAssociation":"CONTRIBUTOR","body":"Thank you @revans2 for sharing these ideas for improving the IO pipeline in the Spark-RAPIDS plugin. I'd like to pick up this thread again now that I'm scoping predicate pushdown in the libcudf parquet reader.\r\n\r\n> in Spark we [could] add in a glue layer that would let the CUDF reader call back into java and the Hadoop File System API\r\n\r\nIs your idea that this glue layer would appear to libcudf as a new `datasource`?\r\n\r\n> CUDF would then need to add in APIs that would allow us to partition the data the same way that Spark does\r\n\r\nFor predicate pushdown we are considering using an AST to represent the predicate. Do you already have tools to convert Spark expressions into libcudf ASTs?\r\n\r\n> possibly pages within a row group being skipped too\r\n\r\nDo you know for sure if Spark does page-level IO filtering with predicates? At least pyarrow, DuckDB and polars only do rowgroup-level filtering with predicates. \r\n\r\n> APIs to do column pruning\r\n\r\nI believe we already support column pruning in `parquet_reader_options._columns`.\r\n\r\n> an API, if it does not exist already, to be able to group multiple smaller files\r\n\r\nI'll need to check on this. We have multi-source support for Parquet reading in cuDF-python, but I'm not sure how that maps into libcudf.\r\n\r\n> Also we can use a thread pool to read data from some file systems\r\n\r\nDoes this have any implications for the API design in libcudf?","createdAt":"2023-04-20T05:40:18Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/9269#issuecomment-1515743905","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5aY5vr","author":{"login":"revans2"},"authorAssociation":"CONTRIBUTOR","body":"> > in Spark we [could] add in a glue layer that would let the CUDF reader call back into java and the Hadoop File System API\r\n> \r\n> Is your idea that this glue layer would appear to libcudf as a new `datasource`?\r\n\r\nYes. Currently we will use the built in Spark and Parquet java code to do predicate push down and see what columns we need to read based off of statistics in the footer and the section of the file that a given task is supposed to process.  After that we will read those parts of the file, write out a new file + new footer to a memory buffer and hand that memory buffer off to cudf to parse.  This gave us flexibility to do predicate push down, column pruning, partitioning, and even combining multiple small files together without having to wait for CUDF to add these features.  It also let us reduce the amount of data we read and parallelize fetching the data.\r\n\r\nBecause we have built so much outside of CUDF it makes me a little nervous to switch everything around unless we can find a way to get at least feature parity. \r\n \r\n> > CUDF would then need to add in APIs that would allow us to partition the data the same way that Spark does\r\n> \r\n> For predicate pushdown we are considering using an AST to represent the predicate. Do you already have tools to convert Spark expressions into libcudf ASTs?\r\n\r\nAST would be fine. The big issue for me is making sure that the AST can support all of the filtering that we currently support.\r\n\r\nhttps://github.com/apache/parquet-mr/blob/master/parquet-column/src/main/java/org/apache/parquet/filter2/predicate/FilterApi.java\r\nhttps://github.com/apache/parquet-mr/blob/master/parquet-column/src/main/java/org/apache/parquet/filter2/predicate/Operators.java\r\n\r\nThe big differences I see are `binaryColumn`, `in`, and `notIn`. We probably could simulate `in` and `notIn` using equality and binary operations.\r\n\r\n> > possibly pages within a row group being skipped too\r\n> \r\n> Do you know for sure if Spark does page-level IO filtering with predicates? At least pyarrow, DuckDB and polars only do rowgroup-level filtering with predicates.\r\n\r\nThat is what triggered me filing this issue.  Parquet-mr added the ability to do this and spark started to do it. They had a bug in their initial implementation which we found so I filed this to explore what it might take to support it here too. It might not be worth it because of the amount of change needed to support a feature like this.\r\n\r\n> > APIs to do column pruning\r\n> \r\n> I believe we already support column pruning in `parquet_reader_options._columns`.\r\n\r\nYes, but technically we can do sub-column pruning too.  Like what if I have a `foo:struct<a:Int, b:Int>`, but I know that foo.a is never touched, so all I want to load is `foo:struct<b:Int>` and skip `a` all together.\r\n\r\n> > an API, if it does not exist already, to be able to group multiple smaller files\r\n> \r\n> I'll need to check on this. We have multi-source support for Parquet reading in cuDF-python, but I'm not sure how that maps into libcudf.\r\n> \r\n> > Also we can use a thread pool to read data from some file systems\r\n> \r\n> Does this have any implications for the API design in libcudf?\r\n\r\nYes. The current data source API is single threaded/single buffer. Read this one buffer starting at offset X and with a size of Y.  Done? OK now read this buffer at offset X2 and size of Y2... \r\n\r\nIf you could provide an API with something like `std::vector<request>` where `request` looks like `struct{buffer_type, offset, size, dst}` then we could decide how we do the I/O to best fulfill that request in the shortest time possible. It would also be great if we could make it async so it could return a `future` so for the small file issue you could send off requests for all of them in parallel so that we could schedule that appropriately too.\r\n\r\nThe reason that we do this is because blob stores can have very high time to first byte (so for random I/O when doing parquet or ORC it is good to coalesce reads and issues multiple requests at once)\r\n\r\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance.html\r\n\r\n>  These applications can achieve consistent small object latencies (and first-byte-out latencies for larger objects) of roughly 100–200 milliseconds.\r\n\r\nWe have also found that throughput can be throttled per connection and we need multiple open connections to saturate the bandwidth. \r\n\r\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance.html\r\n\r\n> You can increase your read or write performance by using parallelization.","createdAt":"2023-04-20T14:55:25Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/9269#issuecomment-1516477419","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5aZq8_","author":{"login":"etseidl"},"authorAssociation":"CONTRIBUTOR","body":"Chiming in here uninvited...\r\n\r\n> Yes, but technically we can do sub-column pruning too. Like what if I have a `foo:struct<a:Int, b:Int>`, but I know that foo.a is never touched, so all I want to load is `foo:struct<b:Int>` and skip `a` all together.\r\n\r\nThe cudf parquet reader supports this as well, you just have to provide the fully quallified path.  You can set `columns` to something like `foo.item.b` (or maybe `foo.element.b` depending on the writer...the schema knows). \r\n\r\nAnd the current API does support multiple input files in the `source_info`, as well as specific row groups from those input files.\r\n\r\nI've experimented with using the column index info on the read side to do page pruning, but for most of my use cases it wasn't worth it.  If I'm reading a range of a single file where the range spans multiple row groups, then I'm only saving a little bit on the first and last row group.  If I'm reading a range from a single row group, there's usually not enough columns being read to saturate the device, so there's no speed up (esp if there are dictionary pages to decompress/decode).  Reading a single row with no dictionaries can be significantly faster, and I'd imagine doing a targeted scan of multiple files would benefit as well.  But since spark can already do page pruning up front (and cudf lacks S3/HDFS support at the c++ layer AFAICT), I don't know if there's an easy win here.\r\n\r\n\r\n\r\n","createdAt":"2023-04-20T17:14:11Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"HEART","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/9269#issuecomment-1516678975","viewerDidAuthor":false}],"createdAt":"2021-09-22T11:58:44Z","id":"I_kwDOBWUGps472wEE","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NjE=","name":"feature request","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwxMTM5NzQwNjY2","name":"libcudf","description":"Affects libcudf (C++/CUDA) code.","color":"c5def5"},{"id":"MDU6TGFiZWwxMTg1MjQ0MTQy","name":"cuIO","description":"cuIO issue","color":"fef2c0"},{"id":"MDU6TGFiZWwxNDA1MTQ2OTc1","name":"Spark","description":"Functionality that helps Spark RAPIDS","color":"7400ff"}],"milestone":{"number":22,"title":"Parquet continuous improvement","description":"","dueOn":null},"number":9269,"projectCards":[{"project":{"name":"Feature Planning"},"column":{"name":"Needs prioritizing"}}],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[FEA] Enable Page-level filtering based on the ColumnIndex feature from parquet 1.11","updatedAt":"2024-02-23T18:42:44Z","url":"https://github.com/rapidsai/cudf/issues/9269"}
