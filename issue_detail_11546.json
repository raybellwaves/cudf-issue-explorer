{"assignees":[],"author":{"id":"MDQ6VXNlcjE1MjIxMjg5","is_bot":false,"login":"jrhemstad","name":"Jake Hemstad"},"body":"**Is your feature request related to a problem? Please describe.**\r\n\r\nIt is important to ensure that all device memory allocated inside of cuDF functions is done through RMM. \r\n\r\nIt is easy to overlook this, e.g., by forgetting to pass the `rmm::exec_policy` to a Thrust algorithm that allocates temporary memory. \r\n\r\n**Describe the solution you'd like**\r\n\r\nIt would be fairly easy to add this to our CI testing by writing a LD_PRELOAD library that overloads `cudaMalloc` to throw an error if it is called more than once.\r\n\r\nThis would ensure that there is only a single `cudaMalloc` call for the pool allocation.\r\n\r\nThere are some things to be aware of with this solution:\r\n- We'd need to ensure the pool is sized such that it won't need to grow for the tests\r\n- It would assume we're using cudaMalloc as the upstream resource for the pool (and not cudaMallocManaged)\r\n","closed":false,"closedAt":null,"comments":[{"id":"IC_kwDOBWUGps5IjGdu","author":{"login":"jrhemstad"},"authorAssociation":"CONTRIBUTOR","body":"The gist of doing LD_PRELOAD injection for `cudaMalloc` is:\r\n\r\n- Write an init function annotated with `__attribute__((constructor))` to ensure it runs during load time\r\n- In the init, use `dlsym(RTLD_NEXT, \"cudaMalloc\")` to get a pointer to the _real_ cudaMalloc function\r\n- Write an overload of `cudaMalloc` with an identical signature to the real one\r\n- In that overload, add a `static` counter that ensures it is only called once and simply invokes the previously stored function pointer\r\n\r\nHere's an example of how I did this in the past for `pthread_mutex_lock/unlock` when I was experimenting with annotating the Python GIL with NVTX (it didn't work because NVTX calls `pthread_mutex_lock` internally and you end up with infinite recursion :( ): \r\n\r\nhttps://github.com/jrhemstad/gil_preload/blob/main/gil_preload.cpp#L62-L80","createdAt":"2022-08-16T21:01:47Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11546#issuecomment-1217161070","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5KbIoN","author":{"login":"github-actions"},"authorAssociation":"NONE","body":"This issue has been labeled `inactive-30d` due to no recent activity in the past 30 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed. This issue will be labeled `inactive-90d` if there is no activity in the next 60 days.","createdAt":"2022-09-15T21:02:59Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/11546#issuecomment-1248627213","viewerDidAuthor":false}],"createdAt":"2022-08-16T20:54:52Z","id":"I_kwDOBWUGps5P7BNv","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NjE=","name":"feature request","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwxMTM5NzQwNjY2","name":"libcudf","description":"Affects libcudf (C++/CUDA) code.","color":"c5def5"}],"milestone":{"number":25,"title":"Helps libcudf C++ integrations","description":"","dueOn":null},"number":11546,"projectCards":[{"project":{"name":"Feature Planning"},"column":{"name":"Needs prioritizing"}}],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[FEA] Extend tests to ensure no memory is allocated outside of RMM  ","updatedAt":"2023-04-02T22:40:35Z","url":"https://github.com/rapidsai/cudf/issues/11546"}
