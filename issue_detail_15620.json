{"assignees":[],"author":{"id":"MDQ6VXNlcjE5MDEwNTk=","is_bot":false,"login":"abellina","name":"Alessandro Bellina"},"body":"We have seen patterns where small `cudaMemcpyAsync` collide with large `cudaMemcpyAsync` being handled by the copy engine. Importantly, the small copy is in a different stream than the large copy.  In the example below, we can see a H2D pinned copy of 51KB that was scheduled with a latency of _12ms_ because there is another pinned H2D copy happening at the same time (the larger copy is ~200MB).\r\n\r\n![2024-04-30_10-35](https://github.com/rapidsai/cudf/assets/1901059/58e6a601-8ffb-490b-931f-1e7deee82dcd)\r\n\r\nThe big issue behind this pattern is that Stream 30 in this case is serializing because nothing else will run in the stream until this small copy is done. Usually when we invoke kernels in cuDF there is a pattern of: small H2Ds, followed by kernel invocation, then small D2Hs. Any of the pre/post copies done around a kernel is a candidate to get stuck, serializing all the work in that stream.\r\n\r\nWe have a PoC that uses `thrust::copy_n` to copy from pinned to device memory and viceversa using SMs instead of the copy engine, as kernels can directly touch pinned memory. When such an approach is followed, the small copy is able to run with much less latency and subsequent work in the stream is unblocked. This leads to kernels running at the same time as large copies, which is a desirable pattern. \r\n\r\nThis issue was created in order to track this work and to figure out how to bring these changes to cuDF in a configurable way so we don't affect serial workloads, as the effect is really prominent for parallel workloads such as Spark.\r\n\r\nThis is NDS q9 at 3TB, looking at the CUDA HW row, we can see how the compute (blue) overlaps more often than not with pinned H2Ds (green).\r\n\r\nBefore:\r\n![2024-04-30_10-43](https://github.com/rapidsai/cudf/assets/1901059/783b32bb-9750-4bdb-a164-daaea6ebab84)\r\n\r\nAfter:\r\n![2024-04-30_10-44](https://github.com/rapidsai/cudf/assets/1901059/54fce8ee-68db-4bb6-9985-782419e19337)\r\n","closed":false,"closedAt":null,"comments":[{"id":"IC_kwDOBWUGps58WoRJ","author":{"login":"bdice"},"authorAssociation":"CONTRIBUTOR","body":"Are there any ways in which CCCL's `cuda::memcpy_async` could help us here? https://nvidia.github.io/cccl/libcudacxx/extended_api/asynchronous_operations/memcpy_async.html\r\n\r\nI'm not super familiar with the facilities libcudacxx provides for this, but perhaps this could be in scope.","createdAt":"2024-04-30T18:01:01Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/15620#issuecomment-2086306889","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps58YATO","author":{"login":"abellina"},"authorAssociation":"CONTRIBUTOR","body":"@bdice I am not familiar with it either. A quick code search is yielding that `cuda::memcpy_async` handles D2D copies (shared, global). I suppose we could refer to pinned memory as another \"device\" memory type, so in that case I see some relation, but the main thing is code we use to orchestrate copies should likely fallback to `cudaMemcpyAsync`: if pinned memory isn't available to bounce the copy through, or if it's disabled. I believe single threaded applications (single stream) may want to use `cudaMemcpyAsync` as is, because there shouldn't be much contention on the CE in those cases.","createdAt":"2024-04-30T19:07:47Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/15620#issuecomment-2086667470","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps58aLP4","author":{"login":"jrhemstad"},"authorAssociation":"CONTRIBUTOR","body":"I suspect many of the small copies in `cuDF` come from `rmm::device_scalar`. We could just update the implementation to use a kernel instead of `cudaMemcpy*`. ","createdAt":"2024-04-30T20:49:54Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"HEART","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/15620#issuecomment-2087236600","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps58yTk3","author":{"login":"vuule"},"authorAssociation":"CONTRIBUTOR","body":"> I suspect many of the small copies in `cuDF` come from `rmm::device_scalar`. We could just update the implementation to use a kernel instead of `cudaMemcpy*`.\r\n\r\nAFAIK we also need to get `device_scalar` to use pinned memory. Which implies that we'd also need to pass a host memory resource at to enable the use of a pinned pool.","createdAt":"2024-05-03T18:40:50Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/15620#issuecomment-2093562167","viewerDidAuthor":false}],"createdAt":"2024-04-30T17:45:40Z","id":"I_kwDOBWUGps6Hbf-x","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NjE=","name":"feature request","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwxNDA1MTQ2OTc1","name":"Spark","description":"Functionality that helps Spark RAPIDS","color":"7400ff"}],"milestone":null,"number":15620,"projectCards":[],"projectItems":[],"reactionGroups":[{"content":"HEART","users":{"totalCount":1}}],"state":"OPEN","title":"[FEA] Use SMs to submit small copies to prevent serialization on a busy copy engine","updatedAt":"2024-05-03T18:40:51Z","url":"https://github.com/rapidsai/cudf/issues/15620"}
