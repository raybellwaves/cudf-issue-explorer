{"assignees":[{"id":"MDQ6VXNlcjUzNzk2MDk5","login":"brandon-b-miller","name":""}],"author":{"id":"MDQ6VXNlcjE2OTI5MTQ=","is_bot":false,"login":"randerzander","name":"Randy Gelhausen"},"body":"Using recent cudf nightly conda package (0.19.0a+250.g8632ca0da3):\r\n\r\n**Int & Decimal Addition**:\r\n```\r\nimport cudf\r\nfrom cudf.core.dtypes import Decimal64Dtype\r\n\r\ndf = cudf.DataFrame({'val': [0.01, 0.02, 0.03]})\r\n\r\ndf['dec_val'] = df['val'].astype(Decimal64Dtype(7,2))\r\ndf['dec_val'] + 1\r\n```\r\n**Result**:\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-8-d4f1761193b6> in <module>\r\n----> 1 df['val'] + 1\r\n\r\n/conda/lib/python3.8/site-packages/cudf/core/series.py in __add__(self, other)\r\n   1600 \r\n   1601     def __add__(self, other):\r\n-> 1602         return self._binaryop(other, \"add\")\r\n   1603 \r\n   1604     def radd(self, other, fill_value=None, axis=0):\r\n\r\n/conda/lib/python3.8/contextlib.py in inner(*args, **kwds)\r\n     73         def inner(*args, **kwds):\r\n     74             with self._recreate_cm():\r\n---> 75                 return func(*args, **kwds)\r\n     76         return inner\r\n     77 \r\n\r\n/conda/lib/python3.8/site-packages/cudf/core/series.py in _binaryop(self, other, fn, fill_value, reflect, can_reindex)\r\n   1515         else:\r\n   1516             lhs, rhs = self, other\r\n-> 1517         rhs = self._normalize_binop_value(rhs)\r\n   1518 \r\n   1519         if fn == \"truediv\":\r\n\r\n/conda/lib/python3.8/site-packages/cudf/core/series.py in _normalize_binop_value(self, other)\r\n   2307             return cudf.Scalar(other, dtype=self.dtype)\r\n   2308         else:\r\n-> 2309             return self._column.normalize_binop_value(other)\r\n   2310 \r\n   2311     def eq(self, other, fill_value=None, axis=0):\r\n\r\nAttributeError: 'DecimalColumn' object has no attribute 'normalize_binop_value'\r\n```\r\n\r\n**Workaround**:\r\n```\r\nimport cudf\r\nfrom cudf.core.dtypes import Decimal64Dtype\r\n\r\ndf = cudf.DataFrame({'val': [0.01, 0.02, 0.03]})\r\n\r\ndf['dec_val'] = df['val'].astype(Decimal64Dtype(7,2))\r\ndf['ones'] = 1.00\r\ndf['dec_val'] + df['ones'].astype(Decimal64Dtype(7,0))\r\n```\r\n```\r\n0    1.01\r\n1    1.02\r\n2    1.03\r\ndtype: decimal\r\n```\r\n\r\n**Decimal & Float Multiplication**:\r\n```\r\nimport cudf\r\nfrom cudf.core.dtypes import Decimal64Dtype\r\n\r\ndf = cudf.DataFrame({'val': [0.01, 0.02, 0.03]})\r\n\r\ndf['dec_val'] = df['val'].astype(Decimal64Dtype(7,2))\r\ndf['val'] * df['dec_val']\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-13-4680a31be74b> in <module>\r\n----> 1 df['val'] * df['dec_val']\r\n\r\n/conda/lib/python3.8/site-packages/cudf/core/series.py in __mul__(self, other)\r\n   1799 \r\n   1800     def __mul__(self, other):\r\n-> 1801         return self._binaryop(other, \"mul\")\r\n   1802 \r\n   1803     def rmul(self, other, fill_value=None, axis=0):\r\n\r\n/conda/lib/python3.8/contextlib.py in inner(*args, **kwds)\r\n     73         def inner(*args, **kwds):\r\n     74             with self._recreate_cm():\r\n---> 75                 return func(*args, **kwds)\r\n     76         return inner\r\n     77 \r\n\r\n/conda/lib/python3.8/site-packages/cudf/core/series.py in _binaryop(self, other, fn, fill_value, reflect, can_reindex)\r\n   1542                     rhs = rhs.fillna(fill_value)\r\n   1543 \r\n-> 1544         outcol = lhs._column.binary_operator(fn, rhs, reflect=reflect)\r\n   1545         result = lhs._copy_construct(data=outcol, name=result_name)\r\n   1546         return result\r\n\r\n/conda/lib/python3.8/site-packages/cudf/core/column/numerical.py in binary_operator(self, binop, rhs, reflect)\r\n    108             ):\r\n    109                 msg = \"{!r} operator not supported between {} and {}\"\r\n--> 110                 raise TypeError(msg.format(binop, type(self), type(rhs)))\r\n    111             out_dtype = np.result_type(self.dtype, rhs.dtype)\r\n    112             if binop in [\"mod\", \"floordiv\"]:\r\n\r\nTypeError: 'mul' operator not supported between <class 'cudf.core.column.numerical.NumericalColumn'> and <class 'cudf.core.column.decimal.DecimalColumn'\r\n```\r\n\r\n**Workaround**:\r\n```\r\nimport cudf\r\nfrom cudf.core.dtypes import Decimal64Dtype\r\n\r\ndf = cudf.DataFrame({'val': [0.01, 0.02, 0.03]})\r\n\r\ndf['dec_val'] = df['val'].astype(Decimal64Dtype(7,2))\r\ndf['dec_val'] * df['val'].astype(Decimal64Dtype(7, 2))\r\n```\r\n```\r\n0    0.0001\r\n1    0.0004\r\n2    0.0009\r\ndtype: decimal\r\n```","closed":false,"closedAt":null,"comments":[{"id":"MDEyOklzc3VlQ29tbWVudDgxMjA2OTY1MA==","author":{"login":"brandon-b-miller"},"authorAssociation":"CONTRIBUTOR","body":"Reopening this as there's a piece of the ask here that isn't implemented yet: `Decimal` vs `int` binary ops. ","createdAt":"2021-04-01T17:49:15Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/7680#issuecomment-812069650","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDgxMzYzMjk0Nw==","author":{"login":"brandon-b-miller"},"authorAssociation":"CONTRIBUTOR","body":"@randerzander PR #7859 should close the second part of this - however I think we decided we can't do decimal<->float as we'd need to do some implicit rounding for the user there. \r\n\r\nThat said since integers are exact numbers I went ahead and added that. ","createdAt":"2021-04-05T20:32:31Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/7680#issuecomment-813632947","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDgxNzk3NjEyNg==","author":{"login":"randerzander"},"authorAssociation":"CONTRIBUTOR","body":"@brandon-b-miller I understand the concern about users not realizing rounding happens in an implicit cast, but it would be nice to allow configurable implicit cast behavior.\r\n\r\nFor what it's worth, Spark automatically converts (somewhat surprisingly) to a Double:\r\n```\r\nfrom pyspark.sql import SparkSession\r\n\r\nspark = SparkSession \\\r\n    .builder \\\r\n    .getOrCreate()\r\n\r\ndf = spark.createDataFrame(\r\n    [\r\n        (1, 1.0), # create your data here, be consistent in the types.\r\n        (2, 2.0),\r\n    ],\r\n    ['id', 'doubleCol'] # add your columns label here\r\n)\r\n\r\ndf = df.withColumn('floatCol', df['doubleCol'].cast('float'))\r\ndf = df.withColumn('decCol', df['doubleCol'].cast('decimal(7,2)'))\r\n\r\ndf.withColumn('mixedResult', df['floatCol'] + df['decCol']).schema\r\n```\r\n```\r\nStructType(List(StructField(id,LongType,true),StructField(doubleCol,DoubleType,true),StructField(floatCol,FloatType,true),StructField(decCol,DecimalType(7,2),true),StructField(mixedResult,DoubleType,true)))\r\n\r\n```","createdAt":"2021-04-12T17:05:20Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/7680#issuecomment-817976126","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDgxODc3MzQ4NA==","author":{"login":"brandon-b-miller"},"authorAssociation":"CONTRIBUTOR","body":"@randerzander thank you for that example. As long as there's some authoritative source for what the rules should be, I'd be comfortable adopting that standard and allowing the behavior. Let me dig into spark a bit and figure out where it derives its casting rules and then follow up here. ","createdAt":"2021-04-13T14:18:12Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/7680#issuecomment-818773484","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDgyMTU3NzM0MA==","author":{"login":"brandon-b-miller"},"authorAssociation":"CONTRIBUTOR","body":"So ideally here we'd like to follow spark's rules because they seem fairly robust and also, because we try and avoid inventing casting rules. Here are those rules ([github link](https://github.com/apache/spark/blob/0bfcf9c21069cabafc6fff47a801aa86ca13c608/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/DecimalPrecision.scala#L56-L62))\r\n\r\n```\r\n * In addition, when mixing non-decimal types with decimals, we use the following rules:\r\n * - BYTE gets turned into DECIMAL(3, 0)\r\n * - SHORT gets turned into DECIMAL(5, 0)\r\n * - INT gets turned into DECIMAL(10, 0)\r\n * - LONG gets turned into DECIMAL(20, 0)\r\n * - FLOAT and DOUBLE cause fixed-length decimals to turn into DOUBLE\r\n * - Literals INT and LONG get turned into DECIMAL with the precision strictly needed by the value\r\n```\r\n\r\nThis basically means that when performing a binary op between a decimal column and an integer column, the integer column is cast to decimal with precision `p`, where `p` is the maximum number of digits in the maximum representable value corresponding to the integer columns dtype. \r\n\r\nThe problem is we only support precision 18, and this number is `9223372036854775807` which contains 19 digits. Technically that number itself is representable as decimal in libcudf, but a `Decimal64Dtype(19,0)` isn't valid in cuDF python, because it implies that _any_ 19 digit number can be represented. \r\n\r\nThis leaves us at a bit of an impasse because if we try and cast an `int64` column to decimal in the way the spark rules specify, we run into our own constraint. Fundamentally spark doesn't have this problem because it supports 38 digits of precision. The problem gets worse when we consider [the rules for precision](https://docs.microsoft.com/en-us/sql/t-sql/data-types/precision-scale-and-length-transact-sql?view=sql-server-ver15) in decimal arithmetic ops. Basically most ops tend to create a result that has a higher precision than the inputs, so even if we elected to use precision 18 for `int64` since it's technically safe there's not much we could do with it at that point, since the resulting precision isn't compatible with cudf.\r\n\r\nThere seem to be 3 options:\r\n1. Use spark's rules and disable ops involving one decimal and one `int64` or `uint64` column\r\n2. Don't use spark's rules, invent our own, possibly scan the data and cast to the minimum required precision\r\n3. Wait for 128 bit types \r\n\r\nNone of these seem like great options for me, but I am open to opinions.","createdAt":"2021-04-16T21:38:45Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/7680#issuecomment-821577340","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDg0MTg4Mzg1NQ==","author":{"login":"github-actions"},"authorAssociation":"NONE","body":"This issue has been labeled `inactive-30d` due to no recent activity in the past 30 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed. This issue will be labeled `inactive-90d` if there is no activity in the next 60 days.","createdAt":"2021-05-16T22:04:30Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/7680#issuecomment-841883855","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps49geQt","author":{"login":"github-actions"},"authorAssociation":"NONE","body":"This issue has been labeled `inactive-90d` due to no recent activity in the past 90 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed.","createdAt":"2022-02-07T21:05:11Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/7680#issuecomment-1031922733","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5GfViC","author":{"login":"vyasr"},"authorAssociation":"CONTRIBUTOR","body":"@shwina @isVoid this seems like another potential candidate for #11193 ","createdAt":"2022-07-13T00:21:39Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/7680#issuecomment-1182619778","viewerDidAuthor":false}],"createdAt":"2021-03-23T14:36:45Z","id":"MDU6SXNzdWU4Mzg3OTUwODA=","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NjE=","name":"feature request","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwxMTM5NzQxMjEz","name":"cuDF (Python)","description":"Affects Python cuDF API.","color":"1d76db"}],"milestone":{"number":3,"title":"Decimal data type and operations","description":"","dueOn":null},"number":7680,"projectCards":[{"project":{"name":"Feature Planning"},"column":{"name":"Needs prioritizing"}}],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[FEA] Mixed precision Decimal math support in cudf Python","updatedAt":"2024-02-23T18:43:05Z","url":"https://github.com/rapidsai/cudf/issues/7680"}
