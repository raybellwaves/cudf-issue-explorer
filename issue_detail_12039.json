{"assignees":[],"author":{"id":"MDQ6VXNlcjExMjY5ODE=","is_bot":false,"login":"wence-","name":"Lawrence Mitchell"},"body":"# Summary\r\n\r\nCUDF is not consistent with Pandas (under a bunch of circumstances) in\r\nits behaviour when upcasting during `__setitem__`. In some cases, we\r\nmight want to mimic pandas behaviour (though they are very keen to use\r\nvalue-based type promotion). In others, where we have more structured\r\ndtypes than pandas, we need to decide what to do (current behaviour is\r\ninternally inconsistent and buggy in a bunch of cases).\r\n\r\nI summarise what I think the current state is (by way of experiment),\r\nand then discuss some options. Opinions welcome!\r\n\r\ncc: @vyasr, @mroeschke, @shwina\r\n# Pandas behaviour\r\n\r\nPandas version 1.5.1, MacOS (Apple Silicon)\r\n\r\nEdit: updated code for generating more tables.\r\n\r\nI should note that these tables are for single index `__setitem__` (`s.iloc[i] = value`). I should check if the same behaviour also occurs for:\r\n- [x] slice-based `__setitem__` with single value `s.iloc[:1] = [value]`\r\n- [x] slice-based `__setitem__` with list of values `s.iloc[:2] = [value for _ in range(2)]`\r\n- [x] mask-based `__setitem__` with singleton value `s.iloc[[True, False]] = [value]`\r\n- [x] mask-based `__setitem__` with multiple values `s.iloc[[True, False, True]] = [value, value]`\r\n- [x] index-based `__setitem__` with single value `s.iloc[[1]] = value`\r\n- [x] index-based `__setitem__` with multiple values `s.iloc[[1, 2]] = [value, value]`\r\n\r\n<details>\r\n<summary>Code to generate tables</summary>\r\n\r\n```python\r\nfrom __future__ import annotations\r\n\r\nimport os\r\nfrom enum import Enum, IntEnum, auto\r\nfrom itertools import filterfalse, repeat\r\nfrom operator import not_\r\nfrom pathlib import Path\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\nimport typer\r\n\r\ntry:\r\n    import cudf\r\n    import cupy\r\n\r\n    class Backend(str, Enum):\r\n        PANDAS = \"pandas\"\r\n        CUDF = \"cudf\"\r\n\r\nexcept ImportError:\r\n\r\n    class Backend(str, Enum):\r\n        PANDAS = \"pandas\"\r\n\r\n\r\ndef numeric_series(values, dtype, *, pandas):\r\n    if pandas:\r\n        return pd.Series(values, dtype=dtype)\r\n    else:\r\n        return cudf.Series(values, dtype=dtype)\r\n\r\n\r\ndef format_val(v):\r\n    try:\r\n        dt = v.dtype\r\n        return f\"np.{dt.type.__name__}({v})\"\r\n    except AttributeError:\r\n        return f\"{v}\"\r\n\r\n\r\nclass IndexType(IntEnum):\r\n    SINGLE_INT = auto()\r\n    SINGLETON_SLICE = auto()\r\n    CONTIG_SLICE = auto()\r\n    STRIDED_SLICE = auto()\r\n    SINGLETON_MASK = auto()\r\n    GENERAL_MASK = auto()\r\n    SINGLETON_SCATTER = auto()\r\n    GENERAL_SCATTER = auto()\r\n\r\n\r\ndef indexing(index_type: IndexType, n: int) -> tuple[int | slice | list, slice | list]:\r\n    assert n >= 3\r\n    if index_type == IndexType.SINGLE_INT:\r\n        return n - 1, slice(0, n - 1, None)\r\n    elif index_type == IndexType.SINGLETON_SLICE:\r\n        return slice(1, 2, 1), [0, *range(2, n)]\r\n    elif index_type == IndexType.CONTIG_SLICE:\r\n        return slice(1, n - 2, 1), [0, *range(n - 2, n)]\r\n    elif index_type == IndexType.STRIDED_SLICE:\r\n        return slice(0, n, 2), slice(1, n, 2)\r\n    elif index_type == IndexType.SINGLETON_MASK:\r\n        yes = [False, True, *repeat(False, n - 2)]\r\n        no = list(map(not_, yes))\r\n        return yes, no\r\n    elif index_type == IndexType.GENERAL_MASK:\r\n        yes = [True, False, True, *repeat(False, n - 3)]\r\n        no = list(map(not_, yes))\r\n        return yes, no\r\n    elif index_type == IndexType.SINGLETON_SCATTER:\r\n        yes = [1]\r\n        # Oh for Haskell-esque sections\r\n        no = list(filterfalse(yes.__contains__, range(n)))\r\n        return yes, no\r\n    elif index_type == IndexType.GENERAL_SCATTER:\r\n        yes = [0, 2]\r\n        no = list(filterfalse(yes.__contains__, range(n)))\r\n        return yes, no\r\n    else:\r\n        raise ValueError(\"Unhandled case\")\r\n\r\n\r\ndef generate_table(f, initial_values, values_to_try, dtype, *, index_type, pandas):\r\n    initial_values = np.asarray(initial_values, dtype=object)\r\n    f.write(\"| Initial dtype | New value | Final dtype | Lossy? |\\n\")\r\n    f.write(\"|---------------|-----------|-------------|--------|\\n\")\r\n\r\n    yes, no = indexing(index_type, len(initial_values))\r\n    for value in values_to_try:\r\n        s = numeric_series(initial_values, dtype=dtype, pandas=pandas)\r\n        otype = f\"np.{type(s.dtype).__name__}\"\r\n        try:\r\n            if index_type == IndexType.SINGLETON_SLICE:\r\n                value = cupy.asarray([value])\r\n            s.iloc[yes] = value\r\n        except BaseException as e:\r\n            f.write(f\"| `{otype}` | `{format_val(value)}` | N/A | {e} |\\n\")\r\n            continue\r\n        ntype = f\"np.{type(s.dtype).__name__}\"\r\n        expect = (np.asarray if pandas else cupy.asarray)(\r\n            initial_values[no], dtype=dtype\r\n        )\r\n        original_lost_info = (s.iloc[no].astype(dtype) != expect).any()\r\n        try:\r\n            new_vals = s.iloc[yes].astype(value.dtype)\r\n        except AttributeError:\r\n            if pandas:\r\n                new_vals = np.asarray(s.iloc[yes])\r\n            else:\r\n                new_vals = cupy.asarray(s.iloc[yes])\r\n        new_lost_info = (new_vals != value).any()\r\n        lossy = \"Yes\" if original_lost_info or new_lost_info else \"No\"\r\n        f.write(f\"| `{otype}` | `{format_val(value)}` | `{ntype}` | {lossy} |\\n\")\r\n\r\n\r\ndef generate_tables(output_directory: Path, backend: Backend, index_type: IndexType):\r\n    integer_column_values_to_try = [\r\n        10,\r\n        np.int64(10),\r\n        2**40,\r\n        np.int64(2**40),\r\n        2**80,\r\n        10.5,\r\n        np.float64(10),\r\n        np.float64(10.5),\r\n        np.float32(10),\r\n        np.float32(10.5),\r\n    ]\r\n    float_column_values_to_try = [\r\n        10,\r\n        np.int64(10),\r\n        2**40,\r\n        np.int64(2**40),\r\n        np.int32(2**31 - 100),\r\n        np.int64(2**63 - 100),\r\n        2**80 - 100,\r\n        10.5,\r\n        np.float64(10),\r\n        np.float64(10.5),\r\n        np.float64(np.finfo(np.float32).max.astype(np.float64) * 10),\r\n        np.float32(10),\r\n        np.float32(10.5),\r\n    ]\r\n\r\n    pandas = backend == Backend.PANDAS\r\n    filename = f\"{backend}-setitem-{index_type.name}.md\"\r\n    with open(output_directory / filename, \"w\") as f:\r\n        if pandas:\r\n            f.write(f\"Pandas {pd.__version__} behaviour for {index_type!r}\\n\\n\")\r\n        else:\r\n            f.write(f\"CUDF {cudf.__version__} behaviour for {index_type!r}\\n\\n\")\r\n\r\n        generate_table(\r\n            f,\r\n            [2**31 - 10, 2**31 - 100, 3, 4, 5],\r\n            integer_column_values_to_try,\r\n            np.int32,\r\n            index_type=index_type,\r\n            pandas=pandas,\r\n        )\r\n        f.write(\"\\n\")\r\n        generate_table(\r\n            f,\r\n            [2**63 - 10, 2**63 - 100, 3, 4, 5],\r\n            integer_column_values_to_try,\r\n            np.int64,\r\n            index_type=index_type,\r\n            pandas=pandas,\r\n        )\r\n        f.write(\"\\n\")\r\n        generate_table(\r\n            f,\r\n            [np.finfo(np.float32).max, np.float32(np.inf), 3, 4, 5],\r\n            float_column_values_to_try,\r\n            np.float32,\r\n            index_type=index_type,\r\n            pandas=pandas,\r\n        )\r\n        f.write(\"\\n\")\r\n        generate_table(\r\n            f,\r\n            [np.finfo(np.float64).max, np.float64(np.inf), 3, 4, 5],\r\n            float_column_values_to_try,\r\n            np.float64,\r\n            index_type=index_type,\r\n            pandas=pandas,\r\n        )\r\n\r\n\r\ndef main(\r\n    output_directory: Path = typer.Argument(Path(\".\"), help=\"Output directory for results\"),\r\n    backend: Backend = typer.Option(\"pandas\", help=\"Dataframe backend to test\"),\r\n):\r\n    os.makedirs(output_directory, exist_ok=True)\r\n    for index_type in IndexType.__members__.values():\r\n        generate_tables(output_directory, backend, index_type)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    typer.run(main)\r\n```\r\n\r\n</details>\r\n\r\n## Numeric columns\r\n\r\n### Integer column dtypes\r\n\r\n#### dtype width < max integer width\r\n\r\nInitial values `[2**31 - 10, 2**31 - 100, 3]`. `np.int32` is\r\nrepresentative of any integer type that is smaller than the max width.\r\n\r\n| Initial dtype     | New value                   | Final dtype          | Lossy? |\r\n|-------------------|-----------------------------|----------------------|--------|\r\n| `np.dtype[int32]` | `10`                        | `np.dtype[int32]`    | No[^1] |\r\n| `np.dtype[int32]` | `np.int64(10)`              | `np.dtype[int32]`    | No[^1] |\r\n| `np.dtype[int32]` | `1099511627776`             | `np.dtype[longlong]` | No[^2] |\r\n| `np.dtype[int32]` | `np.int64(1099511627776)`   | `np.dtype[longlong]` | No[^2] |\r\n| `np.dtype[int32]` | `1208925819614629174706176` | `np.dtype[object_]`  | No[^3] |\r\n| `np.dtype[int32]` | `10.5`                      | `np.dtype[float64]`  | No[^4] |\r\n| `np.dtype[int32]` | `np.float64(10.0)`          | `np.dtype[int32]`    | No[^1] |\r\n| `np.dtype[int32]` | `np.float64(10.5)`          | `np.dtype[float64]`  | No[^2] |\r\n| `np.dtype[int32]` | `np.float32(10.0)`          | `np.dtype[int32]`    | No[^1] |\r\n| `np.dtype[int32]` | `np.float32(10.5)`          | `np.dtype[float64]`  | No[^5] |\r\n\r\n[^1]: value is exact in the initial dtype\r\n[^2]: next largest numpy type that contains the value\r\n[^3]: not representable in a numpy type, so coercion to object column\r\n[^4]: default float type is float64\r\n[^5]: `np.int32` is losslessly convertible to `np.float64`\r\n\r\n#### dtype width == max integer width\r\n\r\nInitial values `[2 ** 63 - 10, 2 ** 63 - 100, 3]`. These provoke edge\r\ncases in upcasting because:\r\n```python\r\nimport numpy as np\r\nnp.find_common_type([], [np.int64, np.float64])\r\n# => np.float64 Noooooo! Hates it\r\n# Yes, I know this is the same as the integer to float promotion in\r\n# C/C++, I'm allowed to hate that too.\r\n```\r\n\r\n| Initial dtype     | New value                   | Final dtype         | Lossy?  |\r\n|-------------------|-----------------------------|---------------------|---------|\r\n| `np.dtype[int64]` | `10`                        | `np.dtype[int64]`   | No[^1]  |\r\n| `np.dtype[int64]` | `np.int64(10)`              | `np.dtype[int64]`   | No[^1]  |\r\n| `np.dtype[int64]` | `1099511627776`             | `np.dtype[int64]`   | No[^1]  |\r\n| `np.dtype[int64]` | `np.int64(1099511627776)`   | `np.dtype[int64]`   | No[^1]  |\r\n| `np.dtype[int64]` | `1208925819614629174706176` | `np.dtype[object_]` | No[^3]  |\r\n| `np.dtype[int64]` | `10.5`                      | `np.dtype[float64]` | Yes[^6] |\r\n| `np.dtype[int64]` | `np.float64(10.0)`          | `np.dtype[int64]`   | No[^1]  |\r\n| `np.dtype[int64]` | `np.float64(10.5)`          | `np.dtype[float64]` | Yes[^6] |\r\n| `np.dtype[int64]` | `np.float32(10.0)`          | `np.dtype[int64]`   | No[^1]  |\r\n| `np.dtype[int64]` | `np.float32(10.5)`          | `np.dtype[float64]` | Yes[^6] |\r\n\r\n[^6]: `np.int64` is _not_ losslessly convertible `np.float64`\r\n\r\n### Float column dtypes\r\n\r\n#### dtype width < max float width\r\n\r\nInitial values `[np.finfo(np.float32).max, np.float32(np.inf), 3]`\r\n\r\n| Initial dtype       | New value                            | Final dtype         | Lossy?   |\r\n|---------------------|--------------------------------------|---------------------|----------|\r\n| `np.dtype[float32]` | `10`                                 | `np.dtype[float32]` | No[^1]   |\r\n| `np.dtype[float32]` | `np.int64(10)`                       | `np.dtype[float32]` | No[^1]   |\r\n| `np.dtype[float32]` | `1099511627776`                      | `np.dtype[float32]` | No[^1]   |\r\n| `np.dtype[float32]` | `np.int64(1099511627776)`            | `np.dtype[float32]` | No[^1]   |\r\n| `np.dtype[float32]` | `np.int32(2147483548)`               | `np.dtype[float64]` | No[^1]   |\r\n| `np.dtype[float32]` | `np.int64(9223372036854775708)`      | `np.dtype[float32]` | Yes[^7] |\r\n| `np.dtype[float32]` | `1208925819614629174706076`          | `np.dtype[object_]` | No[^3]  |\r\n| `np.dtype[float32]` | `10.5`                               | `np.dtype[float32]` | No[^1]   |\r\n| `np.dtype[float32]` | `np.float64(10.0)`                   | `np.dtype[float32]` | No[^1]   |\r\n| `np.dtype[float32]` | `np.float64(10.5)`                   | `np.dtype[float32]` | No[^1]   |\r\n| `np.dtype[float32]` | `np.float64(3.4028234663852886e+39)` | `np.dtype[float64]` | No[^2]  |\r\n| `np.dtype[float32]` | `np.float32(10.0)`                   | `np.dtype[float32]` | No[^1]   |\r\n| `np.dtype[float32]` | `np.float32(10.5)`                   | `np.dtype[float32]` | No[^1]   |\r\n\r\n[^7]: value is not losslessly representable, but also, expecting\r\n    `np.float64`!\r\n\r\n#### dtype width == max float width\r\n\r\nInitial values `[np.finfo(np.float64).max, np.float64(np.inf), 3]`\r\n\r\n| Initial dtype       | New value                            | Final dtype         | Lossy?   |\r\n|---------------------|--------------------------------------|---------------------|----------|\r\n| `np.dtype[float64]` | `10`                                 | `np.dtype[float64]` | No[^1]  |\r\n| `np.dtype[float64]` | `np.int64(10)`                       | `np.dtype[float64]` | No[^1]  |\r\n| `np.dtype[float64]` | `1099511627776`                      | `np.dtype[float64]` | No[^1]  |\r\n| `np.dtype[float64]` | `np.int64(1099511627776)`            | `np.dtype[float64]` | No[^1]  |\r\n| `np.dtype[float64]` | `np.int32(2147483548)`               | `np.dtype[float64]` | No[^1]  |\r\n| `np.dtype[float64]` | `np.int64(9223372036854775708)`      | `np.dtype[float64]` | Yes[^6] |\r\n| `np.dtype[float64]` | `1208925819614629174706076`          | `np.dtype[object_]` | No[^3]  |\r\n| `np.dtype[float64]` | `10.5`                               | `np.dtype[float64]` | No[^1]  |\r\n| `np.dtype[float64]` | `np.float64(10.0)`                   | `np.dtype[float64]` | No[^1]  |\r\n| `np.dtype[float64]` | `np.float64(10.5)`                   | `np.dtype[float64]` | No[^1]  |\r\n| `np.dtype[float64]` | `np.float64(3.4028234663852886e+39)` | `np.dtype[float64]` | No[^1]  |\r\n| `np.dtype[float64]` | `np.float32(10.0)`                   | `np.dtype[float64]` | No[^1]  |\r\n| `np.dtype[float64]` | `np.float32(10.5)`                   | `np.dtype[float64]` | No[^1]  |\r\n\r\n## Everything else\r\n\r\nBasically, you can put anything in a column and you get an object out,\r\nbut numpy types are converted to `object` first.\r\n\r\n# CUDF behaviour\r\n\r\nCUDF trunk, and state in #11904.\r\n\r\n## Numeric columns\r\n\r\n### Integer column dtypes\r\n\r\n#### dtype width < max integer width\r\n\r\nInitial values `[2**31 - 10, 2**31 - 100, 3]`. `np.int32` is\r\nrepresentative of any integer type that is smaller than the max width.\r\n\r\n| Initial dtype     | New value                   | Final dtype (trunk)   | Final dtype (#11904)    | Lossy? (trunk) | Lossy? (#11904) |\r\n|-------------------|-----------------------------|-----------------------|-------------------------|----------------|-----------------|\r\n| `np.dtype[int32]` | `10`                        | `np.dtype[int32]`[^8] | `np.dtype[int64]`[^9]   | No             | No              |\r\n| `np.dtype[int32]` | `np.int64(10)`              | `np.dtype[int32]`[^8] | `np.dtype[int64]`[^9]   | No             | No              |\r\n| `np.dtype[int32]` | `1099511627776`             | `np.dtype[int32]`[^8] | `np.dtype[int64]`[^9]   | Yes            | No              |\r\n| `np.dtype[int32]` | `np.int64(1099511627776)`   | `np.dtype[int32]`[^8] | `np.dtype[int64]`[^9]   | Yes            | No              |\r\n| `np.dtype[int32]` | `1208925819614629174706176` | OverflowError         | OverflowError           | N/A            | N/A             |\r\n| `np.dtype[int32]` | `10.5`                      | `np.dtype[int32]`[^8] | `np.dtype[float64]`[^9] | Yes            | No              |\r\n| `np.dtype[int32]` | `np.float64(10.0)`          | `np.dtype[int32]`[^8] | `np.dtype[float64]`[^9] | No             | No              |\r\n| `np.dtype[int32]` | `np.float64(10.5)`          | `np.dtype[int32]`[^8] | `np.dtype[float64]`[^9] | Yes            | No              |\r\n| `np.dtype[int32]` | `np.float32(10.0)`          | `np.dtype[int32]`[^8] | `np.dtype[float64]`[^9] | No             | No              |\r\n| `np.dtype[int32]` | `np.float32(10.5)`          | `np.dtype[int32]`[^8] | `np.dtype[float64]`[^9] | Yes            | No              |\r\n\r\n[^8]: Bug fixed by #11904\r\n[^9]: CUDF doesn't inspect values, so type-based promotion (difference\r\n    from pandas)\r\n\r\n#### dtype width == max integer width\r\n\r\nInitial values `[2 ** 63 - 10, 2 ** 63 - 100, 3]`.\r\n\r\n| Initial dtype     | New value                   | Final dtype (trunk)   | Final dtype (#11904)    | Lossy? (trunk) | Lossy? (#11904) |\r\n|-------------------|-----------------------------|-----------------------|-------------------------|----------------|-----------------|\r\n| `np.dtype[int64]` | `10`                        | `np.dtype[int64]`[^8] | `np.dtype[int64]`[^9]   | No             | No              |\r\n| `np.dtype[int64]` | `np.int64(10)`              | `np.dtype[int64]`[^8] | `np.dtype[int64]`[^9]   | No             | No              |\r\n| `np.dtype[int64]` | `1099511627776`             | `np.dtype[int64]`[^8] | `np.dtype[int64]`[^9]   | No             | No              |\r\n| `np.dtype[int64]` | `np.int64(1099511627776)`   | `np.dtype[int64]`[^8] | `np.dtype[int64]`[^9]   | No             | No              |\r\n| `np.dtype[int64]` | `1208925819614629174706176` | OverflowError         | OverflowError           | N/A            | N/A             |\r\n| `np.dtype[int64]` | `10.5`                      | `np.dtype[int64]`[^8] | `np.dtype[float64]`[^9] | Yes            | Yes[^6]         |\r\n| `np.dtype[int64]` | `np.float64(10.0)`          | `np.dtype[int64]`[^8] | `np.dtype[float64]`[^9] | No             | Yes[^6]         |\r\n| `np.dtype[int64]` | `np.float64(10.5)`          | `np.dtype[int64]`[^8] | `np.dtype[float64]`[^9] | Yes            | Yes[^6]         |\r\n| `np.dtype[int64]` | `np.float32(10.0)`          | `np.dtype[int64]`[^8] | `np.dtype[float64]`[^9] | No             | Yes[^6]         |\r\n| `np.dtype[int64]` | `np.float32(10.5)`          | `np.dtype[int64]`[^8] | `np.dtype[float64]`[^9] | Yes            | Yes[^6]         |\r\n\r\n### Float column dtypes\r\n\r\n#### dtype width < max float width\r\n\r\nInitial values `[np.finfo(np.float32).max, np.float32(np.inf), 3]`\r\n\r\n| Initial dtype       | New value                            | Final dtype (trunk)     | Final dtype (#11904)    | Lossy? (trunk) | Lossy? (#11904) |\r\n|---------------------|--------------------------------------|-------------------------|-------------------------|----------------|-----------------|\r\n| `np.dtype[float32]` | `10`                                 | `np.dtype[float32]`[^8] | `np.dtype[float64]`[^9] | No             | No              |\r\n| `np.dtype[float32]` | `np.int64(10)`                       | `np.dtype[float32]`[^8] | `np.dtype[float64]`[^9] | No             | No              |\r\n| `np.dtype[float32]` | `1099511627776`                      | `np.dtype[float32]`[^8] | `np.dtype[float64]`[^9] | No             | No              |\r\n| `np.dtype[float32]` | `np.int64(1099511627776)`            | `np.dtype[float32]`[^8] | `np.dtype[float64]`[^9] | No             | No              |\r\n| `np.dtype[float32]` | `np.int32(2147483548)`               | `np.dtype[float32]`[^8] | `np.dtype[float64]`[^9] | Yes[^10]       | No              |\r\n| `np.dtype[float32]` | `np.int64(9223372036854775708)`      | `np.dtype[float32]`[^8] | `np.dtype[float64]`[^9] | Yes[^10]       | Yes[^6]         |\r\n| `np.dtype[float32]` | `1208925819614629174706076`          | OverflowError           | OverflowError           | N/A            | N/A             |\r\n| `np.dtype[float32]` | `10.5`                               | `np.dtype[float32]`[^8] | `np.dtype[float64]`[^9] | No             | No              |\r\n| `np.dtype[float32]` | `np.float64(10.0)`                   | `np.dtype[float32]`[^8] | `np.dtype[float64]`[^9] | No             | No              |\r\n| `np.dtype[float32]` | `np.float64(10.5)`                   | `np.dtype[float32]`[^8] | `np.dtype[float64]`[^9] | No             | No              |\r\n| `np.dtype[float32]` | `np.float64(3.4028234663852886e+39)` | `np.dtype[float32]`[^8] | `np.dtype[float64]`[^9] | Yes[^8]        | No              |\r\n| `np.dtype[float32]` | `np.float32(10.0)`                   | `np.dtype[float32]`[^8] | `np.dtype[float32]`[^9] | No             | No              |\r\n| `np.dtype[float32]` | `np.float32(10.5)`                   | `np.dtype[float32]`[^8] | `np.dtype[float32]`[^9] | No             | No              |\r\n\r\n[^10]: As for [^6], but promotion from `np.int32` to `np.float32` is\r\n    also not lossless.\r\n\r\n#### dtype width == max float width\r\n\r\nInitial values `[np.finfo(np.float64).max, np.float64(np.inf), 3]`\r\n\r\n| Initial dtype       | New value                            | Final dtype (trunk) | Final dtype (#11904) | Lossy? (trunk) | Lossy? (#11904) |\r\n|---------------------|--------------------------------------|---------------------|----------------------|----------------|-----------------|\r\n| `np.dtype[float64]` | `10`                                 | `np.dtype[float64]` | `np.dtype[float64]`  | No             | No              |\r\n| `np.dtype[float64]` | `np.int64(10)`                       | `np.dtype[float64]` | `np.dtype[float64]`  | No             | No              |\r\n| `np.dtype[float64]` | `1099511627776`                      | `np.dtype[float64]` | `np.dtype[float64]`  | No             | No              |\r\n| `np.dtype[float64]` | `np.int64(1099511627776)`            | `np.dtype[float64]` | `np.dtype[float64]`  | No             | No              |\r\n| `np.dtype[float64]` | `np.int32(2147483548)`               | `np.dtype[float64]` | `np.dtype[float64]`  | No             | No              |\r\n| `np.dtype[float64]` | `np.int64(9223372036854775708)`      | `np.dtype[float64]` | `np.dtype[float64]`  | Yes[^6]        | Yes[^6]         |\r\n| `np.dtype[float64]` | `1208925819614629174706076`          | OverflowError       | OverflowError        | N/A            | N/A             |\r\n| `np.dtype[float64]` | `10.5`                               | `np.dtype[float64]` | `np.dtype[float64]`  | No             | No              |\r\n| `np.dtype[float64]` | `np.float64(10.0)`                   | `np.dtype[float64]` | `np.dtype[float64]`  | No             | No              |\r\n| `np.dtype[float64]` | `np.float64(10.5)`                   | `np.dtype[float64]` | `np.dtype[float64]`  | No             | No              |\r\n| `np.dtype[float64]` | `np.float64(3.4028234663852886e+39)` | `np.dtype[float64]` | `np.dtype[float64]`  | No             | No              |\r\n| `np.dtype[float64]` | `np.float32(10.0)`                   | `np.dtype[float64]` | `np.dtype[float64]`  | No             | No              |\r\n| `np.dtype[float64]` | `np.float32(10.5)`                   | `np.dtype[float64]` | `np.dtype[float64]`  | No             | No              |\r\n\r\n## Everything else\r\n\r\nThis is where it starts to get _really_ messy. This section is a work\r\nin progress. We should decide what we _want_ the semantics to be,\r\nbecause in most cases pandas doesn't have the same dtypes that CUDF does.\r\n\r\n### Inserting strings into numerical columns\r\n\r\nThis \"works\", for some value of \"works\" on #11904 if the string value\r\nis parseable as the target dtype.\r\n\r\nSo\r\n\r\n```python\r\ns = cudf.Series([1, 2, 3], dtype=int)\r\ns.iloc[2] = \"4\" # works\r\ns.iloc[2] = \"0xf\" # => ValueError: invalid literal for int() with base 10: '0xf'\r\n```\r\n\r\nAnd similarly for float strings and float dtypes.\r\n\r\nThis is probably a nice feature.\r\n\r\n### Inserting things into string columns\r\n\r\nWorks if the the \"thing\" is convertible to a string (so numbers work),\r\nbut Scalars with list or struct dtypes don't work.\r\n\r\nI would argue that explicit casting from the user here is probably\r\nbetter.\r\n\r\n### List columns\r\n\r\nThe new value must have an identical dtype to that of the target column.\r\n\r\n### Struct columns\r\n\r\nThe new value must have leaf dtypes that are considered compatible in\r\nsome sense, but then the leaves are downcast to the leaf dtypes of the\r\ntarget column. So this is lossy and likely a bug:\r\n\r\n```python\r\n sr = cudf.Series([{\"a\": 1, \"b\": 2}])\r\n sr.iloc[0] = {\"a\": 10.5, \"b\": 2}\r\n sr[0] # => {\"a\": 10, \"b\": 2} (lost data in \"a\")\r\n```\r\n## What I think we want (for composite columns)\r\n\r\nFor composite columns, if the dtype shapes match, I think the casting\r\nrule should be to traverse to the leaf dtypes and promote using the\r\nrules for non-composite columns. If shapes don't match, `__setitem__`\r\nshould not be allowed.\r\n\r\nThis, to me, exhibits principle of least surprise.\r\n","closed":false,"closedAt":null,"comments":[{"id":"IC_kwDOBWUGps5Nixg-","author":{"login":"shwina"},"authorAssociation":"CONTRIBUTOR","body":"What a fantastic analysis. ","createdAt":"2022-11-02T17:20:07Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"HOORAY","users":{"totalCount":2}}],"url":"https://github.com/rapidsai/cudf/issues/12039#issuecomment-1300961342","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5Nps3t","author":{"login":"mroeschke"},"authorAssociation":"CONTRIBUTOR","body":"Just a comment about the `Lossy` classification:\r\n\r\nOne of the factors of `Lossy=\"Yes\"` is if: \r\n\r\n```\r\nexpect = (np.asarray if pandas else cupy.asarray)(\r\n    initial_values[:new_location], dtype=dtype\r\n)\r\noriginal_lost_info = (s.iloc[:new_location].astype(dtype) != expect).any()\r\n```\r\n\r\nAnd in this example from your table had `Lossy=\"Yes\"`:\r\n\r\n```\r\nIn [26]: ser = pd.Series([2 ** 63 - 10, 2 ** 63 - 100, 3], dtype=np.int64)\r\n\r\nIn [27]: ser\r\nOut[27]:\r\n0    9223372036854775798\r\n1    9223372036854775708\r\n2                      3\r\ndtype: int64\r\n\r\nIn [29]: ser.iloc[2] = 10.5\r\n\r\nIn [32]: ser\r\nOut[32]:\r\n0    9.223372e+18\r\n1    9.223372e+18\r\n2    1.050000e+01\r\ndtype: float64\r\n```\r\n\r\nI think this is \"lossy\" because of an `int64` -> `float64` -> `int64` conversion, not necessarily because there's a lossiness in value\r\n\r\n```\r\nIn [47]: ser.iloc[0] == 2 ** 63 - 10\r\nOut[47]: True\r\n\r\nIn [48]: ser.iloc[1] == 2 ** 63 - 100\r\nOut[48]: True\r\n\r\nIn [49]: ser.iloc[2] == 10.5\r\nOut[49]: True\r\n\r\nIn [50]: pd.Series([2 ** 63 - 10, 2 ** 63 - 100, 3], dtype=np.int64)\r\nOut[50]:\r\n0    9223372036854775798\r\n1    9223372036854775708\r\n2                      3\r\ndtype: int64\r\n\r\nIn [51]: pd.Series([2 ** 63 - 10, 2 ** 63 - 100, 3], dtype=np.int64).astype(np.float64).astype(np.int64)\r\nOut[51]:\r\n0   -9223372036854775808\r\n1   -9223372036854775808\r\n2                      3\r\ndtype: int64\r\n\r\nIn [52]: np.array([2 ** 63 - 10, 2 ** 63 - 100, 3], dtype=np.int64).astype(np.float64).astype(np.int64)\r\nOut[52]: array([-9223372036854775808, -9223372036854775808,                    3])\r\n```\r\n","createdAt":"2022-11-03T23:11:36Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12039#issuecomment-1302777325","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5Npw5F","author":{"login":"vyasr"},"authorAssociation":"CONTRIBUTOR","body":"> What a fantastic analysis.\r\n\r\nThis is easily one of the most detailed issues I have ever seen!","createdAt":"2022-11-03T23:39:15Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12039#issuecomment-1302793797","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5NrX_p","author":{"login":"wence-"},"authorAssociation":"CONTRIBUTOR","body":"> Just a comment about the `Lossy` classification:\r\n> \r\n> One of the factors of `Lossy=\"Yes\"` is if:\r\n> \r\n> ```\r\n> expect = (np.asarray if pandas else cupy.asarray)(\r\n>     initial_values[:new_location], dtype=dtype\r\n> )\r\n> original_lost_info = (s.iloc[:new_location].astype(dtype) != expect).any()\r\n> ```\r\n> \r\n> And in this example from your table had `Lossy=\"Yes\"`:\r\n> \r\n> ```\r\n> In [26]: ser = pd.Series([2 ** 63 - 10, 2 ** 63 - 100, 3], dtype=np.int64)\r\n> \r\n> In [27]: ser\r\n> Out[27]:\r\n> 0    9223372036854775798\r\n> 1    9223372036854775708\r\n> 2                      3\r\n> dtype: int64\r\n> \r\n> In [29]: ser.iloc[2] = 10.5\r\n> \r\n> In [32]: ser\r\n> Out[32]:\r\n> 0    9.223372e+18\r\n> 1    9.223372e+18\r\n> 2    1.050000e+01\r\n> dtype: float64\r\n> ```\r\n> \r\n> I think this is \"lossy\" because of an `int64` -> `float64` -> `int64` conversion, not necessarily because there's a lossiness in value\r\n\r\nDepends what you mean by lossy. float64 only has enough precision to exactly represent int53 (because the mantissa has 53 bits). Hence, mapping between int64 and float64 is neither surjective (you can't reach nan and inf) nor injective (multiple int64s can map to the same float64), so you can't roundtrip successfully.\r\n\r\n```python\r\nIn [36]: np.int64(2**63 - 10) == np.int64(2**63 - 100)\r\nOut[36]: False\r\n\r\nIn [37]: np.int64(2**63 - 10).astype(np.float64) == np.int64(2**63 - 100).astype(np.float64)\r\nOut[37]: True\r\n```\r\n> \r\n> ```\r\n> In [47]: ser.iloc[0] == 2 ** 63 - 10\r\n> Out[47]: True\r\n> \r\n> In [48]: ser.iloc[1] == 2 ** 63 - 100\r\n> Out[48]: True\r\n> \r\n> In [49]: ser.iloc[2] == 10.5\r\n> Out[49]: True\r\n\r\nThese are all true because the right operand of equality is promoted to `float64` before the comparison is performed.\r\n\r\n> In [50]: pd.Series([2 ** 63 - 10, 2 ** 63 - 100, 3], dtype=np.int64)\r\n> Out[50]:\r\n> 0    9223372036854775798\r\n> 1    9223372036854775708\r\n> 2                      3\r\n> dtype: int64\r\n> \r\n> In [51]: pd.Series([2 ** 63 - 10, 2 ** 63 - 100, 3], dtype=np.int64).astype(np.float64).astype(np.int64)\r\n> Out[51]:\r\n> 0   -9223372036854775808\r\n> 1   -9223372036854775808\r\n\r\nI characterise this as \"lossy\" because you didn't round-trip successfully. As noted, it isn't really a pandas issue per-se, but rather numpy (and thence promotion via C rules).\r\n\r\nI note that numpy doesn't _quite_ implement [C/C++ promotion rules](https://en.cppreference.com/w/c/language/conversion) for integer and floating point types. Since it widens floating point types in addition between ints and floats.\r\n\r\n[C++](https://gcc.godbolt.org/z/v9sPKbPE4):\r\n```\r\ntypeof(1 + 1.0f) => float32\r\ntypeof(1L + 1.0f) => float32\r\ntypeof(1 + 1.0d) => float64\r\ntypeof(1L + 1.0d) => float64\r\n```\r\nThese promotions are always in-range, since `max(float32) > max(int64)` but lossy since the mapping from integers to floating point types of the same width is not bijective.\r\n\r\nnumpy:\r\n```\r\n(np.int32(1) + np.float32(1)).dtype => float64\r\n(np.int64(1) + np.float32(1)).dtype => float64\r\n(np.int32(1) + np.float64(1)).dtype => float64\r\n(np.int64(1) + np.float64(1)).dtype => float64\r\n```\r\nThese promotions are bijective (if we ignore nan and inf) if the integer type is narrower than the floating point type. (So the int32 promotion can be undone losslessly).\r\n\r\nThese kind of automatic promotions are basically a fight I lost before I was born in old programming languages, though some more modern ones force you to always explicitly cast so that you're aware that you might be doing A Bad Thing.","createdAt":"2022-11-04T10:15:37Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12039#issuecomment-1303216105","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5NuIVl","author":{"login":"wence-"},"authorAssociation":"CONTRIBUTOR","body":"Updated the code to generate tables for a bunch of different scenarios. You can now run and it produces a separate file for each scenario, which you can diff against each other to see the points of difference.\r\n\r\nMostly things work (with #11904), except #12072 means that equality is busted.\r\n\r\nAnd, slice-based setitem is broken for length-one ranges if setting with an array rather than a scalar (that's #12073).","createdAt":"2022-11-04T17:43:45Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12039#issuecomment-1303938405","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5NumzN","author":{"login":"seberg"},"authorAssociation":"CONTRIBUTOR","body":"Sorry for the long post, I hope it doesn't derail or is off topic.  I will mainly try to explain the state of NumPy and what [NEP 50](https://numpy.org/neps/nep-0050-scalar-promotion.html) does.  Happy to re-read again without that in mind!\r\n(I will always make time to talk about these things, because just talking about it, even tangentially, helps me clear up things and move forward.)\r\n\r\nN.B. please avoid `np.find_common_type` it needs to be deprecated.  `np.result_type` (for array inputs or if you need the value-based or future NEP 50 behavior) and `np.promote_types` are the weapons of choice.\r\n\r\nFirst, there are three things that come together to understand the NumPy behavior:\r\n1. Integers are converted to `long -> long long -> unsigned long long -> object` by NumPy based on value.  This is what happens if you do `np.asarray(integer)` and in many other contexts.\r\n2. Storing values may or may not go via `np.asarray()`, if it does not, NumPy will be more strict sometimes.\r\n   In fact, I *just* deprecated storing out-of-bounds integers like to disallow `np.asarray([-1], dtype=np.uint8)`.  This will be part of the next NumPy release, unless someone complains (which could happen).\r\n3. *Promotion/Common DType:* i.e. what happens if you mix two dtypes\r\n   * NumPy promotion rules are wonky, they try to be \"safe\", but make an exception by promoting `in64 + float64 -> float64`  (and worse `int64 + uint64 -> float64`).  The reason for this is that they fall out of the \"safe casting\" concept.  I disagree with this nowadays, while related, I feel it is better to see promotion/common-dtype and safe/non-lossy casting as distinct.  For numerical dtypes, I do agree that round-tripping correctly implies that promotion is acceptable.  (NumPy is implemented that way now after NEP 42.)\r\n     The C promotion rules, etc. have the advantage of being e.g. strictly associative (EDIT) while ours are not: `(int16 + uint16) + float32 == int32 + float32 == float64 != float32 == (float32 + int16) + uint16`  (I/NumPy has a heuristic so that `np.result_type` finds `float32` if it is a single operation, it works always, but you can construct failures if you add new dtypes in the mix.)\r\n   * *Current* promotion inspects values (of \"scalars\" including 0-D arrays, but not when all are scalars).  This allows an integer to match any dtype that will fit its value, and floats also to be \"down-promoted\" so long they are not approximately out of range.\r\n    *NEP 50 is the proposal to remove this*.  Instead, NEP 50 proposes to insert the Python integer, float, complex as undefined/lowest precision.  For integers this means that `np.uint8(1) + 300` will error, because we try to do `uint8 + uint8 -> uint8` and that doesn't work.\r\n   NEP 50 tries to describe this, the [JAX diagram](https://jax.readthedocs.io/en/latest/type_promotion.html) also helps.  This makes the Python scalars \"weak\".\r\n\r\n---\r\n\r\nThere is discussion about *lossy* conversion of Python scalars.  Which touches something that I am currently struggling with to push NEP 50.  So bringing it up, because if you clearly want the notion of \"lossy\", it might inform how I do this for NEP 50 (it is not central to NEP 50, and doesn't have to be; but I need to settle on something).\r\n\r\nFor NumPy dtypes we have that defined through the \"casting safety\" (which can be \"no\", \"equiv\", \"safe\", \"same kind\", \"unsafe\".  For *values* we don't really define it, and I don't think the categories quite work.\r\n`int64 -> int8` is same-kind, but `10000 -> int8` seems \"unsafe\", or is it \"safe\" but fails when you try it?  Whether or not I should try to introduce the notion \"safety\" to scalar assignments in NumPy is one of the things I am struggling with, because the notion of cast-safety doesn't seem to cleanly apply (but NumPy exposes it a lot).","createdAt":"2022-11-04T19:27:40Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12039#issuecomment-1304063181","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5NuoFn","author":{"login":"seberg"},"authorAssociation":"CONTRIBUTOR","body":"NEP 50 itself does not touch the annoying fact that `np.array([2**63, 2**61])` returns a float array.  But, since NEP 50 is likely a NumPy 2.0, I think it does make sense to stop doing it and only ever go to the default integer and otherwise raise an error.","createdAt":"2022-11-04T19:32:39Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12039#issuecomment-1304068455","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5Nvcvk","author":{"login":"mroeschke"},"authorAssociation":"CONTRIBUTOR","body":"I think generally from the pandas point of view (with respect to `__setitem__`), I would say the general philosophy is \"the resulting dtype should be able to store the existing values & new value(s) without losing numeric precision\".\r\n\r\nHere are some examples (related to your original table) where we differ from numpy due to the above:\r\n\r\n\r\n1 . \"Don't truncate so upcast dtype\" example\r\n\r\n```\r\nIn [6]: ser = pd.Series([2**63 - 10, 2**63 - 100, 3], dtype=np.int64)\r\n\r\nIn [7]: ser.iloc[2] = 10.5\r\n\r\nIn [8]: ser\r\nOut[8]:\r\n0    9.223372e+18\r\n1    9.223372e+18\r\n2    1.050000e+01\r\ndtype: float64\r\n\r\nIn [9]: np_arr = np.array([2**63 - 10, 2**63 - 100, 3], dtype=np.int64)\r\n\r\nIn [10]: np_arr[2] = 10.5\r\n\r\nIn [11]: np_arr\r\nOut[11]: array([9223372036854775798, 9223372036854775708,                  10])\r\n\r\nIn [14]: print(np.__version__) ; print(pd.__version__)\r\n1.23.1\r\n2.0.0.dev0+577.g7e9ca6e8af\r\n``` \r\n\r\n\r\n2. \"Bend over backwards to accommodate the values\" examples\r\n```\r\nIn [5]: ser = pd.Series([1, 2, 3], dtype=np.int64)\r\n\r\nIn [6]: ser.iloc[2] = 2**80\r\n\r\nIn [7]: ser\r\nOut[7]:\r\n0                            1\r\n1                            2\r\n2    1208925819614629174706176\r\ndtype: object\r\n\r\nIn [8]: arr = np.array([1, 2, 3], dtype=np.int64)\r\n\r\nIn [9]: arr\r\nOut[9]: array([1, 2, 3])\r\n\r\nIn [10]: arr[2] = 1208925819614629174706176\r\na---------------------------------------------------------------------------\r\nOverflowError                             Traceback (most recent call last)\r\nInput In [10], in <cell line: 1>()\r\n----> 1 arr[2] = 1208925819614629174706176\r\n\r\nOverflowError: Python int too large to convert to C long\r\n```","createdAt":"2022-11-04T21:43:29Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12039#issuecomment-1304284132","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5Nz-Y4","author":{"login":"wence-"},"authorAssociation":"CONTRIBUTOR","body":"> 2\\. \"Bend over backwards to accommodate the values\" examples\r\n\r\nUnless we want to implement bignums in libcudf, I think this is not something we can really contemplate.","createdAt":"2022-11-07T11:29:04Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12039#issuecomment-1305470520","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5Nz_Om","author":{"login":"wence-"},"authorAssociation":"CONTRIBUTOR","body":"> I think generally from the pandas point of view (with respect to `__setitem__`), I would say the general philosophy is \"the resulting dtype should be able to store the existing values & new value(s) without losing numeric precision\".\r\n\r\nThis is a reasonable philosophy, though it does rather depend on what you mean by \"losing numeric precision\". \r\n\r\n> 1 . \"Don't truncate so upcast dtype\" example\r\n> \r\n> ```\r\n> In [6]: ser = pd.Series([2**63 - 10, 2**63 - 100, 3], dtype=np.int64)\r\n> \r\n> In [7]: ser.iloc[2] = 10.5\r\n> \r\n> In [8]: ser\r\n> Out[8]:\r\n> 0    9.223372e+18\r\n> 1    9.223372e+18\r\n> 2    1.050000e+01\r\n> dtype: float64\r\n> ```\r\n\r\nThis seems like a nice thing to do, except that it changes behaviour of the series:\r\n\r\n```\r\nIn [22]: ser = pd.Series([2**63 - 10, 2**63 - 100, 3], dtype=np.int64)\r\n\r\nIn [23]: ser == ser.iloc[0]\r\nOut[23]: \r\n0     True\r\n1    False\r\n2    False\r\ndtype: bool\r\n\r\nIn [24]: ser.iloc[2] = 10.5\r\n\r\nIn [25]: ser == ser.iloc[0]\r\nOut[25]: \r\n0     True\r\n1     True\r\n2    False\r\ndtype: bool\r\n```\r\n\r\nI would argue that this choice _did_ lose numeric precision, and particularly in _existing_ values that weren't touched by the indexing.","createdAt":"2022-11-07T11:32:51Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12039#issuecomment-1305473958","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5N0hmf","author":{"login":"seberg"},"authorAssociation":"CONTRIBUTOR","body":"So pandas (and cudf) use common DType to promote a column when necessary.  That leads to two things:\r\n* The special rule for going to float64 does lose precision for int64/uint64.\r\n* Pandas will even allow going to `object` dtype.\r\n\r\nThe first issue is independent of NEP 50 and I am not sure there is much to do about it.  You could consider disallowing it.  It probably means making a clearer distinction between \"common DType\" (for storing into a single column) and \"promotion\" (for operations).\r\nThat is, the promotion in `float_arr + int64_arr -> float_arr` (i.e. adding two arrays) seems fine, but maybe storing the two into one container is not?\r\nI make the distinction in NumPy, but not convenient to apply it broadly.  My gut feeling is that it is probably best to live with the way it is:  Yes, it loses precision, but it is usually OK.\r\n\r\nFor the second thing, NEP 50 might nudge towards making it an error in `pandas`:  If you apply NEP 50 promotion rules, the column should keep whatever `int` dtype it has, and if the Python integer cannot be assigned it should just error.  Auto-promoting to `object` dtype would then never work (which is also what will happen for `np.int64(1) + 2**500`).","createdAt":"2022-11-07T13:25:40Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12039#issuecomment-1305614751","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5N2Iw1","author":{"login":"mroeschke"},"authorAssociation":"CONTRIBUTOR","body":"> I would argue that this choice did lose numeric precision, and particularly in existing values that weren't touched by the indexing.\r\n\r\nThat's fair. Not 100% sure of the origin of promoting to `float64` in pandas for this case, but I would venture to guess the numpy behavior of keeping int64 and truncating the 10.5 to 10 was a more suprising & more significant loss in precision behavior to pandas users.\r\n\r\n> For the second thing, NEP 50 might nudge towards making it an error in pandas:\r\n\r\nThis is also fair. Maybe this is a case of pandas being too accomodating, but we can always nudge users to `astype(object)` in these cases.\r\n\r\n ","createdAt":"2022-11-07T18:45:54Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12039#issuecomment-1306037301","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5N5eNZ","author":{"login":"wence-"},"authorAssociation":"CONTRIBUTOR","body":"> Not 100% sure of the origin of promoting to `float64` in pandas for this case,\r\n\r\nIf you're going to do integral to floating type promotion and want to maintain accuracy in the largest number of cases (and don't care too much about memory footprint) the promoting to float64 is a reasonable choice.","createdAt":"2022-11-08T09:37:20Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/12039#issuecomment-1306911577","viewerDidAuthor":false}],"createdAt":"2022-11-01T18:01:16Z","id":"I_kwDOBWUGps5VWD2Y","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NjE=","name":"feature request","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWw1OTk2MjY1NjQ=","name":"question","description":"Further information is requested","color":"D4C5F9"},{"id":"MDU6TGFiZWwxMDEzOTg3NTAz","name":"2 - In Progress","description":"Currently a work in progress","color":"fef2c0"},{"id":"MDU6TGFiZWwxMDEzOTg3OTIx","name":"proposal","description":"Change current process or code","color":"2a2c89"},{"id":"MDU6TGFiZWwxMTM5NzQxMjEz","name":"cuDF (Python)","description":"Affects Python cuDF API.","color":"1d76db"},{"id":"MDU6TGFiZWwyNTQ2NTIxMDI0","name":"improvement","description":"Improvement / enhancement to an existing function","color":"bfd4f2"}],"milestone":null,"number":12039,"projectCards":[{"project":{"name":"Feature Planning"},"column":{"name":"Needs prioritizing"}}],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[ENH/QST]: Behaviour of type promotion in `__setitem__`","updatedAt":"2024-05-16T05:12:38Z","url":"https://github.com/rapidsai/cudf/issues/12039"}
