{"assignees":[],"author":{"id":"MDQ6VXNlcjE5MDEwNTk=","is_bot":false,"login":"abellina","name":"Alessandro Bellina"},"body":"As the number of columns increases for `cudf::gather` with the same gather map, we see the number of kernels called increase proportionally and the runtime increases linearly. We are wondering if there are better ways to group or \"batch\" these calls so we perform less kernel invocations that can do more work all at once, in hopes of amortizing some of the cost with many columns or deeply nested schemas.\r\n\r\nA very simple example is below. This creates a column of 10 `int32_t` rows and adds it to a struct N times (where `N` is between 2 and 1024):\r\n\r\n```\r\n#include <cudf/table/table.hpp>\r\n#include <cudf_test/column_wrapper.hpp>\r\n\r\n#include <rmm/mr/device/cuda_memory_resource.hpp>\r\n#include <rmm/mr/device/device_memory_resource.hpp>\r\n#include <rmm/mr/device/pool_memory_resource.hpp>\r\n\r\n#include <memory>\r\n#include <string>\r\n#include <vector>\r\n#include <nvtx3/nvToolsExt.h>\r\n\r\nint main(int argc, char** argv)\r\n{\r\n  rmm::mr::cuda_memory_resource cuda_mr{};\r\n  rmm::mr::pool_memory_resource mr{&cuda_mr};\r\n  rmm::mr::set_current_device_resource(&mr);\r\n  using col_t = cudf::test::fixed_width_column_wrapper<int32_t>;\r\n\r\n  auto const values = std::vector<int32_t>{1,2,3,4,5,6,7,8,9,10};\r\n  for (int num_cols = 2; num_cols <= 1024; num_cols *= 2) {\r\n    std::vector<std::unique_ptr<cudf::column>> members(num_cols);\r\n    for (auto i = 0; i < num_cols; ++i) {\r\n      auto wrapper = col_t(values.begin(), values.end());\r\n      members[i] = wrapper.release();\r\n    }\r\n    auto struct_col = cudf::test::structs_column_wrapper(std::move(members));\r\n    auto gather_map = std::vector<cudf::offset_type>{1}; // gather 1 row\r\n    std::stringstream msg;\r\n    nvtxRangePush(msg.str().c_str()); \r\n    auto result = cudf::gather(\r\n      cudf::table_view{{struct_col}}, \r\n      cudf::test::fixed_width_column_wrapper<int32_t>(gather_map.begin(), gather_map.end()),\r\n      cudf::out_of_bounds_policy::NULLIFY);\r\n    nvtxRangePop();\r\n    std::cout << \"Result: rows: \" << result->num_rows() << \" cols: \" << result->num_columns() << std::endl;\r\n\r\n  }\r\n  return 0;\r\n}\r\n```\r\nAs the column count increases by 2x, the gather kernel takes 2x longer:\r\n\r\n![Screenshot from 2023-06-05 10-18-52](https://github.com/rapidsai/cudf/assets/1901059/500ac308-9ab3-494c-97fa-1ffec7ac875a)\r\n\r\nA similar argument can be made for columns that have nested things like arrays of structs (each with array members). The number of calls to underlying cub calls can increase drastically.\r\n\r\nI am filing this issue to solicit comments/patches to see how we could improve this behavior.\r\n","closed":false,"closedAt":null,"comments":[{"id":"IC_kwDOBWUGps5d_2nw","author":{"login":"abellina"},"authorAssociation":"CONTRIBUTOR","body":"I also believe, that improving the performance for `gather` will help with `copy_if` (for non fixed width, for fix width it looks like we implement our own `scatter` kernel). `copy_if` is another kernel with very similar behavior. I think we can discuss what we want here and we can target `copy_if` as a follow on given what we learn with `gather`.","createdAt":"2023-06-05T15:28:10Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/13509#issuecomment-1577019888","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5d_3p4","author":{"login":"nvdbaranec"},"authorAssociation":"CONTRIBUTOR","body":"If we ignore lists and strings for the moment, I think it would be pretty easy to put together a proof-of-concept for doing batched fixed width gathers as a single kernel invocation (well, maybe 2 - one more for validity).\r\n\r\nStrings is probably not too hard of an extension.  Lists would definitely be tricky.  I'd have to wrap my head around the list gather stuff to remember :) ","createdAt":"2023-06-05T15:31:00Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/13509#issuecomment-1577024120","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5eAIyY","author":{"login":"bdice"},"authorAssociation":"CONTRIBUTOR","body":"At one point cudf (possibly before libcudf!) used a stream pool for gather operations. Each gather is independent, so we can launch all the kernels on separate streams and synchronize them with an event on the input stream. I would love to reimplement this approach and see if it can improve the performance. See also #12086.","createdAt":"2023-06-05T16:15:40Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/13509#issuecomment-1577094296","viewerDidAuthor":false}],"createdAt":"2023-06-05T15:24:45Z","id":"I_kwDOBWUGps5n1SrD","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NjE=","name":"feature request","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwxMDEzOTg3MzUy","name":"0 - Backlog","description":"In queue waiting for assignment","color":"d4c5f9"},{"id":"MDU6TGFiZWwxMTM5NzQwNjY2","name":"libcudf","description":"Affects libcudf (C++/CUDA) code.","color":"c5def5"},{"id":"MDU6TGFiZWwxMzIyMjUyNjE3","name":"Performance","description":"Performance related issue","color":"C2E0C6"},{"id":"MDU6TGFiZWwxNDA1MTQ2OTc1","name":"Spark","description":"Functionality that helps Spark RAPIDS","color":"7400ff"}],"milestone":{"number":20,"title":"Stabilizing large workflows (OOM, spilling, partitioning)","description":"","dueOn":null},"number":13509,"projectCards":[],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[FEA] Improve cudf::gather scalability as number of columns increases","updatedAt":"2023-06-29T21:52:08Z","url":"https://github.com/rapidsai/cudf/issues/13509"}
