{"assignees":[{"id":"MDQ6VXNlcjE2MDA1Njkw","login":"vuule","name":"Vukasin Milovanovic"}],"author":{"id":"MDQ6VXNlcjc2NjAzNjA2","is_bot":false,"login":"Ploverain","name":""},"body":"**Describe the bug**\r\nThe cudf to_csv interface cannot write files larger than 2GB and displays a negative size error.\r\n\r\n\r\n**Steps/Code to reproduce bug**\r\nimport cudf\r\n\r\ndf = cudf.read_csv(\"3G.csv\")\r\ndf.to_csv(\"result.csv\")\r\n\r\n\r\n**Expected behavior**\r\ni hope df.to_csv() create a 3G size csv\r\n\r\n**Environment overview (please complete the following information)**\r\n - Environment location: [Bare-metal, Docker, Cloud(specify cloud provider)]\r\n - Method of cuDF install: [conda, Docker, or from source]\r\n   - If method of install is [Docker], provide `docker pull` & `docker run` commands used\r\n\r\n**Environment details**\r\nPlease run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n","closed":false,"closedAt":null,"comments":[{"id":"IC_kwDOBWUGps5jTKTR","author":{"login":"GregoryKimball"},"authorAssociation":"CONTRIBUTOR","body":"Thank you @Ploverain for sharing this request. It sounds like the CSV writer is limited by our strings column character limit of 2.1B characters (also see #13733). At a minimum we should provide a better error message that recommends partitioning the dataframe.","createdAt":"2023-08-04T17:40:41Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/13785#issuecomment-1665967313","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5jTMk8","author":{"login":"beckernick"},"authorAssociation":"MEMBER","body":"You can use the `chunksize` parameter to get around this issue. E.g.,\r\n\r\n```python\r\ndf = cudf.read_csv(\"3G.csv\")\r\ndf.to_csv(\"result.csv\", chunksize=5e6) # assuming five million rows will work -- you may want to try a higher or lower value\r\n```\r\n\r\nWe could also explore handling this under the hood in the Python layer (via some kind of data introspection or otherwise) (cc @wence- , as this came up in a recent discussion)","createdAt":"2023-08-04T17:49:31Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/13785#issuecomment-1665976636","viewerDidAuthor":false}],"createdAt":"2023-07-31T03:21:24Z","id":"I_kwDOBWUGps5s-g7s","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NTk=","name":"bug","description":"Something isn't working","color":"d73a4a"},{"id":"MDU6TGFiZWwxMDEzOTg3Nzk5","name":"0 - Waiting on Author","description":"Waiting for author to respond to review","color":"ffb88c"},{"id":"MDU6TGFiZWwxMTM5NzQwNjY2","name":"libcudf","description":"Affects libcudf (C++/CUDA) code.","color":"c5def5"},{"id":"MDU6TGFiZWwxMTg1MjQ0MTQy","name":"cuIO","description":"cuIO issue","color":"fef2c0"}],"milestone":{"number":12,"title":"CSV continuous improvement","description":"","dueOn":null},"number":13785,"projectCards":[],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[BUG]The cudf to_csv interface cannot read files larger than 2GB and displays a negative size error.","updatedAt":"2024-05-17T22:11:34Z","url":"https://github.com/rapidsai/cudf/issues/13785"}
