{"assignees":[],"author":{"id":"MDQ6VXNlcjg4MTMwMDI=","is_bot":false,"login":"razajafri","name":"Raza Jafri"},"body":"**Describe the bug**\r\nRunning min aggregate on a table returns the NaN value as its long value instead of the literal \"nan\" as it does for the other aggregates. I haven't gotten around to writing a unit test for this but can do if so required \r\n\r\n**Steps/Code to reproduce bug**\r\nCreate the following table \r\n```\r\nscala> spark.sql(\"\"select * from floatsAndDoubles\"\").show\r\n+-----+------+\r\n|float|double|\r\n+-----+------+\r\n|  NaN|   NaN|\r\n| 1.02|   NaN|\r\n|  NaN|   4.5|\r\n+-----+------+\r\n```\r\n\r\nrunning an aggregate(min) op on the double column will result in the following table \r\n\r\n```\r\n+----------+-----------+\r\n| float    |min(double)|\r\n+----------+-----------+\r\n| 1.020000 | 179769313486231570000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000.000000 |\r\n| NaN      | 4.500000  |\r\n+----------+-----------+\r\n```\r\n\r\n**Expected behavior**\r\nIt should output this \r\n\r\n```\r\nscala> spark.sql(\"\"select float, min(double) from floatsAndDoubles group by float\"\").show\r\n+-----+-----------+\r\n|float|min(double)|\r\n+-----+-----------+\r\n| 1.02|        NaN|\r\n|  NaN|        4.5|\r\n+-----+-----------+\r\n```\r\n\r\n**Additional context**\r\nFor context here is what aggregate(sum) does in cudf\r\n```\r\n+------+-----+\r\n|float | sum |\r\n+------+-----+\r\n| 1.02 | NaN |\r\n| NaN  | NaN |\r\n+------+-----+\r\n```\r\n","closed":false,"closedAt":null,"comments":[{"id":"MDEyOklzc3VlQ29tbWVudDYwNjYzNDgzMA==","author":{"login":"jrhemstad"},"authorAssociation":"CONTRIBUTOR","body":"The example was very confusing until I realized you were describing a _groupby_ aggregation. ","createdAt":"2020-03-31T13:41:28Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/rapidsai/cudf/issues/4753#issuecomment-606634830","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDYwNjYzNjQwMA==","author":{"login":"jrhemstad"},"authorAssociation":"CONTRIBUTOR","body":"From https://github.com/rapidsai/cudf/issues/4754\r\n\r\n```\r\nscala> spark.sql(\"\"select * from floatsAndDoubles\"\").show\r\n+-----+------+\r\n|float|double|\r\n+-----+------+\r\n|  NaN|   NaN|\r\n| 1.02|   NaN|\r\n|  NaN|   4.5|\r\n+-----+------+\r\n```\r\n\r\nrunning an aggregate(max) op on the double-column will result in the following table \r\n\r\n```\r\n+----------+-----------+\r\n| float    |max(double)|\r\n+----------+-----------+\r\n| 1.020000 | -179769313486231570000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000.000000 |\r\n| NaN      | 4.500000  |\r\n+----------+-----------+\r\n```\r\n\r\n**Expected behavior**\r\nIt should output this \r\n\r\n```\r\n\"scala> spark.sql(\"\"select float, max(double) from floatsAndDoubles group by float\"\").show\r\n+-----+-----------+\r\n|float|max(double)|\r\n+-----+-----------+\r\n| 1.02|        NaN|\r\n|  NaN|        NaN|\r\n+-----+-----------+\"\r\n```\r\n\r\n**Additional context**\r\nFor context here is what aggregate(sum) does in cudf\r\n```\r\n+------+-----+\r\n|float | sum |\r\n+------+-----+\r\n| 1.02 | NaN |\r\n| NaN  | NaN |\r\n+------+-----+\r\n```\r\n","createdAt":"2020-03-31T13:44:06Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/4753#issuecomment-606636400","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDYwNjY0Njk2OQ==","author":{"login":"jrhemstad"},"authorAssociation":"CONTRIBUTOR","body":"There's no way to accomplish this behavior in a hash based groupby without significant performance loss. For min/max, we rely on CUDA `atomicMin`/`atomicMax`. There's no way to inject custom logic into these comparisons. So we would instead have to specialize floating point min/max operations to use a `atomicCAS` instead, which is much slower than a native `min/max` atomic. \r\n\r\nSort-based groupby uses the same `aggregate_row` functionality for min/max, so it has the same problem. ","createdAt":"2020-03-31T14:01:49Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/4753#issuecomment-606646969","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDYwNjY1OTkxNw==","author":{"login":"jrhemstad"},"authorAssociation":"CONTRIBUTOR","body":"Similar to https://github.com/rapidsai/cudf/issues/4752, I'm marking this as a feature request rather than a bug. ","createdAt":"2020-03-31T14:23:20Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/4753#issuecomment-606659917","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDYwNjY4NDU5Nw==","author":{"login":"OlivierNV"},"authorAssociation":"CONTRIBUTOR","body":"> There's no way to accomplish this behavior in a hash based groupby without significant performance loss. For min/max, we rely on CUDA `atomicMin`/`atomicMax`.\r\n> \r\n\r\nI think the first part of that statement is only a consequence of the second part. A reduction not based on atomics (like in ORC/Parquet stats) should be just as fast if not faster than an atomic-based implementation (it may just require a tiny 2nd low utilization pass to aggregate multiple results)","createdAt":"2020-03-31T15:04:52Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/4753#issuecomment-606684597","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDYwNjY5MTAyMA==","author":{"login":"jrhemstad"},"authorAssociation":"CONTRIBUTOR","body":"> I think the first part of that statement is only a consequence of the second part. A reduction not based on atomics (like in ORC/Parquet stats) should be just as fast if not faster than an atomic-based implementation (it may just require a tiny 2nd low utilization pass to aggregate multiple results)\r\n\r\nA groupby reduction (reduce by key) and a column-level reduction are very different things. \r\n\r\nThe implementation of groupby that uses a hash table requires the use of atomics. ","createdAt":"2020-03-31T15:15:22Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/4753#issuecomment-606691020","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDYzNzU4ODUyOQ==","author":{"login":"kuhushukla"},"authorAssociation":"CONTRIBUTOR","body":"To add here and possibly this is already known - if an aggregation does not have a grouping key, operations like max() still give a different result than Spark's implementation.","createdAt":"2020-06-02T14:42:57Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/4753#issuecomment-637588529","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDYzNzY2NjQ3Mg==","author":{"login":"jrhemstad"},"authorAssociation":"CONTRIBUTOR","body":"> if an aggregation does not have a grouping key\r\n\r\nWhat does that mean? Is that just a column-level reduction? ","createdAt":"2020-06-02T16:30:26Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/4753#issuecomment-637666472","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDYzNzcwMTA0OA==","author":{"login":"kuhushukla"},"authorAssociation":"CONTRIBUTOR","body":"> Is that just a column-level reduction?\r\n>\r\nYes","createdAt":"2020-06-02T17:38:08Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/4753#issuecomment-637701048","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDYzNzczNjE5Mw==","author":{"login":"jrhemstad"},"authorAssociation":"CONTRIBUTOR","body":"> > Is that just a column-level reduction?\r\n> \r\n> Yes\r\n\r\nBased on discussion in https://github.com/rapidsai/cudf/issues/4760, I believe that is a situation where Spark will need to do additional pre-processing to satisfy Spark's requirements. ","createdAt":"2020-06-02T18:42:08Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/4753#issuecomment-637736193","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDYzNzc0MDkxOQ==","author":{"login":"kuhushukla"},"authorAssociation":"CONTRIBUTOR","body":">Spark will need to do additional pre-processing to satisfy Spark's requirements.\r\n>\r\nOther than not running on the GPU, I am not sure how aggregates/reductions can be pre-processed to handle NaNs. @revans2. Am I missing something here?","createdAt":"2020-06-02T18:51:36Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/4753#issuecomment-637740919","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDYzNzc0Mzk1Mg==","author":{"login":"jlowe"},"authorAssociation":"MEMBER","body":"Some simple reductions like min/max/sum/etc. should be straightforward.  We would just need to replace NaNs with the value desired if the reduction with NaNs doesn't result in NaN. For example, if we're doing a max reduction then we can check if there's a NaN anywhere and if so just return NaN otherwise do the reduction (via copy_if_else).  If we're doing a min then we can just replace NaNs with null or filter them out completely.\r\n\r\nAggregations _might_ be able to be handled similarly, but I suspect some aggregations we won't be able to pre-process properly.","createdAt":"2020-06-02T18:57:19Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/4753#issuecomment-637743952","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDYzNzc0NDkyOQ==","author":{"login":"kuhushukla"},"authorAssociation":"CONTRIBUTOR","body":"Thanks @jlowe , sounds like a plan. I will follow up on the changes we need on our side. Thanks @jrhemstad.\r\n","createdAt":"2020-06-02T18:59:21Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/4753#issuecomment-637744929","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDczMjMxOTk2NQ==","author":{"login":"razajafri"},"authorAssociation":"CONTRIBUTOR","body":"@kuhushukla any update on this? ","createdAt":"2020-11-23T17:46:13Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/4753#issuecomment-732319965","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDczMjMyNDg0MQ==","author":{"login":"kuhushukla"},"authorAssociation":"CONTRIBUTOR","body":"No update at the moment","createdAt":"2020-11-23T17:54:30Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/4753#issuecomment-732324841","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDc5ODk2MjYzOA==","author":{"login":"github-actions"},"authorAssociation":"NONE","body":"This issue has been labeled `inactive-90d` due to no recent activity in the past 90 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed.","createdAt":"2021-03-14T19:13:55Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/4753#issuecomment-798962638","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDc5ODk2MjY0NA==","author":{"login":"github-actions"},"authorAssociation":"NONE","body":"This issue has been labeled `inactive-30d` due to no recent activity in the past 30 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed. This issue will be labeled `inactive-90d` if there is no activity in the next 60 days.","createdAt":"2021-03-14T19:13:56Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/4753#issuecomment-798962644","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps41dnw_","author":{"login":"revans2"},"authorAssociation":"CONTRIBUTOR","body":"I believe that this is still needed. We already have a NaN equality configs for collect_set and merge_set operations. This feels similar. To be clear we could make the proposal https://github.com/rapidsai/cudf/issues/4753#issuecomment-637743952 work for some cases, but not all, and in the cases we can make it work it feels unreasonable to do so.\r\n\r\nInstead of doing a single reduction we would have to \r\n\r\n1. call `is_nan`\r\n2. do an `any` reduction to see if we have to go any further.\r\n3. If there are any nans do an all reduction.\r\n4. If they are all nans stop because the result will be NaN\r\n5. if they are not all nans, then call `replace_nans` to replace the NaN with a min or max value depending on the operation being done.\r\n\r\nThis makes the code a little more complicated, but we can isolate it and it is not likely to cause too many issues.\r\n\r\nFor group by aggregations it would be much more complicated. We could do it but to be able to tell on a per group basis if we need to replace a NaN or not is going to require us to do a group by aggregation to check the same any/all conditions and then find a way to put that back with the original input data.  That means either doing a join with the original input data on the grouping keys or sorting the data. Both of those would have very large performance penalties, even in the common case when there are no NaN values.\r\n\r\nFor windowing, which we now also need to support, there is no way to do this.  We could have a single row that contributes to multiple separate aggregations. Some of which may have all of the values be NaN. Some of which may have only a few of the values be NaNs.  There is no way for us to fix up the input data to work around this.","createdAt":"2021-08-11T16:09:44Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/4753#issuecomment-896957503","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5VJL7t","author":{"login":"GregoryKimball"},"authorAssociation":"CONTRIBUTOR","body":"Based on some offline discussion, we agreed to keep this issue open. The NaN value issue impacts hash-based groupby aggregations with a `float` key type. We expect `float` types to be rarely used as groupby keys, and Spark-RAPIDS has workarounds that are adequate for now.","createdAt":"2023-02-13T18:45:23Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/4753#issuecomment-1428471533","viewerDidAuthor":false}],"createdAt":"2020-03-31T05:12:45Z","id":"MDU6SXNzdWU1OTA4MDAxNjA=","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NjE=","name":"feature request","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwxMDEzOTg3MzUy","name":"0 - Backlog","description":"In queue waiting for assignment","color":"d4c5f9"},{"id":"MDU6TGFiZWwxMTM5NzQwNjY2","name":"libcudf","description":"Affects libcudf (C++/CUDA) code.","color":"c5def5"},{"id":"MDU6TGFiZWwxNDA1MTQ2OTc1","name":"Spark","description":"Functionality that helps Spark RAPIDS","color":"7400ff"}],"milestone":null,"number":4753,"projectCards":[{"project":{"name":"Feature Planning"},"column":{"name":"Needs prioritizing"}}],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[FEA] Groupby MIN/MAX with NaN values does not match what Spark expects","updatedAt":"2023-02-13T18:45:37Z","url":"https://github.com/rapidsai/cudf/issues/4753"}
