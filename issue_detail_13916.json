{"assignees":[],"author":{"id":"MDQ6VXNlcjEyNzI1MTEx","is_bot":false,"login":"GregoryKimball","name":"Gregory Kimball"},"body":"### Background\r\n\r\nThe CSV reader in cuDF/libcudf is a common IO interface for ingesting raw data, and is frequently the first IO interface that new users test when getting started with RAPIDS. There have been many improvements to the CSV reader over the years, but much of the implementation has remained the same from its introduction in #3213 and rework in #5024. We see several opportunities to address the [CSV reader continuous improvement](https://github.com/rapidsai/cudf/milestone/12) milestone, and this story associates open issues with particular functions and kernels in the CSV reading process.\r\n\r\n### Step 1: Decompression and preprocessing\r\nThe CSV reader begins with host-side processing in `select_data_and_row_offsets`. With the exception of decompression, we would like to migrate this processing to be done device side and refactor this function to use a kvikIO data source. Note, this refactor could also include adding support for the `header` parameter and `byte_range` at the same time ([code pointer](https://github.com/rapidsai/cudf/blob/b798a70d608cbbe2c7f372a8c21354455ba56f74/cpp/src/io/csv/reader_impl.cu#L442)).\r\n\r\nThe initial processing interacts with several issues:\r\n* #13797 is small story issue about this topic\r\n* #4999 batch the full read as small chunks\r\n* #5142 \r\n* #11728 describes how the initial byte range parsing to find the first row assumes the byte_range starts in an unquoted state. If a user provides a byte_range that starts in a quoted field, then the reader will fail! The solution described in this issue interacts the next step \"identify row offsets\". \r\n* #12255 needs investigation\r\n* #12582 return empty `metadata.schema_info` when column names are autogenerated\r\n\r\n### Step 2: Identify row offsets (delimiters)\r\nThe next step is identifying record delimiters and computing row offsets in `load_data_and_gather_row_offsets` (invoked by `select_data_and_row_offsets`). This algorithm operates in three main steps: `gather_row_offsets` called with empty data, `select_row_context`, and `gather_row_offsets` called with row context data. The row context state machine is difficult to refactor because it uses a custom data representation that stores several logical values within a single 32-bit or 64-bit physical type ([code pointer](https://github.com/rapidsai/cudf/blob/b798a70d608cbbe2c7f372a8c21354455ba56f74/cpp/src/io/csv/csv_gpu.hpp#L64)). The row context tracks whether the content is in a comment block or in a quoted block. \r\n* `gather_row_offsets` runs a [4-state](https://github.com/rapidsai/cudf/blob/b798a70d608cbbe2c7f372a8c21354455ba56f74/cpp/src/io/csv/csv_gpu.hpp#L40) \"row context\" state machine over 16 KB blocks of characters and returns the number of un-quoted, un-commented record delimiters from the block given each possible initial state\r\n* `select_row_context` is invoked in a host-side loop over `row_ctr` data for each 16 KB block, starting from a `state 0` initial context. \r\n* `gather_row_offsets` is called in a second pass with a valid `all_row_offsets` data parameter.\r\n\r\nMajor design topics:\r\n* We should consider a larger refactor of the \"identify row offsets\" code based on using a new FST instance ([code pointer](https://github.com/rapidsai/cudf/tree/branch-23.10/cpp/src/io/fst)). Using an FST instance would easily allow us to add additional states beyond the existing 4-state machine. Please refer to the [ParPaRaw paper](https://arxiv.org/pdf/1905.13415.pdf) from Elias Stehle et al for more information about parallel algorithms for CSV parsing.\r\n* To unblock Spark-RAPIDS usage of the CSV, we may also choose to support user-provided `all_row_offsets` parameter to the read function or as a reader option. This would allow Spark to bypass the first `gather_row_offsets` pass and `select_row_context` in `load_data_and_gather_row_offsets`. When calling `read_csv` on a strings column, Spark already has the row offsets. \r\n* Also note that refactoring the interface to provide row offsets is relevant to #11728, where we would want to provide pre-computed offsets. For this issue we might prefer a new detail API rather than new parameters in the public API - more design work is needed.\r\n\r\nThe row offsets algorithm interacts with several open issues:\r\n* #6572 complex preprocessing or changes to the row context state machine. \r\n* (Spark blocker) #11984 Pandas and Spark don't have the same escaping conventions, and the row offset state machine doesn't have an escaped state. Needs confirmation - does this impact the row offsets step?\r\n* (Spark blocker) #11948 to handle misplaced quotes. The issue shows a file getting truncated so fields with misplaced quotes seem to compromise the row offset data. #2398 suggests a workaround\r\n* #6305 another quoting/escaping issue \r\n* #13856 commented lines should not emit row offsets\r\n* Issue n/a: Add unit tests for `gather_row_offsets` kernel\r\n\r\n### Step 3: Determine column types\r\nThe next step is determining the data types for each column that does not map to a user-provided data type. The function `determine_column_types` completes this work by collecting the user-provided data types, and then calling `infer_column_types` to handle the unspecified data types. `infer_column_types` invokes the `detect_column_types`->`data_type_detection` kernel to collect statistics about the data in each field, and then use the conventions of the pandas CSV reader to select a column type. \r\n* We should consider refactoring the \"determine\", \"infer\", and \"detect\" function names to improve clarity\r\n* (good first issue) #14066 update thread indexing\r\n* #5080 performance improvements for type inference\r\n* (Spark blocker) #11984 `seek_field_end` supports escape characters within data fields. perhaps field traversal is already Spark-compatible\r\n* (Spark blocker) #11948 misplaced quotes could fail with `seek_field_end`\r\n* #6313 pandas doesn't infer as `float` if there are any nulls\r\n* #9987 would change `seek_field_end`, maybe not much else\r\n* Issue n/a: Add unit tests for `seek_field_end` kernel\r\n\r\n### Step 4: Decode data and populate device buffers\r\nThe final step, `decode_data`, does another pass over the data to decode values according to the determined columns types. The kernel is `decode_row_column_data`->`convert_csv_to_cudf`\r\n* (Spark blocker) #13892 trim white space . Probably a modest change to `trim_whitespaces_quotes`. Related to #6659\r\n* (Spark blocker) #12145 add option to decode `\"\"` as empty strings or `null`. Probably an additional parsing option.\r\n* (Spark blocker) #11984 `convert_csv_to_cudf` also uses `seek_field_end` which nominally supports escape characters\r\n* (Spark blocker) #11948 misplaced quotes could fail with `seek_field_end`\r\n* #4001 support additional `nanValue` options\r\n* #10599 float parsing consistency, this is probably a `wontfix`\r\n\r\n\r\n\r\n","closed":false,"closedAt":null,"comments":[],"createdAt":"2023-08-18T19:27:48Z","id":"I_kwDOBWUGps5usoHy","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NjE=","name":"feature request","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwxMDEzOTg3MzUy","name":"0 - Backlog","description":"In queue waiting for assignment","color":"d4c5f9"},{"id":"MDU6TGFiZWwxMTM5NzQwNjY2","name":"libcudf","description":"Affects libcudf (C++/CUDA) code.","color":"c5def5"},{"id":"MDU6TGFiZWwxMTg1MjQ0MTQy","name":"cuIO","description":"cuIO issue","color":"fef2c0"}],"milestone":{"number":12,"title":"CSV continuous improvement","description":"","dueOn":null},"number":13916,"projectCards":[],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[FEA] Modernize CSV reader and expand reader options","updatedAt":"2023-09-12T21:19:54Z","url":"https://github.com/rapidsai/cudf/issues/13916"}
