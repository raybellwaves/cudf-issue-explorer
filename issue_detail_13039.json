{"assignees":[],"author":{"id":"MDQ6VXNlcjYzNTg4NjY=","is_bot":false,"login":"stucash","name":"QiuxiaoMu"},"body":"I am a new user of `dask`/`dask_cudf`.\r\nI have parquet files of various sizes (11GB, 2.5GB, 1.1GB), all of which failed with `NotImplementedError: large_string`. \r\n\r\nMy `dask.dataframe` backend is `cudf`. When the backend is `pandas`, `read.parquet` works fine.\r\n\r\nHere's an exerpt of what my data looks like in `csv` format:\r\n\r\n    Symbol,Date,Open,High,Low,Close,Volume\r\n    AADR,17-Oct-2017 09:00,57.47,58.3844,57.3645,58.3844,2094\r\n    AADR,17-Oct-2017 10:00,57.27,57.2856,57.25,57.27,627\r\n    AADR,17-Oct-2017 11:00,56.99,56.99,56.99,56.99,100\r\n    AADR,17-Oct-2017 12:00,56.98,57.05,56.98,57.05,200\r\n    AADR,17-Oct-2017 13:00,57.14,57.16,57.14,57.16,700\r\n    AADR,17-Oct-2017 14:00,57.13,57.13,57.13,57.13,100\r\n    AADR,17-Oct-2017 15:00,57.07,57.07,57.07,57.07,200\r\n    AAMC,17-Oct-2017 09:00,87,87,87,87,100\r\n    AAU,17-Oct-2017 09:00,1.1,1.13,1.0832,1.121,67790\r\n    AAU,17-Oct-2017 10:00,1.12,1.12,1.12,1.12,100\r\n    AAU,17-Oct-2017 11:00,1.125,1.125,1.125,1.125,200\r\n    AAU,17-Oct-2017 12:00,1.1332,1.15,1.1332,1.15,27439\r\n    AAU,17-Oct-2017 13:00,1.15,1.15,1.13,1.13,8200\r\n    AAU,17-Oct-2017 14:00,1.1467,1.1467,1.14,1.1467,1750\r\n    AAU,17-Oct-2017 15:00,1.1401,1.1493,1.1401,1.1493,4100\r\n    AAU,17-Oct-2017 16:00,1.13,1.13,1.13,1.13,100\r\n    ABE,17-Oct-2017 09:00,14.64,14.64,14.64,14.64,200\r\n    ABE,17-Oct-2017 10:00,14.67,14.67,14.66,14.66,1200\r\n    ABE,17-Oct-2017 11:00,14.65,14.65,14.65,14.65,600\r\n    ABE,17-Oct-2017 15:00,14.65,14.65,14.65,14.65,836\r\n\r\n\r\nWhat I did was really simple:\r\n\r\n    import dask.dataframe as dd\r\n    import cudf\r\n    import dask_cudf\r\n    \r\n    # Failed with large_string error\r\n    dask_cudf.read_parquet('path/to/my.parquet')\r\n    # Failed with large_string error\r\n    dd.read_parquet('path/to/my.parquet')\r\n\r\nThe only large string I could think of is the timestamp string.\r\n\r\nIs there a way around this in `cudf` as it is not implemented yet? The format is `2023-03-12 09:00:00+00:00`.\r\n\r\n\r\n\r\n","closed":false,"closedAt":null,"comments":[{"id":"IC_kwDOBWUGps5Y4poC","author":{"login":"stucash"},"authorAssociation":"NONE","body":"Team can you please take a look? This is currently a show stopper for me and I am literally freezed with GPU related development. Thanks a lot!","createdAt":"2023-03-31T03:44:23Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/13039#issuecomment-1491245570","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5Y6hgc","author":{"login":"stucash"},"authorAssociation":"NONE","body":"I've got a few more bug-like issues, I'll raise them here shortly. ","createdAt":"2023-03-31T10:54:29Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/13039#issuecomment-1491736604","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5Y6hzw","author":{"login":"stucash"},"authorAssociation":"NONE","body":"In the meanwhile if someone has got a good introduction/tutorial about cudf other than the one already posted like 10-minute series, please throw it in here.","createdAt":"2023-03-31T10:55:29Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/13039#issuecomment-1491737840","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5Y_jej","author":{"login":"stucash"},"authorAssociation":"NONE","body":"I've appended full error log below:\r\n\r\n---------------------------------------------------------------------------\r\n```\r\nNotImplementedError                       Traceback (most recent call last)\r\nFile ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask/backends.py:133, in CreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper(*args, **kwargs)\r\n    132 try:\r\n--> 133     return func(*args, **kwargs)\r\n    134 except Exception as e:\r\n\r\nFile ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask/dataframe/io/parquet/core.py:530, in read_parquet(path, columns, filters, categories, index, storage_options, engine, use_nullable_dtypes, calculate_divisions, ignore_metadata_file, metadata_task_size, split_row_groups, chunksize, aggregate_files, parquet_file_extension, filesystem, **kwargs)\r\n    528     index = [index]\r\n--> 530 read_metadata_result = engine.read_metadata(\r\n    531     fs,\r\n    532     paths,\r\n    533     categories=categories,\r\n    534     index=index,\r\n    535     use_nullable_dtypes=use_nullable_dtypes,\r\n    536     gather_statistics=calculate_divisions,\r\n    537     filters=filters,\r\n    538     split_row_groups=split_row_groups,\r\n    539     chunksize=chunksize,\r\n    540     aggregate_files=aggregate_files,\r\n    541     ignore_metadata_file=ignore_metadata_file,\r\n    542     metadata_task_size=metadata_task_size,\r\n    543     parquet_file_extension=parquet_file_extension,\r\n    544     dataset=dataset_options,\r\n    545     read=read_options,\r\n    546     **other_options,\r\n    547 )\r\n    549 # In the future, we may want to give the engine the\r\n    550 # option to return a dedicated element for `common_kwargs`.\r\n    551 # However, to avoid breaking the API, we just embed this\r\n    552 # data in the first element of `parts` for now.\r\n    553 # The logic below is inteded to handle backward and forward\r\n    554 # compatibility with a user-defined engine.\r\n\r\nFile ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask_cudf/io/parquet.py:41, in CudfEngine.read_metadata(*args, **kwargs)\r\n     39 if parts:\r\n     40     # Re-set \"object\" dtypes align with pa schema\r\n---> 41     set_object_dtypes_from_pa_schema(\r\n     42         new_meta,\r\n     43         parts[0].get(\"common_kwargs\", {}).get(\"schema\", None),\r\n     44     )\r\n     46 # If `strings_to_categorical==True`, convert objects to int32\r\n\r\nFile ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask_cudf/io/parquet.py:403, in set_object_dtypes_from_pa_schema(df, schema)\r\n    402     continue\r\n--> 403 typ = cudf_dtype_from_pa_type(schema.field(col_name).type)\r\n    404 if (\r\n    405     col_name in schema.names\r\n    406     and not isinstance(typ, (cudf.ListDtype, cudf.StructDtype))\r\n    407     and isinstance(col, cudf.core.column.StringColumn)\r\n    408 ):\r\n\r\nFile ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/cudf/utils/dtypes.py:231, in cudf_dtype_from_pa_type(typ)\r\n    230 else:\r\n--> 231     return cudf.api.types.pandas_dtype(typ.to_pandas_dtype())\r\n\r\nFile ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/pyarrow/types.pxi:220, in pyarrow.lib.DataType.to_pandas_dtype()\r\n\r\nNotImplementedError: large_string\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nNotImplementedError                       Traceback (most recent call last)\r\nFile ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask/backends.py:133, in CreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper(*args, **kwargs)\r\n    132 try:\r\n--> 133     return func(*args, **kwargs)\r\n    134 except Exception as e:\r\n\r\nFile ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask_cudf/backends.py:497, in CudfBackendEntrypoint.read_parquet(engine, *args, **kwargs)\r\n    495 from dask_cudf.io.parquet import CudfEngine\r\n--> 497 return _default_backend(\r\n    498     dd.read_parquet,\r\n    499     *args,\r\n    500     engine=CudfEngine,\r\n    501     **kwargs,\r\n    502 )\r\n\r\nFile ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask_cudf/backends.py:446, in _default_backend(func, *args, **kwargs)\r\n    445 with config.set({\"dataframe.backend\": \"pandas\"}):\r\n--> 446     return func(*args, **kwargs)\r\n\r\nFile ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask/backends.py:135, in CreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper(*args, **kwargs)\r\n    134 except Exception as e:\r\n--> 135     raise type(e)(\r\n    136         f\"An error occurred while calling the {funcname(func)} \"\r\n    137         f\"method registered to the {self.backend} backend.\\n\"\r\n    138         f\"Original Message: {e}\"\r\n    139     ) from e\r\n\r\nNotImplementedError: An error occurred while calling the read_parquet method registered to the pandas backend.\r\nOriginal Message: large_string\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nNotImplementedError                       Traceback (most recent call last)\r\nCell In[9], line 1\r\n----> 1 dask_cudf.read_parquet('/home/demo/Live-usb-storage/projects/.share/data/test_cudf.parquet')\r\n\r\nFile ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask_cudf/io/parquet.py:465, in read_parquet(path, columns, **kwargs)\r\n    462         kwargs[\"read\"] = {}\r\n    463     kwargs[\"read\"][\"check_file_size\"] = check_file_size\r\n--> 465 return dd.read_parquet(path, columns=columns, engine=CudfEngine, **kwargs)\r\n\r\nFile ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask/backends.py:135, in CreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper(*args, **kwargs)\r\n    133     return func(*args, **kwargs)\r\n    134 except Exception as e:\r\n--> 135     raise type(e)(\r\n    136         f\"An error occurred while calling the {funcname(func)} \"\r\n    137         f\"method registered to the {self.backend} backend.\\n\"\r\n    138         f\"Original Message: {e}\"\r\n    139     ) from e\r\n\r\nNotImplementedError: An error occurred while calling the read_parquet method registered to the cudf backend.\r\nOriginal Message: An error occurred while calling the read_parquet method registered to the pandas backend.\r\nOriginal Message: large_string\r\n```\r\n","createdAt":"2023-04-01T17:43:41Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/13039#issuecomment-1493055395","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5Y_lBS","author":{"login":"stucash"},"authorAssociation":"NONE","body":"Reading the log, I found that `cudf` is converting `pyarrow` explicitly to `pandas` dtypes (before they become `cudf` dtypes); therefore I tried using `pandas` to write the same data to a parquet file. \r\n\r\nThe pandas-written parquet file was successfully converted to a `dask_cudf` dataframe.\r\n\r\nHere's the highlighted line of code in error log that did this:\r\n\r\n```\r\nFile ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask_cudf/io/parquet.py:41, in CudfEngine.read_metadata(*args, **kwargs)\r\n     39 if parts:\r\n     40     # Re-set \"object\" dtypes align with pa schema\r\n---> 41     **set_object_dtypes_from_pa_schema**(\r\n     42         new_meta,\r\n     43         parts[0].get(\"common_kwargs\", {}).get(\"schema\", None),\r\n     44     )\r\n     46 # If `strings_to_categorical==True`, convert objects to int32\r\n\r\nFile ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask_cudf/io/parquet.py:403, in set_object_dtypes_from_pa_schema(df, schema)\r\n    402     continue\r\n--> 403 typ = **cudf_dtype_from_pa_type(schema.field(col_name).type)**\r\n    404 if (\r\n    405     col_name in schema.names\r\n    406     and not isinstance(typ, (cudf.ListDtype, cudf.StructDtype))\r\n    407     and isinstance(col, cudf.core.column.StringColumn)\r\n    408 ):\r\n\r\nFile ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/cudf/utils/dtypes.py:231, in cudf_dtype_from_pa_type(typ)\r\n    230 else:\r\n--> 231     **return cudf.api.types.pandas_dtype(typ.to_pandas_dtype()**)\r\n```\r\n\r\n`parquet` is simply a data container, whoever writes `parquet` is going to write a `parquet`, after all. \r\n\r\nUntil I found that when we write `parquet` using different library, the parquet file can be different! (The original `parquet` that failed was written by `polars`). \r\n\r\nBy the way, `pandas` and `polars` both called `pa.write_table` for `parquet` creation. \r\n\r\nI am not sure whether this involves support for `polars` or we could replace `pandas` dtype conversion with a python dtypes conversion, but at the moment understanding only `parquet` from `pandas` is fairly limited.\r\n\r\nHope you guys can take a look, thanks a lot! \r\n","createdAt":"2023-04-01T18:02:20Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/13039#issuecomment-1493061714","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5ZEy8p","author":{"login":"rjzamora"},"authorAssociation":"MEMBER","body":"Thanks for raising @stucash - I'll take a look at this today to see how I can help.","createdAt":"2023-04-03T14:26:39Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/13039#issuecomment-1494429481","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5ZFJSf","author":{"login":"rjzamora"},"authorAssociation":"MEMBER","body":"@stucash - I cannot be 100% certain without having a complete write + read reproducer to run locally. However, it looks like your original dataset may contain extremely large row-groups. Unfortunately, until [very recently](https://github.com/apache/arrow/issues/34280) the default row-group size in PyArrow was 64Mi rows, which can sometimes result in string columns that cannot be read back by `cudf` (since `cudf` has a 2B character limit for an individual single string column; see: [cudf#3958](https://github.com/rapidsai/cudf/issues/3958)).\r\n\r\nIf the problem is that the row-groups are too large, you will need to rewrite the files with polars or pandas, passing through `row_group_size=<something-smaller>` to the pyarrow backend.\r\n\r\nIt may also be possible that your row-groups are within the cudf limit, but that pyarrow is choosing to use a [large_string](https://arrow.apache.org/docs/python/generated/pyarrow.large_string.html) when converting the dtype to pandas (note that cudf's read_parquet code currently leans on the arrow's native pandas logic to figure out what cudf dtypes to use). If I can get my hands on a reproducer for this, we can probably resolve the problem in cudf/dask-cudf.","createdAt":"2023-04-03T15:17:57Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/13039#issuecomment-1494520991","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5ZFO0F","author":{"login":"rjzamora"},"authorAssociation":"MEMBER","body":"> I tried using pandas to write the same data to a parquet file... The pandas-written parquet file was successfully converted to a dask_cudf dataframe.\r\n\r\nHmmm, this is interesting. I was expecting the same pyarrow issue to show up for a pandas-written parquet file as well (since both are presumably using arrow as the backend). Good to know.","createdAt":"2023-04-03T15:32:45Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/13039#issuecomment-1494543621","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5ZI7oL","author":{"login":"stucash"},"authorAssociation":"NONE","body":"@rjzamora Thanks for taking the time to investigate;  \r\n\r\nI've got 7 parquet files (1.5GB ish per file) originally from `polars`, all of which failed with `dask_cudf`, and once all were rewritten by `pandas` they could be read by `dask_cudf` in one go with no problem. \r\n\r\nLet me prepare a reproducer with data; in the meanwhile I'll try your suggestion of the `row_group_size` to `pyarrow` backend. ","createdAt":"2023-04-04T07:54:46Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/13039#issuecomment-1495513611","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5eA1HA","author":{"login":"GregoryKimball"},"authorAssociation":"CONTRIBUTOR","body":"Thank you @stucash for posting this. It also occurs to me that libcudf does not support the `large_string` type that we see in [pyarrow](https://arrow.apache.org/docs/python/generated/pyarrow.large_string.html). Have you tried converting your column to a `string` type instead of `long_string`?","createdAt":"2023-06-05T18:33:33Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/13039#issuecomment-1577275840","viewerDidAuthor":false},{"id":"IC_kwDOBWUGps5_Kwxt","author":{"login":"CarloNicolini"},"authorAssociation":"NONE","body":"I've been having this kind of issues using NVTabular that relies on cudf.\r\nEverything smooth when dataframes were saved throught pandas.`to_parquet` function, but lot of problems when using Polars `.write_parquet`.\r\n\r\nGiven that pandas was not an option due to its slowness, I managed to save the polars dataframe through a manually crafted function that uses `pyarrow`.\r\n\r\nThis part simplifies the schema removing the  `pa.types.large_string`, `pa.types.large_list` in favor of their non_large counterpart.\r\n\r\n```python\r\n\r\ndef pyarrow_simplified_schema(schema: pa.Schema) -> pa.Schema:\r\n    \"\"\"\r\n    Convert LargeList<LargeString> fields to LargeList<String> in a PyArrow schema.\r\n\r\n    Parameters\r\n    ----------\r\n    schema : pa.Schema\r\n        The original schema of the PyArrow Table.\r\n\r\n    Returns\r\n    -------\r\n    pa.Schema\r\n        A new schema where all LargeList<LargeString> fields are converted to LargeList<String>.\r\n    \"\"\"\r\n    fields = []\r\n    for field in schema:\r\n        if pa.types.is_float64(field.type):\r\n            warn(\r\n                f\"NVTabular does not support double precision, downcasting {field.name} to float32\"\r\n            )\r\n            fields.append(pa.field(field.name, pa.float32()))\r\n        elif pa.types.is_large_list(field.type) or pa.types.is_list(field.type):\r\n            if pa.types.is_large_string(field.type.value_type):\r\n                fields.append(pa.field(field.name, pa.list_(pa.string())))\r\n            elif pa.types.is_float64(field.type.value_type):\r\n                warn(\r\n                    f\"NVTabular does not support double precision, downcasting {field.name} to float32\"\r\n                )\r\n                fields.append(pa.field(field.name, pa.list_(pa.float32())))\r\n            else:\r\n                # passthrough on other types\r\n                fields.append(pa.field(field.name, pa.list_(field.type.value_type)))\r\n        elif pa.types.is_large_string(field.type):\r\n            fields.append(pa.field(field.name, pa.string()))\r\n        else:\r\n            # passthrough on other types\r\n            fields.append(field)\r\n    return pa.schema(fields)\r\n```\r\n\r\nAfter this I was able to load everything correctly using the cudf implementation of NVTabular.","createdAt":"2024-05-27T13:49:42Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/13039#issuecomment-2133527661","viewerDidAuthor":false}],"createdAt":"2023-03-30T16:14:30Z","id":"I_kwDOBWUGps5iOVB5","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NjQ=","name":"question","description":"Further information is requested","color":"D4C5F9"},{"id":"MDU6TGFiZWwxMDEzOTg3Nzk5","name":"0 - Waiting on Author","description":"Waiting for author to respond to review","color":"ffb88c"}],"milestone":{"number":20,"title":"Stabilizing large workflows (OOM, spilling, partitioning)","description":"","dueOn":null},"number":13039,"projectCards":[],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[QST] `dask_cudf.read_parquet` failed with \"NotImplementedError: large_string\"","updatedAt":"2024-05-27T13:49:59Z","url":"https://github.com/rapidsai/cudf/issues/13039"}
