{"assignees":[],"author":{"id":"MDQ6VXNlcjExMjY5ODE=","is_bot":false,"login":"wence-","name":"Lawrence Mitchell"},"body":"**Describe the bug**\r\n\r\nThis comes out of [a review of #14133](https://github.com/rapidsai/cudf/pull/14133#discussion_r1338480398) which introduces a new `Scalar` type on the python side in pylibcudf, but I think that the issues are pervasive.\r\n\r\n## Background\r\n\r\n`libcudf` uses RMM for all memory allocations, this happens through a `memory_resource` object. Allocating `libcudf` functions all take an explicit `memory_resource` argument (as a raw pointer) that is defaulted to `rmm::get_current_device_resource()` (the resource set by `rmm::set_per_device_resource`). RMM `device_buffer`s hold a raw pointer of their allocating memory resource (needed for deallocation), and it is the user's (in this case `libcudf`'s) responsibility to keep that `memory_resource` alive until the `device_buffer` has been dropped:\r\n\r\nThis is fine:\r\n```c++\r\nmemory_resource mr = ...;\r\n{\r\n    device_buffer buf{..., &mr};\r\n    ... // do things with buf\r\n   ~buf(); // called here, fine.\r\n}\r\n```\r\n\r\nThis is not (this is not a real example, since I think the `device_buffer` constructors don't allow exactly this, but bear with me):\r\n```c++\r\ndevice_buffer buf;\r\n{\r\n   memory_resource mr = ...;\r\n   buf = {..., &mr};\r\n   ... // do things with buf\r\n   ~mr(); // called here, boom;\r\n}\r\nbuf no longer valid\r\n```\r\n\r\n### How does `get_current_device_resource` work?\r\n\r\n`set_per_device_resource` stores a raw pointer to a memory resource in a `std::unordered_map` and `get_current_device_resource` just looks up in the map. Again, the user is responsible for keeping the `mr` alive.\r\n\r\n### How does this work in cudf (python) land?\r\n\r\nThe Cython wrappers in RMM expose memory resources, and `set/get_current_device_resource`. `set_per_device_memory_resource` sets the value in both the C++ level `std::unordered_map` _and_ a Python level dict (https://github.com/rapidsai/rmm/blob/5f07014db51535902f8cdb515596af162d1ae6ca/python/rmm/_lib/memory_resource.pyx#L1041-L1049). `get_current_device_resource` looks up in the Python dict (https://github.com/rapidsai/rmm/blob/5f07014db51535902f8cdb515596af162d1ae6ca/python/rmm/_lib/memory_resource.pyx#L1013-1026). `DeviceBuffer`s keep the \"current\" memory resource alive by storing a reference: https://github.com/rapidsai/rmm/blob/5f07014db51535902f8cdb515596af162d1ae6ca/python/rmm/_lib/device_buffer.pyx#L93-L93, but when taking ownership of a C++-level device buffer https://github.com/rapidsai/rmm/blob/5f07014db51535902f8cdb515596af162d1ae6ca/python/rmm/_lib/device_buffer.pyx#L161-L171, we don't use the mr stored in the C++ struct.\r\n\r\nThe Python level dict is \"necessary\" due to the usual expected way in which people will use things from python. That is they will expect that: `set_per_device_resource(device, CudaMemoryResource())` keeps the created memory resource alive. \r\n\r\nIf a C++ library calls `set_per_device_resource` at some point, C++-land (`libcudf`) and Python-land (cudf) can end up disagreeing about what the current memory resource is (because Python-level `get_current_device_resource` doesn't look at the C++-level map).\r\n\r\n## So what's the problem?\r\n\r\nIf `libcudf` ever allocates memory that it hands off to cudf with a memory resource that is _not_ managed by cudf, we have the potential for use-after-free, because cudf has no way of taking (or sharing) ownership of that memory resource from the RMM buffer.\r\n\r\nAFAICT, cudf never explicitly passes a memory resource into `libcudf`, and so the allocation behaviour is always relying on C++ and Python agreeing about what `get_current_device_resource` returns, _and_ that cudf was the creator of that resource. Effectively the pattern is:\r\n\r\n```python\r\n# In python\r\n...\r\n1. mr = rmm.get_current_device_resource()\r\n# No mr passed here, so C++-level get_current_device_resource() is used\r\n2. new_column = libcudf.some_algorithm(existing_data)\r\n# take ownership of the data, fingers crossed that new_column.data.mr is mr\r\n3. cudf_column = Column.from_unique_ptr(new_column, mr)\r\n```\r\n\r\nIf _either_ someone in C++ has set a different per-device resource _or_ the current thread is pre-empted between lines 1 and 2 (and the current device resource is changed), then line 3 will take \"ownership\" of the data, but not be keeping the correct memory resource for the data alive.\r\n\r\n## How can we fix this?\r\n\r\nIf the Cython layer in cudf always explicitly passes a memory resource to `libcudf` algorithms, this problem goes away. We can then always guarantee that the memory resource we have on the Python side is the one used to allocate the data in libcudf so we can keep it alive.\r\n\r\nAlternately, if the memory_resource pointers in RMM were smart pointers it might be possible to keep things alive that way. Right now we can't make Python and C++ always agree on what the current default resource is (because on the C++ side RMM doesn't have a smart pointer stored, because it doesn't take ownership).","closed":false,"closedAt":null,"comments":[{"id":"IC_kwDOBWUGps5nvqG3","author":{"login":"wence-"},"authorAssociation":"CONTRIBUTOR","body":"After some (long) discussions, my current position is:\r\n\r\n1. cudf Python should explicitly pass memory resources to all calls to libcudf that require them.\r\n2. It was a mistake to provide defaulted `mr = rmm::mr::get_current_device_resource()` in all functions in libcudf that allocate data, since in the presence of a third-party library that is also using RMM we have no control of the provenance of this pointer and therefore cannot guarantee the lifetime of the memory resource it points to.","createdAt":"2023-09-29T08:57:35Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/rapidsai/cudf/issues/14229#issuecomment-1740546487","viewerDidAuthor":false}],"createdAt":"2023-09-28T17:29:46Z","id":"I_kwDOBWUGps5yUgg2","labels":[{"id":"MDU6TGFiZWw1OTk2MjY1NTk=","name":"bug","description":"Something isn't working","color":"d73a4a"},{"id":"MDU6TGFiZWwxMDEzOTg3MzUy","name":"0 - Backlog","description":"In queue waiting for assignment","color":"d4c5f9"},{"id":"MDU6TGFiZWwxMTM5NzQwNjY2","name":"libcudf","description":"Affects libcudf (C++/CUDA) code.","color":"c5def5"},{"id":"MDU6TGFiZWwxMTM5NzQxMjEz","name":"cuDF (Python)","description":"Affects Python cuDF API.","color":"1d76db"}],"milestone":null,"number":14229,"projectCards":[],"projectItems":[],"reactionGroups":[],"state":"OPEN","title":"[BUG] Potential for use-after-free on libcudf/cudf interface boundaries","updatedAt":"2024-02-23T18:00:50Z","url":"https://github.com/rapidsai/cudf/issues/14229"}
